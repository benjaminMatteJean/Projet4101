{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9.0, 7.0)\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from numpy file\n",
    "# debinarizedData = numpy.load('debinarizedData.npy')\n",
    "# numpy.shape(debinarizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from pickle file\n",
    "\n",
    "# infile = open('../Projet/trainDatasetPerdiem-20191028.pkl', 'rb')\n",
    "# data, labels = pickle.load(infile)\n",
    "\n",
    "# infile.close()\n",
    "\n",
    "# data_numpy = data.to_numpy()\n",
    "# print(labels)\n",
    "# data.describe() # give column stats\n",
    "\n",
    "# To one-hot-encode\n",
    "# - Spécialité de médecins        | 33\n",
    "# - Université de graduation      | 14\n",
    "# - Plage horaire de facturation  | 3\n",
    "# - Agence de représentation      | 845\n",
    "# - Établissements                | 241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate labeled and unlabeled data\n",
    "\n",
    "# unlabeled = data_numpy[data_numpy[:, 1182] == -1][:, :-1]\n",
    "# numpy.random.shuffle(unlabeled)\n",
    "# numpy.save('unlabeled.npy', unlabeled)\n",
    "\n",
    "# labeled = data_numpy[data_numpy[:, 1182] != -1]\n",
    "# numpy.random.shuffle(labeled)\n",
    "# numpy.save('labeled.npy', labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1182) (1000, 1182)\n"
     ]
    }
   ],
   "source": [
    "# Make smaller sample for testing\n",
    "\n",
    "# unlabeled = debinarizedData = numpy.load('unlabeled.npy')\n",
    "# smaller_size = 5000\n",
    "# smaller_unlabeled = unlabeled[:smaller_size]\n",
    "# numpy.save('smaller_unlabeled.npy', smaller_unlabeled)\n",
    "\n",
    "smaller_unlabeled = numpy.load('smaller_unlabeled.npy')\n",
    "print(numpy.shape(smaller_unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "class RAMQDatasetForAE(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for elem in data:\n",
    "            if isinstance(elem, numpy.ndarray):\n",
    "                elem = elem.tolist()\n",
    "            elem = torch.Tensor(elem)\n",
    "            # AE targets are same as inputs -> [x, x']\n",
    "            self.data += [(elem, elem)]\n",
    "        self.targets = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering model\n",
    "# Ref tied weights:\n",
    "# https://discuss.pytorch.org/t/how-to-create-and-train-a-tied-autoencoder/2585/13\n",
    "# https://gist.github.com/InnovArul/500e0c57e88300651f8005f9bd0d12bc\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Définition d'un réseau de neurones pleinement connecté\n",
    "    de type autoencoder qui permet de réduire l'erreur de\n",
    "    reconstruction sur le jeu de donnée\n",
    "    \"\"\"\n",
    "    def __init__(self, input_len):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encode_only = False\n",
    "        # Initialisation of the network layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_len, 800), \n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(800, 500),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(500, 200),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 50),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(25, 10),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 25),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(25, 50),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50, 200),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(200, 500),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(500, 800),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(800, input_len),\n",
    "            nn.ReLU(True))\n",
    "    def set_encode_only(self, value):\n",
    "        self.encode_only = value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        if not self.encode_only:\n",
    "            x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AE Training\n",
    "# Pretrain autoencoder to minimise reconstruction loss \n",
    "# and keep model for clustering\n",
    "def train(model):\n",
    "    # Setup cuda if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Training parameters \n",
    "    nb_epoch = 5\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 100     # SGD optimizer\n",
    "    weight_decay = 1e-5  # Adam optimizer\n",
    "\n",
    "    # Load data set and create dataloader\n",
    "    input_d = numpy.shape(smaller_unlabeled)[1]\n",
    "    train_set = RAMQDatasetForAE(smaller_unlabeled)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "\n",
    "    # Load model and setup proper device\n",
    "    model.to(device)\n",
    "    model.set_encode_only(False)\n",
    "\n",
    "    # Reconstruction loss and optimzer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # optimizer = Adam(model.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i_epoch in range(nb_epoch):\n",
    "\n",
    "        start_time, train_losses = time.time(), []\n",
    "        for i_batch, batch in enumerate(train_loader):\n",
    "            # Read batch data\n",
    "            x, y = batch\n",
    "\n",
    "            # Reset gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predictions and loss\n",
    "            x_pred = model(x)\n",
    "            loss = criterion(x_pred, y)\n",
    "\n",
    "            # Backpropagate and gradient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        mean_loss = numpy.mean(train_losses)\n",
    "        losses.append(mean_loss)\n",
    "        print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, mean_loss, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch    1/5, train loss 0.030879 in 1.93s\n",
      " [-] epoch    2/5, train loss 0.030771 in 1.94s\n",
      " [-] epoch    3/5, train loss 0.030663 in 1.92s\n",
      " [-] epoch    4/5, train loss 0.030559 in 1.94s\n",
      " [-] epoch    5/5, train loss 0.030458 in 1.93s\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(input_d)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE clustering optimisation\n",
    "# Must retrain the autoencoder from pretrained weights\n",
    "# now optimising weights to minimise clustering loss\n",
    "# AND reconstruction loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

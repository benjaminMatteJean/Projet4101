{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de donnees\n",
    "\n",
    "TSNE (squasher les donnees en 2 dimensions) avec les donnees non binaires\n",
    "Analyse correlation( 2 variables ex: type de medecin et etablissement) Montrer les correlations Attribution des poids\n",
    "\n",
    "Listes des techniques a essayer pour le pre-traitement(reductions de dimensionnalite):\n",
    "- Debinariser les donnees et le transformer en dataset de pytorch\n",
    "\n",
    "Listes des techniques a essayer pour le deep learning:\n",
    "- Deep Clustering Network (DCN)\n",
    "- Deep Embedding Network (DEN)\n",
    "- Deep Subspace Clustering Networks (DSC-Nets)\n",
    "- Deep Nonparametric Clustering (DNC) (Non-supervise)\n",
    "- Deep Nonparametric Clustering (DNC) (Non-supervise)\n",
    "\n",
    "Listes des techniques a essayer pour le clustering:\n",
    "- K-Means\n",
    "- Expection maximum\n",
    "- DB scan\n",
    "- Mean shift clustering\n",
    "- Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trainDatasetPerdiem-20191028.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ac3a13acfa00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0minfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trainDatasetPerdiem-20191028.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trainDatasetPerdiem-20191028.pkl'"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "\n",
    "import pickle\n",
    "\n",
    "infile = open('../Projet/trainDatasetPerdiem-20191028.pkl', 'rb')\n",
    "data, labels = pickle.load(infile)\n",
    "\n",
    "#piece = isolatedDataByRows.iloc[:, :]\n",
    "# np_array = numpy.asarray(piece.to_numpy())\n",
    "#print(piece)\n",
    "#print(labels)\n",
    "\n",
    "infile.close()\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    array_of_doctors_values = numpy.asarray(np_array[i][0:33])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "labels = ['Spécialité de médecins', 'Sexe de médecin', 'Langue de correspondance', 'Université de graduation', 'Plage horaire de facturation', 'Agence de représentation', 'Établissements', 'ActeMedical', 'Activités médico-admin', 'Activités d’enseignement', 'Entente LE', 'Activité en santé communautaire', 'Activité en santé comm. CSST', 'Activité en santé comm. INSP', 'Année de graduation', 'semaine de l’année associée à la facturation', 'Nombre de jours dès la dernière facture avant le Perdiem', 'Année de naissance', 'Année de début de pratique', 'Expérience', 'Salaire régulier', 'Salaire payé', 'Nombre de Perdiem pendant l’année', 'Jour de la semaine associé à la facturation', 'Nombre de Perdiem', 'Nombre d’heures facturées', 'Nombre de services avec bénéficiaire', 'Nombre de services sans bénéficiaire', 'Nombre de patients', 'Montant réclamé avec bénéficiaire', 'Montant réclamé sans bénéficiaire', 'Nombre de service total', 'Nombre de Perdiem facturé le jour avant', 'Nombre d’heures facturées le jour avant', 'Nombre de services avec bénéficiaire le jour avant', 'Nombre de services sans bénéficiaire le jour avant', 'Nombre de patients le jour avant', 'Montant réclamé avec bénéficiaire le jour avant', 'Montant réclamé sans bénéficiaire le jour avant','Nombre de services total le jour avant', 'âge de médecin le jour avant', 'Nombre de Perdiem facturés la semaine d’avant']\n",
    "\n",
    "# remodeler le format de données \n",
    "numberOfRows = len(data)\n",
    "\n",
    "np_array = data.head(numberOfRows).values\n",
    "new_array_data = [[] for c in range(numberOfRows)];\n",
    "\n",
    "for i in range(len(np_array)):\n",
    "    print(len(np_array[i][0:33]))\n",
    "  array_of_doctors_values = numpy.asarray(np_array[i][0:33])\n",
    "  array_of_university_values = numpy.asarray(np_array[i][37:51])\n",
    "  array_of_daily_moment_values = numpy.asarray(np_array[i][51:54])\n",
    "  array_of_agencies_values = numpy.asarray(np_array[i][54:899])\n",
    "  array_of_establishment_values = numpy.asarray(np_array[i][899:1140])\n",
    "  index_of_doctors = numpy.where(array_of_doctors_values == 1)[0]\n",
    "\n",
    "  index_of_university = numpy.where(array_of_university_values == 1)[0][0]\n",
    "  # donc on va dire 0 homme, 1 femme\n",
    "  # donc on va dire 0 francais, 1 anglais\n",
    "  # donc on va dire 0 AM, 1 PM, 2 all day\n",
    "  # donc on va dire ActeMedical 0 oui 1 non\n",
    "  # donc on va dire Activités médico-admin 0 oui 1 non\n",
    "  # donc on va dire Activités d’enseignement 0 oui 1 non\n",
    "  # donc on va dire Entente LE 0 oui 1 non\n",
    "  # donc on va dire Activité en santé communautaire 0 oui 1 non\n",
    "  # donc on va dire Activité en santé comm. CSST 0 oui 1 non\n",
    "  # donc on va dire Activité en santé comm. INSP 0 oui 1 non\n",
    "  index_of_daily_moment = numpy.where(array_of_daily_moment_values == 1)[0][0]\n",
    "  index_of_agency = numpy.where(array_of_agencies_values == 1)[0][0]\n",
    "  index_of_establishment = numpy.where(array_of_establishment_values == 1)[0][0]\n",
    "  np_array[i][31] = index_of_doctors[0]\n",
    "  new_array_please = numpy.delete(np_array[i], slice(0, 31), 0)\n",
    "  # Chopping one column of sex\n",
    "  new_array_please = numpy.delete(new_array_please, slice(1,2), 0)\n",
    "  # chopping one colum of language\n",
    "  new_array_please = numpy.delete(new_array_please, slice(2,3), 0)\n",
    "  new_array_please[3] = index_of_university\n",
    "  new_array_please = numpy.delete(new_array_please, slice(4, 18), 0)\n",
    "  new_array_please[4] = index_of_daily_moment\n",
    "  new_array_please = numpy.delete(new_array_please, slice(5, 7), 0)\n",
    "  new_array_please[5] = index_of_agency\n",
    "  new_array_please = numpy.delete(new_array_please, slice(6, 851), 0)\n",
    "  new_array_please[6] = index_of_establishment\n",
    "  new_array_please = numpy.delete(new_array_please, slice(7, 248), 0)\n",
    "  new_array_please = numpy.delete(new_array_please, slice(8, 9), 0)\n",
    "  new_array_please = numpy.delete(new_array_please, slice(9, 10), 0)\n",
    "  new_array_please = numpy.delete(new_array_please, slice(10, 11), 0)\n",
    "  new_array_please = numpy.delete(new_array_please, slice(11, 12), 0)\n",
    "  new_array_please = numpy.delete(new_array_please, slice(12, 13), 0)\n",
    "  new_array_please = numpy.delete(new_array_please, slice(13, 14), 0)\n",
    "  new_array_data[i] = new_array_please\n",
    "\n",
    "modifiedData = numpy.array(new_array_data)\n",
    "\n",
    "# save modified data\n",
    "numpy.save('debinarizedData.npy', modifiedData)\n",
    "\n",
    "# # Creation du dataset\n",
    "# train_set = RAMQDataset(modifiedData)\n",
    "# print(train_set.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debinarizedData = numpy.load('debinarizedData.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        #print(self.data)\n",
    "        # Pour faciliter la lecture des targets\n",
    "        self.targets = numpy.array(list(zip(*self.data))[41])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

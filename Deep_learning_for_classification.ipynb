{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning pour Classification\n",
    "\n",
    "Ce notebook regroupe quelques configurations d'algorithmes de Deep Learning pour la classification de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Variable                        Possibilités             Nombre de colonne (+1)\n",
      "Spécialité de médecins\t        33 codes de spécialité\t        1-33\n",
      "Sexe de médecin \t            2\t                            34-35\n",
      "Langue de correspondance \t    2\t                            36-37\n",
      "Université de graduation \t    14 universités\t                38-51\n",
      "Plage horaire de facturation\t3 (AM, PM, toute la journée)\t52-54\n",
      "Agence de représentation\t    845 codes d’agences             55-899\n",
      "Établissements                  241 établissements\t            900-1140\n",
      "ActeMedical                     2 (Oui et Non)\t                1141-1142\n",
      "Activités médico-admin          2 (Oui et Non)\t                1143-1144\n",
      "Activités d’enseignement\t    2 (Oui et Non)\t                1145-1146\n",
      "Entente LE\t                    2 (Oui et Non)\t                1147-1148\n",
      "Activité en santé communautaire\t2 (Oui et Non)\t                1149-1150\n",
      "Activité en santé comm. CSST\t2 (Oui et Non)\t                1151-1152\n",
      "Activité en santé comm. INSP\t2 (Oui et Non)\t                1153-1154\n",
      "Année de graduation\t                                            1155\n",
      "semaine de l’année associée à la facturation\t                1156\n",
      "Nombre de jours dès la dernière facture avant le Perdiem\t    1157\n",
      "Année de naissance\t                                            1158\n",
      "Année de début de pratique\t                                    1159\n",
      "Expérience\t                                                    1160\n",
      "Salaire régulier\t                                            1161\n",
      "Salaire payé\t                                                1162\n",
      "Nombre de Perdiem pendant l’année\t                            1163\n",
      "Jour de la semaine associé à la facturation\t                    1164\n",
      "Nombre de Perdiem\t                                            1165\n",
      "Nombre d’heures facturées\t                                    1166\n",
      "Nombre de services avec bénéficiaire\t                        1167\n",
      "Nombre de services sans bénéficiaire\t                        1168\n",
      "Nombre de patients\t                                            1169\n",
      "Montant réclamé avec bénéficiaire\t                            1170\n",
      "Montant réclamé sans bénéficiaire\t                            1171\n",
      "Nombre de service total\t                                        1172\n",
      "Nombre de Perdiem facturé le jour avant\t                        1173\n",
      "Nombre d’heures facturées le jour avant\t                        1174\n",
      "Nombre de services avec bénéficiaire le jour avant\t            1175\n",
      "Nombre de services sans bénéficiaire le jour avant\t            1176\n",
      "Nombre de patients le jour avant\t                            1177\n",
      "Montant réclamé avec bénéficiaire le jour avant\t                1178\n",
      "Montant réclamé sans bénéficiaire le jour avant\t                1179\n",
      "Nombre de services total le jour avant\t                        1180\n",
      "âge de médecin le jour avant \t                                1181\n",
      "Nombre de Perdiem facturés la semaine d’avant\t                1182\n",
      "\n",
      "Target:\n",
      "Conformité\n",
      "(-1 : non contrôlé, 1 : cas conforme, 0 : cas non-conforme)\t    1183\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enregistrement du jeu de données comportant les données avec étiquettes\n",
    "Ici, le jeu de données comportant uniquement les données avec des étiquettes de classe est téléchargé. Il est ensuite séparé en jeux d'entraînement et de test en plus d'être normalisé dans toutes les dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData = numpy.load('labeled.npy')\n",
    "numberOfData = labeledData.shape[0] # 1441\n",
    "dimensions = labeledData.shape[1] # 1183\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train = minmax_scale(labeledData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test = minmax_scale(labeledData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Déclaration de fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_sampler(dataset):\n",
    "    def make_weights_for_balanced_classes(images, n_classes):                        \n",
    "        count = [0] * n_classes                                                      \n",
    "        for item in images:                                                         \n",
    "            count[int(item[1])] += 1                                                     \n",
    "        weight_per_class = [0.] * n_classes                                      \n",
    "        N = float(sum(count))                                                   \n",
    "        for i in range(n_classes):                                                   \n",
    "            weight_per_class[i] = N/float(count[i])                                 \n",
    "        weight = [0] * len(images)                                              \n",
    "        for idx, val in enumerate(images):                                          \n",
    "            weight[idx] = weight_per_class[int(val[1])]                                  \n",
    "        return weight\n",
    "\n",
    "    n_classes = numpy.unique(dataset.targets)\n",
    "    weights = make_weights_for_balanced_classes(dataset.data, len(n_classes))                                                         \n",
    "    weights = torch.DoubleTensor(weights)                 \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    return sampler\n",
    "\n",
    "def compute_accuracy(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition du Dataset\n",
    "Définition de la classe RAMQDataset, une classe qui hérite de la classe abstraite torch.utils.data.Dataset. Comme mentionné dans la documentation, les méthodes __getitem__ et __len__ sont surchargées afin d'avoir un jeu de données utilisable par PyTorch. Le data accepté en paramètres est un array numpy dont la dernière dimension est la valeur de l'étiquette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Cette classe sert à définir le dataset RAMQ pour PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for elem in data:\n",
    "            if isinstance(elem, numpy.ndarray):\n",
    "                elem = elem.tolist()\n",
    "                elem_data = torch.Tensor(elem[:-1])\n",
    "                elem_target = torch.FloatTensor([elem[-1]])\n",
    "            # garde les paramètres en mémoire\n",
    "            self.data += [(elem_data, elem_target[0])]\n",
    "            \n",
    "        self.targets = numpy.array(data[:, -1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Déclaration d'un réseau de neurones de base: 1 couche - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.F1 = nn.Linear(1182, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.F1(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Déclaration d'un réseau de neurones de type linéraire: multicouches\n",
    "Ce réseau est personnalisable à souhait: il est possible d'y indiquer, en paramètres d'entrée, le nombre de dimensions en input, le nombre de couches linéaires à avoir dans le réseau en question et le type de la fonction d'activation qui y sera relié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_layers, activation = nn.ELU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        reduction_ratio = (1/n_inputs)**(1/n_layers)\n",
    "        \n",
    "        layers = []\n",
    "        layer_lenght = n_inputs\n",
    "        \n",
    "        # Calculate nb of layers and nb of neurons\n",
    "        # Builds a list (ex:[1000, 500, 250, ... until 1])\n",
    "        while layer_lenght > 1:\n",
    "            layers.append(layer_lenght)\n",
    "            layer_lenght = int(layer_lenght*reduction_ratio)\n",
    "        layers.append(1)\n",
    "        \n",
    "        # Build network layers\n",
    "        network_layers = []\n",
    "        for i, n_neurons in enumerate(layers[:-1]):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            network_layers.append(layer)\n",
    "            network_layers.append(activation)\n",
    "            network_layers.append(nn.BatchNorm1d(layers[i+1]))\n",
    "        network_layers[-1] = nn.Sigmoid()\n",
    "            \n",
    "        # Build pytorch sequential network\n",
    "        self.network = nn.Sequential(\n",
    "            *network_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Exécute l'inférence du réseau. L'ordre \n",
    "        # d'exécution ici est important.\n",
    "        x = self.network(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Déclaration de la fonction permettant l'affichage du pourcentage d'efficacité en classement selon 1 à 9 couches d'un réseau de neurones \"x\"\n",
    "Cette méthode n'a besoin, en entrées, que d'un tableau des pourcentages d'efficacité pour 0 à 9 couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(test_accu):\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(test_accu, label='Test accuracy')\n",
    "    ax.set_xlim(1, 9)\n",
    "    pyplot.grid()\n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Déclaration de la fonction permettant la classification par réseau de neurones profond de type multicouches linéaire\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_results(X_train, X_test, ):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cpu\" #if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 250\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Définit le nombre de dimensions des données avec lesquelles on travaille (la dernière dimension étant l'étiquette)\n",
    "    dimensions = X_train.shape[1] - 1  \n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "\n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "\n",
    "    # Instancier un réseau RAMQNet\n",
    "    # dans une variable nommée \"model\"\n",
    "    for i in range(1,10):\n",
    "        print(\"Je vais utiliser \" + str(i) + \" layers\")\n",
    "        model = RAMQNet(dimensions, i)\n",
    "\n",
    "        # Tranfert le réseau au bon endroit\n",
    "        model.to(device)\n",
    "\n",
    "        # Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "        # et la mettre dans une variable nommée criterion\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        # Ne pas oublier de lui donner les hyperparamètres\n",
    "        # d'entraînement : learning rate et momentum!\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer\n",
    "                # une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calcul du taux de réussite en classement d'un SVM linéaire de base sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on all dimensions: 0.8361111111111111\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM performance with all dimensions\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train[:, : -1], X_train[:, -1])\n",
    "score = clf.score(X_test[:, : -1], X_test[:, -1])\n",
    "print(\"Score on all dimensions:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on tente de classifier les 1441 données à 1182 dimensions avec un réseau de neurones à 1 couche linéaire, on obtient un pourcentage de classement de l'ordre d'environ 82%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calcul du taux de réussite en classement d'un réseau linéaire multicouches sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.686894 in 0.03s\n",
      " [-] epoch    2/250, train loss 0.658042 in 0.03s\n",
      " [-] epoch    3/250, train loss 0.630507 in 0.02s\n",
      " [-] epoch    4/250, train loss 0.617247 in 0.03s\n",
      " [-] epoch    5/250, train loss 0.598471 in 0.03s\n",
      " [-] epoch    6/250, train loss 0.585760 in 0.03s\n",
      " [-] epoch    7/250, train loss 0.585472 in 0.03s\n",
      " [-] epoch    8/250, train loss 0.578508 in 0.03s\n",
      " [-] epoch    9/250, train loss 0.559436 in 0.03s\n",
      " [-] epoch   10/250, train loss 0.566538 in 0.03s\n",
      " [-] epoch   11/250, train loss 0.546757 in 0.03s\n",
      " [-] epoch   12/250, train loss 0.543458 in 0.03s\n",
      " [-] epoch   13/250, train loss 0.539289 in 0.03s\n",
      " [-] epoch   14/250, train loss 0.532973 in 0.02s\n",
      " [-] epoch   15/250, train loss 0.528345 in 0.03s\n",
      " [-] epoch   16/250, train loss 0.514023 in 0.03s\n",
      " [-] epoch   17/250, train loss 0.529386 in 0.03s\n",
      " [-] epoch   18/250, train loss 0.517567 in 0.04s\n",
      " [-] epoch   19/250, train loss 0.519313 in 0.03s\n",
      " [-] epoch   20/250, train loss 0.512015 in 0.03s\n",
      " [-] epoch   21/250, train loss 0.506013 in 0.03s\n",
      " [-] epoch   22/250, train loss 0.528303 in 0.03s\n",
      " [-] epoch   23/250, train loss 0.513400 in 0.03s\n",
      " [-] epoch   24/250, train loss 0.506760 in 0.02s\n",
      " [-] epoch   25/250, train loss 0.525287 in 0.04s\n",
      " [-] epoch   26/250, train loss 0.493433 in 0.03s\n",
      " [-] epoch   27/250, train loss 0.483047 in 0.02s\n",
      " [-] epoch   28/250, train loss 0.514353 in 0.04s\n",
      " [-] epoch   29/250, train loss 0.486162 in 0.03s\n",
      " [-] epoch   30/250, train loss 0.486986 in 0.02s\n",
      " [-] epoch   31/250, train loss 0.481819 in 0.04s\n",
      " [-] epoch   32/250, train loss 0.474423 in 0.03s\n",
      " [-] epoch   33/250, train loss 0.496064 in 0.03s\n",
      " [-] epoch   34/250, train loss 0.486675 in 0.03s\n",
      " [-] epoch   35/250, train loss 0.481116 in 0.03s\n",
      " [-] epoch   36/250, train loss 0.475196 in 0.03s\n",
      " [-] epoch   37/250, train loss 0.470870 in 0.03s\n",
      " [-] epoch   38/250, train loss 0.487678 in 0.03s\n",
      " [-] epoch   39/250, train loss 0.465195 in 0.03s\n",
      " [-] epoch   40/250, train loss 0.481104 in 0.04s\n",
      " [-] epoch   41/250, train loss 0.477669 in 0.03s\n",
      " [-] epoch   42/250, train loss 0.468704 in 0.03s\n",
      " [-] epoch   43/250, train loss 0.469356 in 0.03s\n",
      " [-] epoch   44/250, train loss 0.468588 in 0.03s\n",
      " [-] epoch   45/250, train loss 0.469191 in 0.03s\n",
      " [-] epoch   46/250, train loss 0.486303 in 0.03s\n",
      " [-] epoch   47/250, train loss 0.471700 in 0.03s\n",
      " [-] epoch   48/250, train loss 0.477710 in 0.03s\n",
      " [-] epoch   49/250, train loss 0.458941 in 0.03s\n",
      " [-] epoch   50/250, train loss 0.462526 in 0.03s\n",
      " [-] epoch   51/250, train loss 0.464078 in 0.03s\n",
      " [-] epoch   52/250, train loss 0.475905 in 0.03s\n",
      " [-] epoch   53/250, train loss 0.470088 in 0.03s\n",
      " [-] epoch   54/250, train loss 0.464953 in 0.03s\n",
      " [-] epoch   55/250, train loss 0.454509 in 0.03s\n",
      " [-] epoch   56/250, train loss 0.462534 in 0.03s\n",
      " [-] epoch   57/250, train loss 0.440198 in 0.03s\n",
      " [-] epoch   58/250, train loss 0.453093 in 0.03s\n",
      " [-] epoch   59/250, train loss 0.434883 in 0.03s\n",
      " [-] epoch   60/250, train loss 0.469784 in 0.03s\n",
      " [-] epoch   61/250, train loss 0.462267 in 0.03s\n",
      " [-] epoch   62/250, train loss 0.445654 in 0.03s\n",
      " [-] epoch   63/250, train loss 0.439448 in 0.03s\n",
      " [-] epoch   64/250, train loss 0.463466 in 0.03s\n",
      " [-] epoch   65/250, train loss 0.443017 in 0.03s\n",
      " [-] epoch   66/250, train loss 0.427959 in 0.03s\n",
      " [-] epoch   67/250, train loss 0.431752 in 0.03s\n",
      " [-] epoch   68/250, train loss 0.460021 in 0.03s\n",
      " [-] epoch   69/250, train loss 0.447327 in 0.03s\n",
      " [-] epoch   70/250, train loss 0.441008 in 0.03s\n",
      " [-] epoch   71/250, train loss 0.444952 in 0.03s\n",
      " [-] epoch   72/250, train loss 0.449204 in 0.03s\n",
      " [-] epoch   73/250, train loss 0.443091 in 0.03s\n",
      " [-] epoch   74/250, train loss 0.443718 in 0.03s\n",
      " [-] epoch   75/250, train loss 0.443723 in 0.03s\n",
      " [-] epoch   76/250, train loss 0.442998 in 0.03s\n",
      " [-] epoch   77/250, train loss 0.438818 in 0.02s\n",
      " [-] epoch   78/250, train loss 0.425812 in 0.03s\n",
      " [-] epoch   79/250, train loss 0.455550 in 0.03s\n",
      " [-] epoch   80/250, train loss 0.461926 in 0.03s\n",
      " [-] epoch   81/250, train loss 0.437535 in 0.03s\n",
      " [-] epoch   82/250, train loss 0.435652 in 0.02s\n",
      " [-] epoch   83/250, train loss 0.446051 in 0.03s\n",
      " [-] epoch   84/250, train loss 0.454365 in 0.03s\n",
      " [-] epoch   85/250, train loss 0.426782 in 0.03s\n",
      " [-] epoch   86/250, train loss 0.449189 in 0.03s\n",
      " [-] epoch   87/250, train loss 0.439182 in 0.03s\n",
      " [-] epoch   88/250, train loss 0.439271 in 0.03s\n",
      " [-] epoch   89/250, train loss 0.446835 in 0.03s\n",
      " [-] epoch   90/250, train loss 0.432996 in 0.03s\n",
      " [-] epoch   91/250, train loss 0.423893 in 0.04s\n",
      " [-] epoch   92/250, train loss 0.450408 in 0.03s\n",
      " [-] epoch   93/250, train loss 0.426811 in 0.03s\n",
      " [-] epoch   94/250, train loss 0.420783 in 0.03s\n",
      " [-] epoch   95/250, train loss 0.423785 in 0.03s\n",
      " [-] epoch   96/250, train loss 0.426003 in 0.03s\n",
      " [-] epoch   97/250, train loss 0.456690 in 0.03s\n",
      " [-] epoch   98/250, train loss 0.420610 in 0.03s\n",
      " [-] epoch   99/250, train loss 0.426513 in 0.03s\n",
      " [-] epoch  100/250, train loss 0.461158 in 0.03s\n",
      " [-] epoch  101/250, train loss 0.415989 in 0.03s\n",
      " [-] epoch  102/250, train loss 0.426366 in 0.03s\n",
      " [-] epoch  103/250, train loss 0.448316 in 0.03s\n",
      " [-] epoch  104/250, train loss 0.439311 in 0.03s\n",
      " [-] epoch  105/250, train loss 0.445333 in 0.03s\n",
      " [-] epoch  106/250, train loss 0.423601 in 0.03s\n",
      " [-] epoch  107/250, train loss 0.439187 in 0.03s\n",
      " [-] epoch  108/250, train loss 0.433433 in 0.03s\n",
      " [-] epoch  109/250, train loss 0.441506 in 0.03s\n",
      " [-] epoch  110/250, train loss 0.419462 in 0.03s\n",
      " [-] epoch  111/250, train loss 0.432694 in 0.03s\n",
      " [-] epoch  112/250, train loss 0.415232 in 0.03s\n",
      " [-] epoch  113/250, train loss 0.432503 in 0.03s\n",
      " [-] epoch  114/250, train loss 0.418831 in 0.03s\n",
      " [-] epoch  115/250, train loss 0.399350 in 0.03s\n",
      " [-] epoch  116/250, train loss 0.431033 in 0.03s\n",
      " [-] epoch  117/250, train loss 0.415211 in 0.03s\n",
      " [-] epoch  118/250, train loss 0.422882 in 0.03s\n",
      " [-] epoch  119/250, train loss 0.434474 in 0.03s\n",
      " [-] epoch  120/250, train loss 0.415012 in 0.03s\n",
      " [-] epoch  121/250, train loss 0.424558 in 0.03s\n",
      " [-] epoch  122/250, train loss 0.421011 in 0.03s\n",
      " [-] epoch  123/250, train loss 0.417624 in 0.02s\n",
      " [-] epoch  124/250, train loss 0.420680 in 0.03s\n",
      " [-] epoch  125/250, train loss 0.415637 in 0.03s\n",
      " [-] epoch  126/250, train loss 0.412918 in 0.03s\n",
      " [-] epoch  127/250, train loss 0.414603 in 0.03s\n",
      " [-] epoch  128/250, train loss 0.416057 in 0.03s\n",
      " [-] epoch  129/250, train loss 0.412618 in 0.03s\n",
      " [-] epoch  130/250, train loss 0.404668 in 0.03s\n",
      " [-] epoch  131/250, train loss 0.422584 in 0.03s\n",
      " [-] epoch  132/250, train loss 0.426590 in 0.03s\n",
      " [-] epoch  133/250, train loss 0.407028 in 0.03s\n",
      " [-] epoch  134/250, train loss 0.407906 in 0.03s\n",
      " [-] epoch  135/250, train loss 0.409173 in 0.03s\n",
      " [-] epoch  136/250, train loss 0.413573 in 0.03s\n",
      " [-] epoch  137/250, train loss 0.419781 in 0.03s\n",
      " [-] epoch  138/250, train loss 0.418465 in 0.03s\n",
      " [-] epoch  139/250, train loss 0.438541 in 0.03s\n",
      " [-] epoch  140/250, train loss 0.401110 in 0.03s\n",
      " [-] epoch  141/250, train loss 0.435644 in 0.02s\n",
      " [-] epoch  142/250, train loss 0.406052 in 0.03s\n",
      " [-] epoch  143/250, train loss 0.425998 in 0.03s\n",
      " [-] epoch  144/250, train loss 0.390823 in 0.03s\n",
      " [-] epoch  145/250, train loss 0.437552 in 0.03s\n",
      " [-] epoch  146/250, train loss 0.448306 in 0.03s\n",
      " [-] epoch  147/250, train loss 0.410591 in 0.03s\n",
      " [-] epoch  148/250, train loss 0.428829 in 0.03s\n",
      " [-] epoch  149/250, train loss 0.424597 in 0.03s\n",
      " [-] epoch  150/250, train loss 0.414360 in 0.03s\n",
      " [-] epoch  151/250, train loss 0.411201 in 0.03s\n",
      " [-] epoch  152/250, train loss 0.416670 in 0.03s\n",
      " [-] epoch  153/250, train loss 0.407897 in 0.03s\n",
      " [-] epoch  154/250, train loss 0.421654 in 0.03s\n",
      " [-] epoch  155/250, train loss 0.410982 in 0.03s\n",
      " [-] epoch  156/250, train loss 0.409894 in 0.03s\n",
      " [-] epoch  157/250, train loss 0.405318 in 0.03s\n",
      " [-] epoch  158/250, train loss 0.410543 in 0.03s\n",
      " [-] epoch  159/250, train loss 0.418293 in 0.03s\n",
      " [-] epoch  160/250, train loss 0.416968 in 0.03s\n",
      " [-] epoch  161/250, train loss 0.424800 in 0.03s\n",
      " [-] epoch  162/250, train loss 0.418563 in 0.03s\n",
      " [-] epoch  163/250, train loss 0.433209 in 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  164/250, train loss 0.408729 in 0.03s\n",
      " [-] epoch  165/250, train loss 0.422436 in 0.03s\n",
      " [-] epoch  166/250, train loss 0.426027 in 0.03s\n",
      " [-] epoch  167/250, train loss 0.409895 in 0.03s\n",
      " [-] epoch  168/250, train loss 0.409231 in 0.03s\n",
      " [-] epoch  169/250, train loss 0.395823 in 0.03s\n",
      " [-] epoch  170/250, train loss 0.414587 in 0.03s\n",
      " [-] epoch  171/250, train loss 0.411114 in 0.03s\n",
      " [-] epoch  172/250, train loss 0.399019 in 0.03s\n",
      " [-] epoch  173/250, train loss 0.394968 in 0.03s\n",
      " [-] epoch  174/250, train loss 0.406293 in 0.03s\n",
      " [-] epoch  175/250, train loss 0.410021 in 0.03s\n",
      " [-] epoch  176/250, train loss 0.411000 in 0.03s\n",
      " [-] epoch  177/250, train loss 0.426811 in 0.03s\n",
      " [-] epoch  178/250, train loss 0.423709 in 0.03s\n",
      " [-] epoch  179/250, train loss 0.400071 in 0.03s\n",
      " [-] epoch  180/250, train loss 0.390296 in 0.03s\n",
      " [-] epoch  181/250, train loss 0.416739 in 0.03s\n",
      " [-] epoch  182/250, train loss 0.410546 in 0.03s\n",
      " [-] epoch  183/250, train loss 0.399930 in 0.03s\n",
      " [-] epoch  184/250, train loss 0.405117 in 0.03s\n",
      " [-] epoch  185/250, train loss 0.414516 in 0.04s\n",
      " [-] epoch  186/250, train loss 0.417465 in 0.03s\n",
      " [-] epoch  187/250, train loss 0.411473 in 0.03s\n",
      " [-] epoch  188/250, train loss 0.401347 in 0.03s\n",
      " [-] epoch  189/250, train loss 0.407957 in 0.03s\n",
      " [-] epoch  190/250, train loss 0.390860 in 0.03s\n",
      " [-] epoch  191/250, train loss 0.409210 in 0.03s\n",
      " [-] epoch  192/250, train loss 0.389607 in 0.03s\n",
      " [-] epoch  193/250, train loss 0.395724 in 0.03s\n",
      " [-] epoch  194/250, train loss 0.439176 in 0.03s\n",
      " [-] epoch  195/250, train loss 0.408678 in 0.02s\n",
      " [-] epoch  196/250, train loss 0.409444 in 0.03s\n",
      " [-] epoch  197/250, train loss 0.400692 in 0.03s\n",
      " [-] epoch  198/250, train loss 0.386844 in 0.03s\n",
      " [-] epoch  199/250, train loss 0.409035 in 0.03s\n",
      " [-] epoch  200/250, train loss 0.399440 in 0.03s\n",
      " [-] epoch  201/250, train loss 0.400205 in 0.02s\n",
      " [-] epoch  202/250, train loss 0.408856 in 0.03s\n",
      " [-] epoch  203/250, train loss 0.404271 in 0.03s\n",
      " [-] epoch  204/250, train loss 0.402758 in 0.03s\n",
      " [-] epoch  205/250, train loss 0.428574 in 0.03s\n",
      " [-] epoch  206/250, train loss 0.428718 in 0.03s\n",
      " [-] epoch  207/250, train loss 0.388471 in 0.03s\n",
      " [-] epoch  208/250, train loss 0.400042 in 0.03s\n",
      " [-] epoch  209/250, train loss 0.391695 in 0.03s\n",
      " [-] epoch  210/250, train loss 0.427544 in 0.03s\n",
      " [-] epoch  211/250, train loss 0.400224 in 0.03s\n",
      " [-] epoch  212/250, train loss 0.395737 in 0.03s\n",
      " [-] epoch  213/250, train loss 0.407511 in 0.03s\n",
      " [-] epoch  214/250, train loss 0.407626 in 0.03s\n",
      " [-] epoch  215/250, train loss 0.377390 in 0.03s\n",
      " [-] epoch  216/250, train loss 0.412704 in 0.03s\n",
      " [-] epoch  217/250, train loss 0.395338 in 0.03s\n",
      " [-] epoch  218/250, train loss 0.406954 in 0.03s\n",
      " [-] epoch  219/250, train loss 0.393453 in 0.03s\n",
      " [-] epoch  220/250, train loss 0.393603 in 0.03s\n",
      " [-] epoch  221/250, train loss 0.415229 in 0.03s\n",
      " [-] epoch  222/250, train loss 0.399749 in 0.03s\n",
      " [-] epoch  223/250, train loss 0.388951 in 0.02s\n",
      " [-] epoch  224/250, train loss 0.386051 in 0.02s\n",
      " [-] epoch  225/250, train loss 0.414744 in 0.03s\n",
      " [-] epoch  226/250, train loss 0.390653 in 0.03s\n",
      " [-] epoch  227/250, train loss 0.404790 in 0.03s\n",
      " [-] epoch  228/250, train loss 0.377873 in 0.03s\n",
      " [-] epoch  229/250, train loss 0.387934 in 0.03s\n",
      " [-] epoch  230/250, train loss 0.396379 in 0.03s\n",
      " [-] epoch  231/250, train loss 0.424489 in 0.03s\n",
      " [-] epoch  232/250, train loss 0.380163 in 0.03s\n",
      " [-] epoch  233/250, train loss 0.395174 in 0.03s\n",
      " [-] epoch  234/250, train loss 0.400887 in 0.03s\n",
      " [-] epoch  235/250, train loss 0.401479 in 0.03s\n",
      " [-] epoch  236/250, train loss 0.373238 in 0.03s\n",
      " [-] epoch  237/250, train loss 0.378884 in 0.03s\n",
      " [-] epoch  238/250, train loss 0.394642 in 0.02s\n",
      " [-] epoch  239/250, train loss 0.409679 in 0.03s\n",
      " [-] epoch  240/250, train loss 0.375524 in 0.03s\n",
      " [-] epoch  241/250, train loss 0.400764 in 0.03s\n",
      " [-] epoch  242/250, train loss 0.388357 in 0.03s\n",
      " [-] epoch  243/250, train loss 0.394262 in 0.03s\n",
      " [-] epoch  244/250, train loss 0.380884 in 0.03s\n",
      " [-] epoch  245/250, train loss 0.395338 in 0.03s\n",
      " [-] epoch  246/250, train loss 0.413390 in 0.03s\n",
      " [-] epoch  247/250, train loss 0.419087 in 0.02s\n",
      " [-] epoch  248/250, train loss 0.408124 in 0.03s\n",
      " [-] epoch  249/250, train loss 0.370555 in 0.03s\n",
      " [-] epoch  250/250, train loss 0.395318 in 0.03s\n",
      " [-] test acc. 83.055556%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.589388 in 0.06s\n",
      " [-] epoch    2/250, train loss 0.459193 in 0.08s\n",
      " [-] epoch    3/250, train loss 0.472389 in 0.07s\n",
      " [-] epoch    4/250, train loss 0.453155 in 0.06s\n",
      " [-] epoch    5/250, train loss 0.421463 in 0.08s\n",
      " [-] epoch    6/250, train loss 0.393128 in 0.07s\n",
      " [-] epoch    7/250, train loss 0.428014 in 0.07s\n",
      " [-] epoch    8/250, train loss 0.410945 in 0.07s\n",
      " [-] epoch    9/250, train loss 0.375544 in 0.07s\n",
      " [-] epoch   10/250, train loss 0.402918 in 0.07s\n",
      " [-] epoch   11/250, train loss 0.408163 in 0.07s\n",
      " [-] epoch   12/250, train loss 0.363794 in 0.06s\n",
      " [-] epoch   13/250, train loss 0.403182 in 0.07s\n",
      " [-] epoch   14/250, train loss 0.376326 in 0.06s\n",
      " [-] epoch   15/250, train loss 0.365432 in 0.08s\n",
      " [-] epoch   16/250, train loss 0.371595 in 0.07s\n",
      " [-] epoch   17/250, train loss 0.391871 in 0.07s\n",
      " [-] epoch   18/250, train loss 0.355608 in 0.06s\n",
      " [-] epoch   19/250, train loss 0.370593 in 0.07s\n",
      " [-] epoch   20/250, train loss 0.394204 in 0.06s\n",
      " [-] epoch   21/250, train loss 0.362525 in 0.08s\n",
      " [-] epoch   22/250, train loss 0.362596 in 0.06s\n",
      " [-] epoch   23/250, train loss 0.346626 in 0.07s\n",
      " [-] epoch   24/250, train loss 0.376250 in 0.06s\n",
      " [-] epoch   25/250, train loss 0.363399 in 0.08s\n",
      " [-] epoch   26/250, train loss 0.372603 in 0.07s\n",
      " [-] epoch   27/250, train loss 0.371687 in 0.08s\n",
      " [-] epoch   28/250, train loss 0.342641 in 0.06s\n",
      " [-] epoch   29/250, train loss 0.354405 in 0.08s\n",
      " [-] epoch   30/250, train loss 0.357196 in 0.07s\n",
      " [-] epoch   31/250, train loss 0.368604 in 0.08s\n",
      " [-] epoch   32/250, train loss 0.339927 in 0.07s\n",
      " [-] epoch   33/250, train loss 0.365285 in 0.06s\n",
      " [-] epoch   34/250, train loss 0.347617 in 0.08s\n",
      " [-] epoch   35/250, train loss 0.345027 in 0.07s\n",
      " [-] epoch   36/250, train loss 0.369485 in 0.08s\n",
      " [-] epoch   37/250, train loss 0.369957 in 0.07s\n",
      " [-] epoch   38/250, train loss 0.377549 in 0.08s\n",
      " [-] epoch   39/250, train loss 0.350288 in 0.08s\n",
      " [-] epoch   40/250, train loss 0.354258 in 0.08s\n",
      " [-] epoch   41/250, train loss 0.374277 in 0.08s\n",
      " [-] epoch   42/250, train loss 0.360221 in 0.08s\n",
      " [-] epoch   43/250, train loss 0.298430 in 0.07s\n",
      " [-] epoch   44/250, train loss 0.360025 in 0.07s\n",
      " [-] epoch   45/250, train loss 0.306287 in 0.07s\n",
      " [-] epoch   46/250, train loss 0.318142 in 0.07s\n",
      " [-] epoch   47/250, train loss 0.336756 in 0.06s\n",
      " [-] epoch   48/250, train loss 0.322262 in 0.07s\n",
      " [-] epoch   49/250, train loss 0.353159 in 0.07s\n",
      " [-] epoch   50/250, train loss 0.342092 in 0.07s\n",
      " [-] epoch   51/250, train loss 0.346992 in 0.06s\n",
      " [-] epoch   52/250, train loss 0.359186 in 0.07s\n",
      " [-] epoch   53/250, train loss 0.337083 in 0.06s\n",
      " [-] epoch   54/250, train loss 0.336179 in 0.07s\n",
      " [-] epoch   55/250, train loss 0.354946 in 0.08s\n",
      " [-] epoch   56/250, train loss 0.340803 in 0.07s\n",
      " [-] epoch   57/250, train loss 0.339750 in 0.06s\n",
      " [-] epoch   58/250, train loss 0.331449 in 0.08s\n",
      " [-] epoch   59/250, train loss 0.334794 in 0.07s\n",
      " [-] epoch   60/250, train loss 0.372095 in 0.06s\n",
      " [-] epoch   61/250, train loss 0.354032 in 0.07s\n",
      " [-] epoch   62/250, train loss 0.339908 in 0.07s\n",
      " [-] epoch   63/250, train loss 0.329310 in 0.07s\n",
      " [-] epoch   64/250, train loss 0.345802 in 0.07s\n",
      " [-] epoch   65/250, train loss 0.321532 in 0.06s\n",
      " [-] epoch   66/250, train loss 0.335564 in 0.07s\n",
      " [-] epoch   67/250, train loss 0.328030 in 0.06s\n",
      " [-] epoch   68/250, train loss 0.329418 in 0.08s\n",
      " [-] epoch   69/250, train loss 0.344038 in 0.07s\n",
      " [-] epoch   70/250, train loss 0.340202 in 0.07s\n",
      " [-] epoch   71/250, train loss 0.331445 in 0.07s\n",
      " [-] epoch   72/250, train loss 0.366268 in 0.07s\n",
      " [-] epoch   73/250, train loss 0.332901 in 0.07s\n",
      " [-] epoch   74/250, train loss 0.371943 in 0.07s\n",
      " [-] epoch   75/250, train loss 0.326040 in 0.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.333345 in 0.08s\n",
      " [-] epoch   77/250, train loss 0.311390 in 0.07s\n",
      " [-] epoch   78/250, train loss 0.329150 in 0.07s\n",
      " [-] epoch   79/250, train loss 0.329106 in 0.07s\n",
      " [-] epoch   80/250, train loss 0.360876 in 0.07s\n",
      " [-] epoch   81/250, train loss 0.333102 in 0.06s\n",
      " [-] epoch   82/250, train loss 0.335034 in 0.08s\n",
      " [-] epoch   83/250, train loss 0.334150 in 0.08s\n",
      " [-] epoch   84/250, train loss 0.321516 in 0.07s\n",
      " [-] epoch   85/250, train loss 0.308439 in 0.07s\n",
      " [-] epoch   86/250, train loss 0.342272 in 0.08s\n",
      " [-] epoch   87/250, train loss 0.315602 in 0.07s\n",
      " [-] epoch   88/250, train loss 0.310725 in 0.07s\n",
      " [-] epoch   89/250, train loss 0.285735 in 0.07s\n",
      " [-] epoch   90/250, train loss 0.315211 in 0.07s\n",
      " [-] epoch   91/250, train loss 0.354974 in 0.07s\n",
      " [-] epoch   92/250, train loss 0.318265 in 0.07s\n",
      " [-] epoch   93/250, train loss 0.321508 in 0.07s\n",
      " [-] epoch   94/250, train loss 0.317926 in 0.07s\n",
      " [-] epoch   95/250, train loss 0.306919 in 0.08s\n",
      " [-] epoch   96/250, train loss 0.324599 in 0.07s\n",
      " [-] epoch   97/250, train loss 0.337846 in 0.06s\n",
      " [-] epoch   98/250, train loss 0.319472 in 0.07s\n",
      " [-] epoch   99/250, train loss 0.318448 in 0.07s\n",
      " [-] epoch  100/250, train loss 0.300456 in 0.07s\n",
      " [-] epoch  101/250, train loss 0.306435 in 0.07s\n",
      " [-] epoch  102/250, train loss 0.329245 in 0.07s\n",
      " [-] epoch  103/250, train loss 0.314801 in 0.07s\n",
      " [-] epoch  104/250, train loss 0.327013 in 0.07s\n",
      " [-] epoch  105/250, train loss 0.311309 in 0.07s\n",
      " [-] epoch  106/250, train loss 0.316106 in 0.07s\n",
      " [-] epoch  107/250, train loss 0.305279 in 0.06s\n",
      " [-] epoch  108/250, train loss 0.310491 in 0.07s\n",
      " [-] epoch  109/250, train loss 0.327569 in 0.08s\n",
      " [-] epoch  110/250, train loss 0.332822 in 0.07s\n",
      " [-] epoch  111/250, train loss 0.307179 in 0.08s\n",
      " [-] epoch  112/250, train loss 0.294819 in 0.08s\n",
      " [-] epoch  113/250, train loss 0.309085 in 0.07s\n",
      " [-] epoch  114/250, train loss 0.300326 in 0.07s\n",
      " [-] epoch  115/250, train loss 0.302809 in 0.07s\n",
      " [-] epoch  116/250, train loss 0.310339 in 0.07s\n",
      " [-] epoch  117/250, train loss 0.314488 in 0.07s\n",
      " [-] epoch  118/250, train loss 0.306241 in 0.07s\n",
      " [-] epoch  119/250, train loss 0.314247 in 0.07s\n",
      " [-] epoch  120/250, train loss 0.351496 in 0.07s\n",
      " [-] epoch  121/250, train loss 0.320036 in 0.07s\n",
      " [-] epoch  122/250, train loss 0.317873 in 0.06s\n",
      " [-] epoch  123/250, train loss 0.296083 in 0.07s\n",
      " [-] epoch  124/250, train loss 0.321607 in 0.06s\n",
      " [-] epoch  125/250, train loss 0.277357 in 0.08s\n",
      " [-] epoch  126/250, train loss 0.308154 in 0.07s\n",
      " [-] epoch  127/250, train loss 0.312035 in 0.06s\n",
      " [-] epoch  128/250, train loss 0.293939 in 0.08s\n",
      " [-] epoch  129/250, train loss 0.290185 in 0.07s\n",
      " [-] epoch  130/250, train loss 0.321380 in 0.08s\n",
      " [-] epoch  131/250, train loss 0.315790 in 0.07s\n",
      " [-] epoch  132/250, train loss 0.310334 in 0.07s\n",
      " [-] epoch  133/250, train loss 0.295770 in 0.07s\n",
      " [-] epoch  134/250, train loss 0.307225 in 0.07s\n",
      " [-] epoch  135/250, train loss 0.269859 in 0.08s\n",
      " [-] epoch  136/250, train loss 0.300428 in 0.07s\n",
      " [-] epoch  137/250, train loss 0.323631 in 0.07s\n",
      " [-] epoch  138/250, train loss 0.293592 in 0.07s\n",
      " [-] epoch  139/250, train loss 0.289810 in 0.07s\n",
      " [-] epoch  140/250, train loss 0.306743 in 0.07s\n",
      " [-] epoch  141/250, train loss 0.301681 in 0.06s\n",
      " [-] epoch  142/250, train loss 0.297407 in 0.08s\n",
      " [-] epoch  143/250, train loss 0.304401 in 0.07s\n",
      " [-] epoch  144/250, train loss 0.283707 in 0.08s\n",
      " [-] epoch  145/250, train loss 0.269838 in 0.07s\n",
      " [-] epoch  146/250, train loss 0.299356 in 0.08s\n",
      " [-] epoch  147/250, train loss 0.296061 in 0.07s\n",
      " [-] epoch  148/250, train loss 0.307497 in 0.07s\n",
      " [-] epoch  149/250, train loss 0.301179 in 0.08s\n",
      " [-] epoch  150/250, train loss 0.283223 in 0.07s\n",
      " [-] epoch  151/250, train loss 0.267941 in 0.07s\n",
      " [-] epoch  152/250, train loss 0.287276 in 0.07s\n",
      " [-] epoch  153/250, train loss 0.291224 in 0.07s\n",
      " [-] epoch  154/250, train loss 0.298835 in 0.08s\n",
      " [-] epoch  155/250, train loss 0.298897 in 0.08s\n",
      " [-] epoch  156/250, train loss 0.304863 in 0.07s\n",
      " [-] epoch  157/250, train loss 0.282436 in 0.07s\n",
      " [-] epoch  158/250, train loss 0.281890 in 0.07s\n",
      " [-] epoch  159/250, train loss 0.318245 in 0.07s\n",
      " [-] epoch  160/250, train loss 0.318915 in 0.07s\n",
      " [-] epoch  161/250, train loss 0.287761 in 0.07s\n",
      " [-] epoch  162/250, train loss 0.302645 in 0.07s\n",
      " [-] epoch  163/250, train loss 0.279151 in 0.08s\n",
      " [-] epoch  164/250, train loss 0.284099 in 0.07s\n",
      " [-] epoch  165/250, train loss 0.289034 in 0.07s\n",
      " [-] epoch  166/250, train loss 0.297657 in 0.07s\n",
      " [-] epoch  167/250, train loss 0.299889 in 0.08s\n",
      " [-] epoch  168/250, train loss 0.289059 in 0.08s\n",
      " [-] epoch  169/250, train loss 0.301907 in 0.07s\n",
      " [-] epoch  170/250, train loss 0.305862 in 0.07s\n",
      " [-] epoch  171/250, train loss 0.302918 in 0.07s\n",
      " [-] epoch  172/250, train loss 0.292983 in 0.07s\n",
      " [-] epoch  173/250, train loss 0.291203 in 0.08s\n",
      " [-] epoch  174/250, train loss 0.292020 in 0.07s\n",
      " [-] epoch  175/250, train loss 0.287331 in 0.07s\n",
      " [-] epoch  176/250, train loss 0.297907 in 0.07s\n",
      " [-] epoch  177/250, train loss 0.296119 in 0.07s\n",
      " [-] epoch  178/250, train loss 0.296267 in 0.07s\n",
      " [-] epoch  179/250, train loss 0.309634 in 0.08s\n",
      " [-] epoch  180/250, train loss 0.296431 in 0.07s\n",
      " [-] epoch  181/250, train loss 0.287648 in 0.07s\n",
      " [-] epoch  182/250, train loss 0.293996 in 0.08s\n",
      " [-] epoch  183/250, train loss 0.268670 in 0.08s\n",
      " [-] epoch  184/250, train loss 0.324164 in 0.08s\n",
      " [-] epoch  185/250, train loss 0.268052 in 0.07s\n",
      " [-] epoch  186/250, train loss 0.281531 in 0.07s\n",
      " [-] epoch  187/250, train loss 0.288383 in 0.07s\n",
      " [-] epoch  188/250, train loss 0.308977 in 0.07s\n",
      " [-] epoch  189/250, train loss 0.289011 in 0.08s\n",
      " [-] epoch  190/250, train loss 0.275585 in 0.08s\n",
      " [-] epoch  191/250, train loss 0.300489 in 0.07s\n",
      " [-] epoch  192/250, train loss 0.300325 in 0.07s\n",
      " [-] epoch  193/250, train loss 0.279927 in 0.07s\n",
      " [-] epoch  194/250, train loss 0.294871 in 0.07s\n",
      " [-] epoch  195/250, train loss 0.289069 in 0.08s\n",
      " [-] epoch  196/250, train loss 0.293895 in 0.08s\n",
      " [-] epoch  197/250, train loss 0.273155 in 0.07s\n",
      " [-] epoch  198/250, train loss 0.299189 in 0.07s\n",
      " [-] epoch  199/250, train loss 0.279941 in 0.07s\n",
      " [-] epoch  200/250, train loss 0.300155 in 0.07s\n",
      " [-] epoch  201/250, train loss 0.289988 in 0.07s\n",
      " [-] epoch  202/250, train loss 0.284975 in 0.07s\n",
      " [-] epoch  203/250, train loss 0.291274 in 0.07s\n",
      " [-] epoch  204/250, train loss 0.285387 in 0.07s\n",
      " [-] epoch  205/250, train loss 0.284815 in 0.06s\n",
      " [-] epoch  206/250, train loss 0.300427 in 0.07s\n",
      " [-] epoch  207/250, train loss 0.299631 in 0.07s\n",
      " [-] epoch  208/250, train loss 0.270530 in 0.07s\n",
      " [-] epoch  209/250, train loss 0.280674 in 0.07s\n",
      " [-] epoch  210/250, train loss 0.285312 in 0.06s\n",
      " [-] epoch  211/250, train loss 0.279595 in 0.09s\n",
      " [-] epoch  212/250, train loss 0.273938 in 0.08s\n",
      " [-] epoch  213/250, train loss 0.296658 in 0.08s\n",
      " [-] epoch  214/250, train loss 0.270728 in 0.07s\n",
      " [-] epoch  215/250, train loss 0.282286 in 0.07s\n",
      " [-] epoch  216/250, train loss 0.280587 in 0.08s\n",
      " [-] epoch  217/250, train loss 0.282202 in 0.07s\n",
      " [-] epoch  218/250, train loss 0.279520 in 0.07s\n",
      " [-] epoch  219/250, train loss 0.274696 in 0.07s\n",
      " [-] epoch  220/250, train loss 0.271196 in 0.08s\n",
      " [-] epoch  221/250, train loss 0.297711 in 0.07s\n",
      " [-] epoch  222/250, train loss 0.299969 in 0.07s\n",
      " [-] epoch  223/250, train loss 0.293038 in 0.07s\n",
      " [-] epoch  224/250, train loss 0.279633 in 0.07s\n",
      " [-] epoch  225/250, train loss 0.278264 in 0.07s\n",
      " [-] epoch  226/250, train loss 0.264108 in 0.07s\n",
      " [-] epoch  227/250, train loss 0.265084 in 0.07s\n",
      " [-] epoch  228/250, train loss 0.300871 in 0.07s\n",
      " [-] epoch  229/250, train loss 0.261103 in 0.07s\n",
      " [-] epoch  230/250, train loss 0.269833 in 0.07s\n",
      " [-] epoch  231/250, train loss 0.272480 in 0.06s\n",
      " [-] epoch  232/250, train loss 0.291648 in 0.07s\n",
      " [-] epoch  233/250, train loss 0.301868 in 0.07s\n",
      " [-] epoch  234/250, train loss 0.264062 in 0.07s\n",
      " [-] epoch  235/250, train loss 0.308281 in 0.07s\n",
      " [-] epoch  236/250, train loss 0.281242 in 0.07s\n",
      " [-] epoch  237/250, train loss 0.277029 in 0.07s\n",
      " [-] epoch  238/250, train loss 0.264663 in 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  239/250, train loss 0.264546 in 0.07s\n",
      " [-] epoch  240/250, train loss 0.270482 in 0.07s\n",
      " [-] epoch  241/250, train loss 0.262180 in 0.06s\n",
      " [-] epoch  242/250, train loss 0.284999 in 0.07s\n",
      " [-] epoch  243/250, train loss 0.266501 in 0.07s\n",
      " [-] epoch  244/250, train loss 0.288543 in 0.08s\n",
      " [-] epoch  245/250, train loss 0.281126 in 0.06s\n",
      " [-] epoch  246/250, train loss 0.272339 in 0.08s\n",
      " [-] epoch  247/250, train loss 0.311172 in 0.07s\n",
      " [-] epoch  248/250, train loss 0.270497 in 0.07s\n",
      " [-] epoch  249/250, train loss 0.278764 in 0.07s\n",
      " [-] epoch  250/250, train loss 0.278198 in 0.06s\n",
      " [-] test acc. 81.666667%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.548998 in 0.13s\n",
      " [-] epoch    2/250, train loss 0.481776 in 0.12s\n",
      " [-] epoch    3/250, train loss 0.429799 in 0.13s\n",
      " [-] epoch    4/250, train loss 0.413195 in 0.10s\n",
      " [-] epoch    5/250, train loss 0.406080 in 0.13s\n",
      " [-] epoch    6/250, train loss 0.389216 in 0.13s\n",
      " [-] epoch    7/250, train loss 0.394210 in 0.14s\n",
      " [-] epoch    8/250, train loss 0.353340 in 0.13s\n",
      " [-] epoch    9/250, train loss 0.377068 in 0.13s\n",
      " [-] epoch   10/250, train loss 0.359617 in 0.13s\n",
      " [-] epoch   11/250, train loss 0.357816 in 0.12s\n",
      " [-] epoch   12/250, train loss 0.348653 in 0.12s\n",
      " [-] epoch   13/250, train loss 0.351575 in 0.13s\n",
      " [-] epoch   14/250, train loss 0.366971 in 0.13s\n",
      " [-] epoch   15/250, train loss 0.351504 in 0.11s\n",
      " [-] epoch   16/250, train loss 0.323480 in 0.14s\n",
      " [-] epoch   17/250, train loss 0.319987 in 0.12s\n",
      " [-] epoch   18/250, train loss 0.340573 in 0.13s\n",
      " [-] epoch   19/250, train loss 0.326697 in 0.12s\n",
      " [-] epoch   20/250, train loss 0.309886 in 0.13s\n",
      " [-] epoch   21/250, train loss 0.330867 in 0.13s\n",
      " [-] epoch   22/250, train loss 0.305773 in 0.12s\n",
      " [-] epoch   23/250, train loss 0.324815 in 0.13s\n",
      " [-] epoch   24/250, train loss 0.298604 in 0.13s\n",
      " [-] epoch   25/250, train loss 0.307371 in 0.13s\n",
      " [-] epoch   26/250, train loss 0.306367 in 0.12s\n",
      " [-] epoch   27/250, train loss 0.299351 in 0.13s\n",
      " [-] epoch   28/250, train loss 0.309949 in 0.13s\n",
      " [-] epoch   29/250, train loss 0.327910 in 0.12s\n",
      " [-] epoch   30/250, train loss 0.314412 in 0.12s\n",
      " [-] epoch   31/250, train loss 0.334165 in 0.12s\n",
      " [-] epoch   32/250, train loss 0.294656 in 0.12s\n",
      " [-] epoch   33/250, train loss 0.316993 in 0.13s\n",
      " [-] epoch   34/250, train loss 0.311328 in 0.13s\n",
      " [-] epoch   35/250, train loss 0.299661 in 0.12s\n",
      " [-] epoch   36/250, train loss 0.303548 in 0.13s\n",
      " [-] epoch   37/250, train loss 0.303151 in 0.12s\n",
      " [-] epoch   38/250, train loss 0.271339 in 0.13s\n",
      " [-] epoch   39/250, train loss 0.294569 in 0.13s\n",
      " [-] epoch   40/250, train loss 0.274751 in 0.12s\n",
      " [-] epoch   41/250, train loss 0.282481 in 0.12s\n",
      " [-] epoch   42/250, train loss 0.281721 in 0.13s\n",
      " [-] epoch   43/250, train loss 0.280698 in 0.12s\n",
      " [-] epoch   44/250, train loss 0.280372 in 0.13s\n",
      " [-] epoch   45/250, train loss 0.269192 in 0.12s\n",
      " [-] epoch   46/250, train loss 0.263785 in 0.13s\n",
      " [-] epoch   47/250, train loss 0.256362 in 0.14s\n",
      " [-] epoch   48/250, train loss 0.298738 in 0.14s\n",
      " [-] epoch   49/250, train loss 0.293382 in 0.12s\n",
      " [-] epoch   50/250, train loss 0.281513 in 0.12s\n",
      " [-] epoch   51/250, train loss 0.264538 in 0.13s\n",
      " [-] epoch   52/250, train loss 0.260880 in 0.13s\n",
      " [-] epoch   53/250, train loss 0.287145 in 0.11s\n",
      " [-] epoch   54/250, train loss 0.275537 in 0.13s\n",
      " [-] epoch   55/250, train loss 0.274719 in 0.12s\n",
      " [-] epoch   56/250, train loss 0.281327 in 0.12s\n",
      " [-] epoch   57/250, train loss 0.271480 in 0.13s\n",
      " [-] epoch   58/250, train loss 0.266263 in 0.13s\n",
      " [-] epoch   59/250, train loss 0.298652 in 0.14s\n",
      " [-] epoch   60/250, train loss 0.286609 in 0.11s\n",
      " [-] epoch   61/250, train loss 0.293065 in 0.12s\n",
      " [-] epoch   62/250, train loss 0.282532 in 0.15s\n",
      " [-] epoch   63/250, train loss 0.268423 in 0.11s\n",
      " [-] epoch   64/250, train loss 0.284838 in 0.14s\n",
      " [-] epoch   65/250, train loss 0.275953 in 0.13s\n",
      " [-] epoch   66/250, train loss 0.274088 in 0.13s\n",
      " [-] epoch   67/250, train loss 0.261729 in 0.12s\n",
      " [-] epoch   68/250, train loss 0.268225 in 0.14s\n",
      " [-] epoch   69/250, train loss 0.267037 in 0.13s\n",
      " [-] epoch   70/250, train loss 0.273024 in 0.13s\n",
      " [-] epoch   71/250, train loss 0.258149 in 0.13s\n",
      " [-] epoch   72/250, train loss 0.259101 in 0.12s\n",
      " [-] epoch   73/250, train loss 0.277705 in 0.13s\n",
      " [-] epoch   74/250, train loss 0.264052 in 0.13s\n",
      " [-] epoch   75/250, train loss 0.273115 in 0.14s\n",
      " [-] epoch   76/250, train loss 0.268949 in 0.11s\n",
      " [-] epoch   77/250, train loss 0.254993 in 0.13s\n",
      " [-] epoch   78/250, train loss 0.265457 in 0.13s\n",
      " [-] epoch   79/250, train loss 0.272922 in 0.12s\n",
      " [-] epoch   80/250, train loss 0.253913 in 0.12s\n",
      " [-] epoch   81/250, train loss 0.270146 in 0.13s\n",
      " [-] epoch   82/250, train loss 0.249337 in 0.13s\n",
      " [-] epoch   83/250, train loss 0.262249 in 0.12s\n",
      " [-] epoch   84/250, train loss 0.244123 in 0.13s\n",
      " [-] epoch   85/250, train loss 0.279192 in 0.12s\n",
      " [-] epoch   86/250, train loss 0.258330 in 0.13s\n",
      " [-] epoch   87/250, train loss 0.261496 in 0.12s\n",
      " [-] epoch   88/250, train loss 0.248889 in 0.13s\n",
      " [-] epoch   89/250, train loss 0.255721 in 0.12s\n",
      " [-] epoch   90/250, train loss 0.273524 in 0.12s\n",
      " [-] epoch   91/250, train loss 0.253215 in 0.13s\n",
      " [-] epoch   92/250, train loss 0.262645 in 0.13s\n",
      " [-] epoch   93/250, train loss 0.270474 in 0.14s\n",
      " [-] epoch   94/250, train loss 0.241591 in 0.12s\n",
      " [-] epoch   95/250, train loss 0.245767 in 0.13s\n",
      " [-] epoch   96/250, train loss 0.266584 in 0.14s\n",
      " [-] epoch   97/250, train loss 0.258896 in 0.13s\n",
      " [-] epoch   98/250, train loss 0.236993 in 0.13s\n",
      " [-] epoch   99/250, train loss 0.244091 in 0.12s\n",
      " [-] epoch  100/250, train loss 0.244552 in 0.13s\n",
      " [-] epoch  101/250, train loss 0.252870 in 0.12s\n",
      " [-] epoch  102/250, train loss 0.253335 in 0.12s\n",
      " [-] epoch  103/250, train loss 0.247400 in 0.14s\n",
      " [-] epoch  104/250, train loss 0.258899 in 0.13s\n",
      " [-] epoch  105/250, train loss 0.244171 in 0.11s\n",
      " [-] epoch  106/250, train loss 0.236502 in 0.13s\n",
      " [-] epoch  107/250, train loss 0.252546 in 0.13s\n",
      " [-] epoch  108/250, train loss 0.263293 in 0.12s\n",
      " [-] epoch  109/250, train loss 0.247907 in 0.14s\n",
      " [-] epoch  110/250, train loss 0.263875 in 0.13s\n",
      " [-] epoch  111/250, train loss 0.256266 in 0.13s\n",
      " [-] epoch  112/250, train loss 0.251174 in 0.15s\n",
      " [-] epoch  113/250, train loss 0.233793 in 0.13s\n",
      " [-] epoch  114/250, train loss 0.246245 in 0.13s\n",
      " [-] epoch  115/250, train loss 0.254213 in 0.13s\n",
      " [-] epoch  116/250, train loss 0.248738 in 0.12s\n",
      " [-] epoch  117/250, train loss 0.246713 in 0.13s\n",
      " [-] epoch  118/250, train loss 0.268509 in 0.12s\n",
      " [-] epoch  119/250, train loss 0.246201 in 0.13s\n",
      " [-] epoch  120/250, train loss 0.234906 in 0.14s\n",
      " [-] epoch  121/250, train loss 0.237309 in 0.13s\n",
      " [-] epoch  122/250, train loss 0.244022 in 0.12s\n",
      " [-] epoch  123/250, train loss 0.271000 in 0.15s\n",
      " [-] epoch  124/250, train loss 0.248293 in 0.11s\n",
      " [-] epoch  125/250, train loss 0.272604 in 0.15s\n",
      " [-] epoch  126/250, train loss 0.236063 in 0.13s\n",
      " [-] epoch  127/250, train loss 0.249643 in 0.12s\n",
      " [-] epoch  128/250, train loss 0.225916 in 0.11s\n",
      " [-] epoch  129/250, train loss 0.271095 in 0.12s\n",
      " [-] epoch  130/250, train loss 0.243716 in 0.12s\n",
      " [-] epoch  131/250, train loss 0.243114 in 0.13s\n",
      " [-] epoch  132/250, train loss 0.244895 in 0.11s\n",
      " [-] epoch  133/250, train loss 0.254783 in 0.12s\n",
      " [-] epoch  134/250, train loss 0.254038 in 0.14s\n",
      " [-] epoch  135/250, train loss 0.224123 in 0.13s\n",
      " [-] epoch  136/250, train loss 0.249336 in 0.14s\n",
      " [-] epoch  137/250, train loss 0.242031 in 0.13s\n",
      " [-] epoch  138/250, train loss 0.228579 in 0.15s\n",
      " [-] epoch  139/250, train loss 0.260474 in 0.13s\n",
      " [-] epoch  140/250, train loss 0.256818 in 0.13s\n",
      " [-] epoch  141/250, train loss 0.231893 in 0.13s\n",
      " [-] epoch  142/250, train loss 0.245649 in 0.12s\n",
      " [-] epoch  143/250, train loss 0.234837 in 0.13s\n",
      " [-] epoch  144/250, train loss 0.256730 in 0.13s\n",
      " [-] epoch  145/250, train loss 0.225744 in 0.13s\n",
      " [-] epoch  146/250, train loss 0.242575 in 0.12s\n",
      " [-] epoch  147/250, train loss 0.255873 in 0.13s\n",
      " [-] epoch  148/250, train loss 0.242225 in 0.14s\n",
      " [-] epoch  149/250, train loss 0.235139 in 0.11s\n",
      " [-] epoch  150/250, train loss 0.225856 in 0.12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.234843 in 0.13s\n",
      " [-] epoch  152/250, train loss 0.266849 in 0.14s\n",
      " [-] epoch  153/250, train loss 0.239830 in 0.13s\n",
      " [-] epoch  154/250, train loss 0.241307 in 0.13s\n",
      " [-] epoch  155/250, train loss 0.242555 in 0.13s\n",
      " [-] epoch  156/250, train loss 0.246355 in 0.14s\n",
      " [-] epoch  157/250, train loss 0.228121 in 0.13s\n",
      " [-] epoch  158/250, train loss 0.232341 in 0.11s\n",
      " [-] epoch  159/250, train loss 0.233410 in 0.13s\n",
      " [-] epoch  160/250, train loss 0.235731 in 0.13s\n",
      " [-] epoch  161/250, train loss 0.254208 in 0.14s\n",
      " [-] epoch  162/250, train loss 0.249817 in 0.12s\n",
      " [-] epoch  163/250, train loss 0.239944 in 0.12s\n",
      " [-] epoch  164/250, train loss 0.234931 in 0.13s\n",
      " [-] epoch  165/250, train loss 0.231438 in 0.11s\n",
      " [-] epoch  166/250, train loss 0.222969 in 0.13s\n",
      " [-] epoch  167/250, train loss 0.222429 in 0.13s\n",
      " [-] epoch  168/250, train loss 0.206306 in 0.14s\n",
      " [-] epoch  169/250, train loss 0.231362 in 0.11s\n",
      " [-] epoch  170/250, train loss 0.235616 in 0.13s\n",
      " [-] epoch  171/250, train loss 0.246580 in 0.13s\n",
      " [-] epoch  172/250, train loss 0.212873 in 0.14s\n",
      " [-] epoch  173/250, train loss 0.202897 in 0.11s\n",
      " [-] epoch  174/250, train loss 0.238768 in 0.12s\n",
      " [-] epoch  175/250, train loss 0.250133 in 0.13s\n",
      " [-] epoch  176/250, train loss 0.241574 in 0.13s\n",
      " [-] epoch  177/250, train loss 0.227147 in 0.13s\n",
      " [-] epoch  178/250, train loss 0.223253 in 0.12s\n",
      " [-] epoch  179/250, train loss 0.223683 in 0.12s\n",
      " [-] epoch  180/250, train loss 0.254161 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.250416 in 0.13s\n",
      " [-] epoch  182/250, train loss 0.228742 in 0.13s\n",
      " [-] epoch  183/250, train loss 0.225841 in 0.13s\n",
      " [-] epoch  184/250, train loss 0.234085 in 0.13s\n",
      " [-] epoch  185/250, train loss 0.243198 in 0.12s\n",
      " [-] epoch  186/250, train loss 0.235568 in 0.13s\n",
      " [-] epoch  187/250, train loss 0.222218 in 0.13s\n",
      " [-] epoch  188/250, train loss 0.222604 in 0.14s\n",
      " [-] epoch  189/250, train loss 0.231385 in 0.12s\n",
      " [-] epoch  190/250, train loss 0.231110 in 0.13s\n",
      " [-] epoch  191/250, train loss 0.240156 in 0.12s\n",
      " [-] epoch  192/250, train loss 0.233919 in 0.13s\n",
      " [-] epoch  193/250, train loss 0.238423 in 0.13s\n",
      " [-] epoch  194/250, train loss 0.229160 in 0.12s\n",
      " [-] epoch  195/250, train loss 0.230515 in 0.13s\n",
      " [-] epoch  196/250, train loss 0.235833 in 0.13s\n",
      " [-] epoch  197/250, train loss 0.241754 in 0.12s\n",
      " [-] epoch  198/250, train loss 0.213245 in 0.13s\n",
      " [-] epoch  199/250, train loss 0.220915 in 0.14s\n",
      " [-] epoch  200/250, train loss 0.232894 in 0.13s\n",
      " [-] epoch  201/250, train loss 0.224880 in 0.14s\n",
      " [-] epoch  202/250, train loss 0.222788 in 0.13s\n",
      " [-] epoch  203/250, train loss 0.233985 in 0.12s\n",
      " [-] epoch  204/250, train loss 0.235830 in 0.13s\n",
      " [-] epoch  205/250, train loss 0.216534 in 0.14s\n",
      " [-] epoch  206/250, train loss 0.219374 in 0.11s\n",
      " [-] epoch  207/250, train loss 0.220148 in 0.12s\n",
      " [-] epoch  208/250, train loss 0.217893 in 0.13s\n",
      " [-] epoch  209/250, train loss 0.244591 in 0.13s\n",
      " [-] epoch  210/250, train loss 0.249782 in 0.12s\n",
      " [-] epoch  211/250, train loss 0.226870 in 0.13s\n",
      " [-] epoch  212/250, train loss 0.233950 in 0.13s\n",
      " [-] epoch  213/250, train loss 0.222219 in 0.13s\n",
      " [-] epoch  214/250, train loss 0.218724 in 0.12s\n",
      " [-] epoch  215/250, train loss 0.242135 in 0.15s\n",
      " [-] epoch  216/250, train loss 0.227638 in 0.12s\n",
      " [-] epoch  217/250, train loss 0.241059 in 0.12s\n",
      " [-] epoch  218/250, train loss 0.228109 in 0.13s\n",
      " [-] epoch  219/250, train loss 0.228997 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.209795 in 0.12s\n",
      " [-] epoch  221/250, train loss 0.237620 in 0.12s\n",
      " [-] epoch  222/250, train loss 0.217040 in 0.13s\n",
      " [-] epoch  223/250, train loss 0.239071 in 0.15s\n",
      " [-] epoch  224/250, train loss 0.214784 in 0.13s\n",
      " [-] epoch  225/250, train loss 0.226682 in 0.12s\n",
      " [-] epoch  226/250, train loss 0.213082 in 0.12s\n",
      " [-] epoch  227/250, train loss 0.270233 in 0.12s\n",
      " [-] epoch  228/250, train loss 0.216631 in 0.15s\n",
      " [-] epoch  229/250, train loss 0.217919 in 0.12s\n",
      " [-] epoch  230/250, train loss 0.229882 in 0.12s\n",
      " [-] epoch  231/250, train loss 0.227598 in 0.15s\n",
      " [-] epoch  232/250, train loss 0.230850 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.223608 in 0.13s\n",
      " [-] epoch  234/250, train loss 0.259461 in 0.13s\n",
      " [-] epoch  235/250, train loss 0.214967 in 0.12s\n",
      " [-] epoch  236/250, train loss 0.220748 in 0.13s\n",
      " [-] epoch  237/250, train loss 0.216516 in 0.13s\n",
      " [-] epoch  238/250, train loss 0.215944 in 0.14s\n",
      " [-] epoch  239/250, train loss 0.237326 in 0.12s\n",
      " [-] epoch  240/250, train loss 0.222661 in 0.14s\n",
      " [-] epoch  241/250, train loss 0.228059 in 0.13s\n",
      " [-] epoch  242/250, train loss 0.232370 in 0.13s\n",
      " [-] epoch  243/250, train loss 0.214242 in 0.13s\n",
      " [-] epoch  244/250, train loss 0.262731 in 0.14s\n",
      " [-] epoch  245/250, train loss 0.223283 in 0.13s\n",
      " [-] epoch  246/250, train loss 0.231175 in 0.13s\n",
      " [-] epoch  247/250, train loss 0.213466 in 0.14s\n",
      " [-] epoch  248/250, train loss 0.224650 in 0.11s\n",
      " [-] epoch  249/250, train loss 0.230365 in 0.14s\n",
      " [-] epoch  250/250, train loss 0.212486 in 0.14s\n",
      " [-] test acc. 75.833333%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.572536 in 0.20s\n",
      " [-] epoch    2/250, train loss 0.460830 in 0.21s\n",
      " [-] epoch    3/250, train loss 0.459472 in 0.20s\n",
      " [-] epoch    4/250, train loss 0.412910 in 0.21s\n",
      " [-] epoch    5/250, train loss 0.396901 in 0.21s\n",
      " [-] epoch    6/250, train loss 0.406658 in 0.19s\n",
      " [-] epoch    7/250, train loss 0.405068 in 0.19s\n",
      " [-] epoch    8/250, train loss 0.373955 in 0.20s\n",
      " [-] epoch    9/250, train loss 0.370191 in 0.21s\n",
      " [-] epoch   10/250, train loss 0.368443 in 0.20s\n",
      " [-] epoch   11/250, train loss 0.319609 in 0.21s\n",
      " [-] epoch   12/250, train loss 0.388943 in 0.21s\n",
      " [-] epoch   13/250, train loss 0.348861 in 0.19s\n",
      " [-] epoch   14/250, train loss 0.336172 in 0.22s\n",
      " [-] epoch   15/250, train loss 0.317303 in 0.20s\n",
      " [-] epoch   16/250, train loss 0.320564 in 0.21s\n",
      " [-] epoch   17/250, train loss 0.307452 in 0.20s\n",
      " [-] epoch   18/250, train loss 0.330408 in 0.21s\n",
      " [-] epoch   19/250, train loss 0.318575 in 0.18s\n",
      " [-] epoch   20/250, train loss 0.319807 in 0.21s\n",
      " [-] epoch   21/250, train loss 0.331253 in 0.20s\n",
      " [-] epoch   22/250, train loss 0.300432 in 0.22s\n",
      " [-] epoch   23/250, train loss 0.331274 in 0.22s\n",
      " [-] epoch   24/250, train loss 0.328785 in 0.20s\n",
      " [-] epoch   25/250, train loss 0.314998 in 0.22s\n",
      " [-] epoch   26/250, train loss 0.305745 in 0.21s\n",
      " [-] epoch   27/250, train loss 0.305001 in 0.18s\n",
      " [-] epoch   28/250, train loss 0.290372 in 0.20s\n",
      " [-] epoch   29/250, train loss 0.295855 in 0.23s\n",
      " [-] epoch   30/250, train loss 0.309256 in 0.19s\n",
      " [-] epoch   31/250, train loss 0.320750 in 0.19s\n",
      " [-] epoch   32/250, train loss 0.275873 in 0.21s\n",
      " [-] epoch   33/250, train loss 0.293191 in 0.21s\n",
      " [-] epoch   34/250, train loss 0.287080 in 0.20s\n",
      " [-] epoch   35/250, train loss 0.282889 in 0.20s\n",
      " [-] epoch   36/250, train loss 0.247381 in 0.21s\n",
      " [-] epoch   37/250, train loss 0.299133 in 0.20s\n",
      " [-] epoch   38/250, train loss 0.262154 in 0.21s\n",
      " [-] epoch   39/250, train loss 0.285471 in 0.19s\n",
      " [-] epoch   40/250, train loss 0.271809 in 0.23s\n",
      " [-] epoch   41/250, train loss 0.272481 in 0.19s\n",
      " [-] epoch   42/250, train loss 0.276730 in 0.21s\n",
      " [-] epoch   43/250, train loss 0.277207 in 0.20s\n",
      " [-] epoch   44/250, train loss 0.294034 in 0.22s\n",
      " [-] epoch   45/250, train loss 0.295843 in 0.21s\n",
      " [-] epoch   46/250, train loss 0.271388 in 0.21s\n",
      " [-] epoch   47/250, train loss 0.275502 in 0.22s\n",
      " [-] epoch   48/250, train loss 0.276516 in 0.22s\n",
      " [-] epoch   49/250, train loss 0.266488 in 0.21s\n",
      " [-] epoch   50/250, train loss 0.263982 in 0.21s\n",
      " [-] epoch   51/250, train loss 0.262155 in 0.22s\n",
      " [-] epoch   52/250, train loss 0.266975 in 0.21s\n",
      " [-] epoch   53/250, train loss 0.254053 in 0.22s\n",
      " [-] epoch   54/250, train loss 0.262670 in 0.22s\n",
      " [-] epoch   55/250, train loss 0.263575 in 0.20s\n",
      " [-] epoch   56/250, train loss 0.261415 in 0.21s\n",
      " [-] epoch   57/250, train loss 0.250957 in 0.21s\n",
      " [-] epoch   58/250, train loss 0.260746 in 0.21s\n",
      " [-] epoch   59/250, train loss 0.255542 in 0.21s\n",
      " [-] epoch   60/250, train loss 0.264430 in 0.22s\n",
      " [-] epoch   61/250, train loss 0.261399 in 0.19s\n",
      " [-] epoch   62/250, train loss 0.272335 in 0.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   63/250, train loss 0.260636 in 0.22s\n",
      " [-] epoch   64/250, train loss 0.261913 in 0.22s\n",
      " [-] epoch   65/250, train loss 0.267876 in 0.22s\n",
      " [-] epoch   66/250, train loss 0.274463 in 0.21s\n",
      " [-] epoch   67/250, train loss 0.257273 in 0.21s\n",
      " [-] epoch   68/250, train loss 0.277112 in 0.21s\n",
      " [-] epoch   69/250, train loss 0.241130 in 0.20s\n",
      " [-] epoch   70/250, train loss 0.262461 in 0.22s\n",
      " [-] epoch   71/250, train loss 0.254816 in 0.21s\n",
      " [-] epoch   72/250, train loss 0.249694 in 0.20s\n",
      " [-] epoch   73/250, train loss 0.265656 in 0.23s\n",
      " [-] epoch   74/250, train loss 0.257928 in 0.21s\n",
      " [-] epoch   75/250, train loss 0.258374 in 0.22s\n",
      " [-] epoch   76/250, train loss 0.250761 in 0.21s\n",
      " [-] epoch   77/250, train loss 0.258341 in 0.21s\n",
      " [-] epoch   78/250, train loss 0.253590 in 0.19s\n",
      " [-] epoch   79/250, train loss 0.243209 in 0.22s\n",
      " [-] epoch   80/250, train loss 0.220189 in 0.18s\n",
      " [-] epoch   81/250, train loss 0.276070 in 0.20s\n",
      " [-] epoch   82/250, train loss 0.264100 in 0.21s\n",
      " [-] epoch   83/250, train loss 0.248488 in 0.19s\n",
      " [-] epoch   84/250, train loss 0.256401 in 0.22s\n",
      " [-] epoch   85/250, train loss 0.234013 in 0.23s\n",
      " [-] epoch   86/250, train loss 0.254598 in 0.21s\n",
      " [-] epoch   87/250, train loss 0.256024 in 0.22s\n",
      " [-] epoch   88/250, train loss 0.240368 in 0.21s\n",
      " [-] epoch   89/250, train loss 0.251275 in 0.21s\n",
      " [-] epoch   90/250, train loss 0.263941 in 0.21s\n",
      " [-] epoch   91/250, train loss 0.239445 in 0.19s\n",
      " [-] epoch   92/250, train loss 0.251200 in 0.21s\n",
      " [-] epoch   93/250, train loss 0.222514 in 0.19s\n",
      " [-] epoch   94/250, train loss 0.260541 in 0.22s\n",
      " [-] epoch   95/250, train loss 0.236762 in 0.21s\n",
      " [-] epoch   96/250, train loss 0.261780 in 0.22s\n",
      " [-] epoch   97/250, train loss 0.252940 in 0.22s\n",
      " [-] epoch   98/250, train loss 0.220798 in 0.20s\n",
      " [-] epoch   99/250, train loss 0.236238 in 0.21s\n",
      " [-] epoch  100/250, train loss 0.234967 in 0.19s\n",
      " [-] epoch  101/250, train loss 0.228432 in 0.21s\n",
      " [-] epoch  102/250, train loss 0.238935 in 0.22s\n",
      " [-] epoch  103/250, train loss 0.245825 in 0.20s\n",
      " [-] epoch  104/250, train loss 0.248076 in 0.19s\n",
      " [-] epoch  105/250, train loss 0.235966 in 0.21s\n",
      " [-] epoch  106/250, train loss 0.252643 in 0.20s\n",
      " [-] epoch  107/250, train loss 0.232637 in 0.20s\n",
      " [-] epoch  108/250, train loss 0.227560 in 0.22s\n",
      " [-] epoch  109/250, train loss 0.240151 in 0.22s\n",
      " [-] epoch  110/250, train loss 0.233474 in 0.18s\n",
      " [-] epoch  111/250, train loss 0.249697 in 0.21s\n",
      " [-] epoch  112/250, train loss 0.225642 in 0.21s\n",
      " [-] epoch  113/250, train loss 0.229563 in 0.20s\n",
      " [-] epoch  114/250, train loss 0.242120 in 0.21s\n",
      " [-] epoch  115/250, train loss 0.231313 in 0.20s\n",
      " [-] epoch  116/250, train loss 0.247088 in 0.22s\n",
      " [-] epoch  117/250, train loss 0.230189 in 0.19s\n",
      " [-] epoch  118/250, train loss 0.236221 in 0.23s\n",
      " [-] epoch  119/250, train loss 0.226462 in 0.21s\n",
      " [-] epoch  120/250, train loss 0.230407 in 0.21s\n",
      " [-] epoch  121/250, train loss 0.245042 in 0.20s\n",
      " [-] epoch  122/250, train loss 0.219223 in 0.20s\n",
      " [-] epoch  123/250, train loss 0.214356 in 0.18s\n",
      " [-] epoch  124/250, train loss 0.235815 in 0.23s\n",
      " [-] epoch  125/250, train loss 0.228870 in 0.21s\n",
      " [-] epoch  126/250, train loss 0.232052 in 0.21s\n",
      " [-] epoch  127/250, train loss 0.235717 in 0.20s\n",
      " [-] epoch  128/250, train loss 0.225924 in 0.20s\n",
      " [-] epoch  129/250, train loss 0.225592 in 0.20s\n",
      " [-] epoch  130/250, train loss 0.248042 in 0.22s\n",
      " [-] epoch  131/250, train loss 0.253737 in 0.21s\n",
      " [-] epoch  132/250, train loss 0.232231 in 0.20s\n",
      " [-] epoch  133/250, train loss 0.243212 in 0.20s\n",
      " [-] epoch  134/250, train loss 0.228813 in 0.22s\n",
      " [-] epoch  135/250, train loss 0.236813 in 0.21s\n",
      " [-] epoch  136/250, train loss 0.219754 in 0.20s\n",
      " [-] epoch  137/250, train loss 0.251903 in 0.23s\n",
      " [-] epoch  138/250, train loss 0.231044 in 0.20s\n",
      " [-] epoch  139/250, train loss 0.230129 in 0.22s\n",
      " [-] epoch  140/250, train loss 0.210270 in 0.20s\n",
      " [-] epoch  141/250, train loss 0.240337 in 0.21s\n",
      " [-] epoch  142/250, train loss 0.223637 in 0.21s\n",
      " [-] epoch  143/250, train loss 0.230004 in 0.21s\n",
      " [-] epoch  144/250, train loss 0.213134 in 0.19s\n",
      " [-] epoch  145/250, train loss 0.223877 in 0.21s\n",
      " [-] epoch  146/250, train loss 0.231445 in 0.22s\n",
      " [-] epoch  147/250, train loss 0.211316 in 0.19s\n",
      " [-] epoch  148/250, train loss 0.215750 in 0.23s\n",
      " [-] epoch  149/250, train loss 0.228533 in 0.23s\n",
      " [-] epoch  150/250, train loss 0.242415 in 0.21s\n",
      " [-] epoch  151/250, train loss 0.222777 in 0.21s\n",
      " [-] epoch  152/250, train loss 0.224288 in 0.20s\n",
      " [-] epoch  153/250, train loss 0.232864 in 0.19s\n",
      " [-] epoch  154/250, train loss 0.228096 in 0.22s\n",
      " [-] epoch  155/250, train loss 0.214649 in 0.19s\n",
      " [-] epoch  156/250, train loss 0.242022 in 0.21s\n",
      " [-] epoch  157/250, train loss 0.220512 in 0.21s\n",
      " [-] epoch  158/250, train loss 0.211764 in 0.20s\n",
      " [-] epoch  159/250, train loss 0.244099 in 0.22s\n",
      " [-] epoch  160/250, train loss 0.217886 in 0.21s\n",
      " [-] epoch  161/250, train loss 0.216576 in 0.20s\n",
      " [-] epoch  162/250, train loss 0.226786 in 0.21s\n",
      " [-] epoch  163/250, train loss 0.214976 in 0.19s\n",
      " [-] epoch  164/250, train loss 0.229954 in 0.21s\n",
      " [-] epoch  165/250, train loss 0.206799 in 0.21s\n",
      " [-] epoch  166/250, train loss 0.204038 in 0.20s\n",
      " [-] epoch  167/250, train loss 0.203469 in 0.21s\n",
      " [-] epoch  168/250, train loss 0.215607 in 0.19s\n",
      " [-] epoch  169/250, train loss 0.208664 in 0.20s\n",
      " [-] epoch  170/250, train loss 0.220816 in 0.23s\n",
      " [-] epoch  171/250, train loss 0.239777 in 0.21s\n",
      " [-] epoch  172/250, train loss 0.235499 in 0.19s\n",
      " [-] epoch  173/250, train loss 0.226642 in 0.22s\n",
      " [-] epoch  174/250, train loss 0.221699 in 0.21s\n",
      " [-] epoch  175/250, train loss 0.217432 in 0.21s\n",
      " [-] epoch  176/250, train loss 0.223745 in 0.20s\n",
      " [-] epoch  177/250, train loss 0.218495 in 0.20s\n",
      " [-] epoch  178/250, train loss 0.206457 in 0.20s\n",
      " [-] epoch  179/250, train loss 0.212051 in 0.22s\n",
      " [-] epoch  180/250, train loss 0.216446 in 0.21s\n",
      " [-] epoch  181/250, train loss 0.227065 in 0.22s\n",
      " [-] epoch  182/250, train loss 0.234663 in 0.20s\n",
      " [-] epoch  183/250, train loss 0.223362 in 0.20s\n",
      " [-] epoch  184/250, train loss 0.221404 in 0.21s\n",
      " [-] epoch  185/250, train loss 0.232794 in 0.21s\n",
      " [-] epoch  186/250, train loss 0.228450 in 0.21s\n",
      " [-] epoch  187/250, train loss 0.232802 in 0.21s\n",
      " [-] epoch  188/250, train loss 0.218412 in 0.20s\n",
      " [-] epoch  189/250, train loss 0.220684 in 0.22s\n",
      " [-] epoch  190/250, train loss 0.228358 in 0.21s\n",
      " [-] epoch  191/250, train loss 0.227049 in 0.19s\n",
      " [-] epoch  192/250, train loss 0.211411 in 0.21s\n",
      " [-] epoch  193/250, train loss 0.204909 in 0.21s\n",
      " [-] epoch  194/250, train loss 0.211154 in 0.22s\n",
      " [-] epoch  195/250, train loss 0.211739 in 0.21s\n",
      " [-] epoch  196/250, train loss 0.210431 in 0.18s\n",
      " [-] epoch  197/250, train loss 0.206050 in 0.21s\n",
      " [-] epoch  198/250, train loss 0.217859 in 0.23s\n",
      " [-] epoch  199/250, train loss 0.222315 in 0.19s\n",
      " [-] epoch  200/250, train loss 0.217859 in 0.20s\n",
      " [-] epoch  201/250, train loss 0.214647 in 0.24s\n",
      " [-] epoch  202/250, train loss 0.197206 in 0.21s\n",
      " [-] epoch  203/250, train loss 0.215422 in 0.20s\n",
      " [-] epoch  204/250, train loss 0.186006 in 0.21s\n",
      " [-] epoch  205/250, train loss 0.229395 in 0.19s\n",
      " [-] epoch  206/250, train loss 0.217400 in 0.19s\n",
      " [-] epoch  207/250, train loss 0.232518 in 0.22s\n",
      " [-] epoch  208/250, train loss 0.222540 in 0.19s\n",
      " [-] epoch  209/250, train loss 0.215649 in 0.22s\n",
      " [-] epoch  210/250, train loss 0.208649 in 0.20s\n",
      " [-] epoch  211/250, train loss 0.215550 in 0.20s\n",
      " [-] epoch  212/250, train loss 0.205078 in 0.20s\n",
      " [-] epoch  213/250, train loss 0.221486 in 0.22s\n",
      " [-] epoch  214/250, train loss 0.207135 in 0.18s\n",
      " [-] epoch  215/250, train loss 0.228184 in 0.20s\n",
      " [-] epoch  216/250, train loss 0.212064 in 0.20s\n",
      " [-] epoch  217/250, train loss 0.216566 in 0.21s\n",
      " [-] epoch  218/250, train loss 0.214187 in 0.21s\n",
      " [-] epoch  219/250, train loss 0.220381 in 0.20s\n",
      " [-] epoch  220/250, train loss 0.226419 in 0.22s\n",
      " [-] epoch  221/250, train loss 0.210906 in 0.19s\n",
      " [-] epoch  222/250, train loss 0.209546 in 0.21s\n",
      " [-] epoch  223/250, train loss 0.199316 in 0.20s\n",
      " [-] epoch  224/250, train loss 0.198530 in 0.22s\n",
      " [-] epoch  225/250, train loss 0.222716 in 0.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.202808 in 0.19s\n",
      " [-] epoch  227/250, train loss 0.209161 in 0.22s\n",
      " [-] epoch  228/250, train loss 0.212249 in 0.22s\n",
      " [-] epoch  229/250, train loss 0.214586 in 0.21s\n",
      " [-] epoch  230/250, train loss 0.209930 in 0.21s\n",
      " [-] epoch  231/250, train loss 0.212893 in 0.20s\n",
      " [-] epoch  232/250, train loss 0.198827 in 0.19s\n",
      " [-] epoch  233/250, train loss 0.208853 in 0.22s\n",
      " [-] epoch  234/250, train loss 0.210772 in 0.20s\n",
      " [-] epoch  235/250, train loss 0.217120 in 0.20s\n",
      " [-] epoch  236/250, train loss 0.217848 in 0.22s\n",
      " [-] epoch  237/250, train loss 0.208374 in 0.21s\n",
      " [-] epoch  238/250, train loss 0.213925 in 0.21s\n",
      " [-] epoch  239/250, train loss 0.214252 in 0.20s\n",
      " [-] epoch  240/250, train loss 0.208597 in 0.22s\n",
      " [-] epoch  241/250, train loss 0.200781 in 0.19s\n",
      " [-] epoch  242/250, train loss 0.202604 in 0.21s\n",
      " [-] epoch  243/250, train loss 0.219412 in 0.20s\n",
      " [-] epoch  244/250, train loss 0.201550 in 0.22s\n",
      " [-] epoch  245/250, train loss 0.243404 in 0.20s\n",
      " [-] epoch  246/250, train loss 0.202552 in 0.21s\n",
      " [-] epoch  247/250, train loss 0.225751 in 0.21s\n",
      " [-] epoch  248/250, train loss 0.209562 in 0.20s\n",
      " [-] epoch  249/250, train loss 0.198260 in 0.21s\n",
      " [-] epoch  250/250, train loss 0.216227 in 0.21s\n",
      " [-] test acc. 81.111111%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.559365 in 0.30s\n",
      " [-] epoch    2/250, train loss 0.474303 in 0.30s\n",
      " [-] epoch    3/250, train loss 0.449787 in 0.30s\n",
      " [-] epoch    4/250, train loss 0.406066 in 0.29s\n",
      " [-] epoch    5/250, train loss 0.397710 in 0.28s\n",
      " [-] epoch    6/250, train loss 0.366343 in 0.29s\n",
      " [-] epoch    7/250, train loss 0.360887 in 0.31s\n",
      " [-] epoch    8/250, train loss 0.353513 in 0.32s\n",
      " [-] epoch    9/250, train loss 0.368263 in 0.32s\n",
      " [-] epoch   10/250, train loss 0.322781 in 0.30s\n",
      " [-] epoch   11/250, train loss 0.331909 in 0.30s\n",
      " [-] epoch   12/250, train loss 0.345786 in 0.32s\n",
      " [-] epoch   13/250, train loss 0.336821 in 0.29s\n",
      " [-] epoch   14/250, train loss 0.320412 in 0.34s\n",
      " [-] epoch   15/250, train loss 0.342665 in 0.29s\n",
      " [-] epoch   16/250, train loss 0.355893 in 0.30s\n",
      " [-] epoch   17/250, train loss 0.310402 in 0.31s\n",
      " [-] epoch   18/250, train loss 0.338297 in 0.32s\n",
      " [-] epoch   19/250, train loss 0.323058 in 0.32s\n",
      " [-] epoch   20/250, train loss 0.306038 in 0.29s\n",
      " [-] epoch   21/250, train loss 0.300765 in 0.28s\n",
      " [-] epoch   22/250, train loss 0.298256 in 0.32s\n",
      " [-] epoch   23/250, train loss 0.294804 in 0.31s\n",
      " [-] epoch   24/250, train loss 0.268979 in 0.29s\n",
      " [-] epoch   25/250, train loss 0.291884 in 0.32s\n",
      " [-] epoch   26/250, train loss 0.322528 in 0.31s\n",
      " [-] epoch   27/250, train loss 0.275973 in 0.29s\n",
      " [-] epoch   28/250, train loss 0.313792 in 0.30s\n",
      " [-] epoch   29/250, train loss 0.293669 in 0.31s\n",
      " [-] epoch   30/250, train loss 0.290170 in 0.30s\n",
      " [-] epoch   31/250, train loss 0.320042 in 0.30s\n",
      " [-] epoch   32/250, train loss 0.302409 in 0.29s\n",
      " [-] epoch   33/250, train loss 0.310814 in 0.30s\n",
      " [-] epoch   34/250, train loss 0.271252 in 0.30s\n",
      " [-] epoch   35/250, train loss 0.291185 in 0.32s\n",
      " [-] epoch   36/250, train loss 0.281220 in 0.31s\n",
      " [-] epoch   37/250, train loss 0.291094 in 0.29s\n",
      " [-] epoch   38/250, train loss 0.266366 in 0.35s\n",
      " [-] epoch   39/250, train loss 0.261049 in 0.31s\n",
      " [-] epoch   40/250, train loss 0.279213 in 0.31s\n",
      " [-] epoch   41/250, train loss 0.277932 in 0.30s\n",
      " [-] epoch   42/250, train loss 0.303160 in 0.31s\n",
      " [-] epoch   43/250, train loss 0.259338 in 0.30s\n",
      " [-] epoch   44/250, train loss 0.263279 in 0.29s\n",
      " [-] epoch   45/250, train loss 0.263264 in 0.29s\n",
      " [-] epoch   46/250, train loss 0.288649 in 0.29s\n",
      " [-] epoch   47/250, train loss 0.292007 in 0.33s\n",
      " [-] epoch   48/250, train loss 0.273959 in 0.31s\n",
      " [-] epoch   49/250, train loss 0.283108 in 0.30s\n",
      " [-] epoch   50/250, train loss 0.262154 in 0.30s\n",
      " [-] epoch   51/250, train loss 0.241147 in 0.31s\n",
      " [-] epoch   52/250, train loss 0.259903 in 0.30s\n",
      " [-] epoch   53/250, train loss 0.269300 in 0.31s\n",
      " [-] epoch   54/250, train loss 0.261374 in 0.31s\n",
      " [-] epoch   55/250, train loss 0.260212 in 0.31s\n",
      " [-] epoch   56/250, train loss 0.264297 in 0.31s\n",
      " [-] epoch   57/250, train loss 0.264470 in 0.32s\n",
      " [-] epoch   58/250, train loss 0.261110 in 0.30s\n",
      " [-] epoch   59/250, train loss 0.276469 in 0.31s\n",
      " [-] epoch   60/250, train loss 0.251565 in 0.29s\n",
      " [-] epoch   61/250, train loss 0.250381 in 0.27s\n",
      " [-] epoch   62/250, train loss 0.261358 in 0.31s\n",
      " [-] epoch   63/250, train loss 0.262186 in 0.31s\n",
      " [-] epoch   64/250, train loss 0.270290 in 0.33s\n",
      " [-] epoch   65/250, train loss 0.234046 in 0.31s\n",
      " [-] epoch   66/250, train loss 0.251998 in 0.33s\n",
      " [-] epoch   67/250, train loss 0.245488 in 0.32s\n",
      " [-] epoch   68/250, train loss 0.249951 in 0.32s\n",
      " [-] epoch   69/250, train loss 0.239619 in 0.31s\n",
      " [-] epoch   70/250, train loss 0.258894 in 0.31s\n",
      " [-] epoch   71/250, train loss 0.226679 in 0.31s\n",
      " [-] epoch   72/250, train loss 0.273140 in 0.30s\n",
      " [-] epoch   73/250, train loss 0.267879 in 0.30s\n",
      " [-] epoch   74/250, train loss 0.240409 in 0.31s\n",
      " [-] epoch   75/250, train loss 0.240212 in 0.29s\n",
      " [-] epoch   76/250, train loss 0.240801 in 0.33s\n",
      " [-] epoch   77/250, train loss 0.242408 in 0.32s\n",
      " [-] epoch   78/250, train loss 0.252674 in 0.30s\n",
      " [-] epoch   79/250, train loss 0.234116 in 0.30s\n",
      " [-] epoch   80/250, train loss 0.226366 in 0.31s\n",
      " [-] epoch   81/250, train loss 0.254069 in 0.31s\n",
      " [-] epoch   82/250, train loss 0.253929 in 0.29s\n",
      " [-] epoch   83/250, train loss 0.241328 in 0.32s\n",
      " [-] epoch   84/250, train loss 0.235021 in 0.31s\n",
      " [-] epoch   85/250, train loss 0.246328 in 0.30s\n",
      " [-] epoch   86/250, train loss 0.261377 in 0.32s\n",
      " [-] epoch   87/250, train loss 0.211541 in 0.31s\n",
      " [-] epoch   88/250, train loss 0.236196 in 0.32s\n",
      " [-] epoch   89/250, train loss 0.238637 in 0.29s\n",
      " [-] epoch   90/250, train loss 0.227984 in 0.30s\n",
      " [-] epoch   91/250, train loss 0.250745 in 0.28s\n",
      " [-] epoch   92/250, train loss 0.261745 in 0.34s\n",
      " [-] epoch   93/250, train loss 0.243565 in 0.29s\n",
      " [-] epoch   94/250, train loss 0.235681 in 0.31s\n",
      " [-] epoch   95/250, train loss 0.247769 in 0.31s\n",
      " [-] epoch   96/250, train loss 0.257544 in 0.28s\n",
      " [-] epoch   97/250, train loss 0.245783 in 0.29s\n",
      " [-] epoch   98/250, train loss 0.230524 in 0.34s\n",
      " [-] epoch   99/250, train loss 0.249954 in 0.30s\n",
      " [-] epoch  100/250, train loss 0.224078 in 0.32s\n",
      " [-] epoch  101/250, train loss 0.230256 in 0.32s\n",
      " [-] epoch  102/250, train loss 0.217713 in 0.30s\n",
      " [-] epoch  103/250, train loss 0.234390 in 0.31s\n",
      " [-] epoch  104/250, train loss 0.223650 in 0.31s\n",
      " [-] epoch  105/250, train loss 0.253263 in 0.32s\n",
      " [-] epoch  106/250, train loss 0.239015 in 0.31s\n",
      " [-] epoch  107/250, train loss 0.237491 in 0.32s\n",
      " [-] epoch  108/250, train loss 0.235398 in 0.32s\n",
      " [-] epoch  109/250, train loss 0.241581 in 0.32s\n",
      " [-] epoch  110/250, train loss 0.236286 in 0.32s\n",
      " [-] epoch  111/250, train loss 0.226700 in 0.31s\n",
      " [-] epoch  112/250, train loss 0.230768 in 0.28s\n",
      " [-] epoch  113/250, train loss 0.215925 in 0.31s\n",
      " [-] epoch  114/250, train loss 0.205119 in 0.31s\n",
      " [-] epoch  115/250, train loss 0.223898 in 0.33s\n",
      " [-] epoch  116/250, train loss 0.220197 in 0.31s\n",
      " [-] epoch  117/250, train loss 0.219428 in 0.31s\n",
      " [-] epoch  118/250, train loss 0.221842 in 0.30s\n",
      " [-] epoch  119/250, train loss 0.235064 in 0.31s\n",
      " [-] epoch  120/250, train loss 0.217315 in 0.31s\n",
      " [-] epoch  121/250, train loss 0.220712 in 0.31s\n",
      " [-] epoch  122/250, train loss 0.233920 in 0.31s\n",
      " [-] epoch  123/250, train loss 0.212119 in 0.33s\n",
      " [-] epoch  124/250, train loss 0.226997 in 0.31s\n",
      " [-] epoch  125/250, train loss 0.237278 in 0.33s\n",
      " [-] epoch  126/250, train loss 0.233133 in 0.30s\n",
      " [-] epoch  127/250, train loss 0.233827 in 0.31s\n",
      " [-] epoch  128/250, train loss 0.220809 in 0.30s\n",
      " [-] epoch  129/250, train loss 0.226778 in 0.30s\n",
      " [-] epoch  130/250, train loss 0.215021 in 0.29s\n",
      " [-] epoch  131/250, train loss 0.224337 in 0.32s\n",
      " [-] epoch  132/250, train loss 0.219975 in 0.29s\n",
      " [-] epoch  133/250, train loss 0.207293 in 0.32s\n",
      " [-] epoch  134/250, train loss 0.211402 in 0.30s\n",
      " [-] epoch  135/250, train loss 0.215959 in 0.31s\n",
      " [-] epoch  136/250, train loss 0.254175 in 0.31s\n",
      " [-] epoch  137/250, train loss 0.216193 in 0.32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  138/250, train loss 0.225178 in 0.30s\n",
      " [-] epoch  139/250, train loss 0.238280 in 0.30s\n",
      " [-] epoch  140/250, train loss 0.205123 in 0.31s\n",
      " [-] epoch  141/250, train loss 0.215887 in 0.30s\n",
      " [-] epoch  142/250, train loss 0.217657 in 0.30s\n",
      " [-] epoch  143/250, train loss 0.235512 in 0.32s\n",
      " [-] epoch  144/250, train loss 0.227026 in 0.28s\n",
      " [-] epoch  145/250, train loss 0.205447 in 0.28s\n",
      " [-] epoch  146/250, train loss 0.212684 in 0.32s\n",
      " [-] epoch  147/250, train loss 0.245781 in 0.31s\n",
      " [-] epoch  148/250, train loss 0.216611 in 0.32s\n",
      " [-] epoch  149/250, train loss 0.212425 in 0.31s\n",
      " [-] epoch  150/250, train loss 0.212128 in 0.30s\n",
      " [-] epoch  151/250, train loss 0.227811 in 0.30s\n",
      " [-] epoch  152/250, train loss 0.213126 in 0.31s\n",
      " [-] epoch  153/250, train loss 0.236706 in 0.33s\n",
      " [-] epoch  154/250, train loss 0.216155 in 0.30s\n",
      " [-] epoch  155/250, train loss 0.221536 in 0.31s\n",
      " [-] epoch  156/250, train loss 0.206334 in 0.28s\n",
      " [-] epoch  157/250, train loss 0.195101 in 0.30s\n",
      " [-] epoch  158/250, train loss 0.204277 in 0.32s\n",
      " [-] epoch  159/250, train loss 0.201154 in 0.30s\n",
      " [-] epoch  160/250, train loss 0.211628 in 0.30s\n",
      " [-] epoch  161/250, train loss 0.218148 in 0.31s\n",
      " [-] epoch  162/250, train loss 0.224066 in 0.29s\n",
      " [-] epoch  163/250, train loss 0.236891 in 0.31s\n",
      " [-] epoch  164/250, train loss 0.221148 in 0.29s\n",
      " [-] epoch  165/250, train loss 0.207004 in 0.31s\n",
      " [-] epoch  166/250, train loss 0.214705 in 0.29s\n",
      " [-] epoch  167/250, train loss 0.211414 in 0.33s\n",
      " [-] epoch  168/250, train loss 0.215535 in 0.31s\n",
      " [-] epoch  169/250, train loss 0.209496 in 0.33s\n",
      " [-] epoch  170/250, train loss 0.198050 in 0.30s\n",
      " [-] epoch  171/250, train loss 0.205008 in 0.29s\n",
      " [-] epoch  172/250, train loss 0.205621 in 0.31s\n",
      " [-] epoch  173/250, train loss 0.202651 in 0.33s\n",
      " [-] epoch  174/250, train loss 0.208220 in 0.32s\n",
      " [-] epoch  175/250, train loss 0.214223 in 0.30s\n",
      " [-] epoch  176/250, train loss 0.217584 in 0.30s\n",
      " [-] epoch  177/250, train loss 0.216680 in 0.29s\n",
      " [-] epoch  178/250, train loss 0.214155 in 0.31s\n",
      " [-] epoch  179/250, train loss 0.193137 in 0.30s\n",
      " [-] epoch  180/250, train loss 0.202276 in 0.32s\n",
      " [-] epoch  181/250, train loss 0.217257 in 0.30s\n",
      " [-] epoch  182/250, train loss 0.222422 in 0.31s\n",
      " [-] epoch  183/250, train loss 0.207103 in 0.31s\n",
      " [-] epoch  184/250, train loss 0.208362 in 0.30s\n",
      " [-] epoch  185/250, train loss 0.207237 in 0.31s\n",
      " [-] epoch  186/250, train loss 0.194299 in 0.30s\n",
      " [-] epoch  187/250, train loss 0.209790 in 0.29s\n",
      " [-] epoch  188/250, train loss 0.202818 in 0.31s\n",
      " [-] epoch  189/250, train loss 0.225664 in 0.30s\n",
      " [-] epoch  190/250, train loss 0.204240 in 0.31s\n",
      " [-] epoch  191/250, train loss 0.197380 in 0.32s\n",
      " [-] epoch  192/250, train loss 0.204932 in 0.33s\n",
      " [-] epoch  193/250, train loss 0.204571 in 0.30s\n",
      " [-] epoch  194/250, train loss 0.221280 in 0.32s\n",
      " [-] epoch  195/250, train loss 0.201124 in 0.28s\n",
      " [-] epoch  196/250, train loss 0.202120 in 0.30s\n",
      " [-] epoch  197/250, train loss 0.206508 in 0.30s\n",
      " [-] epoch  198/250, train loss 0.200598 in 0.31s\n",
      " [-] epoch  199/250, train loss 0.204162 in 0.31s\n",
      " [-] epoch  200/250, train loss 0.207757 in 0.31s\n",
      " [-] epoch  201/250, train loss 0.224401 in 0.30s\n",
      " [-] epoch  202/250, train loss 0.191727 in 0.32s\n",
      " [-] epoch  203/250, train loss 0.198030 in 0.32s\n",
      " [-] epoch  204/250, train loss 0.194447 in 0.30s\n",
      " [-] epoch  205/250, train loss 0.200706 in 0.27s\n",
      " [-] epoch  206/250, train loss 0.216124 in 0.30s\n",
      " [-] epoch  207/250, train loss 0.204666 in 0.31s\n",
      " [-] epoch  208/250, train loss 0.202701 in 0.31s\n",
      " [-] epoch  209/250, train loss 0.216062 in 0.31s\n",
      " [-] epoch  210/250, train loss 0.211360 in 0.32s\n",
      " [-] epoch  211/250, train loss 0.198946 in 0.31s\n",
      " [-] epoch  212/250, train loss 0.191063 in 0.31s\n",
      " [-] epoch  213/250, train loss 0.202620 in 0.32s\n",
      " [-] epoch  214/250, train loss 0.226943 in 0.31s\n",
      " [-] epoch  215/250, train loss 0.223208 in 0.31s\n",
      " [-] epoch  216/250, train loss 0.203237 in 0.32s\n",
      " [-] epoch  217/250, train loss 0.191074 in 0.32s\n",
      " [-] epoch  218/250, train loss 0.201627 in 0.31s\n",
      " [-] epoch  219/250, train loss 0.194927 in 0.28s\n",
      " [-] epoch  220/250, train loss 0.203719 in 0.31s\n",
      " [-] epoch  221/250, train loss 0.201420 in 0.31s\n",
      " [-] epoch  222/250, train loss 0.202563 in 0.30s\n",
      " [-] epoch  223/250, train loss 0.191297 in 0.33s\n",
      " [-] epoch  224/250, train loss 0.203249 in 0.28s\n",
      " [-] epoch  225/250, train loss 0.184479 in 0.32s\n",
      " [-] epoch  226/250, train loss 0.197731 in 0.30s\n",
      " [-] epoch  227/250, train loss 0.196310 in 0.30s\n",
      " [-] epoch  228/250, train loss 0.202073 in 0.30s\n",
      " [-] epoch  229/250, train loss 0.184929 in 0.30s\n",
      " [-] epoch  230/250, train loss 0.189830 in 0.29s\n",
      " [-] epoch  231/250, train loss 0.192879 in 0.31s\n",
      " [-] epoch  232/250, train loss 0.213231 in 0.31s\n",
      " [-] epoch  233/250, train loss 0.205662 in 0.30s\n",
      " [-] epoch  234/250, train loss 0.203057 in 0.30s\n",
      " [-] epoch  235/250, train loss 0.202783 in 0.31s\n",
      " [-] epoch  236/250, train loss 0.195748 in 0.31s\n",
      " [-] epoch  237/250, train loss 0.212649 in 0.33s\n",
      " [-] epoch  238/250, train loss 0.196296 in 0.30s\n",
      " [-] epoch  239/250, train loss 0.185566 in 0.30s\n",
      " [-] epoch  240/250, train loss 0.198370 in 0.32s\n",
      " [-] epoch  241/250, train loss 0.194606 in 0.31s\n",
      " [-] epoch  242/250, train loss 0.203354 in 0.29s\n",
      " [-] epoch  243/250, train loss 0.236682 in 0.30s\n",
      " [-] epoch  244/250, train loss 0.195741 in 0.30s\n",
      " [-] epoch  245/250, train loss 0.196701 in 0.31s\n",
      " [-] epoch  246/250, train loss 0.203554 in 0.31s\n",
      " [-] epoch  247/250, train loss 0.202776 in 0.31s\n",
      " [-] epoch  248/250, train loss 0.184484 in 0.31s\n",
      " [-] epoch  249/250, train loss 0.206273 in 0.31s\n",
      " [-] epoch  250/250, train loss 0.187154 in 0.34s\n",
      " [-] test acc. 80.833333%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.606700 in 0.40s\n",
      " [-] epoch    2/250, train loss 0.453514 in 0.41s\n",
      " [-] epoch    3/250, train loss 0.460443 in 0.42s\n",
      " [-] epoch    4/250, train loss 0.387659 in 0.42s\n",
      " [-] epoch    5/250, train loss 0.382538 in 0.41s\n",
      " [-] epoch    6/250, train loss 0.368306 in 0.42s\n",
      " [-] epoch    7/250, train loss 0.385961 in 0.41s\n",
      " [-] epoch    8/250, train loss 0.379175 in 0.40s\n",
      " [-] epoch    9/250, train loss 0.361280 in 0.40s\n",
      " [-] epoch   10/250, train loss 0.345835 in 0.42s\n",
      " [-] epoch   11/250, train loss 0.345948 in 0.39s\n",
      " [-] epoch   12/250, train loss 0.353230 in 0.37s\n",
      " [-] epoch   13/250, train loss 0.328064 in 0.40s\n",
      " [-] epoch   14/250, train loss 0.334069 in 0.38s\n",
      " [-] epoch   15/250, train loss 0.335272 in 0.40s\n",
      " [-] epoch   16/250, train loss 0.318465 in 0.41s\n",
      " [-] epoch   17/250, train loss 0.349641 in 0.42s\n",
      " [-] epoch   18/250, train loss 0.322776 in 0.41s\n",
      " [-] epoch   19/250, train loss 0.311687 in 0.41s\n",
      " [-] epoch   20/250, train loss 0.331588 in 0.37s\n",
      " [-] epoch   21/250, train loss 0.331169 in 0.40s\n",
      " [-] epoch   22/250, train loss 0.295651 in 0.38s\n",
      " [-] epoch   23/250, train loss 0.341105 in 0.41s\n",
      " [-] epoch   24/250, train loss 0.283923 in 0.41s\n",
      " [-] epoch   25/250, train loss 0.303558 in 0.38s\n",
      " [-] epoch   26/250, train loss 0.291925 in 0.40s\n",
      " [-] epoch   27/250, train loss 0.312684 in 0.40s\n",
      " [-] epoch   28/250, train loss 0.313453 in 0.39s\n",
      " [-] epoch   29/250, train loss 0.287105 in 0.41s\n",
      " [-] epoch   30/250, train loss 0.312255 in 0.40s\n",
      " [-] epoch   31/250, train loss 0.309914 in 0.37s\n",
      " [-] epoch   32/250, train loss 0.287069 in 0.43s\n",
      " [-] epoch   33/250, train loss 0.283529 in 0.41s\n",
      " [-] epoch   34/250, train loss 0.277173 in 0.39s\n",
      " [-] epoch   35/250, train loss 0.274375 in 0.39s\n",
      " [-] epoch   36/250, train loss 0.291746 in 0.38s\n",
      " [-] epoch   37/250, train loss 0.294398 in 0.38s\n",
      " [-] epoch   38/250, train loss 0.301046 in 0.41s\n",
      " [-] epoch   39/250, train loss 0.266659 in 0.39s\n",
      " [-] epoch   40/250, train loss 0.260513 in 0.39s\n",
      " [-] epoch   41/250, train loss 0.292940 in 0.39s\n",
      " [-] epoch   42/250, train loss 0.261621 in 0.40s\n",
      " [-] epoch   43/250, train loss 0.305818 in 0.41s\n",
      " [-] epoch   44/250, train loss 0.285822 in 0.39s\n",
      " [-] epoch   45/250, train loss 0.262707 in 0.39s\n",
      " [-] epoch   46/250, train loss 0.275028 in 0.37s\n",
      " [-] epoch   47/250, train loss 0.282441 in 0.40s\n",
      " [-] epoch   48/250, train loss 0.271743 in 0.40s\n",
      " [-] epoch   49/250, train loss 0.268962 in 0.40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.271757 in 0.41s\n",
      " [-] epoch   51/250, train loss 0.308569 in 0.43s\n",
      " [-] epoch   52/250, train loss 0.268918 in 0.43s\n",
      " [-] epoch   53/250, train loss 0.272214 in 0.40s\n",
      " [-] epoch   54/250, train loss 0.263229 in 0.41s\n",
      " [-] epoch   55/250, train loss 0.256736 in 0.41s\n",
      " [-] epoch   56/250, train loss 0.253083 in 0.38s\n",
      " [-] epoch   57/250, train loss 0.247069 in 0.42s\n",
      " [-] epoch   58/250, train loss 0.265275 in 0.44s\n",
      " [-] epoch   59/250, train loss 0.270713 in 0.38s\n",
      " [-] epoch   60/250, train loss 0.276519 in 0.40s\n",
      " [-] epoch   61/250, train loss 0.248532 in 0.41s\n",
      " [-] epoch   62/250, train loss 0.247417 in 0.42s\n",
      " [-] epoch   63/250, train loss 0.280878 in 0.40s\n",
      " [-] epoch   64/250, train loss 0.253168 in 0.44s\n",
      " [-] epoch   65/250, train loss 0.256112 in 0.42s\n",
      " [-] epoch   66/250, train loss 0.254210 in 0.40s\n",
      " [-] epoch   67/250, train loss 0.247717 in 0.41s\n",
      " [-] epoch   68/250, train loss 0.259737 in 0.38s\n",
      " [-] epoch   69/250, train loss 0.243274 in 0.40s\n",
      " [-] epoch   70/250, train loss 0.232747 in 0.44s\n",
      " [-] epoch   71/250, train loss 0.245174 in 0.39s\n",
      " [-] epoch   72/250, train loss 0.264671 in 0.40s\n",
      " [-] epoch   73/250, train loss 0.226685 in 0.41s\n",
      " [-] epoch   74/250, train loss 0.280451 in 0.40s\n",
      " [-] epoch   75/250, train loss 0.261491 in 0.39s\n",
      " [-] epoch   76/250, train loss 0.230690 in 0.40s\n",
      " [-] epoch   77/250, train loss 0.234880 in 0.41s\n",
      " [-] epoch   78/250, train loss 0.237076 in 0.39s\n",
      " [-] epoch   79/250, train loss 0.263785 in 0.38s\n",
      " [-] epoch   80/250, train loss 0.263984 in 0.41s\n",
      " [-] epoch   81/250, train loss 0.256483 in 0.42s\n",
      " [-] epoch   82/250, train loss 0.259302 in 0.41s\n",
      " [-] epoch   83/250, train loss 0.248889 in 0.40s\n",
      " [-] epoch   84/250, train loss 0.229319 in 0.42s\n",
      " [-] epoch   85/250, train loss 0.250740 in 0.42s\n",
      " [-] epoch   86/250, train loss 0.237367 in 0.42s\n",
      " [-] epoch   87/250, train loss 0.238393 in 0.40s\n",
      " [-] epoch   88/250, train loss 0.231575 in 0.42s\n",
      " [-] epoch   89/250, train loss 0.239834 in 0.39s\n",
      " [-] epoch   90/250, train loss 0.245359 in 0.38s\n",
      " [-] epoch   91/250, train loss 0.237799 in 0.40s\n",
      " [-] epoch   92/250, train loss 0.227273 in 0.39s\n",
      " [-] epoch   93/250, train loss 0.245116 in 0.40s\n",
      " [-] epoch   94/250, train loss 0.255412 in 0.35s\n",
      " [-] epoch   95/250, train loss 0.263461 in 0.38s\n",
      " [-] epoch   96/250, train loss 0.236792 in 0.39s\n",
      " [-] epoch   97/250, train loss 0.225396 in 0.37s\n",
      " [-] epoch   98/250, train loss 0.238571 in 0.41s\n",
      " [-] epoch   99/250, train loss 0.257403 in 0.40s\n",
      " [-] epoch  100/250, train loss 0.240021 in 0.40s\n",
      " [-] epoch  101/250, train loss 0.238627 in 0.39s\n",
      " [-] epoch  102/250, train loss 0.239762 in 0.40s\n",
      " [-] epoch  103/250, train loss 0.240352 in 0.41s\n",
      " [-] epoch  104/250, train loss 0.232972 in 0.39s\n",
      " [-] epoch  105/250, train loss 0.256917 in 0.40s\n",
      " [-] epoch  106/250, train loss 0.242607 in 0.40s\n",
      " [-] epoch  107/250, train loss 0.253125 in 0.38s\n",
      " [-] epoch  108/250, train loss 0.239590 in 0.40s\n",
      " [-] epoch  109/250, train loss 0.230715 in 0.40s\n",
      " [-] epoch  110/250, train loss 0.251490 in 0.40s\n",
      " [-] epoch  111/250, train loss 0.218639 in 0.40s\n",
      " [-] epoch  112/250, train loss 0.235512 in 0.38s\n",
      " [-] epoch  113/250, train loss 0.219310 in 0.38s\n",
      " [-] epoch  114/250, train loss 0.218812 in 0.39s\n",
      " [-] epoch  115/250, train loss 0.217758 in 0.40s\n",
      " [-] epoch  116/250, train loss 0.230783 in 0.39s\n",
      " [-] epoch  117/250, train loss 0.214615 in 0.40s\n",
      " [-] epoch  118/250, train loss 0.231715 in 0.38s\n",
      " [-] epoch  119/250, train loss 0.194774 in 0.38s\n",
      " [-] epoch  120/250, train loss 0.228888 in 0.41s\n",
      " [-] epoch  121/250, train loss 0.238188 in 0.40s\n",
      " [-] epoch  122/250, train loss 0.224556 in 0.40s\n",
      " [-] epoch  123/250, train loss 0.260977 in 0.41s\n",
      " [-] epoch  124/250, train loss 0.209321 in 0.41s\n",
      " [-] epoch  125/250, train loss 0.216028 in 0.40s\n",
      " [-] epoch  126/250, train loss 0.220493 in 0.42s\n",
      " [-] epoch  127/250, train loss 0.216323 in 0.44s\n",
      " [-] epoch  128/250, train loss 0.205393 in 0.42s\n",
      " [-] epoch  129/250, train loss 0.217060 in 0.44s\n",
      " [-] epoch  130/250, train loss 0.220411 in 0.38s\n",
      " [-] epoch  131/250, train loss 0.208253 in 0.39s\n",
      " [-] epoch  132/250, train loss 0.238756 in 0.39s\n",
      " [-] epoch  133/250, train loss 0.219243 in 0.40s\n",
      " [-] epoch  134/250, train loss 0.215019 in 0.39s\n",
      " [-] epoch  135/250, train loss 0.236422 in 0.37s\n",
      " [-] epoch  136/250, train loss 0.228433 in 0.41s\n",
      " [-] epoch  137/250, train loss 0.246448 in 0.43s\n",
      " [-] epoch  138/250, train loss 0.213095 in 0.42s\n",
      " [-] epoch  139/250, train loss 0.223189 in 0.41s\n",
      " [-] epoch  140/250, train loss 0.222444 in 0.41s\n",
      " [-] epoch  141/250, train loss 0.225862 in 0.39s\n",
      " [-] epoch  142/250, train loss 0.243187 in 0.41s\n",
      " [-] epoch  143/250, train loss 0.222916 in 0.39s\n",
      " [-] epoch  144/250, train loss 0.246214 in 0.40s\n",
      " [-] epoch  145/250, train loss 0.219005 in 0.39s\n",
      " [-] epoch  146/250, train loss 0.225158 in 0.43s\n",
      " [-] epoch  147/250, train loss 0.222127 in 0.41s\n",
      " [-] epoch  148/250, train loss 0.231210 in 0.41s\n",
      " [-] epoch  149/250, train loss 0.206749 in 0.39s\n",
      " [-] epoch  150/250, train loss 0.212790 in 0.40s\n",
      " [-] epoch  151/250, train loss 0.234194 in 0.42s\n",
      " [-] epoch  152/250, train loss 0.231821 in 0.41s\n",
      " [-] epoch  153/250, train loss 0.203613 in 0.42s\n",
      " [-] epoch  154/250, train loss 0.217449 in 0.40s\n",
      " [-] epoch  155/250, train loss 0.232436 in 0.40s\n",
      " [-] epoch  156/250, train loss 0.210756 in 0.42s\n",
      " [-] epoch  157/250, train loss 0.228298 in 0.42s\n",
      " [-] epoch  158/250, train loss 0.209545 in 0.41s\n",
      " [-] epoch  159/250, train loss 0.213645 in 0.42s\n",
      " [-] epoch  160/250, train loss 0.217901 in 0.42s\n",
      " [-] epoch  161/250, train loss 0.220076 in 0.40s\n",
      " [-] epoch  162/250, train loss 0.221281 in 0.41s\n",
      " [-] epoch  163/250, train loss 0.227406 in 0.41s\n",
      " [-] epoch  164/250, train loss 0.210814 in 0.38s\n",
      " [-] epoch  165/250, train loss 0.227018 in 0.37s\n",
      " [-] epoch  166/250, train loss 0.226211 in 0.41s\n",
      " [-] epoch  167/250, train loss 0.215977 in 0.38s\n",
      " [-] epoch  168/250, train loss 0.202118 in 0.39s\n",
      " [-] epoch  169/250, train loss 0.213054 in 0.43s\n",
      " [-] epoch  170/250, train loss 0.213283 in 0.40s\n",
      " [-] epoch  171/250, train loss 0.211885 in 0.42s\n",
      " [-] epoch  172/250, train loss 0.213308 in 0.41s\n",
      " [-] epoch  173/250, train loss 0.213610 in 0.41s\n",
      " [-] epoch  174/250, train loss 0.220448 in 0.39s\n",
      " [-] epoch  175/250, train loss 0.216171 in 0.41s\n",
      " [-] epoch  176/250, train loss 0.218126 in 0.40s\n",
      " [-] epoch  177/250, train loss 0.215108 in 0.40s\n",
      " [-] epoch  178/250, train loss 0.234245 in 0.39s\n",
      " [-] epoch  179/250, train loss 0.219616 in 0.42s\n",
      " [-] epoch  180/250, train loss 0.228013 in 0.42s\n",
      " [-] epoch  181/250, train loss 0.208991 in 0.40s\n",
      " [-] epoch  182/250, train loss 0.206174 in 0.39s\n",
      " [-] epoch  183/250, train loss 0.225221 in 0.38s\n",
      " [-] epoch  184/250, train loss 0.199344 in 0.43s\n",
      " [-] epoch  185/250, train loss 0.209099 in 0.37s\n",
      " [-] epoch  186/250, train loss 0.219488 in 0.39s\n",
      " [-] epoch  187/250, train loss 0.207243 in 0.39s\n",
      " [-] epoch  188/250, train loss 0.222200 in 0.39s\n",
      " [-] epoch  189/250, train loss 0.234067 in 0.40s\n",
      " [-] epoch  190/250, train loss 0.199049 in 0.39s\n",
      " [-] epoch  191/250, train loss 0.208947 in 0.39s\n",
      " [-] epoch  192/250, train loss 0.213520 in 0.41s\n",
      " [-] epoch  193/250, train loss 0.218820 in 0.39s\n",
      " [-] epoch  194/250, train loss 0.216508 in 0.40s\n",
      " [-] epoch  195/250, train loss 0.207567 in 0.41s\n",
      " [-] epoch  196/250, train loss 0.229386 in 0.43s\n",
      " [-] epoch  197/250, train loss 0.191775 in 0.43s\n",
      " [-] epoch  198/250, train loss 0.224843 in 0.40s\n",
      " [-] epoch  199/250, train loss 0.208162 in 0.41s\n",
      " [-] epoch  200/250, train loss 0.235003 in 0.40s\n",
      " [-] epoch  201/250, train loss 0.209682 in 0.39s\n",
      " [-] epoch  202/250, train loss 0.213581 in 0.40s\n",
      " [-] epoch  203/250, train loss 0.243684 in 0.41s\n",
      " [-] epoch  204/250, train loss 0.214649 in 0.41s\n",
      " [-] epoch  205/250, train loss 0.229872 in 0.40s\n",
      " [-] epoch  206/250, train loss 0.213881 in 0.40s\n",
      " [-] epoch  207/250, train loss 0.195453 in 0.40s\n",
      " [-] epoch  208/250, train loss 0.202909 in 0.40s\n",
      " [-] epoch  209/250, train loss 0.224735 in 0.40s\n",
      " [-] epoch  210/250, train loss 0.198440 in 0.40s\n",
      " [-] epoch  211/250, train loss 0.197920 in 0.40s\n",
      " [-] epoch  212/250, train loss 0.209454 in 0.40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  213/250, train loss 0.209138 in 0.40s\n",
      " [-] epoch  214/250, train loss 0.206808 in 0.40s\n",
      " [-] epoch  215/250, train loss 0.216036 in 0.39s\n",
      " [-] epoch  216/250, train loss 0.201850 in 0.39s\n",
      " [-] epoch  217/250, train loss 0.190416 in 0.42s\n",
      " [-] epoch  218/250, train loss 0.210053 in 0.40s\n",
      " [-] epoch  219/250, train loss 0.213334 in 0.40s\n",
      " [-] epoch  220/250, train loss 0.210501 in 0.41s\n",
      " [-] epoch  221/250, train loss 0.211895 in 0.41s\n",
      " [-] epoch  222/250, train loss 0.209103 in 0.41s\n",
      " [-] epoch  223/250, train loss 0.192471 in 0.40s\n",
      " [-] epoch  224/250, train loss 0.231747 in 0.41s\n",
      " [-] epoch  225/250, train loss 0.213436 in 0.42s\n",
      " [-] epoch  226/250, train loss 0.213688 in 0.42s\n",
      " [-] epoch  227/250, train loss 0.219267 in 0.42s\n",
      " [-] epoch  228/250, train loss 0.206710 in 0.40s\n",
      " [-] epoch  229/250, train loss 0.225297 in 0.39s\n",
      " [-] epoch  230/250, train loss 0.204479 in 0.43s\n",
      " [-] epoch  231/250, train loss 0.211790 in 0.41s\n",
      " [-] epoch  232/250, train loss 0.199844 in 0.40s\n",
      " [-] epoch  233/250, train loss 0.229870 in 0.42s\n",
      " [-] epoch  234/250, train loss 0.210197 in 0.41s\n",
      " [-] epoch  235/250, train loss 0.204607 in 0.42s\n",
      " [-] epoch  236/250, train loss 0.190714 in 0.42s\n",
      " [-] epoch  237/250, train loss 0.199299 in 0.40s\n",
      " [-] epoch  238/250, train loss 0.216164 in 0.41s\n",
      " [-] epoch  239/250, train loss 0.220309 in 0.39s\n",
      " [-] epoch  240/250, train loss 0.209018 in 0.40s\n",
      " [-] epoch  241/250, train loss 0.196271 in 0.40s\n",
      " [-] epoch  242/250, train loss 0.190388 in 0.40s\n",
      " [-] epoch  243/250, train loss 0.199562 in 0.40s\n",
      " [-] epoch  244/250, train loss 0.193718 in 0.40s\n",
      " [-] epoch  245/250, train loss 0.207596 in 0.41s\n",
      " [-] epoch  246/250, train loss 0.214986 in 0.41s\n",
      " [-] epoch  247/250, train loss 0.192085 in 0.39s\n",
      " [-] epoch  248/250, train loss 0.203006 in 0.40s\n",
      " [-] epoch  249/250, train loss 0.198135 in 0.39s\n",
      " [-] epoch  250/250, train loss 0.181156 in 0.39s\n",
      " [-] test acc. 84.444444%\n",
      "Je vais utiliser 7 layers\n",
      " [-] epoch    1/250, train loss 0.591937 in 0.48s\n",
      " [-] epoch    2/250, train loss 0.479185 in 0.48s\n",
      " [-] epoch    3/250, train loss 0.420637 in 0.50s\n",
      " [-] epoch    4/250, train loss 0.417657 in 0.50s\n",
      " [-] epoch    5/250, train loss 0.386232 in 0.50s\n",
      " [-] epoch    6/250, train loss 0.377837 in 0.49s\n",
      " [-] epoch    7/250, train loss 0.393516 in 0.49s\n",
      " [-] epoch    8/250, train loss 0.356557 in 0.49s\n",
      " [-] epoch    9/250, train loss 0.334191 in 0.49s\n",
      " [-] epoch   10/250, train loss 0.353813 in 0.51s\n",
      " [-] epoch   11/250, train loss 0.351998 in 0.46s\n",
      " [-] epoch   12/250, train loss 0.370767 in 0.47s\n",
      " [-] epoch   13/250, train loss 0.340188 in 0.49s\n",
      " [-] epoch   14/250, train loss 0.344200 in 0.50s\n",
      " [-] epoch   15/250, train loss 0.329723 in 0.49s\n",
      " [-] epoch   16/250, train loss 0.341649 in 0.47s\n",
      " [-] epoch   17/250, train loss 0.294993 in 0.50s\n",
      " [-] epoch   18/250, train loss 0.351132 in 0.49s\n",
      " [-] epoch   19/250, train loss 0.331827 in 0.52s\n",
      " [-] epoch   20/250, train loss 0.325866 in 0.49s\n",
      " [-] epoch   21/250, train loss 0.324567 in 0.48s\n",
      " [-] epoch   22/250, train loss 0.311482 in 0.49s\n",
      " [-] epoch   23/250, train loss 0.296660 in 0.49s\n",
      " [-] epoch   24/250, train loss 0.338996 in 0.52s\n",
      " [-] epoch   25/250, train loss 0.298048 in 0.50s\n",
      " [-] epoch   26/250, train loss 0.313347 in 0.47s\n",
      " [-] epoch   27/250, train loss 0.321584 in 0.50s\n",
      " [-] epoch   28/250, train loss 0.306441 in 0.51s\n",
      " [-] epoch   29/250, train loss 0.286516 in 0.48s\n",
      " [-] epoch   30/250, train loss 0.303972 in 0.48s\n",
      " [-] epoch   31/250, train loss 0.285196 in 0.52s\n",
      " [-] epoch   32/250, train loss 0.304406 in 0.50s\n",
      " [-] epoch   33/250, train loss 0.302084 in 0.49s\n",
      " [-] epoch   34/250, train loss 0.308401 in 0.48s\n",
      " [-] epoch   35/250, train loss 0.302933 in 0.51s\n",
      " [-] epoch   36/250, train loss 0.301210 in 0.50s\n",
      " [-] epoch   37/250, train loss 0.304352 in 0.50s\n",
      " [-] epoch   38/250, train loss 0.282934 in 0.48s\n",
      " [-] epoch   39/250, train loss 0.283927 in 0.51s\n",
      " [-] epoch   40/250, train loss 0.283809 in 0.49s\n",
      " [-] epoch   41/250, train loss 0.301537 in 0.49s\n",
      " [-] epoch   42/250, train loss 0.296064 in 0.50s\n",
      " [-] epoch   43/250, train loss 0.290242 in 0.49s\n",
      " [-] epoch   44/250, train loss 0.283893 in 0.49s\n",
      " [-] epoch   45/250, train loss 0.286689 in 0.51s\n",
      " [-] epoch   46/250, train loss 0.305901 in 0.49s\n",
      " [-] epoch   47/250, train loss 0.282119 in 0.51s\n",
      " [-] epoch   48/250, train loss 0.279309 in 0.50s\n",
      " [-] epoch   49/250, train loss 0.274727 in 0.50s\n",
      " [-] epoch   50/250, train loss 0.273823 in 0.50s\n",
      " [-] epoch   51/250, train loss 0.301153 in 0.52s\n",
      " [-] epoch   52/250, train loss 0.306574 in 0.51s\n",
      " [-] epoch   53/250, train loss 0.283610 in 0.50s\n",
      " [-] epoch   54/250, train loss 0.248850 in 0.50s\n",
      " [-] epoch   55/250, train loss 0.256658 in 0.49s\n",
      " [-] epoch   56/250, train loss 0.250672 in 0.51s\n",
      " [-] epoch   57/250, train loss 0.286213 in 0.52s\n",
      " [-] epoch   58/250, train loss 0.258981 in 0.47s\n",
      " [-] epoch   59/250, train loss 0.251278 in 0.48s\n",
      " [-] epoch   60/250, train loss 0.258270 in 0.50s\n",
      " [-] epoch   61/250, train loss 0.281365 in 0.47s\n",
      " [-] epoch   62/250, train loss 0.258540 in 0.50s\n",
      " [-] epoch   63/250, train loss 0.289785 in 0.50s\n",
      " [-] epoch   64/250, train loss 0.274304 in 0.50s\n",
      " [-] epoch   65/250, train loss 0.256370 in 0.51s\n",
      " [-] epoch   66/250, train loss 0.242608 in 0.49s\n",
      " [-] epoch   67/250, train loss 0.248337 in 0.47s\n",
      " [-] epoch   68/250, train loss 0.262131 in 0.52s\n",
      " [-] epoch   69/250, train loss 0.264429 in 0.48s\n",
      " [-] epoch   70/250, train loss 0.253881 in 0.50s\n",
      " [-] epoch   71/250, train loss 0.241341 in 0.51s\n",
      " [-] epoch   72/250, train loss 0.259194 in 0.51s\n",
      " [-] epoch   73/250, train loss 0.244279 in 0.49s\n",
      " [-] epoch   74/250, train loss 0.271213 in 0.46s\n",
      " [-] epoch   75/250, train loss 0.246655 in 0.45s\n",
      " [-] epoch   76/250, train loss 0.239493 in 0.46s\n",
      " [-] epoch   77/250, train loss 0.213061 in 0.51s\n",
      " [-] epoch   78/250, train loss 0.235117 in 0.49s\n",
      " [-] epoch   79/250, train loss 0.229936 in 0.47s\n",
      " [-] epoch   80/250, train loss 0.272065 in 0.48s\n",
      " [-] epoch   81/250, train loss 0.240168 in 0.48s\n",
      " [-] epoch   82/250, train loss 0.237052 in 0.50s\n",
      " [-] epoch   83/250, train loss 0.261930 in 0.50s\n",
      " [-] epoch   84/250, train loss 0.249265 in 0.49s\n",
      " [-] epoch   85/250, train loss 0.239507 in 0.48s\n",
      " [-] epoch   86/250, train loss 0.257148 in 0.49s\n",
      " [-] epoch   87/250, train loss 0.257462 in 0.48s\n",
      " [-] epoch   88/250, train loss 0.226485 in 0.51s\n",
      " [-] epoch   89/250, train loss 0.225596 in 0.49s\n",
      " [-] epoch   90/250, train loss 0.231730 in 0.46s\n",
      " [-] epoch   91/250, train loss 0.263489 in 0.50s\n",
      " [-] epoch   92/250, train loss 0.240061 in 0.48s\n",
      " [-] epoch   93/250, train loss 0.235708 in 0.46s\n",
      " [-] epoch   94/250, train loss 0.265646 in 0.49s\n",
      " [-] epoch   95/250, train loss 0.226092 in 0.48s\n",
      " [-] epoch   96/250, train loss 0.255318 in 0.50s\n",
      " [-] epoch   97/250, train loss 0.259138 in 0.49s\n",
      " [-] epoch   98/250, train loss 0.224987 in 0.49s\n",
      " [-] epoch   99/250, train loss 0.224230 in 0.53s\n",
      " [-] epoch  100/250, train loss 0.238999 in 0.49s\n",
      " [-] epoch  101/250, train loss 0.245427 in 0.47s\n",
      " [-] epoch  102/250, train loss 0.234186 in 0.50s\n",
      " [-] epoch  103/250, train loss 0.241411 in 0.48s\n",
      " [-] epoch  104/250, train loss 0.242729 in 0.46s\n",
      " [-] epoch  105/250, train loss 0.253305 in 0.47s\n",
      " [-] epoch  106/250, train loss 0.241339 in 0.48s\n",
      " [-] epoch  107/250, train loss 0.251111 in 0.47s\n",
      " [-] epoch  108/250, train loss 0.226365 in 0.50s\n",
      " [-] epoch  109/250, train loss 0.224902 in 0.48s\n",
      " [-] epoch  110/250, train loss 0.225639 in 0.51s\n",
      " [-] epoch  111/250, train loss 0.232258 in 0.49s\n",
      " [-] epoch  112/250, train loss 0.211369 in 0.51s\n",
      " [-] epoch  113/250, train loss 0.232703 in 0.50s\n",
      " [-] epoch  114/250, train loss 0.246934 in 0.50s\n",
      " [-] epoch  115/250, train loss 0.252784 in 0.49s\n",
      " [-] epoch  116/250, train loss 0.232642 in 0.51s\n",
      " [-] epoch  117/250, train loss 0.215730 in 0.48s\n",
      " [-] epoch  118/250, train loss 0.224764 in 0.49s\n",
      " [-] epoch  119/250, train loss 0.238410 in 0.49s\n",
      " [-] epoch  120/250, train loss 0.234945 in 0.48s\n",
      " [-] epoch  121/250, train loss 0.236579 in 0.48s\n",
      " [-] epoch  122/250, train loss 0.217316 in 0.45s\n",
      " [-] epoch  123/250, train loss 0.238877 in 0.50s\n",
      " [-] epoch  124/250, train loss 0.250522 in 0.52s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  125/250, train loss 0.239737 in 0.51s\n",
      " [-] epoch  126/250, train loss 0.222563 in 0.49s\n",
      " [-] epoch  127/250, train loss 0.212647 in 0.48s\n",
      " [-] epoch  128/250, train loss 0.228657 in 0.49s\n",
      " [-] epoch  129/250, train loss 0.220578 in 0.47s\n",
      " [-] epoch  130/250, train loss 0.223361 in 0.47s\n",
      " [-] epoch  131/250, train loss 0.209026 in 0.46s\n",
      " [-] epoch  132/250, train loss 0.206038 in 0.47s\n",
      " [-] epoch  133/250, train loss 0.210330 in 0.49s\n",
      " [-] epoch  134/250, train loss 0.207651 in 0.50s\n",
      " [-] epoch  135/250, train loss 0.206162 in 0.50s\n",
      " [-] epoch  136/250, train loss 0.204399 in 0.49s\n",
      " [-] epoch  137/250, train loss 0.207253 in 0.49s\n",
      " [-] epoch  138/250, train loss 0.195897 in 0.47s\n",
      " [-] epoch  139/250, train loss 0.251061 in 0.46s\n",
      " [-] epoch  140/250, train loss 0.219254 in 0.48s\n",
      " [-] epoch  141/250, train loss 0.212212 in 0.48s\n",
      " [-] epoch  142/250, train loss 0.206394 in 0.47s\n",
      " [-] epoch  143/250, train loss 0.214266 in 0.49s\n",
      " [-] epoch  144/250, train loss 0.216988 in 0.49s\n",
      " [-] epoch  145/250, train loss 0.202108 in 0.49s\n",
      " [-] epoch  146/250, train loss 0.225979 in 0.46s\n",
      " [-] epoch  147/250, train loss 0.216196 in 0.50s\n",
      " [-] epoch  148/250, train loss 0.226853 in 0.49s\n",
      " [-] epoch  149/250, train loss 0.218104 in 0.48s\n",
      " [-] epoch  150/250, train loss 0.224674 in 0.47s\n",
      " [-] epoch  151/250, train loss 0.228340 in 0.48s\n",
      " [-] epoch  152/250, train loss 0.207931 in 0.47s\n",
      " [-] epoch  153/250, train loss 0.201611 in 0.51s\n",
      " [-] epoch  154/250, train loss 0.220452 in 0.50s\n",
      " [-] epoch  155/250, train loss 0.236123 in 0.49s\n",
      " [-] epoch  156/250, train loss 0.209144 in 0.50s\n",
      " [-] epoch  157/250, train loss 0.208008 in 0.49s\n",
      " [-] epoch  158/250, train loss 0.232478 in 0.48s\n",
      " [-] epoch  159/250, train loss 0.210889 in 0.46s\n",
      " [-] epoch  160/250, train loss 0.208149 in 0.45s\n",
      " [-] epoch  161/250, train loss 0.218570 in 0.49s\n",
      " [-] epoch  162/250, train loss 0.203810 in 0.48s\n",
      " [-] epoch  163/250, train loss 0.201822 in 0.48s\n",
      " [-] epoch  164/250, train loss 0.215841 in 0.47s\n",
      " [-] epoch  165/250, train loss 0.199246 in 0.48s\n",
      " [-] epoch  166/250, train loss 0.201324 in 0.49s\n",
      " [-] epoch  167/250, train loss 0.204690 in 0.49s\n",
      " [-] epoch  168/250, train loss 0.212359 in 0.50s\n",
      " [-] epoch  169/250, train loss 0.213641 in 0.48s\n",
      " [-] epoch  170/250, train loss 0.207144 in 0.48s\n",
      " [-] epoch  171/250, train loss 0.194154 in 0.48s\n",
      " [-] epoch  172/250, train loss 0.213622 in 0.50s\n",
      " [-] epoch  173/250, train loss 0.206390 in 0.47s\n",
      " [-] epoch  174/250, train loss 0.224563 in 0.47s\n",
      " [-] epoch  175/250, train loss 0.232660 in 0.48s\n",
      " [-] epoch  176/250, train loss 0.200014 in 0.48s\n",
      " [-] epoch  177/250, train loss 0.199508 in 0.49s\n",
      " [-] epoch  178/250, train loss 0.217705 in 0.47s\n",
      " [-] epoch  179/250, train loss 0.219301 in 0.48s\n",
      " [-] epoch  180/250, train loss 0.214192 in 0.45s\n",
      " [-] epoch  181/250, train loss 0.192122 in 0.48s\n",
      " [-] epoch  182/250, train loss 0.207378 in 0.49s\n",
      " [-] epoch  183/250, train loss 0.199464 in 0.49s\n",
      " [-] epoch  184/250, train loss 0.194929 in 0.48s\n",
      " [-] epoch  185/250, train loss 0.200407 in 0.47s\n",
      " [-] epoch  186/250, train loss 0.204262 in 0.48s\n",
      " [-] epoch  187/250, train loss 0.203976 in 0.47s\n",
      " [-] epoch  188/250, train loss 0.200786 in 0.47s\n",
      " [-] epoch  189/250, train loss 0.185956 in 0.47s\n",
      " [-] epoch  190/250, train loss 0.188540 in 0.47s\n",
      " [-] epoch  191/250, train loss 0.198994 in 0.50s\n",
      " [-] epoch  192/250, train loss 0.209611 in 0.48s\n",
      " [-] epoch  193/250, train loss 0.202554 in 0.48s\n",
      " [-] epoch  194/250, train loss 0.218512 in 0.49s\n",
      " [-] epoch  195/250, train loss 0.216133 in 0.49s\n",
      " [-] epoch  196/250, train loss 0.208885 in 0.50s\n",
      " [-] epoch  197/250, train loss 0.216077 in 0.46s\n",
      " [-] epoch  198/250, train loss 0.209834 in 0.47s\n",
      " [-] epoch  199/250, train loss 0.204073 in 0.47s\n",
      " [-] epoch  200/250, train loss 0.190042 in 0.48s\n",
      " [-] epoch  201/250, train loss 0.215396 in 0.50s\n",
      " [-] epoch  202/250, train loss 0.216460 in 0.49s\n",
      " [-] epoch  203/250, train loss 0.206644 in 0.48s\n",
      " [-] epoch  204/250, train loss 0.212148 in 0.49s\n",
      " [-] epoch  205/250, train loss 0.199014 in 0.47s\n",
      " [-] epoch  206/250, train loss 0.200953 in 0.48s\n",
      " [-] epoch  207/250, train loss 0.204239 in 0.49s\n",
      " [-] epoch  208/250, train loss 0.193073 in 0.49s\n",
      " [-] epoch  209/250, train loss 0.205402 in 0.47s\n",
      " [-] epoch  210/250, train loss 0.191655 in 0.49s\n",
      " [-] epoch  211/250, train loss 0.187047 in 0.46s\n",
      " [-] epoch  212/250, train loss 0.199553 in 0.49s\n",
      " [-] epoch  213/250, train loss 0.193877 in 0.50s\n",
      " [-] epoch  214/250, train loss 0.189429 in 0.47s\n",
      " [-] epoch  215/250, train loss 0.196355 in 0.49s\n",
      " [-] epoch  216/250, train loss 0.202303 in 0.47s\n",
      " [-] epoch  217/250, train loss 0.194568 in 0.49s\n",
      " [-] epoch  218/250, train loss 0.197684 in 0.49s\n",
      " [-] epoch  219/250, train loss 0.195988 in 0.47s\n",
      " [-] epoch  220/250, train loss 0.182194 in 0.48s\n",
      " [-] epoch  221/250, train loss 0.204256 in 0.49s\n",
      " [-] epoch  222/250, train loss 0.198072 in 0.47s\n",
      " [-] epoch  223/250, train loss 0.211821 in 0.51s\n",
      " [-] epoch  224/250, train loss 0.179679 in 0.50s\n",
      " [-] epoch  225/250, train loss 0.179910 in 0.49s\n",
      " [-] epoch  226/250, train loss 0.188993 in 0.49s\n",
      " [-] epoch  227/250, train loss 0.201424 in 0.49s\n",
      " [-] epoch  228/250, train loss 0.217801 in 0.48s\n",
      " [-] epoch  229/250, train loss 0.201064 in 0.47s\n",
      " [-] epoch  230/250, train loss 0.201355 in 0.46s\n",
      " [-] epoch  231/250, train loss 0.191063 in 0.46s\n",
      " [-] epoch  232/250, train loss 0.191830 in 0.48s\n",
      " [-] epoch  233/250, train loss 0.181575 in 0.48s\n",
      " [-] epoch  234/250, train loss 0.190550 in 0.49s\n",
      " [-] epoch  235/250, train loss 0.188232 in 0.51s\n",
      " [-] epoch  236/250, train loss 0.218292 in 0.49s\n",
      " [-] epoch  237/250, train loss 0.214141 in 0.50s\n",
      " [-] epoch  238/250, train loss 0.200774 in 0.49s\n",
      " [-] epoch  239/250, train loss 0.195989 in 0.50s\n",
      " [-] epoch  240/250, train loss 0.201526 in 0.48s\n",
      " [-] epoch  241/250, train loss 0.177497 in 0.49s\n",
      " [-] epoch  242/250, train loss 0.191480 in 0.44s\n",
      " [-] epoch  243/250, train loss 0.200789 in 0.48s\n",
      " [-] epoch  244/250, train loss 0.203483 in 0.47s\n",
      " [-] epoch  245/250, train loss 0.208318 in 0.46s\n",
      " [-] epoch  246/250, train loss 0.202241 in 0.49s\n",
      " [-] epoch  247/250, train loss 0.199686 in 0.47s\n",
      " [-] epoch  248/250, train loss 0.179736 in 0.49s\n",
      " [-] epoch  249/250, train loss 0.201283 in 0.50s\n",
      " [-] epoch  250/250, train loss 0.199451 in 0.51s\n",
      " [-] test acc. 81.388889%\n",
      "Je vais utiliser 8 layers\n",
      " [-] epoch    1/250, train loss 0.616213 in 0.55s\n",
      " [-] epoch    2/250, train loss 0.483413 in 0.55s\n",
      " [-] epoch    3/250, train loss 0.408916 in 0.58s\n",
      " [-] epoch    4/250, train loss 0.407015 in 0.59s\n",
      " [-] epoch    5/250, train loss 0.396657 in 0.57s\n",
      " [-] epoch    6/250, train loss 0.366113 in 0.56s\n",
      " [-] epoch    7/250, train loss 0.383451 in 0.56s\n",
      " [-] epoch    8/250, train loss 0.376036 in 0.58s\n",
      " [-] epoch    9/250, train loss 0.340128 in 0.57s\n",
      " [-] epoch   10/250, train loss 0.335437 in 0.59s\n",
      " [-] epoch   11/250, train loss 0.354910 in 0.57s\n",
      " [-] epoch   12/250, train loss 0.315041 in 0.56s\n",
      " [-] epoch   13/250, train loss 0.364426 in 0.58s\n",
      " [-] epoch   14/250, train loss 0.344044 in 0.57s\n",
      " [-] epoch   15/250, train loss 0.341144 in 0.58s\n",
      " [-] epoch   16/250, train loss 0.338323 in 0.60s\n",
      " [-] epoch   17/250, train loss 0.292721 in 0.58s\n",
      " [-] epoch   18/250, train loss 0.328544 in 0.57s\n",
      " [-] epoch   19/250, train loss 0.342875 in 0.58s\n",
      " [-] epoch   20/250, train loss 0.300572 in 0.55s\n",
      " [-] epoch   21/250, train loss 0.301667 in 0.57s\n",
      " [-] epoch   22/250, train loss 0.320062 in 0.59s\n",
      " [-] epoch   23/250, train loss 0.337105 in 0.57s\n",
      " [-] epoch   24/250, train loss 0.310677 in 0.56s\n",
      " [-] epoch   25/250, train loss 0.295013 in 0.59s\n",
      " [-] epoch   26/250, train loss 0.295951 in 0.59s\n",
      " [-] epoch   27/250, train loss 0.312645 in 0.58s\n",
      " [-] epoch   28/250, train loss 0.323268 in 0.57s\n",
      " [-] epoch   29/250, train loss 0.314625 in 0.58s\n",
      " [-] epoch   30/250, train loss 0.280283 in 0.57s\n",
      " [-] epoch   31/250, train loss 0.333539 in 0.58s\n",
      " [-] epoch   32/250, train loss 0.289253 in 0.58s\n",
      " [-] epoch   33/250, train loss 0.290666 in 0.57s\n",
      " [-] epoch   34/250, train loss 0.299020 in 0.55s\n",
      " [-] epoch   35/250, train loss 0.300239 in 0.57s\n",
      " [-] epoch   36/250, train loss 0.284712 in 0.56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   37/250, train loss 0.299779 in 0.61s\n",
      " [-] epoch   38/250, train loss 0.291516 in 0.56s\n",
      " [-] epoch   39/250, train loss 0.269598 in 0.57s\n",
      " [-] epoch   40/250, train loss 0.314528 in 0.55s\n",
      " [-] epoch   41/250, train loss 0.321698 in 0.58s\n",
      " [-] epoch   42/250, train loss 0.282191 in 0.56s\n",
      " [-] epoch   43/250, train loss 0.273541 in 0.58s\n",
      " [-] epoch   44/250, train loss 0.266670 in 0.55s\n",
      " [-] epoch   45/250, train loss 0.286236 in 0.59s\n",
      " [-] epoch   46/250, train loss 0.299021 in 0.60s\n",
      " [-] epoch   47/250, train loss 0.259978 in 0.59s\n",
      " [-] epoch   48/250, train loss 0.316587 in 0.54s\n",
      " [-] epoch   49/250, train loss 0.266233 in 0.56s\n",
      " [-] epoch   50/250, train loss 0.290170 in 0.59s\n",
      " [-] epoch   51/250, train loss 0.277379 in 0.57s\n",
      " [-] epoch   52/250, train loss 0.270742 in 0.58s\n",
      " [-] epoch   53/250, train loss 0.263643 in 0.55s\n",
      " [-] epoch   54/250, train loss 0.272798 in 0.58s\n",
      " [-] epoch   55/250, train loss 0.276323 in 0.57s\n",
      " [-] epoch   56/250, train loss 0.266972 in 0.58s\n",
      " [-] epoch   57/250, train loss 0.252322 in 0.58s\n",
      " [-] epoch   58/250, train loss 0.294091 in 0.57s\n",
      " [-] epoch   59/250, train loss 0.279456 in 0.60s\n",
      " [-] epoch   60/250, train loss 0.254924 in 0.56s\n",
      " [-] epoch   61/250, train loss 0.262692 in 0.61s\n",
      " [-] epoch   62/250, train loss 0.270726 in 0.62s\n",
      " [-] epoch   63/250, train loss 0.245592 in 0.57s\n",
      " [-] epoch   64/250, train loss 0.269077 in 0.59s\n",
      " [-] epoch   65/250, train loss 0.250774 in 0.60s\n",
      " [-] epoch   66/250, train loss 0.255796 in 0.57s\n",
      " [-] epoch   67/250, train loss 0.278130 in 0.59s\n",
      " [-] epoch   68/250, train loss 0.274250 in 0.57s\n",
      " [-] epoch   69/250, train loss 0.255594 in 0.59s\n",
      " [-] epoch   70/250, train loss 0.248773 in 0.57s\n",
      " [-] epoch   71/250, train loss 0.247502 in 0.56s\n",
      " [-] epoch   72/250, train loss 0.250067 in 0.58s\n",
      " [-] epoch   73/250, train loss 0.239165 in 0.58s\n",
      " [-] epoch   74/250, train loss 0.279295 in 0.57s\n",
      " [-] epoch   75/250, train loss 0.236805 in 0.56s\n",
      " [-] epoch   76/250, train loss 0.272102 in 0.59s\n",
      " [-] epoch   77/250, train loss 0.263479 in 0.55s\n",
      " [-] epoch   78/250, train loss 0.260750 in 0.58s\n",
      " [-] epoch   79/250, train loss 0.271498 in 0.60s\n",
      " [-] epoch   80/250, train loss 0.272093 in 0.58s\n",
      " [-] epoch   81/250, train loss 0.230526 in 0.55s\n",
      " [-] epoch   82/250, train loss 0.249283 in 0.57s\n",
      " [-] epoch   83/250, train loss 0.268851 in 0.56s\n",
      " [-] epoch   84/250, train loss 0.267701 in 0.59s\n",
      " [-] epoch   85/250, train loss 0.242505 in 0.61s\n",
      " [-] epoch   86/250, train loss 0.238967 in 0.56s\n",
      " [-] epoch   87/250, train loss 0.251073 in 0.58s\n",
      " [-] epoch   88/250, train loss 0.270773 in 0.56s\n",
      " [-] epoch   89/250, train loss 0.254323 in 0.59s\n",
      " [-] epoch   90/250, train loss 0.241496 in 0.57s\n",
      " [-] epoch   91/250, train loss 0.232942 in 0.56s\n",
      " [-] epoch   92/250, train loss 0.271167 in 0.59s\n",
      " [-] epoch   93/250, train loss 0.220678 in 0.60s\n",
      " [-] epoch   94/250, train loss 0.230052 in 0.58s\n",
      " [-] epoch   95/250, train loss 0.230887 in 0.58s\n",
      " [-] epoch   96/250, train loss 0.267939 in 0.59s\n",
      " [-] epoch   97/250, train loss 0.248116 in 0.58s\n",
      " [-] epoch   98/250, train loss 0.259179 in 0.58s\n",
      " [-] epoch   99/250, train loss 0.249922 in 0.56s\n",
      " [-] epoch  100/250, train loss 0.235528 in 0.57s\n",
      " [-] epoch  101/250, train loss 0.227025 in 0.56s\n",
      " [-] epoch  102/250, train loss 0.234813 in 0.58s\n",
      " [-] epoch  103/250, train loss 0.254326 in 0.58s\n",
      " [-] epoch  104/250, train loss 0.244222 in 0.62s\n",
      " [-] epoch  105/250, train loss 0.234947 in 0.59s\n",
      " [-] epoch  106/250, train loss 0.216742 in 0.59s\n",
      " [-] epoch  107/250, train loss 0.226034 in 0.58s\n",
      " [-] epoch  108/250, train loss 0.251198 in 0.61s\n",
      " [-] epoch  109/250, train loss 0.261218 in 0.60s\n",
      " [-] epoch  110/250, train loss 0.250654 in 0.60s\n",
      " [-] epoch  111/250, train loss 0.252785 in 0.59s\n",
      " [-] epoch  112/250, train loss 0.245189 in 0.58s\n",
      " [-] epoch  113/250, train loss 0.223775 in 0.57s\n",
      " [-] epoch  114/250, train loss 0.224828 in 0.57s\n",
      " [-] epoch  115/250, train loss 0.277755 in 0.58s\n",
      " [-] epoch  116/250, train loss 0.240072 in 0.60s\n",
      " [-] epoch  117/250, train loss 0.240581 in 0.58s\n",
      " [-] epoch  118/250, train loss 0.224391 in 0.58s\n",
      " [-] epoch  119/250, train loss 0.214005 in 0.56s\n",
      " [-] epoch  120/250, train loss 0.225255 in 0.55s\n",
      " [-] epoch  121/250, train loss 0.245933 in 0.57s\n",
      " [-] epoch  122/250, train loss 0.254401 in 0.58s\n",
      " [-] epoch  123/250, train loss 0.236581 in 0.56s\n",
      " [-] epoch  124/250, train loss 0.220929 in 0.59s\n",
      " [-] epoch  125/250, train loss 0.257823 in 0.58s\n",
      " [-] epoch  126/250, train loss 0.242898 in 0.61s\n",
      " [-] epoch  127/250, train loss 0.242448 in 0.56s\n",
      " [-] epoch  128/250, train loss 0.230927 in 0.58s\n",
      " [-] epoch  129/250, train loss 0.219465 in 0.56s\n",
      " [-] epoch  130/250, train loss 0.246433 in 0.57s\n",
      " [-] epoch  131/250, train loss 0.228713 in 0.58s\n",
      " [-] epoch  132/250, train loss 0.226956 in 0.59s\n",
      " [-] epoch  133/250, train loss 0.226488 in 0.59s\n",
      " [-] epoch  134/250, train loss 0.226709 in 0.57s\n",
      " [-] epoch  135/250, train loss 0.250415 in 0.59s\n",
      " [-] epoch  136/250, train loss 0.229784 in 0.56s\n",
      " [-] epoch  137/250, train loss 0.223749 in 0.58s\n",
      " [-] epoch  138/250, train loss 0.215539 in 0.58s\n",
      " [-] epoch  139/250, train loss 0.258058 in 0.59s\n",
      " [-] epoch  140/250, train loss 0.219164 in 0.56s\n",
      " [-] epoch  141/250, train loss 0.220361 in 0.55s\n",
      " [-] epoch  142/250, train loss 0.207597 in 0.56s\n",
      " [-] epoch  143/250, train loss 0.214276 in 0.60s\n",
      " [-] epoch  144/250, train loss 0.218826 in 0.58s\n",
      " [-] epoch  145/250, train loss 0.224957 in 0.60s\n",
      " [-] epoch  146/250, train loss 0.198736 in 0.61s\n",
      " [-] epoch  147/250, train loss 0.208145 in 0.59s\n",
      " [-] epoch  148/250, train loss 0.227875 in 1.40s\n",
      " [-] epoch  149/250, train loss 0.224663 in 0.57s\n",
      " [-] epoch  150/250, train loss 0.223615 in 0.64s\n",
      " [-] epoch  151/250, train loss 0.231538 in 0.61s\n",
      " [-] epoch  152/250, train loss 0.224113 in 0.64s\n",
      " [-] epoch  153/250, train loss 0.238493 in 0.63s\n",
      " [-] epoch  154/250, train loss 0.229699 in 0.60s\n",
      " [-] epoch  155/250, train loss 0.217996 in 0.58s\n",
      " [-] epoch  156/250, train loss 0.224237 in 0.59s\n",
      " [-] epoch  157/250, train loss 0.216829 in 0.61s\n",
      " [-] epoch  158/250, train loss 0.208842 in 0.61s\n",
      " [-] epoch  159/250, train loss 0.217542 in 0.62s\n",
      " [-] epoch  160/250, train loss 0.224661 in 0.60s\n",
      " [-] epoch  161/250, train loss 0.209661 in 0.63s\n",
      " [-] epoch  162/250, train loss 0.215434 in 0.60s\n",
      " [-] epoch  163/250, train loss 0.226331 in 0.62s\n",
      " [-] epoch  164/250, train loss 0.227830 in 0.63s\n",
      " [-] epoch  165/250, train loss 0.222442 in 0.60s\n",
      " [-] epoch  166/250, train loss 0.215356 in 0.60s\n",
      " [-] epoch  167/250, train loss 0.249449 in 0.58s\n",
      " [-] epoch  168/250, train loss 0.219558 in 0.59s\n",
      " [-] epoch  169/250, train loss 0.210925 in 0.62s\n",
      " [-] epoch  170/250, train loss 0.223666 in 0.58s\n",
      " [-] epoch  171/250, train loss 0.217438 in 0.57s\n",
      " [-] epoch  172/250, train loss 0.221340 in 0.63s\n",
      " [-] epoch  173/250, train loss 0.218945 in 0.62s\n",
      " [-] epoch  174/250, train loss 0.211500 in 0.59s\n",
      " [-] epoch  175/250, train loss 0.209791 in 0.57s\n",
      " [-] epoch  176/250, train loss 0.237698 in 0.56s\n",
      " [-] epoch  177/250, train loss 0.214638 in 0.57s\n",
      " [-] epoch  178/250, train loss 0.212401 in 0.57s\n",
      " [-] epoch  179/250, train loss 0.210982 in 0.57s\n",
      " [-] epoch  180/250, train loss 0.231963 in 0.60s\n",
      " [-] epoch  181/250, train loss 0.222586 in 0.59s\n",
      " [-] epoch  182/250, train loss 0.207480 in 0.58s\n",
      " [-] epoch  183/250, train loss 0.208328 in 0.57s\n",
      " [-] epoch  184/250, train loss 0.228910 in 0.57s\n",
      " [-] epoch  185/250, train loss 0.218615 in 0.55s\n",
      " [-] epoch  186/250, train loss 0.202587 in 0.55s\n",
      " [-] epoch  187/250, train loss 0.235842 in 0.58s\n",
      " [-] epoch  188/250, train loss 0.203604 in 0.57s\n",
      " [-] epoch  189/250, train loss 0.216416 in 0.55s\n",
      " [-] epoch  190/250, train loss 0.207159 in 0.56s\n",
      " [-] epoch  191/250, train loss 0.213399 in 0.57s\n",
      " [-] epoch  192/250, train loss 0.198932 in 0.57s\n",
      " [-] epoch  193/250, train loss 0.207996 in 0.57s\n",
      " [-] epoch  194/250, train loss 0.221641 in 0.58s\n",
      " [-] epoch  195/250, train loss 0.217739 in 0.55s\n",
      " [-] epoch  196/250, train loss 0.215844 in 0.58s\n",
      " [-] epoch  197/250, train loss 0.226283 in 0.60s\n",
      " [-] epoch  198/250, train loss 0.214398 in 0.56s\n",
      " [-] epoch  199/250, train loss 0.198907 in 0.57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  200/250, train loss 0.214673 in 0.58s\n",
      " [-] epoch  201/250, train loss 0.214800 in 0.58s\n",
      " [-] epoch  202/250, train loss 0.208075 in 0.59s\n",
      " [-] epoch  203/250, train loss 0.226570 in 0.58s\n",
      " [-] epoch  204/250, train loss 0.221069 in 0.61s\n",
      " [-] epoch  205/250, train loss 0.213942 in 0.59s\n",
      " [-] epoch  206/250, train loss 0.215976 in 0.60s\n",
      " [-] epoch  207/250, train loss 0.208525 in 0.61s\n",
      " [-] epoch  208/250, train loss 0.237495 in 0.59s\n",
      " [-] epoch  209/250, train loss 0.211913 in 0.57s\n",
      " [-] epoch  210/250, train loss 0.239292 in 0.59s\n",
      " [-] epoch  211/250, train loss 0.200678 in 0.59s\n",
      " [-] epoch  212/250, train loss 0.224072 in 0.59s\n",
      " [-] epoch  213/250, train loss 0.227767 in 0.60s\n",
      " [-] epoch  214/250, train loss 0.202910 in 0.56s\n",
      " [-] epoch  215/250, train loss 0.209129 in 0.61s\n",
      " [-] epoch  216/250, train loss 0.206841 in 0.58s\n",
      " [-] epoch  217/250, train loss 0.210480 in 0.59s\n",
      " [-] epoch  218/250, train loss 0.203098 in 0.57s\n",
      " [-] epoch  219/250, train loss 0.192173 in 0.59s\n",
      " [-] epoch  220/250, train loss 0.217930 in 0.59s\n",
      " [-] epoch  221/250, train loss 0.229103 in 0.57s\n",
      " [-] epoch  222/250, train loss 0.215128 in 0.58s\n",
      " [-] epoch  223/250, train loss 0.196934 in 0.58s\n",
      " [-] epoch  224/250, train loss 0.197197 in 0.54s\n",
      " [-] epoch  225/250, train loss 0.203478 in 0.58s\n",
      " [-] epoch  226/250, train loss 0.194193 in 0.61s\n",
      " [-] epoch  227/250, train loss 0.214508 in 0.57s\n",
      " [-] epoch  228/250, train loss 0.198409 in 0.58s\n",
      " [-] epoch  229/250, train loss 0.202089 in 0.57s\n",
      " [-] epoch  230/250, train loss 0.228223 in 0.59s\n",
      " [-] epoch  231/250, train loss 0.203344 in 0.60s\n",
      " [-] epoch  232/250, train loss 0.208016 in 0.57s\n",
      " [-] epoch  233/250, train loss 0.205473 in 0.57s\n",
      " [-] epoch  234/250, train loss 0.217558 in 0.58s\n",
      " [-] epoch  235/250, train loss 0.228977 in 0.58s\n",
      " [-] epoch  236/250, train loss 0.186410 in 0.61s\n",
      " [-] epoch  237/250, train loss 0.186610 in 0.61s\n",
      " [-] epoch  238/250, train loss 0.215958 in 0.57s\n",
      " [-] epoch  239/250, train loss 0.203208 in 0.58s\n",
      " [-] epoch  240/250, train loss 0.198448 in 0.57s\n",
      " [-] epoch  241/250, train loss 0.215309 in 0.59s\n",
      " [-] epoch  242/250, train loss 0.197670 in 0.59s\n",
      " [-] epoch  243/250, train loss 0.189917 in 0.57s\n",
      " [-] epoch  244/250, train loss 0.211341 in 0.61s\n",
      " [-] epoch  245/250, train loss 0.204141 in 0.60s\n",
      " [-] epoch  246/250, train loss 0.211105 in 0.58s\n",
      " [-] epoch  247/250, train loss 0.219167 in 0.55s\n",
      " [-] epoch  248/250, train loss 0.214459 in 0.57s\n",
      " [-] epoch  249/250, train loss 0.213074 in 0.61s\n",
      " [-] epoch  250/250, train loss 0.227424 in 0.61s\n",
      " [-] test acc. 82.777778%\n",
      "Je vais utiliser 9 layers\n",
      " [-] epoch    1/250, train loss 0.546251 in 0.64s\n",
      " [-] epoch    2/250, train loss 0.459414 in 0.66s\n",
      " [-] epoch    3/250, train loss 0.424939 in 0.65s\n",
      " [-] epoch    4/250, train loss 0.417108 in 0.63s\n",
      " [-] epoch    5/250, train loss 0.414335 in 0.67s\n",
      " [-] epoch    6/250, train loss 0.397302 in 0.66s\n",
      " [-] epoch    7/250, train loss 0.348281 in 0.65s\n",
      " [-] epoch    8/250, train loss 0.379900 in 0.65s\n",
      " [-] epoch    9/250, train loss 0.372328 in 0.65s\n",
      " [-] epoch   10/250, train loss 0.374787 in 0.66s\n",
      " [-] epoch   11/250, train loss 0.374740 in 0.66s\n",
      " [-] epoch   12/250, train loss 0.342352 in 0.65s\n",
      " [-] epoch   13/250, train loss 0.328286 in 0.64s\n",
      " [-] epoch   14/250, train loss 0.328217 in 0.67s\n",
      " [-] epoch   15/250, train loss 0.311887 in 0.66s\n",
      " [-] epoch   16/250, train loss 0.342111 in 0.63s\n",
      " [-] epoch   17/250, train loss 0.338922 in 0.67s\n",
      " [-] epoch   18/250, train loss 0.349616 in 0.64s\n",
      " [-] epoch   19/250, train loss 0.319776 in 0.64s\n",
      " [-] epoch   20/250, train loss 0.325586 in 0.64s\n",
      " [-] epoch   21/250, train loss 0.310156 in 0.65s\n",
      " [-] epoch   22/250, train loss 0.333342 in 0.65s\n",
      " [-] epoch   23/250, train loss 0.330995 in 0.65s\n",
      " [-] epoch   24/250, train loss 0.307687 in 0.65s\n",
      " [-] epoch   25/250, train loss 0.298159 in 0.65s\n",
      " [-] epoch   26/250, train loss 0.299943 in 0.65s\n",
      " [-] epoch   27/250, train loss 0.310173 in 0.65s\n",
      " [-] epoch   28/250, train loss 0.305567 in 0.65s\n",
      " [-] epoch   29/250, train loss 0.311666 in 0.63s\n",
      " [-] epoch   30/250, train loss 0.305794 in 0.66s\n",
      " [-] epoch   31/250, train loss 0.301834 in 0.65s\n",
      " [-] epoch   32/250, train loss 0.305430 in 0.64s\n",
      " [-] epoch   33/250, train loss 0.294749 in 0.63s\n",
      " [-] epoch   34/250, train loss 0.313157 in 0.64s\n",
      " [-] epoch   35/250, train loss 0.297973 in 0.64s\n",
      " [-] epoch   36/250, train loss 0.271468 in 0.65s\n",
      " [-] epoch   37/250, train loss 0.265160 in 0.66s\n",
      " [-] epoch   38/250, train loss 0.264115 in 0.64s\n",
      " [-] epoch   39/250, train loss 0.267407 in 0.63s\n",
      " [-] epoch   40/250, train loss 0.294956 in 0.64s\n",
      " [-] epoch   41/250, train loss 0.293474 in 0.63s\n",
      " [-] epoch   42/250, train loss 0.285384 in 0.64s\n",
      " [-] epoch   43/250, train loss 0.273016 in 0.62s\n",
      " [-] epoch   44/250, train loss 0.282820 in 0.65s\n",
      " [-] epoch   45/250, train loss 0.286726 in 0.65s\n",
      " [-] epoch   46/250, train loss 0.284823 in 0.65s\n",
      " [-] epoch   47/250, train loss 0.255793 in 0.62s\n",
      " [-] epoch   48/250, train loss 0.284713 in 0.65s\n",
      " [-] epoch   49/250, train loss 0.268988 in 0.66s\n",
      " [-] epoch   50/250, train loss 0.258425 in 0.63s\n",
      " [-] epoch   51/250, train loss 0.283257 in 0.65s\n",
      " [-] epoch   52/250, train loss 0.271468 in 0.64s\n",
      " [-] epoch   53/250, train loss 0.259428 in 0.67s\n",
      " [-] epoch   54/250, train loss 0.279385 in 0.65s\n",
      " [-] epoch   55/250, train loss 0.272660 in 0.66s\n",
      " [-] epoch   56/250, train loss 0.261790 in 0.66s\n",
      " [-] epoch   57/250, train loss 0.291125 in 0.69s\n",
      " [-] epoch   58/250, train loss 0.280818 in 0.68s\n",
      " [-] epoch   59/250, train loss 0.263964 in 0.66s\n",
      " [-] epoch   60/250, train loss 0.275688 in 0.67s\n",
      " [-] epoch   61/250, train loss 0.255807 in 0.67s\n",
      " [-] epoch   62/250, train loss 0.258189 in 0.63s\n",
      " [-] epoch   63/250, train loss 0.263859 in 0.65s\n",
      " [-] epoch   64/250, train loss 0.295290 in 0.68s\n",
      " [-] epoch   65/250, train loss 0.273281 in 0.66s\n",
      " [-] epoch   66/250, train loss 0.249287 in 0.66s\n",
      " [-] epoch   67/250, train loss 0.248453 in 0.66s\n",
      " [-] epoch   68/250, train loss 0.267408 in 0.66s\n",
      " [-] epoch   69/250, train loss 0.260972 in 0.64s\n",
      " [-] epoch   70/250, train loss 0.272227 in 0.69s\n",
      " [-] epoch   71/250, train loss 0.273932 in 0.67s\n",
      " [-] epoch   72/250, train loss 0.280199 in 0.66s\n",
      " [-] epoch   73/250, train loss 0.251397 in 0.64s\n",
      " [-] epoch   74/250, train loss 0.254109 in 0.66s\n",
      " [-] epoch   75/250, train loss 0.263793 in 0.67s\n",
      " [-] epoch   76/250, train loss 0.243663 in 0.63s\n",
      " [-] epoch   77/250, train loss 0.277482 in 0.67s\n",
      " [-] epoch   78/250, train loss 0.254269 in 0.66s\n",
      " [-] epoch   79/250, train loss 0.229119 in 0.67s\n",
      " [-] epoch   80/250, train loss 0.241030 in 0.62s\n",
      " [-] epoch   81/250, train loss 0.232231 in 0.64s\n",
      " [-] epoch   82/250, train loss 0.249952 in 0.68s\n",
      " [-] epoch   83/250, train loss 0.255646 in 0.68s\n",
      " [-] epoch   84/250, train loss 0.266183 in 0.65s\n",
      " [-] epoch   85/250, train loss 0.254743 in 0.65s\n",
      " [-] epoch   86/250, train loss 0.261809 in 0.64s\n",
      " [-] epoch   87/250, train loss 0.233894 in 0.63s\n",
      " [-] epoch   88/250, train loss 0.234176 in 0.66s\n",
      " [-] epoch   89/250, train loss 0.234443 in 0.66s\n",
      " [-] epoch   90/250, train loss 0.249661 in 0.65s\n",
      " [-] epoch   91/250, train loss 0.248524 in 0.65s\n",
      " [-] epoch   92/250, train loss 0.214799 in 0.65s\n",
      " [-] epoch   93/250, train loss 0.246609 in 0.68s\n",
      " [-] epoch   94/250, train loss 0.235516 in 0.68s\n",
      " [-] epoch   95/250, train loss 0.245852 in 0.66s\n",
      " [-] epoch   96/250, train loss 0.239470 in 0.64s\n",
      " [-] epoch   97/250, train loss 0.242883 in 0.68s\n",
      " [-] epoch   98/250, train loss 0.248428 in 0.68s\n",
      " [-] epoch   99/250, train loss 0.239207 in 0.65s\n",
      " [-] epoch  100/250, train loss 0.236659 in 0.64s\n",
      " [-] epoch  101/250, train loss 0.235656 in 0.66s\n",
      " [-] epoch  102/250, train loss 0.234036 in 0.65s\n",
      " [-] epoch  103/250, train loss 0.225030 in 0.65s\n",
      " [-] epoch  104/250, train loss 0.222554 in 0.63s\n",
      " [-] epoch  105/250, train loss 0.238939 in 0.62s\n",
      " [-] epoch  106/250, train loss 0.270166 in 0.67s\n",
      " [-] epoch  107/250, train loss 0.250537 in 0.66s\n",
      " [-] epoch  108/250, train loss 0.247351 in 0.68s\n",
      " [-] epoch  109/250, train loss 0.226334 in 0.68s\n",
      " [-] epoch  110/250, train loss 0.225956 in 0.67s\n",
      " [-] epoch  111/250, train loss 0.208791 in 0.65s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  112/250, train loss 0.219947 in 0.67s\n",
      " [-] epoch  113/250, train loss 0.222286 in 0.63s\n",
      " [-] epoch  114/250, train loss 0.217177 in 0.64s\n",
      " [-] epoch  115/250, train loss 0.250263 in 0.66s\n",
      " [-] epoch  116/250, train loss 0.261881 in 0.65s\n",
      " [-] epoch  117/250, train loss 0.244741 in 0.65s\n",
      " [-] epoch  118/250, train loss 0.227767 in 0.64s\n",
      " [-] epoch  119/250, train loss 0.229019 in 0.65s\n",
      " [-] epoch  120/250, train loss 0.229182 in 0.66s\n",
      " [-] epoch  121/250, train loss 0.221093 in 0.64s\n",
      " [-] epoch  122/250, train loss 0.212851 in 0.68s\n",
      " [-] epoch  123/250, train loss 0.224749 in 0.66s\n",
      " [-] epoch  124/250, train loss 0.218725 in 0.66s\n",
      " [-] epoch  125/250, train loss 0.219165 in 0.66s\n",
      " [-] epoch  126/250, train loss 0.222087 in 0.63s\n",
      " [-] epoch  127/250, train loss 0.242980 in 0.66s\n",
      " [-] epoch  128/250, train loss 0.236165 in 0.65s\n",
      " [-] epoch  129/250, train loss 0.254408 in 0.67s\n",
      " [-] epoch  130/250, train loss 0.221239 in 0.65s\n",
      " [-] epoch  131/250, train loss 0.244963 in 0.66s\n",
      " [-] epoch  132/250, train loss 0.225788 in 0.66s\n",
      " [-] epoch  133/250, train loss 0.254189 in 0.67s\n",
      " [-] epoch  134/250, train loss 0.221333 in 0.65s\n",
      " [-] epoch  135/250, train loss 0.223956 in 0.67s\n",
      " [-] epoch  136/250, train loss 0.214772 in 0.67s\n",
      " [-] epoch  137/250, train loss 0.218113 in 0.65s\n",
      " [-] epoch  138/250, train loss 0.212121 in 0.67s\n",
      " [-] epoch  139/250, train loss 0.218068 in 0.65s\n",
      " [-] epoch  140/250, train loss 0.229265 in 0.65s\n",
      " [-] epoch  141/250, train loss 0.227914 in 0.61s\n",
      " [-] epoch  142/250, train loss 0.214215 in 0.67s\n",
      " [-] epoch  143/250, train loss 0.217437 in 0.67s\n",
      " [-] epoch  144/250, train loss 0.205202 in 0.65s\n",
      " [-] epoch  145/250, train loss 0.216867 in 0.63s\n",
      " [-] epoch  146/250, train loss 0.207274 in 0.64s\n",
      " [-] epoch  147/250, train loss 0.218029 in 0.66s\n",
      " [-] epoch  148/250, train loss 0.234817 in 0.64s\n",
      " [-] epoch  149/250, train loss 0.228869 in 0.67s\n",
      " [-] epoch  150/250, train loss 0.217003 in 0.65s\n",
      " [-] epoch  151/250, train loss 0.225089 in 0.63s\n",
      " [-] epoch  152/250, train loss 0.192140 in 0.64s\n",
      " [-] epoch  153/250, train loss 0.242755 in 0.66s\n",
      " [-] epoch  154/250, train loss 0.230665 in 0.68s\n",
      " [-] epoch  155/250, train loss 0.209902 in 0.64s\n",
      " [-] epoch  156/250, train loss 0.223983 in 0.66s\n",
      " [-] epoch  157/250, train loss 0.221927 in 0.66s\n",
      " [-] epoch  158/250, train loss 0.211966 in 0.66s\n",
      " [-] epoch  159/250, train loss 0.229507 in 0.66s\n",
      " [-] epoch  160/250, train loss 0.230010 in 0.66s\n",
      " [-] epoch  161/250, train loss 0.221764 in 0.67s\n",
      " [-] epoch  162/250, train loss 0.238751 in 0.66s\n",
      " [-] epoch  163/250, train loss 0.214824 in 0.67s\n",
      " [-] epoch  164/250, train loss 0.224800 in 0.68s\n",
      " [-] epoch  165/250, train loss 0.210375 in 0.67s\n",
      " [-] epoch  166/250, train loss 0.212587 in 0.66s\n",
      " [-] epoch  167/250, train loss 0.221106 in 0.65s\n",
      " [-] epoch  168/250, train loss 0.199001 in 0.66s\n",
      " [-] epoch  169/250, train loss 0.206457 in 0.69s\n",
      " [-] epoch  170/250, train loss 0.192192 in 0.65s\n",
      " [-] epoch  171/250, train loss 0.212904 in 0.65s\n",
      " [-] epoch  172/250, train loss 0.204371 in 0.66s\n",
      " [-] epoch  173/250, train loss 0.222085 in 0.64s\n",
      " [-] epoch  174/250, train loss 0.216415 in 0.66s\n",
      " [-] epoch  175/250, train loss 0.206479 in 0.65s\n",
      " [-] epoch  176/250, train loss 0.208424 in 0.65s\n",
      " [-] epoch  177/250, train loss 0.202903 in 0.66s\n",
      " [-] epoch  178/250, train loss 0.216646 in 0.66s\n",
      " [-] epoch  179/250, train loss 0.205825 in 0.65s\n",
      " [-] epoch  180/250, train loss 0.229041 in 0.67s\n",
      " [-] epoch  181/250, train loss 0.193595 in 0.63s\n",
      " [-] epoch  182/250, train loss 0.211190 in 0.61s\n",
      " [-] epoch  183/250, train loss 0.234175 in 0.65s\n",
      " [-] epoch  184/250, train loss 0.203041 in 0.64s\n",
      " [-] epoch  185/250, train loss 0.207518 in 0.63s\n",
      " [-] epoch  186/250, train loss 0.205973 in 0.65s\n",
      " [-] epoch  187/250, train loss 0.210523 in 0.64s\n",
      " [-] epoch  188/250, train loss 0.191772 in 0.63s\n",
      " [-] epoch  189/250, train loss 0.202828 in 0.65s\n",
      " [-] epoch  190/250, train loss 0.188556 in 0.66s\n",
      " [-] epoch  191/250, train loss 0.204347 in 0.64s\n",
      " [-] epoch  192/250, train loss 0.203378 in 0.66s\n",
      " [-] epoch  193/250, train loss 0.198037 in 0.67s\n",
      " [-] epoch  194/250, train loss 0.212428 in 0.70s\n",
      " [-] epoch  195/250, train loss 0.214295 in 0.71s\n",
      " [-] epoch  196/250, train loss 0.208756 in 0.67s\n",
      " [-] epoch  197/250, train loss 0.200137 in 0.67s\n",
      " [-] epoch  198/250, train loss 0.229618 in 0.65s\n",
      " [-] epoch  199/250, train loss 0.207485 in 0.67s\n",
      " [-] epoch  200/250, train loss 0.229762 in 0.66s\n",
      " [-] epoch  201/250, train loss 0.199144 in 0.68s\n",
      " [-] epoch  202/250, train loss 0.221109 in 0.65s\n",
      " [-] epoch  203/250, train loss 0.224342 in 0.63s\n",
      " [-] epoch  204/250, train loss 0.198324 in 0.64s\n",
      " [-] epoch  205/250, train loss 0.202090 in 0.63s\n",
      " [-] epoch  206/250, train loss 0.209954 in 0.61s\n",
      " [-] epoch  207/250, train loss 0.219596 in 0.66s\n",
      " [-] epoch  208/250, train loss 0.207544 in 0.69s\n",
      " [-] epoch  209/250, train loss 0.211461 in 0.68s\n",
      " [-] epoch  210/250, train loss 0.204275 in 0.65s\n",
      " [-] epoch  211/250, train loss 0.203228 in 0.62s\n",
      " [-] epoch  212/250, train loss 0.203832 in 0.66s\n",
      " [-] epoch  213/250, train loss 0.191327 in 0.65s\n",
      " [-] epoch  214/250, train loss 0.194773 in 0.64s\n",
      " [-] epoch  215/250, train loss 0.185193 in 0.63s\n",
      " [-] epoch  216/250, train loss 0.204839 in 0.63s\n",
      " [-] epoch  217/250, train loss 0.216171 in 0.63s\n",
      " [-] epoch  218/250, train loss 0.223021 in 0.62s\n",
      " [-] epoch  219/250, train loss 0.207417 in 0.66s\n",
      " [-] epoch  220/250, train loss 0.196738 in 0.65s\n",
      " [-] epoch  221/250, train loss 0.201172 in 0.65s\n",
      " [-] epoch  222/250, train loss 0.210403 in 0.65s\n",
      " [-] epoch  223/250, train loss 0.201821 in 0.64s\n",
      " [-] epoch  224/250, train loss 0.200721 in 0.65s\n",
      " [-] epoch  225/250, train loss 0.212590 in 0.63s\n",
      " [-] epoch  226/250, train loss 0.215002 in 0.67s\n",
      " [-] epoch  227/250, train loss 0.210116 in 0.63s\n",
      " [-] epoch  228/250, train loss 0.215797 in 0.64s\n",
      " [-] epoch  229/250, train loss 0.211706 in 0.66s\n",
      " [-] epoch  230/250, train loss 0.205333 in 0.65s\n",
      " [-] epoch  231/250, train loss 0.229323 in 0.65s\n",
      " [-] epoch  232/250, train loss 0.191890 in 0.64s\n",
      " [-] epoch  233/250, train loss 0.204198 in 0.64s\n",
      " [-] epoch  234/250, train loss 0.193622 in 0.65s\n",
      " [-] epoch  235/250, train loss 0.202900 in 0.64s\n",
      " [-] epoch  236/250, train loss 0.203300 in 0.64s\n",
      " [-] epoch  237/250, train loss 0.226142 in 0.64s\n",
      " [-] epoch  238/250, train loss 0.211688 in 0.66s\n",
      " [-] epoch  239/250, train loss 0.202726 in 0.66s\n",
      " [-] epoch  240/250, train loss 0.199189 in 0.67s\n",
      " [-] epoch  241/250, train loss 0.193989 in 0.66s\n",
      " [-] epoch  242/250, train loss 0.208169 in 0.64s\n",
      " [-] epoch  243/250, train loss 0.208205 in 0.67s\n",
      " [-] epoch  244/250, train loss 0.197655 in 0.67s\n",
      " [-] epoch  245/250, train loss 0.215348 in 0.67s\n",
      " [-] epoch  246/250, train loss 0.215167 in 0.66s\n",
      " [-] epoch  247/250, train loss 0.214619 in 0.65s\n",
      " [-] epoch  248/250, train loss 0.192809 in 0.65s\n",
      " [-] epoch  249/250, train loss 0.195971 in 0.65s\n",
      " [-] epoch  250/250, train loss 0.191923 in 0.67s\n",
      " [-] test acc. 84.166667%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_linear_results(X_train, X_test) # all dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calcul du taux de réussite en classement d'un réseau linéaire multicouches sur les données réduites\n",
    "Cette deuxième étape consiste à avoir les résultats sur les dimensions réduites engendrées par la réduction de dimensionnalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.699588 in 0.97s\n",
      " [-] epoch    2/250, train loss 0.693963 in 0.02s\n",
      " [-] epoch    3/250, train loss 0.689958 in 0.02s\n",
      " [-] epoch    4/250, train loss 0.684737 in 0.02s\n",
      " [-] epoch    5/250, train loss 0.681853 in 0.02s\n",
      " [-] epoch    6/250, train loss 0.681242 in 0.02s\n",
      " [-] epoch    7/250, train loss 0.682814 in 0.02s\n",
      " [-] epoch    8/250, train loss 0.678852 in 0.02s\n",
      " [-] epoch    9/250, train loss 0.677086 in 0.02s\n",
      " [-] epoch   10/250, train loss 0.679487 in 0.02s\n",
      " [-] epoch   11/250, train loss 0.679945 in 0.02s\n",
      " [-] epoch   12/250, train loss 0.681844 in 0.02s\n",
      " [-] epoch   13/250, train loss 0.678720 in 0.02s\n",
      " [-] epoch   14/250, train loss 0.677489 in 0.02s\n",
      " [-] epoch   15/250, train loss 0.681580 in 0.02s\n",
      " [-] epoch   16/250, train loss 0.679560 in 0.02s\n",
      " [-] epoch   17/250, train loss 0.677039 in 0.02s\n",
      " [-] epoch   18/250, train loss 0.685467 in 0.02s\n",
      " [-] epoch   19/250, train loss 0.680034 in 0.02s\n",
      " [-] epoch   20/250, train loss 0.675869 in 0.02s\n",
      " [-] epoch   21/250, train loss 0.685485 in 0.02s\n",
      " [-] epoch   22/250, train loss 0.679358 in 0.02s\n",
      " [-] epoch   23/250, train loss 0.677735 in 0.02s\n",
      " [-] epoch   24/250, train loss 0.679651 in 0.02s\n",
      " [-] epoch   25/250, train loss 0.672353 in 0.02s\n",
      " [-] epoch   26/250, train loss 0.676303 in 0.02s\n",
      " [-] epoch   27/250, train loss 0.677538 in 0.02s\n",
      " [-] epoch   28/250, train loss 0.672612 in 0.02s\n",
      " [-] epoch   29/250, train loss 0.678515 in 0.02s\n",
      " [-] epoch   30/250, train loss 0.665456 in 0.02s\n",
      " [-] epoch   31/250, train loss 0.663631 in 0.02s\n",
      " [-] epoch   32/250, train loss 0.673436 in 0.02s\n",
      " [-] epoch   33/250, train loss 0.671643 in 0.02s\n",
      " [-] epoch   34/250, train loss 0.678637 in 0.02s\n",
      " [-] epoch   35/250, train loss 0.672781 in 0.02s\n",
      " [-] epoch   36/250, train loss 0.672411 in 0.02s\n",
      " [-] epoch   37/250, train loss 0.671066 in 0.02s\n",
      " [-] epoch   38/250, train loss 0.670206 in 0.02s\n",
      " [-] epoch   39/250, train loss 0.673996 in 0.02s\n",
      " [-] epoch   40/250, train loss 0.673108 in 0.02s\n",
      " [-] epoch   41/250, train loss 0.670647 in 0.02s\n",
      " [-] epoch   42/250, train loss 0.669771 in 0.02s\n",
      " [-] epoch   43/250, train loss 0.675833 in 0.02s\n",
      " [-] epoch   44/250, train loss 0.665244 in 0.02s\n",
      " [-] epoch   45/250, train loss 0.672683 in 0.02s\n",
      " [-] epoch   46/250, train loss 0.671811 in 0.02s\n",
      " [-] epoch   47/250, train loss 0.669659 in 0.02s\n",
      " [-] epoch   48/250, train loss 0.670346 in 0.02s\n",
      " [-] epoch   49/250, train loss 0.669046 in 0.02s\n",
      " [-] epoch   50/250, train loss 0.665259 in 0.02s\n",
      " [-] epoch   51/250, train loss 0.665125 in 0.02s\n",
      " [-] epoch   52/250, train loss 0.675700 in 0.02s\n",
      " [-] epoch   53/250, train loss 0.661415 in 0.02s\n",
      " [-] epoch   54/250, train loss 0.667589 in 0.02s\n",
      " [-] epoch   55/250, train loss 0.676741 in 0.02s\n",
      " [-] epoch   56/250, train loss 0.667192 in 0.02s\n",
      " [-] epoch   57/250, train loss 0.669227 in 0.02s\n",
      " [-] epoch   58/250, train loss 0.660046 in 0.02s\n",
      " [-] epoch   59/250, train loss 0.663682 in 0.02s\n",
      " [-] epoch   60/250, train loss 0.666613 in 0.02s\n",
      " [-] epoch   61/250, train loss 0.665921 in 0.02s\n",
      " [-] epoch   62/250, train loss 0.667267 in 0.02s\n",
      " [-] epoch   63/250, train loss 0.663134 in 0.02s\n",
      " [-] epoch   64/250, train loss 0.669858 in 0.02s\n",
      " [-] epoch   65/250, train loss 0.666912 in 0.02s\n",
      " [-] epoch   66/250, train loss 0.669709 in 0.02s\n",
      " [-] epoch   67/250, train loss 0.668469 in 0.02s\n",
      " [-] epoch   68/250, train loss 0.666852 in 0.02s\n",
      " [-] epoch   69/250, train loss 0.665506 in 0.02s\n",
      " [-] epoch   70/250, train loss 0.658884 in 0.02s\n",
      " [-] epoch   71/250, train loss 0.668166 in 0.02s\n",
      " [-] epoch   72/250, train loss 0.657939 in 0.02s\n",
      " [-] epoch   73/250, train loss 0.661576 in 0.02s\n",
      " [-] epoch   74/250, train loss 0.659035 in 0.02s\n",
      " [-] epoch   75/250, train loss 0.651344 in 0.02s\n",
      " [-] epoch   76/250, train loss 0.668308 in 0.02s\n",
      " [-] epoch   77/250, train loss 0.664858 in 0.02s\n",
      " [-] epoch   78/250, train loss 0.651235 in 0.02s\n",
      " [-] epoch   79/250, train loss 0.661077 in 0.02s\n",
      " [-] epoch   80/250, train loss 0.659385 in 0.02s\n",
      " [-] epoch   81/250, train loss 0.670998 in 0.02s\n",
      " [-] epoch   82/250, train loss 0.660709 in 0.02s\n",
      " [-] epoch   83/250, train loss 0.658327 in 0.02s\n",
      " [-] epoch   84/250, train loss 0.656420 in 0.02s\n",
      " [-] epoch   85/250, train loss 0.655242 in 0.02s\n",
      " [-] epoch   86/250, train loss 0.650643 in 0.02s\n",
      " [-] epoch   87/250, train loss 0.657972 in 0.02s\n",
      " [-] epoch   88/250, train loss 0.654668 in 0.02s\n",
      " [-] epoch   89/250, train loss 0.653118 in 0.02s\n",
      " [-] epoch   90/250, train loss 0.658504 in 0.02s\n",
      " [-] epoch   91/250, train loss 0.664027 in 0.02s\n",
      " [-] epoch   92/250, train loss 0.656517 in 0.02s\n",
      " [-] epoch   93/250, train loss 0.656537 in 0.02s\n",
      " [-] epoch   94/250, train loss 0.645823 in 0.02s\n",
      " [-] epoch   95/250, train loss 0.658749 in 0.02s\n",
      " [-] epoch   96/250, train loss 0.645611 in 0.02s\n",
      " [-] epoch   97/250, train loss 0.652266 in 0.02s\n",
      " [-] epoch   98/250, train loss 0.662395 in 0.02s\n",
      " [-] epoch   99/250, train loss 0.664863 in 0.02s\n",
      " [-] epoch  100/250, train loss 0.657126 in 0.02s\n",
      " [-] epoch  101/250, train loss 0.654702 in 0.02s\n",
      " [-] epoch  102/250, train loss 0.658366 in 0.02s\n",
      " [-] epoch  103/250, train loss 0.658009 in 0.02s\n",
      " [-] epoch  104/250, train loss 0.653981 in 0.02s\n",
      " [-] epoch  105/250, train loss 0.655100 in 0.02s\n",
      " [-] epoch  106/250, train loss 0.657087 in 0.02s\n",
      " [-] epoch  107/250, train loss 0.662395 in 0.02s\n",
      " [-] epoch  108/250, train loss 0.647951 in 0.02s\n",
      " [-] epoch  109/250, train loss 0.657608 in 0.02s\n",
      " [-] epoch  110/250, train loss 0.653179 in 0.02s\n",
      " [-] epoch  111/250, train loss 0.654789 in 0.02s\n",
      " [-] epoch  112/250, train loss 0.652602 in 0.02s\n",
      " [-] epoch  113/250, train loss 0.658678 in 0.02s\n",
      " [-] epoch  114/250, train loss 0.652865 in 0.02s\n",
      " [-] epoch  115/250, train loss 0.648326 in 0.02s\n",
      " [-] epoch  116/250, train loss 0.649428 in 0.02s\n",
      " [-] epoch  117/250, train loss 0.654780 in 0.02s\n",
      " [-] epoch  118/250, train loss 0.641783 in 0.02s\n",
      " [-] epoch  119/250, train loss 0.660111 in 0.02s\n",
      " [-] epoch  120/250, train loss 0.656927 in 0.02s\n",
      " [-] epoch  121/250, train loss 0.649308 in 0.02s\n",
      " [-] epoch  122/250, train loss 0.651783 in 0.02s\n",
      " [-] epoch  123/250, train loss 0.657108 in 0.02s\n",
      " [-] epoch  124/250, train loss 0.644682 in 0.02s\n",
      " [-] epoch  125/250, train loss 0.654156 in 0.02s\n",
      " [-] epoch  126/250, train loss 0.654363 in 0.02s\n",
      " [-] epoch  127/250, train loss 0.660839 in 0.02s\n",
      " [-] epoch  128/250, train loss 0.652508 in 0.02s\n",
      " [-] epoch  129/250, train loss 0.654773 in 0.02s\n",
      " [-] epoch  130/250, train loss 0.651677 in 0.02s\n",
      " [-] epoch  131/250, train loss 0.648245 in 0.02s\n",
      " [-] epoch  132/250, train loss 0.653213 in 0.02s\n",
      " [-] epoch  133/250, train loss 0.646720 in 0.02s\n",
      " [-] epoch  134/250, train loss 0.654284 in 0.02s\n",
      " [-] epoch  135/250, train loss 0.656253 in 0.02s\n",
      " [-] epoch  136/250, train loss 0.646377 in 0.02s\n",
      " [-] epoch  137/250, train loss 0.649375 in 0.02s\n",
      " [-] epoch  138/250, train loss 0.656028 in 0.02s\n",
      " [-] epoch  139/250, train loss 0.650967 in 0.02s\n",
      " [-] epoch  140/250, train loss 0.649275 in 0.02s\n",
      " [-] epoch  141/250, train loss 0.649081 in 0.02s\n",
      " [-] epoch  142/250, train loss 0.646504 in 0.02s\n",
      " [-] epoch  143/250, train loss 0.651310 in 0.02s\n",
      " [-] epoch  144/250, train loss 0.652369 in 0.02s\n",
      " [-] epoch  145/250, train loss 0.645531 in 0.02s\n",
      " [-] epoch  146/250, train loss 0.649054 in 0.02s\n",
      " [-] epoch  147/250, train loss 0.640628 in 0.02s\n",
      " [-] epoch  148/250, train loss 0.649229 in 0.02s\n",
      " [-] epoch  149/250, train loss 0.646858 in 0.02s\n",
      " [-] epoch  150/250, train loss 0.646067 in 0.02s\n",
      " [-] epoch  151/250, train loss 0.651005 in 0.02s\n",
      " [-] epoch  152/250, train loss 0.652840 in 0.02s\n",
      " [-] epoch  153/250, train loss 0.643093 in 0.02s\n",
      " [-] epoch  154/250, train loss 0.645571 in 0.02s\n",
      " [-] epoch  155/250, train loss 0.649943 in 0.02s\n",
      " [-] epoch  156/250, train loss 0.646809 in 0.02s\n",
      " [-] epoch  157/250, train loss 0.650506 in 0.02s\n",
      " [-] epoch  158/250, train loss 0.646046 in 0.02s\n",
      " [-] epoch  159/250, train loss 0.640070 in 0.02s\n",
      " [-] epoch  160/250, train loss 0.642362 in 0.02s\n",
      " [-] epoch  161/250, train loss 0.639280 in 0.02s\n",
      " [-] epoch  162/250, train loss 0.648078 in 0.02s\n",
      " [-] epoch  163/250, train loss 0.655825 in 0.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  164/250, train loss 0.648496 in 0.02s\n",
      " [-] epoch  165/250, train loss 0.644844 in 0.02s\n",
      " [-] epoch  166/250, train loss 0.652542 in 0.02s\n",
      " [-] epoch  167/250, train loss 0.641290 in 0.02s\n",
      " [-] epoch  168/250, train loss 0.648040 in 0.02s\n",
      " [-] epoch  169/250, train loss 0.650045 in 0.02s\n",
      " [-] epoch  170/250, train loss 0.641886 in 0.02s\n",
      " [-] epoch  171/250, train loss 0.637924 in 0.02s\n",
      " [-] epoch  172/250, train loss 0.653387 in 0.02s\n",
      " [-] epoch  173/250, train loss 0.646076 in 0.02s\n",
      " [-] epoch  174/250, train loss 0.643927 in 0.02s\n",
      " [-] epoch  175/250, train loss 0.648954 in 0.02s\n",
      " [-] epoch  176/250, train loss 0.653178 in 0.02s\n",
      " [-] epoch  177/250, train loss 0.651946 in 0.02s\n",
      " [-] epoch  178/250, train loss 0.633357 in 0.02s\n",
      " [-] epoch  179/250, train loss 0.652615 in 0.02s\n",
      " [-] epoch  180/250, train loss 0.654714 in 0.02s\n",
      " [-] epoch  181/250, train loss 0.646417 in 0.02s\n",
      " [-] epoch  182/250, train loss 0.634121 in 0.02s\n",
      " [-] epoch  183/250, train loss 0.636042 in 0.02s\n",
      " [-] epoch  184/250, train loss 0.651243 in 0.02s\n",
      " [-] epoch  185/250, train loss 0.641130 in 0.02s\n",
      " [-] epoch  186/250, train loss 0.648575 in 0.02s\n",
      " [-] epoch  187/250, train loss 0.635832 in 0.02s\n",
      " [-] epoch  188/250, train loss 0.647475 in 0.02s\n",
      " [-] epoch  189/250, train loss 0.643141 in 0.02s\n",
      " [-] epoch  190/250, train loss 0.646147 in 0.02s\n",
      " [-] epoch  191/250, train loss 0.637265 in 0.02s\n",
      " [-] epoch  192/250, train loss 0.645433 in 0.02s\n",
      " [-] epoch  193/250, train loss 0.653633 in 0.02s\n",
      " [-] epoch  194/250, train loss 0.651426 in 0.02s\n",
      " [-] epoch  195/250, train loss 0.646858 in 0.02s\n",
      " [-] epoch  196/250, train loss 0.632239 in 0.02s\n",
      " [-] epoch  197/250, train loss 0.645501 in 0.02s\n",
      " [-] epoch  198/250, train loss 0.650700 in 0.02s\n",
      " [-] epoch  199/250, train loss 0.643475 in 0.02s\n",
      " [-] epoch  200/250, train loss 0.644851 in 0.02s\n",
      " [-] epoch  201/250, train loss 0.628946 in 0.02s\n",
      " [-] epoch  202/250, train loss 0.650286 in 0.02s\n",
      " [-] epoch  203/250, train loss 0.649161 in 0.02s\n",
      " [-] epoch  204/250, train loss 0.637415 in 0.02s\n",
      " [-] epoch  205/250, train loss 0.646326 in 0.02s\n",
      " [-] epoch  206/250, train loss 0.642844 in 0.02s\n",
      " [-] epoch  207/250, train loss 0.645194 in 0.02s\n",
      " [-] epoch  208/250, train loss 0.644228 in 0.02s\n",
      " [-] epoch  209/250, train loss 0.642798 in 0.02s\n",
      " [-] epoch  210/250, train loss 0.631459 in 0.02s\n",
      " [-] epoch  211/250, train loss 0.644154 in 0.02s\n",
      " [-] epoch  212/250, train loss 0.640399 in 0.02s\n",
      " [-] epoch  213/250, train loss 0.653184 in 0.02s\n",
      " [-] epoch  214/250, train loss 0.638333 in 0.02s\n",
      " [-] epoch  215/250, train loss 0.628680 in 0.02s\n",
      " [-] epoch  216/250, train loss 0.641000 in 0.02s\n",
      " [-] epoch  217/250, train loss 0.639546 in 0.02s\n",
      " [-] epoch  218/250, train loss 0.641364 in 0.02s\n",
      " [-] epoch  219/250, train loss 0.636693 in 0.02s\n",
      " [-] epoch  220/250, train loss 0.630186 in 0.02s\n",
      " [-] epoch  221/250, train loss 0.641438 in 0.02s\n",
      " [-] epoch  222/250, train loss 0.640977 in 0.02s\n",
      " [-] epoch  223/250, train loss 0.635274 in 0.02s\n",
      " [-] epoch  224/250, train loss 0.638060 in 0.02s\n",
      " [-] epoch  225/250, train loss 0.636707 in 0.02s\n",
      " [-] epoch  226/250, train loss 0.635686 in 0.02s\n",
      " [-] epoch  227/250, train loss 0.626355 in 0.02s\n",
      " [-] epoch  228/250, train loss 0.629754 in 0.02s\n",
      " [-] epoch  229/250, train loss 0.646850 in 0.02s\n",
      " [-] epoch  230/250, train loss 0.631385 in 0.02s\n",
      " [-] epoch  231/250, train loss 0.633802 in 0.02s\n",
      " [-] epoch  232/250, train loss 0.638131 in 0.02s\n",
      " [-] epoch  233/250, train loss 0.647182 in 0.02s\n",
      " [-] epoch  234/250, train loss 0.631312 in 0.02s\n",
      " [-] epoch  235/250, train loss 0.627334 in 0.02s\n",
      " [-] epoch  236/250, train loss 0.632976 in 0.02s\n",
      " [-] epoch  237/250, train loss 0.642627 in 0.02s\n",
      " [-] epoch  238/250, train loss 0.628327 in 0.02s\n",
      " [-] epoch  239/250, train loss 0.631020 in 0.02s\n",
      " [-] epoch  240/250, train loss 0.634799 in 0.02s\n",
      " [-] epoch  241/250, train loss 0.632128 in 0.02s\n",
      " [-] epoch  242/250, train loss 0.633481 in 0.02s\n",
      " [-] epoch  243/250, train loss 0.626312 in 0.02s\n",
      " [-] epoch  244/250, train loss 0.637531 in 0.02s\n",
      " [-] epoch  245/250, train loss 0.643025 in 0.02s\n",
      " [-] epoch  246/250, train loss 0.628353 in 0.02s\n",
      " [-] epoch  247/250, train loss 0.645016 in 0.02s\n",
      " [-] epoch  248/250, train loss 0.621361 in 0.02s\n",
      " [-] epoch  249/250, train loss 0.634236 in 0.02s\n",
      " [-] epoch  250/250, train loss 0.641663 in 0.02s\n",
      " [-] test acc. 61.666667%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.701555 in 0.06s\n",
      " [-] epoch    2/250, train loss 0.675066 in 0.06s\n",
      " [-] epoch    3/250, train loss 0.667329 in 0.06s\n",
      " [-] epoch    4/250, train loss 0.634092 in 0.05s\n",
      " [-] epoch    5/250, train loss 0.625002 in 0.05s\n",
      " [-] epoch    6/250, train loss 0.636553 in 0.06s\n",
      " [-] epoch    7/250, train loss 0.647424 in 0.05s\n",
      " [-] epoch    8/250, train loss 0.625319 in 0.05s\n",
      " [-] epoch    9/250, train loss 0.599645 in 0.05s\n",
      " [-] epoch   10/250, train loss 0.609011 in 0.05s\n",
      " [-] epoch   11/250, train loss 0.632850 in 0.05s\n",
      " [-] epoch   12/250, train loss 0.626731 in 0.05s\n",
      " [-] epoch   13/250, train loss 0.606335 in 0.06s\n",
      " [-] epoch   14/250, train loss 0.600003 in 0.06s\n",
      " [-] epoch   15/250, train loss 0.614854 in 0.05s\n",
      " [-] epoch   16/250, train loss 0.624906 in 0.05s\n",
      " [-] epoch   17/250, train loss 0.619051 in 0.05s\n",
      " [-] epoch   18/250, train loss 0.594875 in 0.05s\n",
      " [-] epoch   19/250, train loss 0.608418 in 0.06s\n",
      " [-] epoch   20/250, train loss 0.622206 in 0.06s\n",
      " [-] epoch   21/250, train loss 0.612012 in 0.05s\n",
      " [-] epoch   22/250, train loss 0.583658 in 0.06s\n",
      " [-] epoch   23/250, train loss 0.593144 in 0.05s\n",
      " [-] epoch   24/250, train loss 0.613989 in 0.05s\n",
      " [-] epoch   25/250, train loss 0.571667 in 0.06s\n",
      " [-] epoch   26/250, train loss 0.601640 in 0.05s\n",
      " [-] epoch   27/250, train loss 0.587189 in 0.06s\n",
      " [-] epoch   28/250, train loss 0.603804 in 0.05s\n",
      " [-] epoch   29/250, train loss 0.596222 in 0.05s\n",
      " [-] epoch   30/250, train loss 0.591921 in 0.05s\n",
      " [-] epoch   31/250, train loss 0.574584 in 0.06s\n",
      " [-] epoch   32/250, train loss 0.591826 in 0.05s\n",
      " [-] epoch   33/250, train loss 0.616790 in 0.04s\n",
      " [-] epoch   34/250, train loss 0.576732 in 0.05s\n",
      " [-] epoch   35/250, train loss 0.605305 in 0.05s\n",
      " [-] epoch   36/250, train loss 0.589106 in 0.05s\n",
      " [-] epoch   37/250, train loss 0.597427 in 0.04s\n",
      " [-] epoch   38/250, train loss 0.565320 in 0.05s\n",
      " [-] epoch   39/250, train loss 0.588072 in 0.05s\n",
      " [-] epoch   40/250, train loss 0.592756 in 0.05s\n",
      " [-] epoch   41/250, train loss 0.575011 in 0.05s\n",
      " [-] epoch   42/250, train loss 0.589942 in 0.06s\n",
      " [-] epoch   43/250, train loss 0.589946 in 0.05s\n",
      " [-] epoch   44/250, train loss 0.567692 in 0.05s\n",
      " [-] epoch   45/250, train loss 0.593989 in 0.05s\n",
      " [-] epoch   46/250, train loss 0.626367 in 0.06s\n",
      " [-] epoch   47/250, train loss 0.571127 in 0.05s\n",
      " [-] epoch   48/250, train loss 0.571301 in 0.04s\n",
      " [-] epoch   49/250, train loss 0.588179 in 0.06s\n",
      " [-] epoch   50/250, train loss 0.577158 in 0.05s\n",
      " [-] epoch   51/250, train loss 0.604153 in 0.06s\n",
      " [-] epoch   52/250, train loss 0.603501 in 0.04s\n",
      " [-] epoch   53/250, train loss 0.577289 in 0.05s\n",
      " [-] epoch   54/250, train loss 0.574235 in 0.05s\n",
      " [-] epoch   55/250, train loss 0.571047 in 0.06s\n",
      " [-] epoch   56/250, train loss 0.582753 in 0.05s\n",
      " [-] epoch   57/250, train loss 0.604759 in 0.05s\n",
      " [-] epoch   58/250, train loss 0.574958 in 0.05s\n",
      " [-] epoch   59/250, train loss 0.597502 in 0.06s\n",
      " [-] epoch   60/250, train loss 0.568791 in 0.05s\n",
      " [-] epoch   61/250, train loss 0.591327 in 0.04s\n",
      " [-] epoch   62/250, train loss 0.585100 in 0.07s\n",
      " [-] epoch   63/250, train loss 0.585044 in 0.05s\n",
      " [-] epoch   64/250, train loss 0.559448 in 0.05s\n",
      " [-] epoch   65/250, train loss 0.555550 in 0.05s\n",
      " [-] epoch   66/250, train loss 0.585718 in 0.05s\n",
      " [-] epoch   67/250, train loss 0.577955 in 0.05s\n",
      " [-] epoch   68/250, train loss 0.573459 in 0.05s\n",
      " [-] epoch   69/250, train loss 0.606114 in 0.05s\n",
      " [-] epoch   70/250, train loss 0.596250 in 0.05s\n",
      " [-] epoch   71/250, train loss 0.586220 in 0.06s\n",
      " [-] epoch   72/250, train loss 0.598525 in 0.05s\n",
      " [-] epoch   73/250, train loss 0.585197 in 0.06s\n",
      " [-] epoch   74/250, train loss 0.566200 in 0.05s\n",
      " [-] epoch   75/250, train loss 0.578152 in 0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.568162 in 0.06s\n",
      " [-] epoch   77/250, train loss 0.576756 in 0.05s\n",
      " [-] epoch   78/250, train loss 0.577617 in 0.04s\n",
      " [-] epoch   79/250, train loss 0.561704 in 0.05s\n",
      " [-] epoch   80/250, train loss 0.567482 in 0.05s\n",
      " [-] epoch   81/250, train loss 0.583934 in 0.05s\n",
      " [-] epoch   82/250, train loss 0.592420 in 0.05s\n",
      " [-] epoch   83/250, train loss 0.576433 in 0.05s\n",
      " [-] epoch   84/250, train loss 0.557118 in 0.05s\n",
      " [-] epoch   85/250, train loss 0.565266 in 0.05s\n",
      " [-] epoch   86/250, train loss 0.554026 in 0.05s\n",
      " [-] epoch   87/250, train loss 0.579862 in 0.05s\n",
      " [-] epoch   88/250, train loss 0.567575 in 0.04s\n",
      " [-] epoch   89/250, train loss 0.589943 in 0.05s\n",
      " [-] epoch   90/250, train loss 0.589386 in 0.04s\n",
      " [-] epoch   91/250, train loss 0.579167 in 0.05s\n",
      " [-] epoch   92/250, train loss 0.566441 in 0.05s\n",
      " [-] epoch   93/250, train loss 0.567382 in 0.05s\n",
      " [-] epoch   94/250, train loss 0.592275 in 0.05s\n",
      " [-] epoch   95/250, train loss 0.574556 in 0.05s\n",
      " [-] epoch   96/250, train loss 0.565848 in 0.05s\n",
      " [-] epoch   97/250, train loss 0.552620 in 0.05s\n",
      " [-] epoch   98/250, train loss 0.542452 in 0.04s\n",
      " [-] epoch   99/250, train loss 0.569748 in 0.05s\n",
      " [-] epoch  100/250, train loss 0.597736 in 0.05s\n",
      " [-] epoch  101/250, train loss 0.572395 in 0.05s\n",
      " [-] epoch  102/250, train loss 0.562322 in 0.05s\n",
      " [-] epoch  103/250, train loss 0.566375 in 0.05s\n",
      " [-] epoch  104/250, train loss 0.583252 in 0.05s\n",
      " [-] epoch  105/250, train loss 0.574483 in 0.05s\n",
      " [-] epoch  106/250, train loss 0.559738 in 0.06s\n",
      " [-] epoch  107/250, train loss 0.581664 in 0.06s\n",
      " [-] epoch  108/250, train loss 0.567862 in 0.05s\n",
      " [-] epoch  109/250, train loss 0.553676 in 0.04s\n",
      " [-] epoch  110/250, train loss 0.605644 in 0.04s\n",
      " [-] epoch  111/250, train loss 0.577217 in 0.04s\n",
      " [-] epoch  112/250, train loss 0.576277 in 0.04s\n",
      " [-] epoch  113/250, train loss 0.580972 in 0.04s\n",
      " [-] epoch  114/250, train loss 0.558254 in 0.04s\n",
      " [-] epoch  115/250, train loss 0.568869 in 0.05s\n",
      " [-] epoch  116/250, train loss 0.578211 in 0.05s\n",
      " [-] epoch  117/250, train loss 0.565722 in 0.05s\n",
      " [-] epoch  118/250, train loss 0.590214 in 0.05s\n",
      " [-] epoch  119/250, train loss 0.559241 in 0.05s\n",
      " [-] epoch  120/250, train loss 0.577409 in 0.05s\n",
      " [-] epoch  121/250, train loss 0.571694 in 0.05s\n",
      " [-] epoch  122/250, train loss 0.550102 in 0.05s\n",
      " [-] epoch  123/250, train loss 0.560658 in 0.05s\n",
      " [-] epoch  124/250, train loss 0.578581 in 0.05s\n",
      " [-] epoch  125/250, train loss 0.569052 in 0.05s\n",
      " [-] epoch  126/250, train loss 0.575468 in 0.05s\n",
      " [-] epoch  127/250, train loss 0.567119 in 0.05s\n",
      " [-] epoch  128/250, train loss 0.550538 in 0.05s\n",
      " [-] epoch  129/250, train loss 0.546104 in 0.05s\n",
      " [-] epoch  130/250, train loss 0.539494 in 0.04s\n",
      " [-] epoch  131/250, train loss 0.588755 in 0.05s\n",
      " [-] epoch  132/250, train loss 0.579527 in 0.05s\n",
      " [-] epoch  133/250, train loss 0.578679 in 0.05s\n",
      " [-] epoch  134/250, train loss 0.563693 in 0.05s\n",
      " [-] epoch  135/250, train loss 0.582959 in 0.05s\n",
      " [-] epoch  136/250, train loss 0.579820 in 0.05s\n",
      " [-] epoch  137/250, train loss 0.579001 in 0.05s\n",
      " [-] epoch  138/250, train loss 0.568847 in 0.04s\n",
      " [-] epoch  139/250, train loss 0.565518 in 0.04s\n",
      " [-] epoch  140/250, train loss 0.549851 in 0.05s\n",
      " [-] epoch  141/250, train loss 0.589585 in 0.05s\n",
      " [-] epoch  142/250, train loss 0.556281 in 0.05s\n",
      " [-] epoch  143/250, train loss 0.561880 in 0.05s\n",
      " [-] epoch  144/250, train loss 0.582226 in 0.06s\n",
      " [-] epoch  145/250, train loss 0.565422 in 0.05s\n",
      " [-] epoch  146/250, train loss 0.569354 in 0.04s\n",
      " [-] epoch  147/250, train loss 0.553575 in 0.05s\n",
      " [-] epoch  148/250, train loss 0.572239 in 0.05s\n",
      " [-] epoch  149/250, train loss 0.577230 in 0.05s\n",
      " [-] epoch  150/250, train loss 0.558338 in 0.04s\n",
      " [-] epoch  151/250, train loss 0.573029 in 0.04s\n",
      " [-] epoch  152/250, train loss 0.587140 in 0.05s\n",
      " [-] epoch  153/250, train loss 0.582969 in 0.05s\n",
      " [-] epoch  154/250, train loss 0.571970 in 0.05s\n",
      " [-] epoch  155/250, train loss 0.546039 in 0.05s\n",
      " [-] epoch  156/250, train loss 0.558009 in 0.05s\n",
      " [-] epoch  157/250, train loss 0.565265 in 0.04s\n",
      " [-] epoch  158/250, train loss 0.578170 in 0.05s\n",
      " [-] epoch  159/250, train loss 0.566915 in 0.05s\n",
      " [-] epoch  160/250, train loss 0.567142 in 0.05s\n",
      " [-] epoch  161/250, train loss 0.567136 in 0.05s\n",
      " [-] epoch  162/250, train loss 0.560087 in 0.04s\n",
      " [-] epoch  163/250, train loss 0.544495 in 0.05s\n",
      " [-] epoch  164/250, train loss 0.561198 in 0.05s\n",
      " [-] epoch  165/250, train loss 0.560177 in 0.05s\n",
      " [-] epoch  166/250, train loss 0.548454 in 0.04s\n",
      " [-] epoch  167/250, train loss 0.566131 in 0.05s\n",
      " [-] epoch  168/250, train loss 0.593708 in 0.04s\n",
      " [-] epoch  169/250, train loss 0.573977 in 0.05s\n",
      " [-] epoch  170/250, train loss 0.563738 in 0.05s\n",
      " [-] epoch  171/250, train loss 0.567038 in 0.05s\n",
      " [-] epoch  172/250, train loss 0.574425 in 0.05s\n",
      " [-] epoch  173/250, train loss 0.556863 in 0.05s\n",
      " [-] epoch  174/250, train loss 0.554686 in 0.05s\n",
      " [-] epoch  175/250, train loss 0.575218 in 0.05s\n",
      " [-] epoch  176/250, train loss 0.565605 in 0.05s\n",
      " [-] epoch  177/250, train loss 0.548862 in 0.05s\n",
      " [-] epoch  178/250, train loss 0.532588 in 0.05s\n",
      " [-] epoch  179/250, train loss 0.574461 in 0.05s\n",
      " [-] epoch  180/250, train loss 0.587193 in 0.05s\n",
      " [-] epoch  181/250, train loss 0.559704 in 0.05s\n",
      " [-] epoch  182/250, train loss 0.553965 in 0.04s\n",
      " [-] epoch  183/250, train loss 0.572002 in 0.06s\n",
      " [-] epoch  184/250, train loss 0.576780 in 0.05s\n",
      " [-] epoch  185/250, train loss 0.582842 in 0.05s\n",
      " [-] epoch  186/250, train loss 0.551534 in 0.04s\n",
      " [-] epoch  187/250, train loss 0.579005 in 0.05s\n",
      " [-] epoch  188/250, train loss 0.585551 in 0.05s\n",
      " [-] epoch  189/250, train loss 0.582265 in 0.06s\n",
      " [-] epoch  190/250, train loss 0.554491 in 0.05s\n",
      " [-] epoch  191/250, train loss 0.549569 in 0.05s\n",
      " [-] epoch  192/250, train loss 0.563159 in 0.05s\n",
      " [-] epoch  193/250, train loss 0.557503 in 0.05s\n",
      " [-] epoch  194/250, train loss 0.557981 in 0.06s\n",
      " [-] epoch  195/250, train loss 0.546914 in 0.05s\n",
      " [-] epoch  196/250, train loss 0.582415 in 0.05s\n",
      " [-] epoch  197/250, train loss 0.547517 in 0.05s\n",
      " [-] epoch  198/250, train loss 0.556512 in 0.06s\n",
      " [-] epoch  199/250, train loss 0.573804 in 0.05s\n",
      " [-] epoch  200/250, train loss 0.566279 in 0.05s\n",
      " [-] epoch  201/250, train loss 0.552670 in 0.06s\n",
      " [-] epoch  202/250, train loss 0.545042 in 0.05s\n",
      " [-] epoch  203/250, train loss 0.545088 in 0.05s\n",
      " [-] epoch  204/250, train loss 0.578434 in 0.06s\n",
      " [-] epoch  205/250, train loss 0.567705 in 0.05s\n",
      " [-] epoch  206/250, train loss 0.551066 in 0.05s\n",
      " [-] epoch  207/250, train loss 0.578575 in 0.05s\n",
      " [-] epoch  208/250, train loss 0.547293 in 0.05s\n",
      " [-] epoch  209/250, train loss 0.566086 in 0.05s\n",
      " [-] epoch  210/250, train loss 0.569616 in 0.03s\n",
      " [-] epoch  211/250, train loss 0.560650 in 0.06s\n",
      " [-] epoch  212/250, train loss 0.557054 in 0.06s\n",
      " [-] epoch  213/250, train loss 0.568413 in 0.05s\n",
      " [-] epoch  214/250, train loss 0.568845 in 0.05s\n",
      " [-] epoch  215/250, train loss 0.578374 in 0.05s\n",
      " [-] epoch  216/250, train loss 0.569076 in 0.05s\n",
      " [-] epoch  217/250, train loss 0.545509 in 0.04s\n",
      " [-] epoch  218/250, train loss 0.569765 in 0.05s\n",
      " [-] epoch  219/250, train loss 0.559613 in 0.04s\n",
      " [-] epoch  220/250, train loss 0.579891 in 0.05s\n",
      " [-] epoch  221/250, train loss 0.568138 in 0.04s\n",
      " [-] epoch  222/250, train loss 0.559983 in 0.05s\n",
      " [-] epoch  223/250, train loss 0.547237 in 0.06s\n",
      " [-] epoch  224/250, train loss 0.554611 in 0.04s\n",
      " [-] epoch  225/250, train loss 0.580082 in 0.05s\n",
      " [-] epoch  226/250, train loss 0.534972 in 0.05s\n",
      " [-] epoch  227/250, train loss 0.592878 in 0.05s\n",
      " [-] epoch  228/250, train loss 0.564030 in 0.05s\n",
      " [-] epoch  229/250, train loss 0.554000 in 0.05s\n",
      " [-] epoch  230/250, train loss 0.540501 in 0.05s\n",
      " [-] epoch  231/250, train loss 0.567694 in 0.03s\n",
      " [-] epoch  232/250, train loss 0.562002 in 0.05s\n",
      " [-] epoch  233/250, train loss 0.571236 in 0.05s\n",
      " [-] epoch  234/250, train loss 0.541567 in 0.05s\n",
      " [-] epoch  235/250, train loss 0.577502 in 0.05s\n",
      " [-] epoch  236/250, train loss 0.558554 in 0.04s\n",
      " [-] epoch  237/250, train loss 0.586150 in 0.05s\n",
      " [-] epoch  238/250, train loss 0.574865 in 0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  239/250, train loss 0.570598 in 0.06s\n",
      " [-] epoch  240/250, train loss 0.566496 in 0.05s\n",
      " [-] epoch  241/250, train loss 0.565825 in 0.05s\n",
      " [-] epoch  242/250, train loss 0.569378 in 0.05s\n",
      " [-] epoch  243/250, train loss 0.538932 in 0.05s\n",
      " [-] epoch  244/250, train loss 0.551658 in 0.05s\n",
      " [-] epoch  245/250, train loss 0.556656 in 0.05s\n",
      " [-] epoch  246/250, train loss 0.548178 in 0.05s\n",
      " [-] epoch  247/250, train loss 0.575215 in 0.05s\n",
      " [-] epoch  248/250, train loss 0.558915 in 0.05s\n",
      " [-] epoch  249/250, train loss 0.556194 in 0.04s\n",
      " [-] epoch  250/250, train loss 0.546188 in 0.05s\n",
      " [-] test acc. 51.388889%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.670980 in 0.08s\n",
      " [-] epoch    2/250, train loss 0.642764 in 0.07s\n",
      " [-] epoch    3/250, train loss 0.631759 in 0.07s\n",
      " [-] epoch    4/250, train loss 0.644739 in 0.08s\n",
      " [-] epoch    5/250, train loss 0.632667 in 0.07s\n",
      " [-] epoch    6/250, train loss 0.594128 in 0.07s\n",
      " [-] epoch    7/250, train loss 0.620398 in 0.08s\n",
      " [-] epoch    8/250, train loss 0.607978 in 0.07s\n",
      " [-] epoch    9/250, train loss 0.582960 in 0.08s\n",
      " [-] epoch   10/250, train loss 0.606429 in 0.07s\n",
      " [-] epoch   11/250, train loss 0.585186 in 0.08s\n",
      " [-] epoch   12/250, train loss 0.592252 in 0.07s\n",
      " [-] epoch   13/250, train loss 0.593686 in 0.08s\n",
      " [-] epoch   14/250, train loss 0.600385 in 0.07s\n",
      " [-] epoch   15/250, train loss 0.596936 in 0.08s\n",
      " [-] epoch   16/250, train loss 0.594082 in 0.06s\n",
      " [-] epoch   17/250, train loss 0.563669 in 0.07s\n",
      " [-] epoch   18/250, train loss 0.571764 in 0.07s\n",
      " [-] epoch   19/250, train loss 0.579706 in 0.07s\n",
      " [-] epoch   20/250, train loss 0.574859 in 0.08s\n",
      " [-] epoch   21/250, train loss 0.567283 in 0.07s\n",
      " [-] epoch   22/250, train loss 0.544851 in 0.07s\n",
      " [-] epoch   23/250, train loss 0.575177 in 0.08s\n",
      " [-] epoch   24/250, train loss 0.566294 in 0.08s\n",
      " [-] epoch   25/250, train loss 0.578403 in 0.07s\n",
      " [-] epoch   26/250, train loss 0.555025 in 0.08s\n",
      " [-] epoch   27/250, train loss 0.550125 in 0.08s\n",
      " [-] epoch   28/250, train loss 0.549151 in 0.07s\n",
      " [-] epoch   29/250, train loss 0.573404 in 0.06s\n",
      " [-] epoch   30/250, train loss 0.545941 in 0.08s\n",
      " [-] epoch   31/250, train loss 0.570471 in 0.07s\n",
      " [-] epoch   32/250, train loss 0.587274 in 0.08s\n",
      " [-] epoch   33/250, train loss 0.551579 in 0.07s\n",
      " [-] epoch   34/250, train loss 0.551166 in 0.08s\n",
      " [-] epoch   35/250, train loss 0.551651 in 0.08s\n",
      " [-] epoch   36/250, train loss 0.562000 in 0.07s\n",
      " [-] epoch   37/250, train loss 0.549751 in 0.08s\n",
      " [-] epoch   38/250, train loss 0.539335 in 0.06s\n",
      " [-] epoch   39/250, train loss 0.548931 in 0.07s\n",
      " [-] epoch   40/250, train loss 0.566413 in 0.07s\n",
      " [-] epoch   41/250, train loss 0.565275 in 0.07s\n",
      " [-] epoch   42/250, train loss 0.542595 in 0.08s\n",
      " [-] epoch   43/250, train loss 0.571628 in 0.07s\n",
      " [-] epoch   44/250, train loss 0.564627 in 0.07s\n",
      " [-] epoch   45/250, train loss 0.562052 in 0.07s\n",
      " [-] epoch   46/250, train loss 0.565260 in 0.07s\n",
      " [-] epoch   47/250, train loss 0.546528 in 0.08s\n",
      " [-] epoch   48/250, train loss 0.534401 in 0.07s\n",
      " [-] epoch   49/250, train loss 0.576778 in 0.06s\n",
      " [-] epoch   50/250, train loss 0.565943 in 0.06s\n",
      " [-] epoch   51/250, train loss 0.553559 in 0.07s\n",
      " [-] epoch   52/250, train loss 0.559867 in 0.08s\n",
      " [-] epoch   53/250, train loss 0.542006 in 0.06s\n",
      " [-] epoch   54/250, train loss 0.531706 in 0.08s\n",
      " [-] epoch   55/250, train loss 0.544157 in 0.07s\n",
      " [-] epoch   56/250, train loss 0.528350 in 0.08s\n",
      " [-] epoch   57/250, train loss 0.544978 in 0.07s\n",
      " [-] epoch   58/250, train loss 0.533467 in 0.06s\n",
      " [-] epoch   59/250, train loss 0.540283 in 0.08s\n",
      " [-] epoch   60/250, train loss 0.543385 in 0.06s\n",
      " [-] epoch   61/250, train loss 0.559783 in 0.07s\n",
      " [-] epoch   62/250, train loss 0.568924 in 0.07s\n",
      " [-] epoch   63/250, train loss 0.563246 in 0.08s\n",
      " [-] epoch   64/250, train loss 0.527526 in 0.06s\n",
      " [-] epoch   65/250, train loss 0.550011 in 0.07s\n",
      " [-] epoch   66/250, train loss 0.576323 in 0.06s\n",
      " [-] epoch   67/250, train loss 0.570741 in 0.08s\n",
      " [-] epoch   68/250, train loss 0.544217 in 0.07s\n",
      " [-] epoch   69/250, train loss 0.524424 in 0.07s\n",
      " [-] epoch   70/250, train loss 0.548281 in 0.07s\n",
      " [-] epoch   71/250, train loss 0.536050 in 0.07s\n",
      " [-] epoch   72/250, train loss 0.565749 in 0.08s\n",
      " [-] epoch   73/250, train loss 0.522699 in 0.07s\n",
      " [-] epoch   74/250, train loss 0.537660 in 0.08s\n",
      " [-] epoch   75/250, train loss 0.528044 in 0.07s\n",
      " [-] epoch   76/250, train loss 0.541444 in 0.07s\n",
      " [-] epoch   77/250, train loss 0.532896 in 0.08s\n",
      " [-] epoch   78/250, train loss 0.519005 in 0.07s\n",
      " [-] epoch   79/250, train loss 0.527697 in 0.07s\n",
      " [-] epoch   80/250, train loss 0.543072 in 0.07s\n",
      " [-] epoch   81/250, train loss 0.533757 in 0.08s\n",
      " [-] epoch   82/250, train loss 0.524916 in 0.07s\n",
      " [-] epoch   83/250, train loss 0.529907 in 0.08s\n",
      " [-] epoch   84/250, train loss 0.556020 in 0.07s\n",
      " [-] epoch   85/250, train loss 0.527427 in 0.08s\n",
      " [-] epoch   86/250, train loss 0.552461 in 0.08s\n",
      " [-] epoch   87/250, train loss 0.527663 in 0.07s\n",
      " [-] epoch   88/250, train loss 0.551585 in 0.07s\n",
      " [-] epoch   89/250, train loss 0.564016 in 0.08s\n",
      " [-] epoch   90/250, train loss 0.544282 in 0.07s\n",
      " [-] epoch   91/250, train loss 0.528144 in 0.08s\n",
      " [-] epoch   92/250, train loss 0.535959 in 0.07s\n",
      " [-] epoch   93/250, train loss 0.538725 in 0.06s\n",
      " [-] epoch   94/250, train loss 0.550626 in 0.07s\n",
      " [-] epoch   95/250, train loss 0.542862 in 0.09s\n",
      " [-] epoch   96/250, train loss 0.537962 in 0.08s\n",
      " [-] epoch   97/250, train loss 0.549387 in 0.07s\n",
      " [-] epoch   98/250, train loss 0.502886 in 0.07s\n",
      " [-] epoch   99/250, train loss 0.526882 in 0.07s\n",
      " [-] epoch  100/250, train loss 0.531041 in 0.07s\n",
      " [-] epoch  101/250, train loss 0.532240 in 0.07s\n",
      " [-] epoch  102/250, train loss 0.528139 in 0.07s\n",
      " [-] epoch  103/250, train loss 0.550686 in 0.07s\n",
      " [-] epoch  104/250, train loss 0.527079 in 0.06s\n",
      " [-] epoch  105/250, train loss 0.546773 in 0.09s\n",
      " [-] epoch  106/250, train loss 0.510211 in 0.07s\n",
      " [-] epoch  107/250, train loss 0.539543 in 0.08s\n",
      " [-] epoch  108/250, train loss 0.508141 in 0.08s\n",
      " [-] epoch  109/250, train loss 0.543716 in 0.07s\n",
      " [-] epoch  110/250, train loss 0.500940 in 0.07s\n",
      " [-] epoch  111/250, train loss 0.551883 in 0.06s\n",
      " [-] epoch  112/250, train loss 0.524756 in 0.07s\n",
      " [-] epoch  113/250, train loss 0.513142 in 0.07s\n",
      " [-] epoch  114/250, train loss 0.526269 in 0.08s\n",
      " [-] epoch  115/250, train loss 0.536515 in 0.07s\n",
      " [-] epoch  116/250, train loss 0.519817 in 0.08s\n",
      " [-] epoch  117/250, train loss 0.522590 in 0.07s\n",
      " [-] epoch  118/250, train loss 0.529535 in 0.06s\n",
      " [-] epoch  119/250, train loss 0.548020 in 0.08s\n",
      " [-] epoch  120/250, train loss 0.541804 in 0.08s\n",
      " [-] epoch  121/250, train loss 0.534096 in 0.06s\n",
      " [-] epoch  122/250, train loss 0.516104 in 0.10s\n",
      " [-] epoch  123/250, train loss 0.525528 in 0.08s\n",
      " [-] epoch  124/250, train loss 0.511544 in 0.08s\n",
      " [-] epoch  125/250, train loss 0.553932 in 0.08s\n",
      " [-] epoch  126/250, train loss 0.536015 in 0.08s\n",
      " [-] epoch  127/250, train loss 0.534172 in 0.08s\n",
      " [-] epoch  128/250, train loss 0.525143 in 0.07s\n",
      " [-] epoch  129/250, train loss 0.516758 in 0.08s\n",
      " [-] epoch  130/250, train loss 0.525336 in 0.06s\n",
      " [-] epoch  131/250, train loss 0.534815 in 0.07s\n",
      " [-] epoch  132/250, train loss 0.513759 in 0.07s\n",
      " [-] epoch  133/250, train loss 0.523050 in 0.07s\n",
      " [-] epoch  134/250, train loss 0.527253 in 0.07s\n",
      " [-] epoch  135/250, train loss 0.518945 in 0.06s\n",
      " [-] epoch  136/250, train loss 0.519961 in 0.08s\n",
      " [-] epoch  137/250, train loss 0.544550 in 0.07s\n",
      " [-] epoch  138/250, train loss 0.505295 in 0.06s\n",
      " [-] epoch  139/250, train loss 0.544394 in 0.07s\n",
      " [-] epoch  140/250, train loss 0.512273 in 0.07s\n",
      " [-] epoch  141/250, train loss 0.521463 in 0.08s\n",
      " [-] epoch  142/250, train loss 0.501664 in 0.07s\n",
      " [-] epoch  143/250, train loss 0.483009 in 0.06s\n",
      " [-] epoch  144/250, train loss 0.541124 in 0.07s\n",
      " [-] epoch  145/250, train loss 0.496546 in 0.07s\n",
      " [-] epoch  146/250, train loss 0.526097 in 0.08s\n",
      " [-] epoch  147/250, train loss 0.512319 in 0.08s\n",
      " [-] epoch  148/250, train loss 0.507945 in 0.06s\n",
      " [-] epoch  149/250, train loss 0.534347 in 0.08s\n",
      " [-] epoch  150/250, train loss 0.551059 in 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.506544 in 0.07s\n",
      " [-] epoch  152/250, train loss 0.503866 in 0.07s\n",
      " [-] epoch  153/250, train loss 0.518956 in 0.08s\n",
      " [-] epoch  154/250, train loss 0.504376 in 0.07s\n",
      " [-] epoch  155/250, train loss 0.513468 in 0.07s\n",
      " [-] epoch  156/250, train loss 0.513675 in 0.07s\n",
      " [-] epoch  157/250, train loss 0.504888 in 0.07s\n",
      " [-] epoch  158/250, train loss 0.496389 in 0.07s\n",
      " [-] epoch  159/250, train loss 0.518441 in 0.07s\n",
      " [-] epoch  160/250, train loss 0.484161 in 0.06s\n",
      " [-] epoch  161/250, train loss 0.532724 in 0.07s\n",
      " [-] epoch  162/250, train loss 0.519187 in 0.07s\n",
      " [-] epoch  163/250, train loss 0.532349 in 0.06s\n",
      " [-] epoch  164/250, train loss 0.527673 in 0.08s\n",
      " [-] epoch  165/250, train loss 0.538184 in 0.07s\n",
      " [-] epoch  166/250, train loss 0.511561 in 0.07s\n",
      " [-] epoch  167/250, train loss 0.514848 in 0.06s\n",
      " [-] epoch  168/250, train loss 0.551769 in 0.07s\n",
      " [-] epoch  169/250, train loss 0.520724 in 0.07s\n",
      " [-] epoch  170/250, train loss 0.513213 in 0.07s\n",
      " [-] epoch  171/250, train loss 0.531022 in 0.08s\n",
      " [-] epoch  172/250, train loss 0.522665 in 0.07s\n",
      " [-] epoch  173/250, train loss 0.499735 in 0.07s\n",
      " [-] epoch  174/250, train loss 0.511698 in 0.06s\n",
      " [-] epoch  175/250, train loss 0.518642 in 0.06s\n",
      " [-] epoch  176/250, train loss 0.523331 in 0.08s\n",
      " [-] epoch  177/250, train loss 0.516302 in 0.08s\n",
      " [-] epoch  178/250, train loss 0.518321 in 0.08s\n",
      " [-] epoch  179/250, train loss 0.531802 in 0.07s\n",
      " [-] epoch  180/250, train loss 0.527463 in 0.07s\n",
      " [-] epoch  181/250, train loss 0.523691 in 0.07s\n",
      " [-] epoch  182/250, train loss 0.487131 in 0.07s\n",
      " [-] epoch  183/250, train loss 0.486793 in 0.06s\n",
      " [-] epoch  184/250, train loss 0.481804 in 0.07s\n",
      " [-] epoch  185/250, train loss 0.541083 in 0.08s\n",
      " [-] epoch  186/250, train loss 0.512977 in 0.07s\n",
      " [-] epoch  187/250, train loss 0.543427 in 0.06s\n",
      " [-] epoch  188/250, train loss 0.507064 in 0.08s\n",
      " [-] epoch  189/250, train loss 0.501849 in 0.07s\n",
      " [-] epoch  190/250, train loss 0.511388 in 0.08s\n",
      " [-] epoch  191/250, train loss 0.490877 in 0.07s\n",
      " [-] epoch  192/250, train loss 0.531518 in 0.07s\n",
      " [-] epoch  193/250, train loss 0.508077 in 0.08s\n",
      " [-] epoch  194/250, train loss 0.499037 in 0.06s\n",
      " [-] epoch  195/250, train loss 0.528717 in 0.07s\n",
      " [-] epoch  196/250, train loss 0.512887 in 0.08s\n",
      " [-] epoch  197/250, train loss 0.522316 in 0.08s\n",
      " [-] epoch  198/250, train loss 0.506826 in 0.07s\n",
      " [-] epoch  199/250, train loss 0.520457 in 0.08s\n",
      " [-] epoch  200/250, train loss 0.522361 in 0.07s\n",
      " [-] epoch  201/250, train loss 0.522745 in 0.08s\n",
      " [-] epoch  202/250, train loss 0.486269 in 0.06s\n",
      " [-] epoch  203/250, train loss 0.541329 in 0.08s\n",
      " [-] epoch  204/250, train loss 0.542996 in 0.08s\n",
      " [-] epoch  205/250, train loss 0.521044 in 0.07s\n",
      " [-] epoch  206/250, train loss 0.521903 in 0.07s\n",
      " [-] epoch  207/250, train loss 0.493026 in 0.07s\n",
      " [-] epoch  208/250, train loss 0.489008 in 0.07s\n",
      " [-] epoch  209/250, train loss 0.488946 in 0.06s\n",
      " [-] epoch  210/250, train loss 0.512819 in 0.08s\n",
      " [-] epoch  211/250, train loss 0.529388 in 0.08s\n",
      " [-] epoch  212/250, train loss 0.522534 in 0.08s\n",
      " [-] epoch  213/250, train loss 0.513055 in 0.07s\n",
      " [-] epoch  214/250, train loss 0.483691 in 0.07s\n",
      " [-] epoch  215/250, train loss 0.510581 in 0.07s\n",
      " [-] epoch  216/250, train loss 0.487117 in 0.08s\n",
      " [-] epoch  217/250, train loss 0.496576 in 0.07s\n",
      " [-] epoch  218/250, train loss 0.515321 in 0.07s\n",
      " [-] epoch  219/250, train loss 0.539199 in 0.08s\n",
      " [-] epoch  220/250, train loss 0.521098 in 0.06s\n",
      " [-] epoch  221/250, train loss 0.504170 in 0.07s\n",
      " [-] epoch  222/250, train loss 0.487791 in 0.08s\n",
      " [-] epoch  223/250, train loss 0.514729 in 0.08s\n",
      " [-] epoch  224/250, train loss 0.516528 in 0.07s\n",
      " [-] epoch  225/250, train loss 0.504406 in 0.07s\n",
      " [-] epoch  226/250, train loss 0.512643 in 0.07s\n",
      " [-] epoch  227/250, train loss 0.511652 in 0.07s\n",
      " [-] epoch  228/250, train loss 0.502336 in 0.07s\n",
      " [-] epoch  229/250, train loss 0.520250 in 0.06s\n",
      " [-] epoch  230/250, train loss 0.507054 in 0.07s\n",
      " [-] epoch  231/250, train loss 0.464088 in 0.08s\n",
      " [-] epoch  232/250, train loss 0.525024 in 0.07s\n",
      " [-] epoch  233/250, train loss 0.508897 in 0.08s\n",
      " [-] epoch  234/250, train loss 0.521073 in 0.07s\n",
      " [-] epoch  235/250, train loss 0.506536 in 0.07s\n",
      " [-] epoch  236/250, train loss 0.508350 in 0.08s\n",
      " [-] epoch  237/250, train loss 0.478312 in 0.08s\n",
      " [-] epoch  238/250, train loss 0.521880 in 0.07s\n",
      " [-] epoch  239/250, train loss 0.505793 in 0.06s\n",
      " [-] epoch  240/250, train loss 0.537197 in 0.07s\n",
      " [-] epoch  241/250, train loss 0.514354 in 0.05s\n",
      " [-] epoch  242/250, train loss 0.517943 in 0.07s\n",
      " [-] epoch  243/250, train loss 0.508909 in 0.07s\n",
      " [-] epoch  244/250, train loss 0.509764 in 0.06s\n",
      " [-] epoch  245/250, train loss 0.497220 in 0.08s\n",
      " [-] epoch  246/250, train loss 0.496872 in 0.07s\n",
      " [-] epoch  247/250, train loss 0.522212 in 0.07s\n",
      " [-] epoch  248/250, train loss 0.501942 in 0.07s\n",
      " [-] epoch  249/250, train loss 0.498218 in 0.08s\n",
      " [-] epoch  250/250, train loss 0.515897 in 0.07s\n",
      " [-] test acc. 63.333333%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.696427 in 0.10s\n",
      " [-] epoch    2/250, train loss 0.670199 in 0.09s\n",
      " [-] epoch    3/250, train loss 0.648106 in 0.09s\n",
      " [-] epoch    4/250, train loss 0.627015 in 0.09s\n",
      " [-] epoch    5/250, train loss 0.611955 in 0.09s\n",
      " [-] epoch    6/250, train loss 0.597159 in 0.10s\n",
      " [-] epoch    7/250, train loss 0.587879 in 0.08s\n",
      " [-] epoch    8/250, train loss 0.576187 in 0.09s\n",
      " [-] epoch    9/250, train loss 0.589560 in 0.09s\n",
      " [-] epoch   10/250, train loss 0.594567 in 0.09s\n",
      " [-] epoch   11/250, train loss 0.573239 in 0.09s\n",
      " [-] epoch   12/250, train loss 0.582800 in 0.09s\n",
      " [-] epoch   13/250, train loss 0.571267 in 0.09s\n",
      " [-] epoch   14/250, train loss 0.567437 in 0.09s\n",
      " [-] epoch   15/250, train loss 0.565784 in 0.08s\n",
      " [-] epoch   16/250, train loss 0.576625 in 0.10s\n",
      " [-] epoch   17/250, train loss 0.562561 in 0.08s\n",
      " [-] epoch   18/250, train loss 0.550909 in 0.10s\n",
      " [-] epoch   19/250, train loss 0.566541 in 0.10s\n",
      " [-] epoch   20/250, train loss 0.556644 in 0.11s\n",
      " [-] epoch   21/250, train loss 0.566130 in 0.10s\n",
      " [-] epoch   22/250, train loss 0.560976 in 0.10s\n",
      " [-] epoch   23/250, train loss 0.550890 in 0.08s\n",
      " [-] epoch   24/250, train loss 0.582500 in 0.10s\n",
      " [-] epoch   25/250, train loss 0.544800 in 0.10s\n",
      " [-] epoch   26/250, train loss 0.561754 in 0.10s\n",
      " [-] epoch   27/250, train loss 0.557346 in 0.10s\n",
      " [-] epoch   28/250, train loss 0.573567 in 0.10s\n",
      " [-] epoch   29/250, train loss 0.548653 in 0.08s\n",
      " [-] epoch   30/250, train loss 0.540907 in 0.09s\n",
      " [-] epoch   31/250, train loss 0.547950 in 0.09s\n",
      " [-] epoch   32/250, train loss 0.540807 in 0.10s\n",
      " [-] epoch   33/250, train loss 0.567744 in 0.10s\n",
      " [-] epoch   34/250, train loss 0.554420 in 0.09s\n",
      " [-] epoch   35/250, train loss 0.561647 in 0.09s\n",
      " [-] epoch   36/250, train loss 0.568614 in 0.10s\n",
      " [-] epoch   37/250, train loss 0.545379 in 0.11s\n",
      " [-] epoch   38/250, train loss 0.528379 in 0.09s\n",
      " [-] epoch   39/250, train loss 0.555044 in 0.10s\n",
      " [-] epoch   40/250, train loss 0.544206 in 0.10s\n",
      " [-] epoch   41/250, train loss 0.539242 in 0.10s\n",
      " [-] epoch   42/250, train loss 0.552255 in 0.08s\n",
      " [-] epoch   43/250, train loss 0.548211 in 0.07s\n",
      " [-] epoch   44/250, train loss 0.543560 in 0.09s\n",
      " [-] epoch   45/250, train loss 0.515062 in 0.11s\n",
      " [-] epoch   46/250, train loss 0.542338 in 0.09s\n",
      " [-] epoch   47/250, train loss 0.552037 in 0.07s\n",
      " [-] epoch   48/250, train loss 0.537914 in 0.10s\n",
      " [-] epoch   49/250, train loss 0.523163 in 0.09s\n",
      " [-] epoch   50/250, train loss 0.523370 in 0.06s\n",
      " [-] epoch   51/250, train loss 0.565601 in 0.09s\n",
      " [-] epoch   52/250, train loss 0.514863 in 0.12s\n",
      " [-] epoch   53/250, train loss 0.531802 in 0.10s\n",
      " [-] epoch   54/250, train loss 0.547051 in 0.11s\n",
      " [-] epoch   55/250, train loss 0.527619 in 0.11s\n",
      " [-] epoch   56/250, train loss 0.519309 in 0.11s\n",
      " [-] epoch   57/250, train loss 0.518112 in 0.11s\n",
      " [-] epoch   58/250, train loss 0.532041 in 0.10s\n",
      " [-] epoch   59/250, train loss 0.528878 in 0.10s\n",
      " [-] epoch   60/250, train loss 0.533348 in 0.11s\n",
      " [-] epoch   61/250, train loss 0.535993 in 0.10s\n",
      " [-] epoch   62/250, train loss 0.536418 in 0.11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   63/250, train loss 0.530053 in 0.08s\n",
      " [-] epoch   64/250, train loss 0.532654 in 0.07s\n",
      " [-] epoch   65/250, train loss 0.513441 in 0.10s\n",
      " [-] epoch   66/250, train loss 0.506802 in 0.09s\n",
      " [-] epoch   67/250, train loss 0.528260 in 0.10s\n",
      " [-] epoch   68/250, train loss 0.528008 in 0.13s\n",
      " [-] epoch   69/250, train loss 0.527279 in 0.11s\n",
      " [-] epoch   70/250, train loss 0.532113 in 0.11s\n",
      " [-] epoch   71/250, train loss 0.528357 in 0.11s\n",
      " [-] epoch   72/250, train loss 0.526023 in 0.11s\n",
      " [-] epoch   73/250, train loss 0.506593 in 0.10s\n",
      " [-] epoch   74/250, train loss 0.538348 in 0.10s\n",
      " [-] epoch   75/250, train loss 0.524143 in 0.11s\n",
      " [-] epoch   76/250, train loss 0.525577 in 0.11s\n",
      " [-] epoch   77/250, train loss 0.523762 in 0.09s\n",
      " [-] epoch   78/250, train loss 0.539804 in 0.09s\n",
      " [-] epoch   79/250, train loss 0.540981 in 0.08s\n",
      " [-] epoch   80/250, train loss 0.509885 in 0.10s\n",
      " [-] epoch   81/250, train loss 0.526146 in 0.09s\n",
      " [-] epoch   82/250, train loss 0.531267 in 0.08s\n",
      " [-] epoch   83/250, train loss 0.513165 in 0.10s\n",
      " [-] epoch   84/250, train loss 0.516741 in 0.09s\n",
      " [-] epoch   85/250, train loss 0.507974 in 0.09s\n",
      " [-] epoch   86/250, train loss 0.521897 in 0.08s\n",
      " [-] epoch   87/250, train loss 0.498649 in 0.07s\n",
      " [-] epoch   88/250, train loss 0.512946 in 0.10s\n",
      " [-] epoch   89/250, train loss 0.506749 in 0.09s\n",
      " [-] epoch   90/250, train loss 0.519172 in 0.10s\n",
      " [-] epoch   91/250, train loss 0.542549 in 0.09s\n",
      " [-] epoch   92/250, train loss 0.522017 in 0.09s\n",
      " [-] epoch   93/250, train loss 0.535805 in 0.09s\n",
      " [-] epoch   94/250, train loss 0.518805 in 0.08s\n",
      " [-] epoch   95/250, train loss 0.527213 in 0.10s\n",
      " [-] epoch   96/250, train loss 0.527041 in 0.08s\n",
      " [-] epoch   97/250, train loss 0.498125 in 0.08s\n",
      " [-] epoch   98/250, train loss 0.529289 in 0.09s\n",
      " [-] epoch   99/250, train loss 0.510341 in 0.09s\n",
      " [-] epoch  100/250, train loss 0.510503 in 0.11s\n",
      " [-] epoch  101/250, train loss 0.549747 in 0.10s\n",
      " [-] epoch  102/250, train loss 0.516924 in 0.09s\n",
      " [-] epoch  103/250, train loss 0.513810 in 0.08s\n",
      " [-] epoch  104/250, train loss 0.501374 in 0.09s\n",
      " [-] epoch  105/250, train loss 0.512298 in 0.09s\n",
      " [-] epoch  106/250, train loss 0.516416 in 0.10s\n",
      " [-] epoch  107/250, train loss 0.545296 in 0.11s\n",
      " [-] epoch  108/250, train loss 0.489733 in 0.07s\n",
      " [-] epoch  109/250, train loss 0.465329 in 0.08s\n",
      " [-] epoch  110/250, train loss 0.487021 in 0.11s\n",
      " [-] epoch  111/250, train loss 0.500749 in 0.08s\n",
      " [-] epoch  112/250, train loss 0.510135 in 0.10s\n",
      " [-] epoch  113/250, train loss 0.522942 in 0.06s\n",
      " [-] epoch  114/250, train loss 0.517541 in 0.08s\n",
      " [-] epoch  115/250, train loss 0.505516 in 0.10s\n",
      " [-] epoch  116/250, train loss 0.513723 in 0.08s\n",
      " [-] epoch  117/250, train loss 0.495096 in 0.10s\n",
      " [-] epoch  118/250, train loss 0.497355 in 0.09s\n",
      " [-] epoch  119/250, train loss 0.500387 in 0.08s\n",
      " [-] epoch  120/250, train loss 0.528712 in 0.09s\n",
      " [-] epoch  121/250, train loss 0.522168 in 0.06s\n",
      " [-] epoch  122/250, train loss 0.508532 in 0.09s\n",
      " [-] epoch  123/250, train loss 0.506250 in 0.11s\n",
      " [-] epoch  124/250, train loss 0.493235 in 0.11s\n",
      " [-] epoch  125/250, train loss 0.500655 in 0.10s\n",
      " [-] epoch  126/250, train loss 0.522181 in 0.09s\n",
      " [-] epoch  127/250, train loss 0.518288 in 0.10s\n",
      " [-] epoch  128/250, train loss 0.488999 in 0.09s\n",
      " [-] epoch  129/250, train loss 0.491217 in 0.10s\n",
      " [-] epoch  130/250, train loss 0.488746 in 0.08s\n",
      " [-] epoch  131/250, train loss 0.500515 in 0.09s\n",
      " [-] epoch  132/250, train loss 0.499985 in 0.09s\n",
      " [-] epoch  133/250, train loss 0.469915 in 0.11s\n",
      " [-] epoch  134/250, train loss 0.513526 in 0.08s\n",
      " [-] epoch  135/250, train loss 0.478495 in 0.09s\n",
      " [-] epoch  136/250, train loss 0.508709 in 0.11s\n",
      " [-] epoch  137/250, train loss 0.477570 in 0.09s\n",
      " [-] epoch  138/250, train loss 0.527115 in 0.11s\n",
      " [-] epoch  139/250, train loss 0.473829 in 0.09s\n",
      " [-] epoch  140/250, train loss 0.503847 in 0.09s\n",
      " [-] epoch  141/250, train loss 0.491962 in 0.11s\n",
      " [-] epoch  142/250, train loss 0.479796 in 0.10s\n",
      " [-] epoch  143/250, train loss 0.500223 in 0.09s\n",
      " [-] epoch  144/250, train loss 0.471775 in 0.06s\n",
      " [-] epoch  145/250, train loss 0.501562 in 0.09s\n",
      " [-] epoch  146/250, train loss 0.508979 in 0.12s\n",
      " [-] epoch  147/250, train loss 0.499591 in 0.06s\n",
      " [-] epoch  148/250, train loss 0.494599 in 0.09s\n",
      " [-] epoch  149/250, train loss 0.517672 in 0.10s\n",
      " [-] epoch  150/250, train loss 0.492257 in 0.11s\n",
      " [-] epoch  151/250, train loss 0.478965 in 0.07s\n",
      " [-] epoch  152/250, train loss 0.490218 in 0.08s\n",
      " [-] epoch  153/250, train loss 0.510470 in 0.10s\n",
      " [-] epoch  154/250, train loss 0.487475 in 0.06s\n",
      " [-] epoch  155/250, train loss 0.494817 in 0.10s\n",
      " [-] epoch  156/250, train loss 0.478610 in 0.10s\n",
      " [-] epoch  157/250, train loss 0.466374 in 0.09s\n",
      " [-] epoch  158/250, train loss 0.472017 in 0.07s\n",
      " [-] epoch  159/250, train loss 0.475916 in 0.08s\n",
      " [-] epoch  160/250, train loss 0.525616 in 0.11s\n",
      " [-] epoch  161/250, train loss 0.498302 in 0.10s\n",
      " [-] epoch  162/250, train loss 0.487431 in 0.06s\n",
      " [-] epoch  163/250, train loss 0.491665 in 0.08s\n",
      " [-] epoch  164/250, train loss 0.467268 in 0.10s\n",
      " [-] epoch  165/250, train loss 0.459086 in 0.06s\n",
      " [-] epoch  166/250, train loss 0.513601 in 0.10s\n",
      " [-] epoch  167/250, train loss 0.483698 in 0.10s\n",
      " [-] epoch  168/250, train loss 0.490125 in 0.10s\n",
      " [-] epoch  169/250, train loss 0.475569 in 0.09s\n",
      " [-] epoch  170/250, train loss 0.498784 in 0.06s\n",
      " [-] epoch  171/250, train loss 0.477623 in 0.09s\n",
      " [-] epoch  172/250, train loss 0.499893 in 0.10s\n",
      " [-] epoch  173/250, train loss 0.460555 in 0.08s\n",
      " [-] epoch  174/250, train loss 0.496658 in 0.09s\n",
      " [-] epoch  175/250, train loss 0.473447 in 0.10s\n",
      " [-] epoch  176/250, train loss 0.490751 in 0.08s\n",
      " [-] epoch  177/250, train loss 0.492216 in 0.08s\n",
      " [-] epoch  178/250, train loss 0.458989 in 0.09s\n",
      " [-] epoch  179/250, train loss 0.484610 in 0.10s\n",
      " [-] epoch  180/250, train loss 0.480573 in 0.09s\n",
      " [-] epoch  181/250, train loss 0.489633 in 0.07s\n",
      " [-] epoch  182/250, train loss 0.492213 in 0.09s\n",
      " [-] epoch  183/250, train loss 0.482570 in 0.08s\n",
      " [-] epoch  184/250, train loss 0.469171 in 0.09s\n",
      " [-] epoch  185/250, train loss 0.479908 in 0.09s\n",
      " [-] epoch  186/250, train loss 0.499308 in 0.07s\n",
      " [-] epoch  187/250, train loss 0.482410 in 0.09s\n",
      " [-] epoch  188/250, train loss 0.493824 in 0.12s\n",
      " [-] epoch  189/250, train loss 0.502993 in 0.10s\n",
      " [-] epoch  190/250, train loss 0.492371 in 0.09s\n",
      " [-] epoch  191/250, train loss 0.508762 in 0.07s\n",
      " [-] epoch  192/250, train loss 0.483907 in 0.09s\n",
      " [-] epoch  193/250, train loss 0.461894 in 0.09s\n",
      " [-] epoch  194/250, train loss 0.474949 in 0.09s\n",
      " [-] epoch  195/250, train loss 0.453073 in 0.08s\n",
      " [-] epoch  196/250, train loss 0.474096 in 0.08s\n",
      " [-] epoch  197/250, train loss 0.462492 in 0.10s\n",
      " [-] epoch  198/250, train loss 0.498085 in 0.08s\n",
      " [-] epoch  199/250, train loss 0.475669 in 0.10s\n",
      " [-] epoch  200/250, train loss 0.477698 in 0.09s\n",
      " [-] epoch  201/250, train loss 0.506601 in 0.10s\n",
      " [-] epoch  202/250, train loss 0.471080 in 0.10s\n",
      " [-] epoch  203/250, train loss 0.471026 in 0.09s\n",
      " [-] epoch  204/250, train loss 0.490527 in 0.10s\n",
      " [-] epoch  205/250, train loss 0.498989 in 0.11s\n",
      " [-] epoch  206/250, train loss 0.483846 in 0.09s\n",
      " [-] epoch  207/250, train loss 0.460730 in 0.10s\n",
      " [-] epoch  208/250, train loss 0.481615 in 0.09s\n",
      " [-] epoch  209/250, train loss 0.476905 in 0.08s\n",
      " [-] epoch  210/250, train loss 0.477738 in 0.10s\n",
      " [-] epoch  211/250, train loss 0.458190 in 0.09s\n",
      " [-] epoch  212/250, train loss 0.483242 in 0.10s\n",
      " [-] epoch  213/250, train loss 0.477816 in 0.10s\n",
      " [-] epoch  214/250, train loss 0.463208 in 0.06s\n",
      " [-] epoch  215/250, train loss 0.455682 in 0.11s\n",
      " [-] epoch  216/250, train loss 0.490242 in 0.10s\n",
      " [-] epoch  217/250, train loss 0.459744 in 0.07s\n",
      " [-] epoch  218/250, train loss 0.477127 in 0.11s\n",
      " [-] epoch  219/250, train loss 0.453399 in 0.10s\n",
      " [-] epoch  220/250, train loss 0.462557 in 0.09s\n",
      " [-] epoch  221/250, train loss 0.455965 in 0.09s\n",
      " [-] epoch  222/250, train loss 0.460976 in 0.10s\n",
      " [-] epoch  223/250, train loss 0.458275 in 0.08s\n",
      " [-] epoch  224/250, train loss 0.478187 in 0.10s\n",
      " [-] epoch  225/250, train loss 0.447902 in 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.448957 in 0.10s\n",
      " [-] epoch  227/250, train loss 0.473046 in 0.09s\n",
      " [-] epoch  228/250, train loss 0.435853 in 0.09s\n",
      " [-] epoch  229/250, train loss 0.457609 in 0.11s\n",
      " [-] epoch  230/250, train loss 0.474849 in 0.09s\n",
      " [-] epoch  231/250, train loss 0.470169 in 0.08s\n",
      " [-] epoch  232/250, train loss 0.453836 in 0.09s\n",
      " [-] epoch  233/250, train loss 0.455011 in 0.09s\n",
      " [-] epoch  234/250, train loss 0.468605 in 0.08s\n",
      " [-] epoch  235/250, train loss 0.482848 in 0.09s\n",
      " [-] epoch  236/250, train loss 0.457883 in 0.06s\n",
      " [-] epoch  237/250, train loss 0.428624 in 0.08s\n",
      " [-] epoch  238/250, train loss 0.480946 in 0.08s\n",
      " [-] epoch  239/250, train loss 0.468535 in 0.07s\n",
      " [-] epoch  240/250, train loss 0.465419 in 0.10s\n",
      " [-] epoch  241/250, train loss 0.441897 in 0.08s\n",
      " [-] epoch  242/250, train loss 0.470704 in 0.08s\n",
      " [-] epoch  243/250, train loss 0.469508 in 0.10s\n",
      " [-] epoch  244/250, train loss 0.471745 in 0.08s\n",
      " [-] epoch  245/250, train loss 0.443746 in 0.11s\n",
      " [-] epoch  246/250, train loss 0.489155 in 0.08s\n",
      " [-] epoch  247/250, train loss 0.474084 in 0.11s\n",
      " [-] epoch  248/250, train loss 0.462145 in 0.07s\n",
      " [-] epoch  249/250, train loss 0.467882 in 0.08s\n",
      " [-] epoch  250/250, train loss 0.476367 in 0.11s\n",
      " [-] test acc. 60.277778%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.693873 in 0.08s\n",
      " [-] epoch    2/250, train loss 0.622055 in 0.09s\n",
      " [-] epoch    3/250, train loss 0.624390 in 0.11s\n",
      " [-] epoch    4/250, train loss 0.634040 in 0.08s\n",
      " [-] epoch    5/250, train loss 0.587295 in 0.09s\n",
      " [-] epoch    6/250, train loss 0.598141 in 0.07s\n",
      " [-] epoch    7/250, train loss 0.583117 in 0.11s\n",
      " [-] epoch    8/250, train loss 0.573266 in 0.09s\n",
      " [-] epoch    9/250, train loss 0.604054 in 0.12s\n",
      " [-] epoch   10/250, train loss 0.579151 in 0.08s\n",
      " [-] epoch   11/250, train loss 0.567362 in 0.11s\n",
      " [-] epoch   12/250, train loss 0.563180 in 0.10s\n",
      " [-] epoch   13/250, train loss 0.570402 in 0.10s\n",
      " [-] epoch   14/250, train loss 0.536371 in 0.10s\n",
      " [-] epoch   15/250, train loss 0.570182 in 0.08s\n",
      " [-] epoch   16/250, train loss 0.572612 in 0.07s\n",
      " [-] epoch   17/250, train loss 0.568373 in 0.08s\n",
      " [-] epoch   18/250, train loss 0.579552 in 0.09s\n",
      " [-] epoch   19/250, train loss 0.546073 in 0.10s\n",
      " [-] epoch   20/250, train loss 0.557758 in 0.09s\n",
      " [-] epoch   21/250, train loss 0.553588 in 0.12s\n",
      " [-] epoch   22/250, train loss 0.554560 in 0.08s\n",
      " [-] epoch   23/250, train loss 0.546288 in 0.12s\n",
      " [-] epoch   24/250, train loss 0.553254 in 0.09s\n",
      " [-] epoch   25/250, train loss 0.541755 in 0.12s\n",
      " [-] epoch   26/250, train loss 0.573089 in 0.10s\n",
      " [-] epoch   27/250, train loss 0.562381 in 0.09s\n",
      " [-] epoch   28/250, train loss 0.560041 in 0.09s\n",
      " [-] epoch   29/250, train loss 0.567237 in 0.10s\n",
      " [-] epoch   30/250, train loss 0.558370 in 0.10s\n",
      " [-] epoch   31/250, train loss 0.528843 in 0.09s\n",
      " [-] epoch   32/250, train loss 0.540607 in 0.12s\n",
      " [-] epoch   33/250, train loss 0.543368 in 0.10s\n",
      " [-] epoch   34/250, train loss 0.543493 in 0.10s\n",
      " [-] epoch   35/250, train loss 0.530353 in 0.10s\n",
      " [-] epoch   36/250, train loss 0.568861 in 0.10s\n",
      " [-] epoch   37/250, train loss 0.543116 in 0.09s\n",
      " [-] epoch   38/250, train loss 0.536867 in 0.08s\n",
      " [-] epoch   39/250, train loss 0.521266 in 0.11s\n",
      " [-] epoch   40/250, train loss 0.520359 in 0.08s\n",
      " [-] epoch   41/250, train loss 0.526786 in 0.10s\n",
      " [-] epoch   42/250, train loss 0.518387 in 0.09s\n",
      " [-] epoch   43/250, train loss 0.532320 in 0.09s\n",
      " [-] epoch   44/250, train loss 0.526554 in 0.08s\n",
      " [-] epoch   45/250, train loss 0.553763 in 0.09s\n",
      " [-] epoch   46/250, train loss 0.515671 in 0.08s\n",
      " [-] epoch   47/250, train loss 0.518869 in 0.11s\n",
      " [-] epoch   48/250, train loss 0.543460 in 0.10s\n",
      " [-] epoch   49/250, train loss 0.565202 in 0.08s\n",
      " [-] epoch   50/250, train loss 0.529324 in 0.09s\n",
      " [-] epoch   51/250, train loss 0.537438 in 0.10s\n",
      " [-] epoch   52/250, train loss 0.495848 in 0.10s\n",
      " [-] epoch   53/250, train loss 0.553156 in 0.08s\n",
      " [-] epoch   54/250, train loss 0.514955 in 0.11s\n",
      " [-] epoch   55/250, train loss 0.532194 in 0.09s\n",
      " [-] epoch   56/250, train loss 0.527767 in 0.08s\n",
      " [-] epoch   57/250, train loss 0.518002 in 0.07s\n",
      " [-] epoch   58/250, train loss 0.526497 in 0.08s\n",
      " [-] epoch   59/250, train loss 0.518532 in 0.10s\n",
      " [-] epoch   60/250, train loss 0.530846 in 0.11s\n",
      " [-] epoch   61/250, train loss 0.543249 in 0.09s\n",
      " [-] epoch   62/250, train loss 0.511424 in 0.10s\n",
      " [-] epoch   63/250, train loss 0.504529 in 0.09s\n",
      " [-] epoch   64/250, train loss 0.508256 in 0.06s\n",
      " [-] epoch   65/250, train loss 0.535092 in 0.11s\n",
      " [-] epoch   66/250, train loss 0.507951 in 0.09s\n",
      " [-] epoch   67/250, train loss 0.517700 in 0.09s\n",
      " [-] epoch   68/250, train loss 0.504565 in 0.08s\n",
      " [-] epoch   69/250, train loss 0.511774 in 0.07s\n",
      " [-] epoch   70/250, train loss 0.497402 in 0.10s\n",
      " [-] epoch   71/250, train loss 0.500878 in 0.06s\n",
      " [-] epoch   72/250, train loss 0.517794 in 0.06s\n",
      " [-] epoch   73/250, train loss 0.508374 in 0.09s\n",
      " [-] epoch   74/250, train loss 0.514028 in 0.11s\n",
      " [-] epoch   75/250, train loss 0.502566 in 0.10s\n",
      " [-] epoch   76/250, train loss 0.515083 in 0.08s\n",
      " [-] epoch   77/250, train loss 0.523928 in 0.08s\n",
      " [-] epoch   78/250, train loss 0.484913 in 0.09s\n",
      " [-] epoch   79/250, train loss 0.483047 in 0.11s\n",
      " [-] epoch   80/250, train loss 0.477871 in 0.09s\n",
      " [-] epoch   81/250, train loss 0.479872 in 0.07s\n",
      " [-] epoch   82/250, train loss 0.486271 in 0.08s\n",
      " [-] epoch   83/250, train loss 0.468658 in 0.11s\n",
      " [-] epoch   84/250, train loss 0.495835 in 0.11s\n",
      " [-] epoch   85/250, train loss 0.494228 in 0.06s\n",
      " [-] epoch   86/250, train loss 0.517269 in 0.12s\n",
      " [-] epoch   87/250, train loss 0.512962 in 0.08s\n",
      " [-] epoch   88/250, train loss 0.499333 in 0.11s\n",
      " [-] epoch   89/250, train loss 0.502143 in 0.11s\n",
      " [-] epoch   90/250, train loss 0.523000 in 0.10s\n",
      " [-] epoch   91/250, train loss 0.498379 in 0.11s\n",
      " [-] epoch   92/250, train loss 0.493313 in 0.11s\n",
      " [-] epoch   93/250, train loss 0.477590 in 0.11s\n",
      " [-] epoch   94/250, train loss 0.500685 in 0.11s\n",
      " [-] epoch   95/250, train loss 0.475690 in 0.13s\n",
      " [-] epoch   96/250, train loss 0.495209 in 0.10s\n",
      " [-] epoch   97/250, train loss 0.484296 in 0.10s\n",
      " [-] epoch   98/250, train loss 0.503687 in 0.10s\n",
      " [-] epoch   99/250, train loss 0.470326 in 0.10s\n",
      " [-] epoch  100/250, train loss 0.470734 in 0.09s\n",
      " [-] epoch  101/250, train loss 0.465403 in 0.06s\n",
      " [-] epoch  102/250, train loss 0.495085 in 0.09s\n",
      " [-] epoch  103/250, train loss 0.473163 in 0.08s\n",
      " [-] epoch  104/250, train loss 0.485358 in 0.11s\n",
      " [-] epoch  105/250, train loss 0.501832 in 0.09s\n",
      " [-] epoch  106/250, train loss 0.470767 in 0.10s\n",
      " [-] epoch  107/250, train loss 0.477420 in 0.11s\n",
      " [-] epoch  108/250, train loss 0.472229 in 0.10s\n",
      " [-] epoch  109/250, train loss 0.462649 in 0.08s\n",
      " [-] epoch  110/250, train loss 0.497751 in 0.12s\n",
      " [-] epoch  111/250, train loss 0.499487 in 0.09s\n",
      " [-] epoch  112/250, train loss 0.514666 in 0.10s\n",
      " [-] epoch  113/250, train loss 0.477562 in 0.11s\n",
      " [-] epoch  114/250, train loss 0.501402 in 0.09s\n",
      " [-] epoch  115/250, train loss 0.478358 in 0.08s\n",
      " [-] epoch  116/250, train loss 0.496483 in 0.09s\n",
      " [-] epoch  117/250, train loss 0.476379 in 0.09s\n",
      " [-] epoch  118/250, train loss 0.477725 in 0.09s\n",
      " [-] epoch  119/250, train loss 0.497402 in 0.11s\n",
      " [-] epoch  120/250, train loss 0.485325 in 0.09s\n",
      " [-] epoch  121/250, train loss 0.470894 in 0.10s\n",
      " [-] epoch  122/250, train loss 0.507942 in 0.10s\n",
      " [-] epoch  123/250, train loss 0.476002 in 0.10s\n",
      " [-] epoch  124/250, train loss 0.479178 in 0.10s\n",
      " [-] epoch  125/250, train loss 0.470373 in 0.10s\n",
      " [-] epoch  126/250, train loss 0.474382 in 0.10s\n",
      " [-] epoch  127/250, train loss 0.469639 in 0.10s\n",
      " [-] epoch  128/250, train loss 0.476145 in 0.11s\n",
      " [-] epoch  129/250, train loss 0.484836 in 0.10s\n",
      " [-] epoch  130/250, train loss 0.486673 in 0.10s\n",
      " [-] epoch  131/250, train loss 0.447486 in 0.10s\n",
      " [-] epoch  132/250, train loss 0.494960 in 0.09s\n",
      " [-] epoch  133/250, train loss 0.473958 in 0.10s\n",
      " [-] epoch  134/250, train loss 0.469699 in 0.09s\n",
      " [-] epoch  135/250, train loss 0.466644 in 0.10s\n",
      " [-] epoch  136/250, train loss 0.486188 in 0.10s\n",
      " [-] epoch  137/250, train loss 0.470551 in 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  138/250, train loss 0.485694 in 0.10s\n",
      " [-] epoch  139/250, train loss 0.488929 in 0.10s\n",
      " [-] epoch  140/250, train loss 0.483273 in 0.11s\n",
      " [-] epoch  141/250, train loss 0.479674 in 0.10s\n",
      " [-] epoch  142/250, train loss 0.440939 in 0.10s\n",
      " [-] epoch  143/250, train loss 0.463177 in 0.10s\n",
      " [-] epoch  144/250, train loss 0.472081 in 0.10s\n",
      " [-] epoch  145/250, train loss 0.449436 in 0.09s\n",
      " [-] epoch  146/250, train loss 0.469585 in 0.10s\n",
      " [-] epoch  147/250, train loss 0.463137 in 0.10s\n",
      " [-] epoch  148/250, train loss 0.467003 in 0.09s\n",
      " [-] epoch  149/250, train loss 0.462374 in 0.10s\n",
      " [-] epoch  150/250, train loss 0.469984 in 0.10s\n",
      " [-] epoch  151/250, train loss 0.455460 in 0.10s\n",
      " [-] epoch  152/250, train loss 0.461901 in 0.10s\n",
      " [-] epoch  153/250, train loss 0.447965 in 0.10s\n",
      " [-] epoch  154/250, train loss 0.455865 in 0.10s\n",
      " [-] epoch  155/250, train loss 0.476988 in 0.10s\n",
      " [-] epoch  156/250, train loss 0.443513 in 0.10s\n",
      " [-] epoch  157/250, train loss 0.456356 in 0.10s\n",
      " [-] epoch  158/250, train loss 0.439522 in 0.09s\n",
      " [-] epoch  159/250, train loss 0.482903 in 0.10s\n",
      " [-] epoch  160/250, train loss 0.455757 in 0.10s\n",
      " [-] epoch  161/250, train loss 0.449882 in 0.09s\n",
      " [-] epoch  162/250, train loss 0.474383 in 0.10s\n",
      " [-] epoch  163/250, train loss 0.456979 in 0.10s\n",
      " [-] epoch  164/250, train loss 0.446484 in 0.10s\n",
      " [-] epoch  165/250, train loss 0.442090 in 0.10s\n",
      " [-] epoch  166/250, train loss 0.461712 in 0.10s\n",
      " [-] epoch  167/250, train loss 0.421479 in 0.09s\n",
      " [-] epoch  168/250, train loss 0.443969 in 0.09s\n",
      " [-] epoch  169/250, train loss 0.453313 in 0.10s\n",
      " [-] epoch  170/250, train loss 0.448971 in 0.09s\n",
      " [-] epoch  171/250, train loss 0.418429 in 0.10s\n",
      " [-] epoch  172/250, train loss 0.473770 in 0.09s\n",
      " [-] epoch  173/250, train loss 0.465801 in 0.10s\n",
      " [-] epoch  174/250, train loss 0.444713 in 0.09s\n",
      " [-] epoch  175/250, train loss 0.448860 in 0.12s\n",
      " [-] epoch  176/250, train loss 0.437044 in 0.08s\n",
      " [-] epoch  177/250, train loss 0.489436 in 0.10s\n",
      " [-] epoch  178/250, train loss 0.443532 in 0.08s\n",
      " [-] epoch  179/250, train loss 0.448306 in 0.09s\n",
      " [-] epoch  180/250, train loss 0.445723 in 0.11s\n",
      " [-] epoch  181/250, train loss 0.453379 in 0.10s\n",
      " [-] epoch  182/250, train loss 0.440975 in 0.09s\n",
      " [-] epoch  183/250, train loss 0.467307 in 0.10s\n",
      " [-] epoch  184/250, train loss 0.427238 in 0.09s\n",
      " [-] epoch  185/250, train loss 0.429332 in 0.10s\n",
      " [-] epoch  186/250, train loss 0.408967 in 0.10s\n",
      " [-] epoch  187/250, train loss 0.434764 in 0.10s\n",
      " [-] epoch  188/250, train loss 0.464608 in 0.10s\n",
      " [-] epoch  189/250, train loss 0.430871 in 0.10s\n",
      " [-] epoch  190/250, train loss 0.450076 in 0.10s\n",
      " [-] epoch  191/250, train loss 0.475465 in 0.10s\n",
      " [-] epoch  192/250, train loss 0.430947 in 0.10s\n",
      " [-] epoch  193/250, train loss 0.480238 in 0.11s\n",
      " [-] epoch  194/250, train loss 0.445544 in 0.10s\n",
      " [-] epoch  195/250, train loss 0.433230 in 0.10s\n",
      " [-] epoch  196/250, train loss 0.420766 in 0.10s\n",
      " [-] epoch  197/250, train loss 0.441886 in 0.10s\n",
      " [-] epoch  198/250, train loss 0.440877 in 0.10s\n",
      " [-] epoch  199/250, train loss 0.422964 in 0.09s\n",
      " [-] epoch  200/250, train loss 0.436059 in 0.09s\n",
      " [-] epoch  201/250, train loss 0.443493 in 0.09s\n",
      " [-] epoch  202/250, train loss 0.448445 in 0.09s\n",
      " [-] epoch  203/250, train loss 0.406480 in 0.09s\n",
      " [-] epoch  204/250, train loss 0.448630 in 0.10s\n",
      " [-] epoch  205/250, train loss 0.455718 in 0.11s\n",
      " [-] epoch  206/250, train loss 0.444015 in 0.10s\n",
      " [-] epoch  207/250, train loss 0.416801 in 0.11s\n",
      " [-] epoch  208/250, train loss 0.425078 in 0.10s\n",
      " [-] epoch  209/250, train loss 0.462257 in 0.11s\n",
      " [-] epoch  210/250, train loss 0.442801 in 0.10s\n",
      " [-] epoch  211/250, train loss 0.460051 in 0.11s\n",
      " [-] epoch  212/250, train loss 0.440577 in 0.09s\n",
      " [-] epoch  213/250, train loss 0.446160 in 0.09s\n",
      " [-] epoch  214/250, train loss 0.444596 in 0.10s\n",
      " [-] epoch  215/250, train loss 0.457043 in 0.10s\n",
      " [-] epoch  216/250, train loss 0.419921 in 0.10s\n",
      " [-] epoch  217/250, train loss 0.421300 in 0.09s\n",
      " [-] epoch  218/250, train loss 0.441929 in 0.10s\n",
      " [-] epoch  219/250, train loss 0.418295 in 0.10s\n",
      " [-] epoch  220/250, train loss 0.425345 in 0.11s\n",
      " [-] epoch  221/250, train loss 0.452549 in 0.10s\n",
      " [-] epoch  222/250, train loss 0.449853 in 0.10s\n",
      " [-] epoch  223/250, train loss 0.417551 in 0.11s\n",
      " [-] epoch  224/250, train loss 0.438993 in 0.10s\n",
      " [-] epoch  225/250, train loss 0.466072 in 0.11s\n",
      " [-] epoch  226/250, train loss 0.426556 in 0.10s\n",
      " [-] epoch  227/250, train loss 0.463693 in 0.09s\n",
      " [-] epoch  228/250, train loss 0.423761 in 0.08s\n",
      " [-] epoch  229/250, train loss 0.429109 in 0.11s\n",
      " [-] epoch  230/250, train loss 0.422671 in 0.07s\n",
      " [-] epoch  231/250, train loss 0.425623 in 0.12s\n",
      " [-] epoch  232/250, train loss 0.435444 in 0.11s\n",
      " [-] epoch  233/250, train loss 0.458777 in 0.09s\n",
      " [-] epoch  234/250, train loss 0.383754 in 0.08s\n",
      " [-] epoch  235/250, train loss 0.438723 in 0.10s\n",
      " [-] epoch  236/250, train loss 0.412495 in 0.08s\n",
      " [-] epoch  237/250, train loss 0.420983 in 0.09s\n",
      " [-] epoch  238/250, train loss 0.428496 in 0.11s\n",
      " [-] epoch  239/250, train loss 0.439901 in 0.09s\n",
      " [-] epoch  240/250, train loss 0.394737 in 0.11s\n",
      " [-] epoch  241/250, train loss 0.425092 in 0.10s\n",
      " [-] epoch  242/250, train loss 0.393427 in 0.09s\n",
      " [-] epoch  243/250, train loss 0.408154 in 0.10s\n",
      " [-] epoch  244/250, train loss 0.417863 in 0.10s\n",
      " [-] epoch  245/250, train loss 0.410976 in 0.10s\n",
      " [-] epoch  246/250, train loss 0.408696 in 0.10s\n",
      " [-] epoch  247/250, train loss 0.440358 in 0.09s\n",
      " [-] epoch  248/250, train loss 0.449648 in 0.08s\n",
      " [-] epoch  249/250, train loss 0.414735 in 0.09s\n",
      " [-] epoch  250/250, train loss 0.407762 in 0.10s\n",
      " [-] test acc. 66.666667%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.731728 in 0.12s\n",
      " [-] epoch    2/250, train loss 0.664856 in 0.13s\n",
      " [-] epoch    3/250, train loss 0.631958 in 0.13s\n",
      " [-] epoch    4/250, train loss 0.610029 in 0.12s\n",
      " [-] epoch    5/250, train loss 0.600741 in 0.11s\n",
      " [-] epoch    6/250, train loss 0.605326 in 0.13s\n",
      " [-] epoch    7/250, train loss 0.600206 in 0.10s\n",
      " [-] epoch    8/250, train loss 0.597576 in 0.13s\n",
      " [-] epoch    9/250, train loss 0.583435 in 0.13s\n",
      " [-] epoch   10/250, train loss 0.594109 in 0.12s\n",
      " [-] epoch   11/250, train loss 0.565023 in 0.12s\n",
      " [-] epoch   12/250, train loss 0.552912 in 0.13s\n",
      " [-] epoch   13/250, train loss 0.555957 in 0.12s\n",
      " [-] epoch   14/250, train loss 0.603069 in 0.12s\n",
      " [-] epoch   15/250, train loss 0.545129 in 0.14s\n",
      " [-] epoch   16/250, train loss 0.561678 in 0.12s\n",
      " [-] epoch   17/250, train loss 0.563716 in 0.11s\n",
      " [-] epoch   18/250, train loss 0.541179 in 0.10s\n",
      " [-] epoch   19/250, train loss 0.567727 in 0.14s\n",
      " [-] epoch   20/250, train loss 0.538005 in 0.13s\n",
      " [-] epoch   21/250, train loss 0.560033 in 0.12s\n",
      " [-] epoch   22/250, train loss 0.550413 in 0.13s\n",
      " [-] epoch   23/250, train loss 0.535003 in 0.11s\n",
      " [-] epoch   24/250, train loss 0.534025 in 0.11s\n",
      " [-] epoch   25/250, train loss 0.546608 in 0.11s\n",
      " [-] epoch   26/250, train loss 0.550320 in 0.12s\n",
      " [-] epoch   27/250, train loss 0.536848 in 0.13s\n",
      " [-] epoch   28/250, train loss 0.538883 in 0.10s\n",
      " [-] epoch   29/250, train loss 0.542687 in 0.10s\n",
      " [-] epoch   30/250, train loss 0.532798 in 0.14s\n",
      " [-] epoch   31/250, train loss 0.537540 in 0.13s\n",
      " [-] epoch   32/250, train loss 0.540416 in 0.11s\n",
      " [-] epoch   33/250, train loss 0.563096 in 0.13s\n",
      " [-] epoch   34/250, train loss 0.531653 in 0.12s\n",
      " [-] epoch   35/250, train loss 0.512506 in 0.13s\n",
      " [-] epoch   36/250, train loss 0.531790 in 0.13s\n",
      " [-] epoch   37/250, train loss 0.526670 in 0.11s\n",
      " [-] epoch   38/250, train loss 0.510823 in 0.12s\n",
      " [-] epoch   39/250, train loss 0.541509 in 0.13s\n",
      " [-] epoch   40/250, train loss 0.512892 in 0.10s\n",
      " [-] epoch   41/250, train loss 0.510059 in 0.13s\n",
      " [-] epoch   42/250, train loss 0.512270 in 0.12s\n",
      " [-] epoch   43/250, train loss 0.532712 in 0.12s\n",
      " [-] epoch   44/250, train loss 0.526852 in 0.13s\n",
      " [-] epoch   45/250, train loss 0.542255 in 0.12s\n",
      " [-] epoch   46/250, train loss 0.528614 in 0.12s\n",
      " [-] epoch   47/250, train loss 0.509668 in 0.12s\n",
      " [-] epoch   48/250, train loss 0.533371 in 0.13s\n",
      " [-] epoch   49/250, train loss 0.542604 in 0.12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.518758 in 0.11s\n",
      " [-] epoch   51/250, train loss 0.530759 in 0.11s\n",
      " [-] epoch   52/250, train loss 0.518516 in 0.13s\n",
      " [-] epoch   53/250, train loss 0.523403 in 0.13s\n",
      " [-] epoch   54/250, train loss 0.508058 in 0.10s\n",
      " [-] epoch   55/250, train loss 0.511509 in 0.12s\n",
      " [-] epoch   56/250, train loss 0.507567 in 0.10s\n",
      " [-] epoch   57/250, train loss 0.488933 in 0.10s\n",
      " [-] epoch   58/250, train loss 0.527835 in 0.12s\n",
      " [-] epoch   59/250, train loss 0.517019 in 0.14s\n",
      " [-] epoch   60/250, train loss 0.518882 in 0.12s\n",
      " [-] epoch   61/250, train loss 0.515167 in 0.13s\n",
      " [-] epoch   62/250, train loss 0.505473 in 0.12s\n",
      " [-] epoch   63/250, train loss 0.506838 in 0.11s\n",
      " [-] epoch   64/250, train loss 0.510054 in 0.12s\n",
      " [-] epoch   65/250, train loss 0.525933 in 0.13s\n",
      " [-] epoch   66/250, train loss 0.516125 in 0.14s\n",
      " [-] epoch   67/250, train loss 0.497706 in 0.11s\n",
      " [-] epoch   68/250, train loss 0.520885 in 0.10s\n",
      " [-] epoch   69/250, train loss 0.518188 in 0.11s\n",
      " [-] epoch   70/250, train loss 0.497533 in 0.14s\n",
      " [-] epoch   71/250, train loss 0.497352 in 0.11s\n",
      " [-] epoch   72/250, train loss 0.509581 in 0.11s\n",
      " [-] epoch   73/250, train loss 0.482027 in 0.13s\n",
      " [-] epoch   74/250, train loss 0.478341 in 0.12s\n",
      " [-] epoch   75/250, train loss 0.516584 in 0.12s\n",
      " [-] epoch   76/250, train loss 0.505791 in 0.13s\n",
      " [-] epoch   77/250, train loss 0.460943 in 0.12s\n",
      " [-] epoch   78/250, train loss 0.474650 in 0.12s\n",
      " [-] epoch   79/250, train loss 0.510981 in 0.13s\n",
      " [-] epoch   80/250, train loss 0.495362 in 0.13s\n",
      " [-] epoch   81/250, train loss 0.496751 in 0.13s\n",
      " [-] epoch   82/250, train loss 0.484881 in 0.11s\n",
      " [-] epoch   83/250, train loss 0.508239 in 0.08s\n",
      " [-] epoch   84/250, train loss 0.507317 in 0.13s\n",
      " [-] epoch   85/250, train loss 0.512645 in 0.09s\n",
      " [-] epoch   86/250, train loss 0.491175 in 0.09s\n",
      " [-] epoch   87/250, train loss 0.494556 in 0.09s\n",
      " [-] epoch   88/250, train loss 0.491950 in 0.13s\n",
      " [-] epoch   89/250, train loss 0.488437 in 0.10s\n",
      " [-] epoch   90/250, train loss 0.480559 in 0.14s\n",
      " [-] epoch   91/250, train loss 0.480669 in 0.12s\n",
      " [-] epoch   92/250, train loss 0.490186 in 0.10s\n",
      " [-] epoch   93/250, train loss 0.492149 in 0.12s\n",
      " [-] epoch   94/250, train loss 0.506479 in 0.09s\n",
      " [-] epoch   95/250, train loss 0.493378 in 0.11s\n",
      " [-] epoch   96/250, train loss 0.499179 in 0.13s\n",
      " [-] epoch   97/250, train loss 0.509766 in 0.13s\n",
      " [-] epoch   98/250, train loss 0.486100 in 0.12s\n",
      " [-] epoch   99/250, train loss 0.467606 in 0.07s\n",
      " [-] epoch  100/250, train loss 0.492948 in 0.13s\n",
      " [-] epoch  101/250, train loss 0.479268 in 0.11s\n",
      " [-] epoch  102/250, train loss 0.483545 in 0.09s\n",
      " [-] epoch  103/250, train loss 0.475278 in 0.12s\n",
      " [-] epoch  104/250, train loss 0.483739 in 0.12s\n",
      " [-] epoch  105/250, train loss 0.481627 in 0.11s\n",
      " [-] epoch  106/250, train loss 0.492903 in 0.14s\n",
      " [-] epoch  107/250, train loss 0.463598 in 0.11s\n",
      " [-] epoch  108/250, train loss 0.493996 in 0.11s\n",
      " [-] epoch  109/250, train loss 0.467795 in 0.11s\n",
      " [-] epoch  110/250, train loss 0.483303 in 0.13s\n",
      " [-] epoch  111/250, train loss 0.468380 in 0.13s\n",
      " [-] epoch  112/250, train loss 0.490613 in 0.13s\n",
      " [-] epoch  113/250, train loss 0.489720 in 0.14s\n",
      " [-] epoch  114/250, train loss 0.494592 in 0.14s\n",
      " [-] epoch  115/250, train loss 0.504766 in 0.13s\n",
      " [-] epoch  116/250, train loss 0.470843 in 0.11s\n",
      " [-] epoch  117/250, train loss 0.507841 in 0.13s\n",
      " [-] epoch  118/250, train loss 0.501156 in 0.11s\n",
      " [-] epoch  119/250, train loss 0.495204 in 0.14s\n",
      " [-] epoch  120/250, train loss 0.487105 in 0.12s\n",
      " [-] epoch  121/250, train loss 0.453000 in 0.11s\n",
      " [-] epoch  122/250, train loss 0.477748 in 0.11s\n",
      " [-] epoch  123/250, train loss 0.458295 in 0.11s\n",
      " [-] epoch  124/250, train loss 0.497882 in 0.12s\n",
      " [-] epoch  125/250, train loss 0.491923 in 0.13s\n",
      " [-] epoch  126/250, train loss 0.476311 in 0.12s\n",
      " [-] epoch  127/250, train loss 0.487593 in 0.09s\n",
      " [-] epoch  128/250, train loss 0.469043 in 0.09s\n",
      " [-] epoch  129/250, train loss 0.448810 in 0.10s\n",
      " [-] epoch  130/250, train loss 0.480623 in 0.11s\n",
      " [-] epoch  131/250, train loss 0.445012 in 0.10s\n",
      " [-] epoch  132/250, train loss 0.443854 in 0.12s\n",
      " [-] epoch  133/250, train loss 0.480331 in 0.13s\n",
      " [-] epoch  134/250, train loss 0.471175 in 0.12s\n",
      " [-] epoch  135/250, train loss 0.455611 in 0.11s\n",
      " [-] epoch  136/250, train loss 0.464218 in 0.14s\n",
      " [-] epoch  137/250, train loss 0.458679 in 0.09s\n",
      " [-] epoch  138/250, train loss 0.465573 in 0.09s\n",
      " [-] epoch  139/250, train loss 0.488138 in 0.12s\n",
      " [-] epoch  140/250, train loss 0.482773 in 0.16s\n",
      " [-] epoch  141/250, train loss 0.463114 in 0.12s\n",
      " [-] epoch  142/250, train loss 0.455300 in 0.11s\n",
      " [-] epoch  143/250, train loss 0.479994 in 0.10s\n",
      " [-] epoch  144/250, train loss 0.485728 in 0.12s\n",
      " [-] epoch  145/250, train loss 0.467575 in 0.09s\n",
      " [-] epoch  146/250, train loss 0.457641 in 0.11s\n",
      " [-] epoch  147/250, train loss 0.476545 in 0.12s\n",
      " [-] epoch  148/250, train loss 0.461598 in 0.11s\n",
      " [-] epoch  149/250, train loss 0.458919 in 0.12s\n",
      " [-] epoch  150/250, train loss 0.463628 in 0.12s\n",
      " [-] epoch  151/250, train loss 0.445020 in 0.11s\n",
      " [-] epoch  152/250, train loss 0.475437 in 0.10s\n",
      " [-] epoch  153/250, train loss 0.456991 in 0.09s\n",
      " [-] epoch  154/250, train loss 0.447834 in 0.10s\n",
      " [-] epoch  155/250, train loss 0.450014 in 0.10s\n",
      " [-] epoch  156/250, train loss 0.445784 in 0.11s\n",
      " [-] epoch  157/250, train loss 0.463659 in 0.12s\n",
      " [-] epoch  158/250, train loss 0.467723 in 0.14s\n",
      " [-] epoch  159/250, train loss 0.485521 in 0.11s\n",
      " [-] epoch  160/250, train loss 0.430701 in 0.12s\n",
      " [-] epoch  161/250, train loss 0.506986 in 0.07s\n",
      " [-] epoch  162/250, train loss 0.453836 in 0.09s\n",
      " [-] epoch  163/250, train loss 0.434705 in 0.12s\n",
      " [-] epoch  164/250, train loss 0.443313 in 0.11s\n",
      " [-] epoch  165/250, train loss 0.464093 in 0.09s\n",
      " [-] epoch  166/250, train loss 0.450734 in 0.11s\n",
      " [-] epoch  167/250, train loss 0.440592 in 0.09s\n",
      " [-] epoch  168/250, train loss 0.458481 in 0.12s\n",
      " [-] epoch  169/250, train loss 0.445578 in 0.13s\n",
      " [-] epoch  170/250, train loss 0.465567 in 0.10s\n",
      " [-] epoch  171/250, train loss 0.449301 in 0.09s\n",
      " [-] epoch  172/250, train loss 0.463229 in 0.11s\n",
      " [-] epoch  173/250, train loss 0.430992 in 0.13s\n",
      " [-] epoch  174/250, train loss 0.413288 in 0.09s\n",
      " [-] epoch  175/250, train loss 0.416947 in 0.14s\n",
      " [-] epoch  176/250, train loss 0.461339 in 0.09s\n",
      " [-] epoch  177/250, train loss 0.461600 in 0.12s\n",
      " [-] epoch  178/250, train loss 0.443663 in 0.13s\n",
      " [-] epoch  179/250, train loss 0.458128 in 0.07s\n",
      " [-] epoch  180/250, train loss 0.473456 in 0.14s\n",
      " [-] epoch  181/250, train loss 0.415951 in 0.11s\n",
      " [-] epoch  182/250, train loss 0.442025 in 0.12s\n",
      " [-] epoch  183/250, train loss 0.433296 in 0.10s\n",
      " [-] epoch  184/250, train loss 0.449647 in 0.12s\n",
      " [-] epoch  185/250, train loss 0.448828 in 0.11s\n",
      " [-] epoch  186/250, train loss 0.452759 in 0.09s\n",
      " [-] epoch  187/250, train loss 0.444118 in 0.14s\n",
      " [-] epoch  188/250, train loss 0.471050 in 0.12s\n",
      " [-] epoch  189/250, train loss 0.447405 in 0.11s\n",
      " [-] epoch  190/250, train loss 0.425584 in 0.08s\n",
      " [-] epoch  191/250, train loss 0.454261 in 0.12s\n",
      " [-] epoch  192/250, train loss 0.465095 in 0.12s\n",
      " [-] epoch  193/250, train loss 0.434535 in 0.11s\n",
      " [-] epoch  194/250, train loss 0.429543 in 0.12s\n",
      " [-] epoch  195/250, train loss 0.430629 in 0.09s\n",
      " [-] epoch  196/250, train loss 0.448512 in 0.11s\n",
      " [-] epoch  197/250, train loss 0.435330 in 0.12s\n",
      " [-] epoch  198/250, train loss 0.456873 in 0.12s\n",
      " [-] epoch  199/250, train loss 0.417690 in 0.12s\n",
      " [-] epoch  200/250, train loss 0.444369 in 0.11s\n",
      " [-] epoch  201/250, train loss 0.453206 in 0.11s\n",
      " [-] epoch  202/250, train loss 0.439849 in 0.10s\n",
      " [-] epoch  203/250, train loss 0.424728 in 0.11s\n",
      " [-] epoch  204/250, train loss 0.424144 in 0.14s\n",
      " [-] epoch  205/250, train loss 0.426084 in 0.12s\n",
      " [-] epoch  206/250, train loss 0.437765 in 0.13s\n",
      " [-] epoch  207/250, train loss 0.399280 in 0.11s\n",
      " [-] epoch  208/250, train loss 0.459837 in 0.13s\n",
      " [-] epoch  209/250, train loss 0.423968 in 0.13s\n",
      " [-] epoch  210/250, train loss 0.472883 in 0.13s\n",
      " [-] epoch  211/250, train loss 0.408871 in 0.13s\n",
      " [-] epoch  212/250, train loss 0.442434 in 0.13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  213/250, train loss 0.440575 in 0.14s\n",
      " [-] epoch  214/250, train loss 0.433854 in 0.11s\n",
      " [-] epoch  215/250, train loss 0.421098 in 0.12s\n",
      " [-] epoch  216/250, train loss 0.429224 in 0.12s\n",
      " [-] epoch  217/250, train loss 0.435059 in 0.12s\n",
      " [-] epoch  218/250, train loss 0.424636 in 0.12s\n",
      " [-] epoch  219/250, train loss 0.401210 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.440567 in 0.13s\n",
      " [-] epoch  221/250, train loss 0.419834 in 0.13s\n",
      " [-] epoch  222/250, train loss 0.398456 in 0.11s\n",
      " [-] epoch  223/250, train loss 0.442772 in 0.13s\n",
      " [-] epoch  224/250, train loss 0.437491 in 0.11s\n",
      " [-] epoch  225/250, train loss 0.457696 in 0.13s\n",
      " [-] epoch  226/250, train loss 0.426642 in 0.13s\n",
      " [-] epoch  227/250, train loss 0.450474 in 0.13s\n",
      " [-] epoch  228/250, train loss 0.435738 in 0.11s\n",
      " [-] epoch  229/250, train loss 0.427730 in 0.11s\n",
      " [-] epoch  230/250, train loss 0.427362 in 0.13s\n",
      " [-] epoch  231/250, train loss 0.435524 in 0.13s\n",
      " [-] epoch  232/250, train loss 0.434076 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.432859 in 0.12s\n",
      " [-] epoch  234/250, train loss 0.421124 in 0.12s\n",
      " [-] epoch  235/250, train loss 0.445103 in 0.12s\n",
      " [-] epoch  236/250, train loss 0.441937 in 0.11s\n",
      " [-] epoch  237/250, train loss 0.413344 in 0.12s\n",
      " [-] epoch  238/250, train loss 0.420015 in 0.11s\n",
      " [-] epoch  239/250, train loss 0.419932 in 0.12s\n",
      " [-] epoch  240/250, train loss 0.395018 in 0.13s\n",
      " [-] epoch  241/250, train loss 0.405923 in 0.12s\n",
      " [-] epoch  242/250, train loss 0.422172 in 0.11s\n",
      " [-] epoch  243/250, train loss 0.422361 in 0.12s\n",
      " [-] epoch  244/250, train loss 0.426605 in 0.12s\n",
      " [-] epoch  245/250, train loss 0.411562 in 0.13s\n",
      " [-] epoch  246/250, train loss 0.421513 in 0.10s\n",
      " [-] epoch  247/250, train loss 0.444279 in 0.13s\n",
      " [-] epoch  248/250, train loss 0.424091 in 0.12s\n",
      " [-] epoch  249/250, train loss 0.420689 in 0.10s\n",
      " [-] epoch  250/250, train loss 0.418804 in 0.12s\n",
      " [-] test acc. 58.888889%\n",
      "Je vais utiliser 7 layers\n",
      " [-] epoch    1/250, train loss 0.668348 in 0.13s\n",
      " [-] epoch    2/250, train loss 0.627658 in 0.15s\n",
      " [-] epoch    3/250, train loss 0.605365 in 0.12s\n",
      " [-] epoch    4/250, train loss 0.615309 in 0.16s\n",
      " [-] epoch    5/250, train loss 0.589234 in 0.15s\n",
      " [-] epoch    6/250, train loss 0.590099 in 0.15s\n",
      " [-] epoch    7/250, train loss 0.580681 in 0.14s\n",
      " [-] epoch    8/250, train loss 0.568654 in 0.15s\n",
      " [-] epoch    9/250, train loss 0.585477 in 0.13s\n",
      " [-] epoch   10/250, train loss 0.575794 in 0.13s\n",
      " [-] epoch   11/250, train loss 0.571864 in 0.15s\n",
      " [-] epoch   12/250, train loss 0.552814 in 0.14s\n",
      " [-] epoch   13/250, train loss 0.562737 in 0.13s\n",
      " [-] epoch   14/250, train loss 0.568148 in 0.15s\n",
      " [-] epoch   15/250, train loss 0.552044 in 0.15s\n",
      " [-] epoch   16/250, train loss 0.544788 in 0.15s\n",
      " [-] epoch   17/250, train loss 0.555376 in 0.14s\n",
      " [-] epoch   18/250, train loss 0.531728 in 0.12s\n",
      " [-] epoch   19/250, train loss 0.569313 in 0.14s\n",
      " [-] epoch   20/250, train loss 0.560362 in 0.13s\n",
      " [-] epoch   21/250, train loss 0.549466 in 0.15s\n",
      " [-] epoch   22/250, train loss 0.532987 in 0.15s\n",
      " [-] epoch   23/250, train loss 0.536235 in 0.13s\n",
      " [-] epoch   24/250, train loss 0.548471 in 0.15s\n",
      " [-] epoch   25/250, train loss 0.544120 in 0.13s\n",
      " [-] epoch   26/250, train loss 0.517872 in 0.13s\n",
      " [-] epoch   27/250, train loss 0.563087 in 0.14s\n",
      " [-] epoch   28/250, train loss 0.518752 in 0.13s\n",
      " [-] epoch   29/250, train loss 0.535025 in 0.15s\n",
      " [-] epoch   30/250, train loss 0.530827 in 0.14s\n",
      " [-] epoch   31/250, train loss 0.535899 in 0.14s\n",
      " [-] epoch   32/250, train loss 0.538723 in 0.13s\n",
      " [-] epoch   33/250, train loss 0.570497 in 0.14s\n",
      " [-] epoch   34/250, train loss 0.531011 in 0.15s\n",
      " [-] epoch   35/250, train loss 0.557359 in 0.15s\n",
      " [-] epoch   36/250, train loss 0.529899 in 0.13s\n",
      " [-] epoch   37/250, train loss 0.528993 in 0.14s\n",
      " [-] epoch   38/250, train loss 0.507085 in 0.13s\n",
      " [-] epoch   39/250, train loss 0.521108 in 0.15s\n",
      " [-] epoch   40/250, train loss 0.531612 in 0.14s\n",
      " [-] epoch   41/250, train loss 0.527244 in 0.15s\n",
      " [-] epoch   42/250, train loss 0.541084 in 0.15s\n",
      " [-] epoch   43/250, train loss 0.528537 in 0.15s\n",
      " [-] epoch   44/250, train loss 0.520042 in 0.15s\n",
      " [-] epoch   45/250, train loss 0.523418 in 0.14s\n",
      " [-] epoch   46/250, train loss 0.508297 in 0.13s\n",
      " [-] epoch   47/250, train loss 0.514557 in 0.14s\n",
      " [-] epoch   48/250, train loss 0.530569 in 0.14s\n",
      " [-] epoch   49/250, train loss 0.519530 in 0.12s\n",
      " [-] epoch   50/250, train loss 0.537686 in 0.12s\n",
      " [-] epoch   51/250, train loss 0.514450 in 0.14s\n",
      " [-] epoch   52/250, train loss 0.510971 in 0.15s\n",
      " [-] epoch   53/250, train loss 0.504164 in 0.14s\n",
      " [-] epoch   54/250, train loss 0.512402 in 0.14s\n",
      " [-] epoch   55/250, train loss 0.496588 in 0.14s\n",
      " [-] epoch   56/250, train loss 0.525842 in 0.14s\n",
      " [-] epoch   57/250, train loss 0.510128 in 0.13s\n",
      " [-] epoch   58/250, train loss 0.533944 in 0.13s\n",
      " [-] epoch   59/250, train loss 0.510749 in 0.14s\n",
      " [-] epoch   60/250, train loss 0.500310 in 0.14s\n",
      " [-] epoch   61/250, train loss 0.500496 in 0.14s\n",
      " [-] epoch   62/250, train loss 0.519213 in 0.15s\n",
      " [-] epoch   63/250, train loss 0.537684 in 0.13s\n",
      " [-] epoch   64/250, train loss 0.505869 in 0.14s\n",
      " [-] epoch   65/250, train loss 0.479199 in 0.14s\n",
      " [-] epoch   66/250, train loss 0.498527 in 0.16s\n",
      " [-] epoch   67/250, train loss 0.545741 in 0.14s\n",
      " [-] epoch   68/250, train loss 0.508782 in 0.15s\n",
      " [-] epoch   69/250, train loss 0.523472 in 0.14s\n",
      " [-] epoch   70/250, train loss 0.510469 in 0.13s\n",
      " [-] epoch   71/250, train loss 0.507819 in 0.13s\n",
      " [-] epoch   72/250, train loss 0.512197 in 0.15s\n",
      " [-] epoch   73/250, train loss 0.487172 in 0.15s\n",
      " [-] epoch   74/250, train loss 0.496804 in 0.15s\n",
      " [-] epoch   75/250, train loss 0.482306 in 0.13s\n",
      " [-] epoch   76/250, train loss 0.534886 in 0.13s\n",
      " [-] epoch   77/250, train loss 0.494998 in 0.17s\n",
      " [-] epoch   78/250, train loss 0.511478 in 0.13s\n",
      " [-] epoch   79/250, train loss 0.493369 in 0.15s\n",
      " [-] epoch   80/250, train loss 0.477881 in 0.14s\n",
      " [-] epoch   81/250, train loss 0.511930 in 0.10s\n",
      " [-] epoch   82/250, train loss 0.492079 in 0.13s\n",
      " [-] epoch   83/250, train loss 0.518855 in 0.14s\n",
      " [-] epoch   84/250, train loss 0.518556 in 0.14s\n",
      " [-] epoch   85/250, train loss 0.477795 in 0.13s\n",
      " [-] epoch   86/250, train loss 0.498832 in 0.14s\n",
      " [-] epoch   87/250, train loss 0.476177 in 0.14s\n",
      " [-] epoch   88/250, train loss 0.495456 in 0.12s\n",
      " [-] epoch   89/250, train loss 0.452106 in 0.13s\n",
      " [-] epoch   90/250, train loss 0.482924 in 0.15s\n",
      " [-] epoch   91/250, train loss 0.471829 in 0.15s\n",
      " [-] epoch   92/250, train loss 0.434329 in 0.14s\n",
      " [-] epoch   93/250, train loss 0.511299 in 0.11s\n",
      " [-] epoch   94/250, train loss 0.468440 in 0.15s\n",
      " [-] epoch   95/250, train loss 0.488579 in 0.12s\n",
      " [-] epoch   96/250, train loss 0.480243 in 0.14s\n",
      " [-] epoch   97/250, train loss 0.467567 in 0.15s\n",
      " [-] epoch   98/250, train loss 0.491932 in 0.13s\n",
      " [-] epoch   99/250, train loss 0.474268 in 0.13s\n",
      " [-] epoch  100/250, train loss 0.478784 in 0.15s\n",
      " [-] epoch  101/250, train loss 0.477706 in 0.13s\n",
      " [-] epoch  102/250, train loss 0.504531 in 0.13s\n",
      " [-] epoch  103/250, train loss 0.465644 in 0.15s\n",
      " [-] epoch  104/250, train loss 0.480248 in 0.13s\n",
      " [-] epoch  105/250, train loss 0.474136 in 0.15s\n",
      " [-] epoch  106/250, train loss 0.443428 in 0.15s\n",
      " [-] epoch  107/250, train loss 0.465106 in 0.12s\n",
      " [-] epoch  108/250, train loss 0.486391 in 0.17s\n",
      " [-] epoch  109/250, train loss 0.455841 in 0.13s\n",
      " [-] epoch  110/250, train loss 0.466519 in 0.13s\n",
      " [-] epoch  111/250, train loss 0.486380 in 0.13s\n",
      " [-] epoch  112/250, train loss 0.479099 in 0.15s\n",
      " [-] epoch  113/250, train loss 0.478766 in 0.12s\n",
      " [-] epoch  114/250, train loss 0.462193 in 0.13s\n",
      " [-] epoch  115/250, train loss 0.476532 in 0.15s\n",
      " [-] epoch  116/250, train loss 0.462512 in 0.15s\n",
      " [-] epoch  117/250, train loss 0.464367 in 0.13s\n",
      " [-] epoch  118/250, train loss 0.437212 in 0.15s\n",
      " [-] epoch  119/250, train loss 0.484644 in 0.14s\n",
      " [-] epoch  120/250, train loss 0.451043 in 0.15s\n",
      " [-] epoch  121/250, train loss 0.425202 in 0.12s\n",
      " [-] epoch  122/250, train loss 0.480595 in 0.14s\n",
      " [-] epoch  123/250, train loss 0.467772 in 0.14s\n",
      " [-] epoch  124/250, train loss 0.453648 in 0.15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  125/250, train loss 0.483973 in 0.15s\n",
      " [-] epoch  126/250, train loss 0.475235 in 0.15s\n",
      " [-] epoch  127/250, train loss 0.482331 in 0.13s\n",
      " [-] epoch  128/250, train loss 0.450740 in 0.15s\n",
      " [-] epoch  129/250, train loss 0.454594 in 0.16s\n",
      " [-] epoch  130/250, train loss 0.456522 in 0.14s\n",
      " [-] epoch  131/250, train loss 0.437589 in 0.15s\n",
      " [-] epoch  132/250, train loss 0.465521 in 0.15s\n",
      " [-] epoch  133/250, train loss 0.423180 in 0.17s\n",
      " [-] epoch  134/250, train loss 0.441346 in 0.15s\n",
      " [-] epoch  135/250, train loss 0.457446 in 0.14s\n",
      " [-] epoch  136/250, train loss 0.442240 in 0.13s\n",
      " [-] epoch  137/250, train loss 0.463426 in 0.13s\n",
      " [-] epoch  138/250, train loss 0.412903 in 0.15s\n",
      " [-] epoch  139/250, train loss 0.476610 in 0.14s\n",
      " [-] epoch  140/250, train loss 0.448758 in 0.15s\n",
      " [-] epoch  141/250, train loss 0.457247 in 0.14s\n",
      " [-] epoch  142/250, train loss 0.457743 in 0.12s\n",
      " [-] epoch  143/250, train loss 0.447241 in 0.15s\n",
      " [-] epoch  144/250, train loss 0.443811 in 0.14s\n",
      " [-] epoch  145/250, train loss 0.436473 in 0.15s\n",
      " [-] epoch  146/250, train loss 0.439183 in 0.12s\n",
      " [-] epoch  147/250, train loss 0.439064 in 0.15s\n",
      " [-] epoch  148/250, train loss 0.438597 in 0.14s\n",
      " [-] epoch  149/250, train loss 0.424581 in 0.14s\n",
      " [-] epoch  150/250, train loss 0.422662 in 0.17s\n",
      " [-] epoch  151/250, train loss 0.427346 in 0.14s\n",
      " [-] epoch  152/250, train loss 0.426640 in 0.16s\n",
      " [-] epoch  153/250, train loss 0.440607 in 0.14s\n",
      " [-] epoch  154/250, train loss 0.462595 in 0.15s\n",
      " [-] epoch  155/250, train loss 0.424353 in 0.12s\n",
      " [-] epoch  156/250, train loss 0.427797 in 0.15s\n",
      " [-] epoch  157/250, train loss 0.437612 in 0.15s\n",
      " [-] epoch  158/250, train loss 0.432901 in 0.13s\n",
      " [-] epoch  159/250, train loss 0.410467 in 0.16s\n",
      " [-] epoch  160/250, train loss 0.413017 in 0.14s\n",
      " [-] epoch  161/250, train loss 0.431195 in 0.14s\n",
      " [-] epoch  162/250, train loss 0.435541 in 0.15s\n",
      " [-] epoch  163/250, train loss 0.410548 in 0.14s\n",
      " [-] epoch  164/250, train loss 0.425601 in 0.15s\n",
      " [-] epoch  165/250, train loss 0.442960 in 0.13s\n",
      " [-] epoch  166/250, train loss 0.469330 in 0.15s\n",
      " [-] epoch  167/250, train loss 0.419582 in 0.15s\n",
      " [-] epoch  168/250, train loss 0.424311 in 0.13s\n",
      " [-] epoch  169/250, train loss 0.432893 in 0.15s\n",
      " [-] epoch  170/250, train loss 0.462510 in 0.15s\n",
      " [-] epoch  171/250, train loss 0.403391 in 0.15s\n",
      " [-] epoch  172/250, train loss 0.447540 in 0.13s\n",
      " [-] epoch  173/250, train loss 0.427438 in 0.15s\n",
      " [-] epoch  174/250, train loss 0.435227 in 0.13s\n",
      " [-] epoch  175/250, train loss 0.413510 in 0.13s\n",
      " [-] epoch  176/250, train loss 0.389221 in 0.12s\n",
      " [-] epoch  177/250, train loss 0.385436 in 0.15s\n",
      " [-] epoch  178/250, train loss 0.430361 in 0.15s\n",
      " [-] epoch  179/250, train loss 0.440776 in 0.13s\n",
      " [-] epoch  180/250, train loss 0.417773 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.433745 in 0.13s\n",
      " [-] epoch  182/250, train loss 0.423932 in 0.15s\n",
      " [-] epoch  183/250, train loss 0.424011 in 0.17s\n",
      " [-] epoch  184/250, train loss 0.441498 in 0.16s\n",
      " [-] epoch  185/250, train loss 0.411889 in 0.16s\n",
      " [-] epoch  186/250, train loss 0.408023 in 0.14s\n",
      " [-] epoch  187/250, train loss 0.432258 in 0.15s\n",
      " [-] epoch  188/250, train loss 0.432536 in 0.13s\n",
      " [-] epoch  189/250, train loss 0.422338 in 0.15s\n",
      " [-] epoch  190/250, train loss 0.422448 in 0.13s\n",
      " [-] epoch  191/250, train loss 0.396414 in 0.12s\n",
      " [-] epoch  192/250, train loss 0.443671 in 0.17s\n",
      " [-] epoch  193/250, train loss 0.434577 in 0.15s\n",
      " [-] epoch  194/250, train loss 0.413008 in 0.12s\n",
      " [-] epoch  195/250, train loss 0.421334 in 0.17s\n",
      " [-] epoch  196/250, train loss 0.439617 in 0.13s\n",
      " [-] epoch  197/250, train loss 0.406058 in 0.15s\n",
      " [-] epoch  198/250, train loss 0.398490 in 0.16s\n",
      " [-] epoch  199/250, train loss 0.412577 in 0.14s\n",
      " [-] epoch  200/250, train loss 0.401661 in 0.15s\n",
      " [-] epoch  201/250, train loss 0.414881 in 0.12s\n",
      " [-] epoch  202/250, train loss 0.405297 in 0.14s\n",
      " [-] epoch  203/250, train loss 0.420933 in 0.15s\n",
      " [-] epoch  204/250, train loss 0.406665 in 0.14s\n",
      " [-] epoch  205/250, train loss 0.424532 in 0.14s\n",
      " [-] epoch  206/250, train loss 0.414810 in 0.13s\n",
      " [-] epoch  207/250, train loss 0.412379 in 0.13s\n",
      " [-] epoch  208/250, train loss 0.462659 in 0.15s\n",
      " [-] epoch  209/250, train loss 0.424484 in 0.14s\n",
      " [-] epoch  210/250, train loss 0.383980 in 0.15s\n",
      " [-] epoch  211/250, train loss 0.435924 in 0.14s\n",
      " [-] epoch  212/250, train loss 0.441401 in 0.15s\n",
      " [-] epoch  213/250, train loss 0.409530 in 0.17s\n",
      " [-] epoch  214/250, train loss 0.414445 in 0.16s\n",
      " [-] epoch  215/250, train loss 0.425719 in 0.16s\n",
      " [-] epoch  216/250, train loss 0.421248 in 0.15s\n",
      " [-] epoch  217/250, train loss 0.422232 in 0.14s\n",
      " [-] epoch  218/250, train loss 0.432903 in 0.16s\n",
      " [-] epoch  219/250, train loss 0.441131 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.385153 in 0.17s\n",
      " [-] epoch  221/250, train loss 0.409632 in 0.15s\n",
      " [-] epoch  222/250, train loss 0.393531 in 0.16s\n",
      " [-] epoch  223/250, train loss 0.412469 in 0.14s\n",
      " [-] epoch  224/250, train loss 0.387794 in 0.13s\n",
      " [-] epoch  225/250, train loss 0.372990 in 0.15s\n",
      " [-] epoch  226/250, train loss 0.388126 in 0.15s\n",
      " [-] epoch  227/250, train loss 0.385531 in 0.15s\n",
      " [-] epoch  228/250, train loss 0.426105 in 0.13s\n",
      " [-] epoch  229/250, train loss 0.385796 in 0.13s\n",
      " [-] epoch  230/250, train loss 0.411489 in 0.15s\n",
      " [-] epoch  231/250, train loss 0.372028 in 0.15s\n",
      " [-] epoch  232/250, train loss 0.373330 in 0.15s\n",
      " [-] epoch  233/250, train loss 0.387967 in 0.15s\n",
      " [-] epoch  234/250, train loss 0.413462 in 0.14s\n",
      " [-] epoch  235/250, train loss 0.374300 in 0.14s\n",
      " [-] epoch  236/250, train loss 0.414612 in 0.14s\n",
      " [-] epoch  237/250, train loss 0.431428 in 0.14s\n",
      " [-] epoch  238/250, train loss 0.422431 in 0.13s\n",
      " [-] epoch  239/250, train loss 0.408428 in 0.14s\n",
      " [-] epoch  240/250, train loss 0.407305 in 0.14s\n",
      " [-] epoch  241/250, train loss 0.399763 in 0.15s\n",
      " [-] epoch  242/250, train loss 0.407936 in 0.12s\n",
      " [-] epoch  243/250, train loss 0.403045 in 0.13s\n",
      " [-] epoch  244/250, train loss 0.392065 in 0.15s\n",
      " [-] epoch  245/250, train loss 0.394985 in 0.15s\n",
      " [-] epoch  246/250, train loss 0.420360 in 0.14s\n",
      " [-] epoch  247/250, train loss 0.381797 in 0.14s\n",
      " [-] epoch  248/250, train loss 0.378952 in 0.13s\n",
      " [-] epoch  249/250, train loss 0.432450 in 0.13s\n",
      " [-] epoch  250/250, train loss 0.410325 in 0.13s\n",
      " [-] test acc. 63.055556%\n",
      "Je vais utiliser 8 layers\n",
      " [-] epoch    1/250, train loss 0.645195 in 0.13s\n",
      " [-] epoch    2/250, train loss 0.612919 in 0.16s\n",
      " [-] epoch    3/250, train loss 0.606722 in 0.14s\n",
      " [-] epoch    4/250, train loss 0.591285 in 0.15s\n",
      " [-] epoch    5/250, train loss 0.603356 in 0.16s\n",
      " [-] epoch    6/250, train loss 0.583650 in 0.14s\n",
      " [-] epoch    7/250, train loss 0.571901 in 0.16s\n",
      " [-] epoch    8/250, train loss 0.578737 in 0.16s\n",
      " [-] epoch    9/250, train loss 0.551821 in 0.11s\n",
      " [-] epoch   10/250, train loss 0.556950 in 0.13s\n",
      " [-] epoch   11/250, train loss 0.565055 in 0.12s\n",
      " [-] epoch   12/250, train loss 0.552148 in 0.15s\n",
      " [-] epoch   13/250, train loss 0.578186 in 0.15s\n",
      " [-] epoch   14/250, train loss 0.582091 in 0.14s\n",
      " [-] epoch   15/250, train loss 0.553320 in 0.12s\n",
      " [-] epoch   16/250, train loss 0.551210 in 0.15s\n",
      " [-] epoch   17/250, train loss 0.576849 in 0.13s\n",
      " [-] epoch   18/250, train loss 0.574708 in 0.15s\n",
      " [-] epoch   19/250, train loss 0.529728 in 0.13s\n",
      " [-] epoch   20/250, train loss 0.539185 in 0.15s\n",
      " [-] epoch   21/250, train loss 0.533472 in 0.17s\n",
      " [-] epoch   22/250, train loss 0.538628 in 0.14s\n",
      " [-] epoch   23/250, train loss 0.542147 in 0.15s\n",
      " [-] epoch   24/250, train loss 0.571330 in 0.17s\n",
      " [-] epoch   25/250, train loss 0.535639 in 0.16s\n",
      " [-] epoch   26/250, train loss 0.556176 in 0.13s\n",
      " [-] epoch   27/250, train loss 0.531002 in 0.15s\n",
      " [-] epoch   28/250, train loss 0.520724 in 0.13s\n",
      " [-] epoch   29/250, train loss 0.560444 in 0.15s\n",
      " [-] epoch   30/250, train loss 0.542694 in 0.15s\n",
      " [-] epoch   31/250, train loss 0.526692 in 0.16s\n",
      " [-] epoch   32/250, train loss 0.532310 in 0.15s\n",
      " [-] epoch   33/250, train loss 0.531647 in 0.14s\n",
      " [-] epoch   34/250, train loss 0.515703 in 0.15s\n",
      " [-] epoch   35/250, train loss 0.531513 in 0.14s\n",
      " [-] epoch   36/250, train loss 0.525438 in 0.14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   37/250, train loss 0.561570 in 0.16s\n",
      " [-] epoch   38/250, train loss 0.543749 in 0.15s\n",
      " [-] epoch   39/250, train loss 0.504352 in 0.15s\n",
      " [-] epoch   40/250, train loss 0.531472 in 0.13s\n",
      " [-] epoch   41/250, train loss 0.533706 in 0.14s\n",
      " [-] epoch   42/250, train loss 0.524093 in 0.15s\n",
      " [-] epoch   43/250, train loss 0.520347 in 0.14s\n",
      " [-] epoch   44/250, train loss 0.528909 in 0.15s\n",
      " [-] epoch   45/250, train loss 0.515007 in 0.15s\n",
      " [-] epoch   46/250, train loss 0.500251 in 0.14s\n",
      " [-] epoch   47/250, train loss 0.492562 in 0.14s\n",
      " [-] epoch   48/250, train loss 0.516545 in 0.15s\n",
      " [-] epoch   49/250, train loss 0.485419 in 0.15s\n",
      " [-] epoch   50/250, train loss 0.524040 in 0.13s\n",
      " [-] epoch   51/250, train loss 0.499868 in 0.12s\n",
      " [-] epoch   52/250, train loss 0.519548 in 0.13s\n",
      " [-] epoch   53/250, train loss 0.517316 in 0.16s\n",
      " [-] epoch   54/250, train loss 0.507186 in 0.17s\n",
      " [-] epoch   55/250, train loss 0.510498 in 0.15s\n",
      " [-] epoch   56/250, train loss 0.503067 in 0.16s\n",
      " [-] epoch   57/250, train loss 0.526622 in 0.15s\n",
      " [-] epoch   58/250, train loss 0.527902 in 0.16s\n",
      " [-] epoch   59/250, train loss 0.500963 in 0.16s\n",
      " [-] epoch   60/250, train loss 0.511684 in 0.15s\n",
      " [-] epoch   61/250, train loss 0.523613 in 0.15s\n",
      " [-] epoch   62/250, train loss 0.495848 in 0.15s\n",
      " [-] epoch   63/250, train loss 0.456664 in 0.15s\n",
      " [-] epoch   64/250, train loss 0.512718 in 0.15s\n",
      " [-] epoch   65/250, train loss 0.483120 in 0.14s\n",
      " [-] epoch   66/250, train loss 0.498965 in 0.18s\n",
      " [-] epoch   67/250, train loss 0.512591 in 0.16s\n",
      " [-] epoch   68/250, train loss 0.494213 in 0.15s\n",
      " [-] epoch   69/250, train loss 0.493048 in 0.16s\n",
      " [-] epoch   70/250, train loss 0.487239 in 0.14s\n",
      " [-] epoch   71/250, train loss 0.522927 in 0.15s\n",
      " [-] epoch   72/250, train loss 0.514442 in 0.15s\n",
      " [-] epoch   73/250, train loss 0.486463 in 0.15s\n",
      " [-] epoch   74/250, train loss 0.507071 in 0.15s\n",
      " [-] epoch   75/250, train loss 0.494774 in 0.14s\n",
      " [-] epoch   76/250, train loss 0.504514 in 0.17s\n",
      " [-] epoch   77/250, train loss 0.464253 in 0.13s\n",
      " [-] epoch   78/250, train loss 0.481924 in 0.12s\n",
      " [-] epoch   79/250, train loss 0.491211 in 0.15s\n",
      " [-] epoch   80/250, train loss 0.490850 in 0.15s\n",
      " [-] epoch   81/250, train loss 0.478783 in 0.14s\n",
      " [-] epoch   82/250, train loss 0.484730 in 0.14s\n",
      " [-] epoch   83/250, train loss 0.457349 in 0.15s\n",
      " [-] epoch   84/250, train loss 0.497430 in 0.15s\n",
      " [-] epoch   85/250, train loss 0.490376 in 0.14s\n",
      " [-] epoch   86/250, train loss 0.492746 in 0.15s\n",
      " [-] epoch   87/250, train loss 0.483142 in 0.14s\n",
      " [-] epoch   88/250, train loss 0.468900 in 0.15s\n",
      " [-] epoch   89/250, train loss 0.489427 in 0.15s\n",
      " [-] epoch   90/250, train loss 0.449791 in 0.14s\n",
      " [-] epoch   91/250, train loss 0.466800 in 0.12s\n",
      " [-] epoch   92/250, train loss 0.452908 in 0.15s\n",
      " [-] epoch   93/250, train loss 0.484800 in 0.12s\n",
      " [-] epoch   94/250, train loss 0.457493 in 0.14s\n",
      " [-] epoch   95/250, train loss 0.454506 in 0.13s\n",
      " [-] epoch   96/250, train loss 0.479609 in 0.14s\n",
      " [-] epoch   97/250, train loss 0.492921 in 0.13s\n",
      " [-] epoch   98/250, train loss 0.475997 in 0.14s\n",
      " [-] epoch   99/250, train loss 0.485937 in 0.15s\n",
      " [-] epoch  100/250, train loss 0.474605 in 0.14s\n",
      " [-] epoch  101/250, train loss 0.470832 in 0.15s\n",
      " [-] epoch  102/250, train loss 0.479663 in 0.13s\n",
      " [-] epoch  103/250, train loss 0.475878 in 0.14s\n",
      " [-] epoch  104/250, train loss 0.449241 in 0.15s\n",
      " [-] epoch  105/250, train loss 0.445797 in 0.14s\n",
      " [-] epoch  106/250, train loss 0.472354 in 0.15s\n",
      " [-] epoch  107/250, train loss 0.445152 in 0.16s\n",
      " [-] epoch  108/250, train loss 0.466074 in 0.17s\n",
      " [-] epoch  109/250, train loss 0.486826 in 0.17s\n",
      " [-] epoch  110/250, train loss 0.427257 in 0.17s\n",
      " [-] epoch  111/250, train loss 0.465558 in 0.15s\n",
      " [-] epoch  112/250, train loss 0.450771 in 0.15s\n",
      " [-] epoch  113/250, train loss 0.465046 in 0.15s\n",
      " [-] epoch  114/250, train loss 0.444957 in 0.15s\n",
      " [-] epoch  115/250, train loss 0.455815 in 0.14s\n",
      " [-] epoch  116/250, train loss 0.470462 in 0.15s\n",
      " [-] epoch  117/250, train loss 0.470383 in 0.15s\n",
      " [-] epoch  118/250, train loss 0.476605 in 0.15s\n",
      " [-] epoch  119/250, train loss 0.467151 in 0.16s\n",
      " [-] epoch  120/250, train loss 0.469674 in 0.16s\n",
      " [-] epoch  121/250, train loss 0.451264 in 0.15s\n",
      " [-] epoch  122/250, train loss 0.433861 in 0.16s\n",
      " [-] epoch  123/250, train loss 0.502849 in 0.14s\n",
      " [-] epoch  124/250, train loss 0.436396 in 0.13s\n",
      " [-] epoch  125/250, train loss 0.441674 in 0.14s\n",
      " [-] epoch  126/250, train loss 0.437867 in 0.14s\n",
      " [-] epoch  127/250, train loss 0.443845 in 0.14s\n",
      " [-] epoch  128/250, train loss 0.442998 in 0.14s\n",
      " [-] epoch  129/250, train loss 0.474203 in 0.15s\n",
      " [-] epoch  130/250, train loss 0.447516 in 0.15s\n",
      " [-] epoch  131/250, train loss 0.450781 in 0.13s\n",
      " [-] epoch  132/250, train loss 0.442307 in 0.15s\n",
      " [-] epoch  133/250, train loss 0.506613 in 0.14s\n",
      " [-] epoch  134/250, train loss 0.465054 in 0.15s\n",
      " [-] epoch  135/250, train loss 0.451146 in 0.13s\n",
      " [-] epoch  136/250, train loss 0.454993 in 0.15s\n",
      " [-] epoch  137/250, train loss 0.433168 in 0.16s\n",
      " [-] epoch  138/250, train loss 0.431483 in 0.14s\n",
      " [-] epoch  139/250, train loss 0.464418 in 0.15s\n",
      " [-] epoch  140/250, train loss 0.441046 in 0.14s\n",
      " [-] epoch  141/250, train loss 0.461788 in 0.15s\n",
      " [-] epoch  142/250, train loss 0.469392 in 0.14s\n",
      " [-] epoch  143/250, train loss 0.465734 in 0.15s\n",
      " [-] epoch  144/250, train loss 0.424777 in 0.14s\n",
      " [-] epoch  145/250, train loss 0.424881 in 0.14s\n",
      " [-] epoch  146/250, train loss 0.473367 in 0.14s\n",
      " [-] epoch  147/250, train loss 0.450087 in 0.15s\n",
      " [-] epoch  148/250, train loss 0.459486 in 0.14s\n",
      " [-] epoch  149/250, train loss 0.448148 in 0.14s\n",
      " [-] epoch  150/250, train loss 0.440927 in 0.14s\n",
      " [-] epoch  151/250, train loss 0.451006 in 0.15s\n",
      " [-] epoch  152/250, train loss 0.438168 in 0.15s\n",
      " [-] epoch  153/250, train loss 0.445572 in 0.16s\n",
      " [-] epoch  154/250, train loss 0.422069 in 0.16s\n",
      " [-] epoch  155/250, train loss 0.424095 in 0.16s\n",
      " [-] epoch  156/250, train loss 0.413131 in 0.15s\n",
      " [-] epoch  157/250, train loss 0.458972 in 0.16s\n",
      " [-] epoch  158/250, train loss 0.447396 in 0.15s\n",
      " [-] epoch  159/250, train loss 0.425951 in 0.15s\n",
      " [-] epoch  160/250, train loss 0.456967 in 0.16s\n",
      " [-] epoch  161/250, train loss 0.438006 in 0.16s\n",
      " [-] epoch  162/250, train loss 0.423183 in 0.15s\n",
      " [-] epoch  163/250, train loss 0.449937 in 0.14s\n",
      " [-] epoch  164/250, train loss 0.459096 in 0.15s\n",
      " [-] epoch  165/250, train loss 0.443423 in 0.15s\n",
      " [-] epoch  166/250, train loss 0.428786 in 0.16s\n",
      " [-] epoch  167/250, train loss 0.437777 in 0.13s\n",
      " [-] epoch  168/250, train loss 0.457678 in 0.15s\n",
      " [-] epoch  169/250, train loss 0.418151 in 0.13s\n",
      " [-] epoch  170/250, train loss 0.429675 in 0.16s\n",
      " [-] epoch  171/250, train loss 0.458983 in 0.16s\n",
      " [-] epoch  172/250, train loss 0.431021 in 0.15s\n",
      " [-] epoch  173/250, train loss 0.414771 in 0.15s\n",
      " [-] epoch  174/250, train loss 0.422736 in 0.15s\n",
      " [-] epoch  175/250, train loss 0.437494 in 0.15s\n",
      " [-] epoch  176/250, train loss 0.432778 in 0.13s\n",
      " [-] epoch  177/250, train loss 0.422577 in 0.15s\n",
      " [-] epoch  178/250, train loss 0.454869 in 0.15s\n",
      " [-] epoch  179/250, train loss 0.424897 in 0.15s\n",
      " [-] epoch  180/250, train loss 0.407397 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.467781 in 0.15s\n",
      " [-] epoch  182/250, train loss 0.440027 in 0.13s\n",
      " [-] epoch  183/250, train loss 0.426901 in 0.15s\n",
      " [-] epoch  184/250, train loss 0.435469 in 0.12s\n",
      " [-] epoch  185/250, train loss 0.438487 in 0.13s\n",
      " [-] epoch  186/250, train loss 0.424813 in 0.16s\n",
      " [-] epoch  187/250, train loss 0.442239 in 0.14s\n",
      " [-] epoch  188/250, train loss 0.439384 in 0.13s\n",
      " [-] epoch  189/250, train loss 0.426287 in 0.13s\n",
      " [-] epoch  190/250, train loss 0.417863 in 0.13s\n",
      " [-] epoch  191/250, train loss 0.443430 in 0.12s\n",
      " [-] epoch  192/250, train loss 0.438637 in 0.13s\n",
      " [-] epoch  193/250, train loss 0.422375 in 0.15s\n",
      " [-] epoch  194/250, train loss 0.410286 in 0.12s\n",
      " [-] epoch  195/250, train loss 0.408458 in 0.15s\n",
      " [-] epoch  196/250, train loss 0.423434 in 0.16s\n",
      " [-] epoch  197/250, train loss 0.429506 in 0.15s\n",
      " [-] epoch  198/250, train loss 0.418694 in 0.15s\n",
      " [-] epoch  199/250, train loss 0.436535 in 0.14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  200/250, train loss 0.442231 in 0.14s\n",
      " [-] epoch  201/250, train loss 0.423277 in 0.15s\n",
      " [-] epoch  202/250, train loss 0.419039 in 0.15s\n",
      " [-] epoch  203/250, train loss 0.408873 in 0.15s\n",
      " [-] epoch  204/250, train loss 0.437081 in 0.13s\n",
      " [-] epoch  205/250, train loss 0.427275 in 0.17s\n",
      " [-] epoch  206/250, train loss 0.395967 in 0.14s\n",
      " [-] epoch  207/250, train loss 0.419660 in 0.12s\n",
      " [-] epoch  208/250, train loss 0.413485 in 0.12s\n",
      " [-] epoch  209/250, train loss 0.433410 in 0.14s\n",
      " [-] epoch  210/250, train loss 0.440627 in 0.14s\n",
      " [-] epoch  211/250, train loss 0.413826 in 0.15s\n",
      " [-] epoch  212/250, train loss 0.411977 in 0.14s\n",
      " [-] epoch  213/250, train loss 0.388583 in 0.16s\n",
      " [-] epoch  214/250, train loss 0.440047 in 0.15s\n",
      " [-] epoch  215/250, train loss 0.435936 in 0.15s\n",
      " [-] epoch  216/250, train loss 0.398911 in 0.15s\n",
      " [-] epoch  217/250, train loss 0.393241 in 0.14s\n",
      " [-] epoch  218/250, train loss 0.447178 in 0.15s\n",
      " [-] epoch  219/250, train loss 0.406722 in 0.16s\n",
      " [-] epoch  220/250, train loss 0.400381 in 0.15s\n",
      " [-] epoch  221/250, train loss 0.416195 in 0.17s\n",
      " [-] epoch  222/250, train loss 0.412524 in 0.15s\n",
      " [-] epoch  223/250, train loss 0.424912 in 0.14s\n",
      " [-] epoch  224/250, train loss 0.431176 in 0.15s\n",
      " [-] epoch  225/250, train loss 0.420748 in 0.15s\n",
      " [-] epoch  226/250, train loss 0.418395 in 0.15s\n",
      " [-] epoch  227/250, train loss 0.420012 in 0.13s\n",
      " [-] epoch  228/250, train loss 0.424944 in 0.16s\n",
      " [-] epoch  229/250, train loss 0.401863 in 0.19s\n",
      " [-] epoch  230/250, train loss 0.415541 in 0.16s\n",
      " [-] epoch  231/250, train loss 0.412919 in 0.15s\n",
      " [-] epoch  232/250, train loss 0.385790 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.432914 in 0.17s\n",
      " [-] epoch  234/250, train loss 0.450308 in 0.17s\n",
      " [-] epoch  235/250, train loss 0.428376 in 0.17s\n",
      " [-] epoch  236/250, train loss 0.385508 in 0.18s\n",
      " [-] epoch  237/250, train loss 0.418631 in 0.16s\n",
      " [-] epoch  238/250, train loss 0.393965 in 0.14s\n",
      " [-] epoch  239/250, train loss 0.410282 in 0.14s\n",
      " [-] epoch  240/250, train loss 0.425983 in 0.15s\n",
      " [-] epoch  241/250, train loss 0.433448 in 0.16s\n",
      " [-] epoch  242/250, train loss 0.449936 in 0.14s\n",
      " [-] epoch  243/250, train loss 0.399295 in 0.17s\n",
      " [-] epoch  244/250, train loss 0.386482 in 0.16s\n",
      " [-] epoch  245/250, train loss 0.388484 in 0.16s\n",
      " [-] epoch  246/250, train loss 0.396440 in 0.16s\n",
      " [-] epoch  247/250, train loss 0.434500 in 0.15s\n",
      " [-] epoch  248/250, train loss 0.409651 in 0.12s\n",
      " [-] epoch  249/250, train loss 0.402070 in 0.14s\n",
      " [-] epoch  250/250, train loss 0.394856 in 0.15s\n",
      " [-] test acc. 68.055556%\n",
      "Je vais utiliser 9 layers\n",
      " [-] epoch    1/250, train loss 0.691856 in 0.28s\n",
      " [-] epoch    2/250, train loss 0.637791 in 0.18s\n",
      " [-] epoch    3/250, train loss 0.605839 in 0.17s\n",
      " [-] epoch    4/250, train loss 0.594180 in 0.16s\n",
      " [-] epoch    5/250, train loss 0.584131 in 0.16s\n",
      " [-] epoch    6/250, train loss 0.617989 in 0.17s\n",
      " [-] epoch    7/250, train loss 0.559667 in 0.16s\n",
      " [-] epoch    8/250, train loss 0.559600 in 0.18s\n",
      " [-] epoch    9/250, train loss 0.577843 in 0.16s\n",
      " [-] epoch   10/250, train loss 0.568487 in 0.16s\n",
      " [-] epoch   11/250, train loss 0.564626 in 0.17s\n",
      " [-] epoch   12/250, train loss 0.564264 in 0.19s\n",
      " [-] epoch   13/250, train loss 0.558048 in 0.19s\n",
      " [-] epoch   14/250, train loss 0.612223 in 0.17s\n",
      " [-] epoch   15/250, train loss 0.559832 in 0.19s\n",
      " [-] epoch   16/250, train loss 0.568284 in 0.16s\n",
      " [-] epoch   17/250, train loss 0.548106 in 0.16s\n",
      " [-] epoch   18/250, train loss 0.571513 in 0.16s\n",
      " [-] epoch   19/250, train loss 0.542952 in 0.18s\n",
      " [-] epoch   20/250, train loss 0.558376 in 0.16s\n",
      " [-] epoch   21/250, train loss 0.554979 in 0.16s\n",
      " [-] epoch   22/250, train loss 0.540343 in 0.15s\n",
      " [-] epoch   23/250, train loss 0.552360 in 0.16s\n",
      " [-] epoch   24/250, train loss 0.525177 in 0.15s\n",
      " [-] epoch   25/250, train loss 0.526671 in 0.18s\n",
      " [-] epoch   26/250, train loss 0.549482 in 0.17s\n",
      " [-] epoch   27/250, train loss 0.554012 in 0.15s\n",
      " [-] epoch   28/250, train loss 0.557214 in 0.17s\n",
      " [-] epoch   29/250, train loss 0.533861 in 0.18s\n",
      " [-] epoch   30/250, train loss 0.559691 in 0.16s\n",
      " [-] epoch   31/250, train loss 0.542120 in 0.15s\n",
      " [-] epoch   32/250, train loss 0.534558 in 0.15s\n",
      " [-] epoch   33/250, train loss 0.539748 in 0.17s\n",
      " [-] epoch   34/250, train loss 0.525600 in 0.16s\n",
      " [-] epoch   35/250, train loss 0.518558 in 0.17s\n",
      " [-] epoch   36/250, train loss 0.520532 in 0.18s\n",
      " [-] epoch   37/250, train loss 0.526884 in 0.16s\n",
      " [-] epoch   38/250, train loss 0.520943 in 0.17s\n",
      " [-] epoch   39/250, train loss 0.534394 in 0.16s\n",
      " [-] epoch   40/250, train loss 0.510739 in 0.16s\n",
      " [-] epoch   41/250, train loss 0.528951 in 0.17s\n",
      " [-] epoch   42/250, train loss 0.500598 in 0.18s\n",
      " [-] epoch   43/250, train loss 0.498246 in 0.15s\n",
      " [-] epoch   44/250, train loss 0.485157 in 0.15s\n",
      " [-] epoch   45/250, train loss 0.525621 in 0.17s\n",
      " [-] epoch   46/250, train loss 0.522538 in 0.16s\n",
      " [-] epoch   47/250, train loss 0.515130 in 0.17s\n",
      " [-] epoch   48/250, train loss 0.541111 in 0.16s\n",
      " [-] epoch   49/250, train loss 0.512665 in 0.17s\n",
      " [-] epoch   50/250, train loss 0.492565 in 0.17s\n",
      " [-] epoch   51/250, train loss 0.482299 in 0.17s\n",
      " [-] epoch   52/250, train loss 0.502362 in 0.15s\n",
      " [-] epoch   53/250, train loss 0.491808 in 0.15s\n",
      " [-] epoch   54/250, train loss 0.512696 in 0.14s\n",
      " [-] epoch   55/250, train loss 0.503326 in 0.16s\n",
      " [-] epoch   56/250, train loss 0.500595 in 0.16s\n",
      " [-] epoch   57/250, train loss 0.503780 in 0.16s\n",
      " [-] epoch   58/250, train loss 0.493306 in 0.15s\n",
      " [-] epoch   59/250, train loss 0.520493 in 0.17s\n",
      " [-] epoch   60/250, train loss 0.513816 in 0.14s\n",
      " [-] epoch   61/250, train loss 0.520069 in 0.16s\n",
      " [-] epoch   62/250, train loss 0.494455 in 0.16s\n",
      " [-] epoch   63/250, train loss 0.510059 in 0.18s\n",
      " [-] epoch   64/250, train loss 0.462387 in 0.17s\n",
      " [-] epoch   65/250, train loss 0.480588 in 0.17s\n",
      " [-] epoch   66/250, train loss 0.503977 in 0.15s\n",
      " [-] epoch   67/250, train loss 0.487984 in 0.17s\n",
      " [-] epoch   68/250, train loss 0.496103 in 0.15s\n",
      " [-] epoch   69/250, train loss 0.482112 in 0.17s\n",
      " [-] epoch   70/250, train loss 0.504878 in 0.17s\n",
      " [-] epoch   71/250, train loss 0.484869 in 0.15s\n",
      " [-] epoch   72/250, train loss 0.512411 in 0.15s\n",
      " [-] epoch   73/250, train loss 0.502806 in 0.18s\n",
      " [-] epoch   74/250, train loss 0.483765 in 0.16s\n",
      " [-] epoch   75/250, train loss 0.498568 in 0.17s\n",
      " [-] epoch   76/250, train loss 0.487452 in 0.17s\n",
      " [-] epoch   77/250, train loss 0.468231 in 0.16s\n",
      " [-] epoch   78/250, train loss 0.459257 in 0.16s\n",
      " [-] epoch   79/250, train loss 0.500654 in 0.15s\n",
      " [-] epoch   80/250, train loss 0.503345 in 0.17s\n",
      " [-] epoch   81/250, train loss 0.500478 in 0.15s\n",
      " [-] epoch   82/250, train loss 0.480309 in 0.17s\n",
      " [-] epoch   83/250, train loss 0.467018 in 0.18s\n",
      " [-] epoch   84/250, train loss 0.461770 in 0.15s\n",
      " [-] epoch   85/250, train loss 0.436287 in 0.17s\n",
      " [-] epoch   86/250, train loss 0.491600 in 0.15s\n",
      " [-] epoch   87/250, train loss 0.508516 in 0.14s\n",
      " [-] epoch   88/250, train loss 0.478279 in 0.18s\n",
      " [-] epoch   89/250, train loss 0.458973 in 0.17s\n",
      " [-] epoch   90/250, train loss 0.465523 in 0.17s\n",
      " [-] epoch   91/250, train loss 0.467211 in 0.17s\n",
      " [-] epoch   92/250, train loss 0.451127 in 0.16s\n",
      " [-] epoch   93/250, train loss 0.496919 in 0.17s\n",
      " [-] epoch   94/250, train loss 0.490551 in 0.16s\n",
      " [-] epoch   95/250, train loss 0.455315 in 0.15s\n",
      " [-] epoch   96/250, train loss 0.438099 in 0.20s\n",
      " [-] epoch   97/250, train loss 0.472203 in 0.16s\n",
      " [-] epoch   98/250, train loss 0.475499 in 0.18s\n",
      " [-] epoch   99/250, train loss 0.494147 in 0.17s\n",
      " [-] epoch  100/250, train loss 0.477710 in 0.15s\n",
      " [-] epoch  101/250, train loss 0.481225 in 0.17s\n",
      " [-] epoch  102/250, train loss 0.441914 in 0.19s\n",
      " [-] epoch  103/250, train loss 0.496486 in 0.16s\n",
      " [-] epoch  104/250, train loss 0.446409 in 0.17s\n",
      " [-] epoch  105/250, train loss 0.476382 in 0.20s\n",
      " [-] epoch  106/250, train loss 0.476540 in 0.15s\n",
      " [-] epoch  107/250, train loss 0.453860 in 0.18s\n",
      " [-] epoch  108/250, train loss 0.452139 in 0.18s\n",
      " [-] epoch  109/250, train loss 0.416680 in 0.17s\n",
      " [-] epoch  110/250, train loss 0.465130 in 0.17s\n",
      " [-] epoch  111/250, train loss 0.451726 in 0.18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  112/250, train loss 0.463229 in 0.17s\n",
      " [-] epoch  113/250, train loss 0.445401 in 0.17s\n",
      " [-] epoch  114/250, train loss 0.437705 in 0.16s\n",
      " [-] epoch  115/250, train loss 0.468409 in 0.15s\n",
      " [-] epoch  116/250, train loss 0.469755 in 0.18s\n",
      " [-] epoch  117/250, train loss 0.469536 in 0.14s\n",
      " [-] epoch  118/250, train loss 0.459925 in 0.16s\n",
      " [-] epoch  119/250, train loss 0.449934 in 0.16s\n",
      " [-] epoch  120/250, train loss 0.459350 in 0.15s\n",
      " [-] epoch  121/250, train loss 0.431083 in 0.17s\n",
      " [-] epoch  122/250, train loss 0.472547 in 0.16s\n",
      " [-] epoch  123/250, train loss 0.435609 in 0.17s\n",
      " [-] epoch  124/250, train loss 0.456561 in 0.16s\n",
      " [-] epoch  125/250, train loss 0.446590 in 0.13s\n",
      " [-] epoch  126/250, train loss 0.412483 in 0.17s\n",
      " [-] epoch  127/250, train loss 0.423554 in 0.16s\n",
      " [-] epoch  128/250, train loss 0.473127 in 0.17s\n",
      " [-] epoch  129/250, train loss 0.458911 in 0.14s\n",
      " [-] epoch  130/250, train loss 0.446151 in 0.18s\n",
      " [-] epoch  131/250, train loss 0.500747 in 0.16s\n",
      " [-] epoch  132/250, train loss 0.464346 in 0.17s\n",
      " [-] epoch  133/250, train loss 0.467865 in 0.16s\n",
      " [-] epoch  134/250, train loss 0.446522 in 0.16s\n",
      " [-] epoch  135/250, train loss 0.436809 in 0.14s\n",
      " [-] epoch  136/250, train loss 0.444726 in 0.16s\n",
      " [-] epoch  137/250, train loss 0.456228 in 0.16s\n",
      " [-] epoch  138/250, train loss 0.459767 in 0.18s\n",
      " [-] epoch  139/250, train loss 0.439741 in 0.15s\n",
      " [-] epoch  140/250, train loss 0.426666 in 0.18s\n",
      " [-] epoch  141/250, train loss 0.457795 in 0.16s\n",
      " [-] epoch  142/250, train loss 0.425640 in 0.17s\n",
      " [-] epoch  143/250, train loss 0.406807 in 0.15s\n",
      " [-] epoch  144/250, train loss 0.427770 in 0.19s\n",
      " [-] epoch  145/250, train loss 0.442567 in 0.16s\n",
      " [-] epoch  146/250, train loss 0.419609 in 0.18s\n",
      " [-] epoch  147/250, train loss 0.426817 in 0.13s\n",
      " [-] epoch  148/250, train loss 0.441934 in 0.16s\n",
      " [-] epoch  149/250, train loss 0.448285 in 0.15s\n",
      " [-] epoch  150/250, train loss 0.465703 in 0.18s\n",
      " [-] epoch  151/250, train loss 0.423646 in 0.15s\n",
      " [-] epoch  152/250, train loss 0.432377 in 0.18s\n",
      " [-] epoch  153/250, train loss 0.433916 in 0.15s\n",
      " [-] epoch  154/250, train loss 0.418333 in 0.16s\n",
      " [-] epoch  155/250, train loss 0.464955 in 0.15s\n",
      " [-] epoch  156/250, train loss 0.426354 in 0.19s\n",
      " [-] epoch  157/250, train loss 0.430793 in 0.15s\n",
      " [-] epoch  158/250, train loss 0.431829 in 0.17s\n",
      " [-] epoch  159/250, train loss 0.460160 in 0.17s\n",
      " [-] epoch  160/250, train loss 0.439100 in 0.17s\n",
      " [-] epoch  161/250, train loss 0.399949 in 0.17s\n",
      " [-] epoch  162/250, train loss 0.456224 in 0.16s\n",
      " [-] epoch  163/250, train loss 0.445251 in 0.14s\n",
      " [-] epoch  164/250, train loss 0.436185 in 0.13s\n",
      " [-] epoch  165/250, train loss 0.407285 in 0.16s\n",
      " [-] epoch  166/250, train loss 0.464185 in 0.15s\n",
      " [-] epoch  167/250, train loss 0.427204 in 0.15s\n",
      " [-] epoch  168/250, train loss 0.426799 in 0.15s\n",
      " [-] epoch  169/250, train loss 0.412149 in 0.13s\n",
      " [-] epoch  170/250, train loss 0.424449 in 0.17s\n",
      " [-] epoch  171/250, train loss 0.411430 in 0.16s\n",
      " [-] epoch  172/250, train loss 0.428209 in 0.15s\n",
      " [-] epoch  173/250, train loss 0.422238 in 0.17s\n",
      " [-] epoch  174/250, train loss 0.415653 in 0.17s\n",
      " [-] epoch  175/250, train loss 0.421508 in 0.15s\n",
      " [-] epoch  176/250, train loss 0.417860 in 0.16s\n",
      " [-] epoch  177/250, train loss 0.421894 in 0.17s\n",
      " [-] epoch  178/250, train loss 0.434092 in 0.17s\n",
      " [-] epoch  179/250, train loss 0.406981 in 0.18s\n",
      " [-] epoch  180/250, train loss 0.399066 in 0.16s\n",
      " [-] epoch  181/250, train loss 0.407779 in 0.17s\n",
      " [-] epoch  182/250, train loss 0.433704 in 0.17s\n",
      " [-] epoch  183/250, train loss 0.379502 in 0.17s\n",
      " [-] epoch  184/250, train loss 0.422055 in 0.17s\n",
      " [-] epoch  185/250, train loss 0.441536 in 0.17s\n",
      " [-] epoch  186/250, train loss 0.410302 in 0.16s\n",
      " [-] epoch  187/250, train loss 0.402867 in 0.17s\n",
      " [-] epoch  188/250, train loss 0.421475 in 0.16s\n",
      " [-] epoch  189/250, train loss 0.441798 in 0.16s\n",
      " [-] epoch  190/250, train loss 0.398677 in 0.17s\n",
      " [-] epoch  191/250, train loss 0.411036 in 0.14s\n",
      " [-] epoch  192/250, train loss 0.395596 in 0.17s\n",
      " [-] epoch  193/250, train loss 0.400252 in 0.16s\n",
      " [-] epoch  194/250, train loss 0.431405 in 0.17s\n",
      " [-] epoch  195/250, train loss 0.411866 in 0.15s\n",
      " [-] epoch  196/250, train loss 0.394028 in 0.17s\n",
      " [-] epoch  197/250, train loss 0.370216 in 0.16s\n",
      " [-] epoch  198/250, train loss 0.420864 in 0.17s\n",
      " [-] epoch  199/250, train loss 0.413909 in 0.14s\n",
      " [-] epoch  200/250, train loss 0.408885 in 0.18s\n",
      " [-] epoch  201/250, train loss 0.430669 in 0.18s\n",
      " [-] epoch  202/250, train loss 0.424471 in 0.19s\n",
      " [-] epoch  203/250, train loss 0.379510 in 0.16s\n",
      " [-] epoch  204/250, train loss 0.405663 in 0.18s\n",
      " [-] epoch  205/250, train loss 0.408998 in 0.18s\n",
      " [-] epoch  206/250, train loss 0.425201 in 0.17s\n",
      " [-] epoch  207/250, train loss 0.431579 in 0.17s\n",
      " [-] epoch  208/250, train loss 0.420024 in 0.16s\n",
      " [-] epoch  209/250, train loss 0.380634 in 0.17s\n",
      " [-] epoch  210/250, train loss 0.388802 in 0.15s\n",
      " [-] epoch  211/250, train loss 0.389768 in 0.17s\n",
      " [-] epoch  212/250, train loss 0.439042 in 0.18s\n",
      " [-] epoch  213/250, train loss 0.404206 in 0.16s\n",
      " [-] epoch  214/250, train loss 0.390067 in 0.17s\n",
      " [-] epoch  215/250, train loss 0.398374 in 0.15s\n",
      " [-] epoch  216/250, train loss 0.387137 in 0.16s\n",
      " [-] epoch  217/250, train loss 0.411547 in 0.14s\n",
      " [-] epoch  218/250, train loss 0.428924 in 0.18s\n",
      " [-] epoch  219/250, train loss 0.380166 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.390399 in 0.18s\n",
      " [-] epoch  221/250, train loss 0.398624 in 0.16s\n",
      " [-] epoch  222/250, train loss 0.394080 in 0.17s\n",
      " [-] epoch  223/250, train loss 0.373760 in 0.14s\n",
      " [-] epoch  224/250, train loss 0.376787 in 0.16s\n",
      " [-] epoch  225/250, train loss 0.402542 in 0.17s\n",
      " [-] epoch  226/250, train loss 0.400234 in 0.19s\n",
      " [-] epoch  227/250, train loss 0.395579 in 0.17s\n",
      " [-] epoch  228/250, train loss 0.379767 in 0.17s\n",
      " [-] epoch  229/250, train loss 0.416389 in 0.13s\n",
      " [-] epoch  230/250, train loss 0.401972 in 0.17s\n",
      " [-] epoch  231/250, train loss 0.382821 in 0.17s\n",
      " [-] epoch  232/250, train loss 0.380214 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.413928 in 0.17s\n",
      " [-] epoch  234/250, train loss 0.408345 in 0.16s\n",
      " [-] epoch  235/250, train loss 0.394038 in 0.15s\n",
      " [-] epoch  236/250, train loss 0.385091 in 0.15s\n",
      " [-] epoch  237/250, train loss 0.407044 in 0.18s\n",
      " [-] epoch  238/250, train loss 0.387683 in 0.15s\n",
      " [-] epoch  239/250, train loss 0.372455 in 0.18s\n",
      " [-] epoch  240/250, train loss 0.412706 in 0.17s\n",
      " [-] epoch  241/250, train loss 0.434048 in 0.15s\n",
      " [-] epoch  242/250, train loss 0.409289 in 0.17s\n",
      " [-] epoch  243/250, train loss 0.416088 in 0.16s\n",
      " [-] epoch  244/250, train loss 0.382848 in 0.15s\n",
      " [-] epoch  245/250, train loss 0.394219 in 0.17s\n",
      " [-] epoch  246/250, train loss 0.393722 in 0.18s\n",
      " [-] epoch  247/250, train loss 0.367358 in 0.14s\n",
      " [-] epoch  248/250, train loss 0.387658 in 0.17s\n",
      " [-] epoch  249/250, train loss 0.387717 in 0.14s\n",
      " [-] epoch  250/250, train loss 0.373609 in 0.17s\n",
      " [-] test acc. 58.888889%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXiU5b3G8e+PhLAFCEvCFmQn7ItsAoJERaFVcO1RWypapVax2tpabXtstbZ16Wm11S5WFO1ppXGPFqFViSwqAsoOYVcgQAAFDAghye/8MYMnxsAMMMlkXu7PdeUy78yTd+7B5J53nveZGXN3REQkWGrFO4CIiMSeyl1EJIBU7iIiAaRyFxEJIJW7iEgAJcfrhtPS0rxz587xuvmo7d+/nwYNGsQ7RkTKGTuJkBGUM9YSJeeiRYt2uXt6xIHuHvELGAPkA+uAOyq5/nfA4vDXGmBPpH127drVE8GsWbPiHSEqyhk7iZDRXTljLVFyAgs9it6OeORuZknAo8BoYAuwwMxy3X1luQeI75UbfzPQP9pHIRERib1o5twHA+vcfYO7FwPTgPHHGH8l8EwswomIyImJptzbAJvLbW8JX/YlZtYO6AC8efLRRETkRJlHePsBM7scON/drwtvTwAGu/vNlYz9EZBZ2XXh6ycBkwDS09MH5OTknGT8qldUVERqamq8Y0SknLGTCBlBOWMtUXJmZ2cvcveBEQdGmpQHhgIzy23fCdx5lLEfAMOimezXCdXYUs7YSYSM7soZa4mSkyhPqEYzLbMA6GJmHcwsBbgCyK04yMyygCbAO1E9/IiISJWJWO7uXgJMBmYCq4Acd19hZveY2bhyQ68EpoUfWUREJI6iehGTu08Hple47K4K2z+PXSwRkeqx7+Bh3li1g02FJZzljpnFO1JMxO0VqiIi8XLwcClvrCokd8lWZuXvpLikDIAl+xdw70W9yGxSP84JT57KXUROCYdLy5i7dhe5Swr494rt7C8uJb1hHa4afBoX9m3N87MW8tKGjznvd7O57bwsJg5rT1KtxD2KV7mLSGCVlTnzN35M7pICXlu+jT0HDtO4Xm0u7NuaC/u25oyOzT4v8E/b1+bG8cP575eW84tXV5K7eCu/vqQPPVo3ivO9ODEqd5FyNn98gNlbDtO76BDNUuvEO46cAHdn6Za95C4p4NWlBezYd4h6tZMY3aMF4/q2ZmTXdFKSK19LktmkPk9MHMQrS7dxzysruPCRuUwa2ZFbzulC3dpJ1XxPTo7KXQRY9OEnTJm7gRnLt1PmkLM2j2+P7Mi3RnSgfor+TBLBmh2fkru4gFeWFvDh7gOkJNXirKx0xvVtzTndM6L+/2hmoQeBLs351fRV/ClvPdOXbeNXF/dmeOfmVXwvYke/tXLKKikt47Xl25kydyOLN++hUd1kJo3sRJPPtrKoqDH/8581PP3uh9x6bhe+NrAttZP08Qc1zeaPD5C7pIBXlhSwevun1DIY1qk5N43qzPm9WtK4Xu0T3nda/RQeuKwvF/Vvw49fWMbXH5/PZQMy+clXutOkQUoM70XVULnLKWfvZ4f554KPeOrtD9m65zPaN6vPPeN7cunpmTSok0xe3na+fclAFn34Mb+evpqfvLicKXM28sPzsxjTq2VglsolqsJ9B3l16TZylxSwePMeAAa0a8Ld43oytndLMhrWjentDevUnBm3juT3b6zlsdkbmLW6kLsu7MG4vq1r9O+Cyl1OGR/tPsAT8zby7MLN7C8u5YyOTbl7XE/O7pZBrUpWRQxo15RnbxjK66sKeWDGar7z9/fp2zaNO8d244yOzeJwD05dew8c5rXloUJ/d8Nuyhy6t2rEj8Z044I+rWjbtGqXLtatncTtY7pxYd/W3PH8Um6ZtpgXP9hao5dNqtwl0NydhR9+wuNzNvDvlTtIrmVc2Kc1157ZgV5tGkf8eTNjdI8WZGel88L7W/ntf9ZwxWPvcna3DG4fk0W3lom5kiIR7D9UwuurdpC7uIDZa3dyuNTp0LwBk8/uwri+reic0bDaM3Vv1YgXbhzOU29v4jf/zq/RyyZV7hJIh0vLmL5sG1PmbmTplr2k1a/NjaM68c2h7WnR6Pifticn1eJrg9oyrl9rpr69iUdnrWPsw3O4pH8m3z+vK23S6lXBvTj1HCop5a38neQuKeCNVYV8driUlo3qMnFYe8b1bUOvNo3iPhWSVMu49swOnNezxefLJl9evJX7atiySZW7BMreA4d5ZsFHPPX2JrbtPUjH5g2496JeXHp6JvVSTn4pW93aSdxwVieuGNSWP+atZ+rbm3hlaQFXD23HjaM6J8SJtpqmpLSMdzbs5pUlBcxYvp19B0to2iCFSwe04cI+rRnUvmml02bxdmTZ5KtLt3F3eNnk9SM6cuu5NWPZpMpdAmHTrv08OW8jzy7awoHiUoZ3bsYvL+7FqK6Vz6efrLT6Kfz4K925elh7fvefNTw+dyPTFmzmxlGduWZ4+xrxx12TuTvvf/QJuYsL+NeybewqKia1TjLn9QytRR/euXlCrE4yMy7s25oR4WWTf35rPa8trxnLJlXukrDcQ68+nDJ3I6+vCs2nj+/XhmuHd6i2p8dt0urxm8v7ct2IDjw4I5/7Z6zmqbc38b3RXbj09EySE6Cgqou7s3LbPl5Zso1XlhSwdc9n1EmuxTndMxjXtzWjsjIS9kGxJi6bVLknuF1Fh/jFqyvZtv0g65I20K9tGj1bN47JFERNVVxSxr+WFTBl7kaWb91Hk/q1mZzdmQlntCPjBObTY6Fby0ZMmTiIdzfs5r7XVvOj55fx+JyN3D6mG+d2z4j7PHE8bd9fxsOvryV3yVbW79xPUi1jRJfm3HZeV0b3aEHDuie+Fr2mqUnLJlXuCWzVtn1c99RCdhUdIjXZufdfq4DQCZ9uLRvSt20a/TLT6HdaGp3SU2vc2fzjtedAMX+f/xFPv7OJHfsO0TkjlV9d3JtLTm9TY474zujYjBdvHMbMFdt5YEY+1z+9kIHtmnDH2G4MbN803vGqxeHSMt7/8BNm5e8kL7+Q1ds/w2wNg9s35ZrhHfhK71Y0DfC5iS8sm3xh2efLJn8xvleVL9ksT+WeoGau2M73/rmYhnWTee6GYexe9wE9Tj+DJVv2smTzHhZv3sMrSwr4x/yPAGiQkkTvzMb0bZtG/7Zp9G2bRstGdRPiiHLDziKemLeR5xdt5bPDpYzo0pz7L+3DyC7pNfJEm5kxplcrzu3egpyFW/jd62u47M/vMLpHC24/P4suLap/CV9VK/z0IG/l7yQvfyez1+7k04MlJNcyBndoypXdUvjuxWfSqvGptaKoe6tGvPCdYTz9ziYenHlk2WRXrhneoVoOtFTuCcbd+WPeeh6cmU/fzMY89s2BtGhUl7x1kNGoLqN71GV0jxZA6B3xNu7e/3nZL9m8hyfmbuRwaejDsjIa1gkd3Ye/emc2plENeYrs7ryzYTdT5mzkjdWFpCTV4qL+ofXpibK2PDmpFlcNOY2L+rfmibkb+fNbGzj/odlcPqAt3xvdlZaN4zOFFAulZc6SLXvIW13IrPydLNu6F4AWjerw1d6tGJWVwfDOzWhYtzZ5eXmnXLEfkVTLuGZ4B87r2ZL/fmk59/5rFblLCqpl2aTKPYEcPFzKj55fysuLCxjfrzX3X9rnmNMRtWoZndJT6ZSeyiWnZwKhdcQrC/axZPOez4/y/7NyBwBm0Ck9lb6ZafRrGzrK79ay0VHfQa8qFJeU8cqS0Hz6ym37aNYghVvO6cI3zmhHesPEfJfG+inJTD67C1cNaccjb67jb+9u4qXFW7n2zA7ccFank3r/k+r0yf5iZq/dyazVhby1ZiefHDhMLQu99P+H52eRnZVB91YNE+LZYHVrk1aPKVcPrNZlk3Er9+37y1i/s4hO6anxipBQCvcd5Pq/LWLJ5j388PwsbhzV6YT+iOokJ9H/tCb0P63J55ftOVDM0nLTOXn5hTz//hYAUpJr0bN1I/pmptH/tDT6ZqbRrln9mP8Bf7y/mH/M/5Cn3vmQnZ8eoktGKvdf2pvx/WrOfPrJatoghbsu7ME1w9vz2/+s4c9vrecf8z8KnQwe2q7G3c+ystDqllmrC5mVX8jizXso89D9yM7KILtbBiO6NCetfnDnz2OpupdNxq3cD5bCBb+fy10X9uCKQW31aH8My7bs5fqnF7Lv4GH+MmEA5/dsGdP9p9VPYWTXdEZ2TQdCUyJb93z2+VTOks17+eeCzUx9e1N4fG36ZB6ZzmlM38y0E37v83WFR+bTt3CopIyRXdP5n8s7MKJL88D+TrRtWp/f/Vc/rhvRgftn5PPL6auY+vYmvj+6Kxf1bxPXE9/7Dh5m7tpdzFpdSN6anez89BAAfTMbc/PZXcjulkGfNo1r5LmORFHZsslLT8/kp1+N7bLJqMrdzMYADwNJwOPufl8lY74G/BxwYIm7X3WsfTZKMU5vl8adLywjL7+Q+y7po1f3VeJfS7dx27OLadagDs/dMKxa1m+bGZlN6pPZpD4X9GkNhF5FuGZHEUu27Pn8CP+RN9dSFpq+J7NJvc/n7vu2TaPXMZZjujvz1u1mytwNzMrfSUpyLS7p34Zrz+xA1wCebDyanq0b8/S1g5m3bhf3vbaa255dwl/nbOBHY7oxKiu9Wh7c3J01O4qYlV/IrNWFLPrwE0rKnEZ1kxnZNZ3srAxGdk1P2CmxmuzIssk/vLmWv7y1gbz82C6bjFjuZpYEPAqMBrYAC8ws191XlhvTBbgTGO7un5hZRqT9Nqht/O3aITw+dwMPzsxn7MNz+O3X+jIsgd4MvyqVlTkPv7GWh99Yy8B2TfjzhAE0j+MnAyUn1aJH60b0aN2IKwefBoTe2Gn51r2hI/wte/jgoz28unQbEDqRlNWi4RdW55zWtD5zthzmvofnsHr7pzRPTeF753bl62ecFtf7Fm/DOzfn5ZuGM335Nh6cmc81UxcwpENT7hjb7QvTZ7Gy/1AJb6/fzaz8QvJWF1Kw9yAQWt0xaWRHsrtl0L9tml6AVQ3q1k7ih+d344I+/79s8oX3Q+82ebLLJqM5ch8MrHP3DQBmNg0YD6wsN+Z64FF3/wTA3Qsj7TQlKXTCb9LITgzr1JzvTvuAr0+Zz6QRHbntvKxqPYlX03xWXMptzy5m+rLtXDYgk19e3Is6yTVrPhagQZ1khnRsxpByb39b+OlBlmzeGz5hu4dXlxbwzHuh5Zhm4A7dWtbhgcv6MK5v6xo3zxwvtWoZF/RpzXk9WjJtwUf8/o21XPzHt/lK75b84LwsOp7kuakNO4s+X3c+f8PHFJeW0SAliTO7NOe753ThrKz0U3ZFS01wtGWTE4e1P+EH2WjKvQ2wudz2FmBIhTFdAcxsHqGpm5+7+4xj7bT8k45ebRrzr5tH8It/reQvszcwb/0uHvqv/nTOOPVOtm7b+xnXP72QFQX7+MlXunPdiA4JNfec0fDoyzHX7Cii4f6t3HjpiIS6T9UpJbkW3xzanktOz+TxORt4bPYGZq7YwRWD2nLLOV2ifgXuwcOlvLthN3n5O5mVX8iHuw8A0DkjlauHtSM7K4OB7Zue0gdRNU1lyyZfXlzAfZf2pmfryG9PXZG5+7EHmF0OnO/u14W3JwCD3f3mcmNeBQ4DXwMygTlAL3ffU2Ffk4BJAOnp6QNycnK+dHuLdpTw5PJDFJfCVd1TOCszOa5FUFRURGpq9TzIrN9Tyu8/OMShEuc7/erQNz36893VmfNkJELOmpRx7yEnd30xeZtLSKoFY9rXZmyH2tRLti/l3HmgjKW7Slm6s5RVu0spLoPataB7syT6NE+iT3oSGfWrv8xr0r/nsdSknO7Oe9tL+fuqQxQdDv1/H9+5NnWSjOzs7EXuPjDSPqJpjy1A23LbmUBBJWPedffDwEYzywe6AAsqBH4MeAwgKyvLR40a9aUbGwV8Y+xBbstZwtQVuyjwJtx3aZ+4vVw5Ly+PynLG2osfbOH+15fRslFoPezxvoqxunKerETIWdMyjif0rpe/+Xc+uUu3MXe7cfPZnWlVdxMpbXuFjs5XF7K28DMA2jatx5VDMhjVLYOhHZvFfeqrpv17Hk1Ny5kNfPtAMb+evpp/LtzM8r21+dXFvaL++WjKfQHQxcw6AFuBK4CKK2FeAq4EpppZc0LTNBuiTlFBi0Z1efrawUyZu5EHZq5mzEOz+e3X+nFml+CdbC0rcx78dz5/ylvPGR2b8qevD9CqIfmS9s0b8MhVpzNp5B7ue201d7+yEgOc+dROMoZ0aMZ/DWpLdrcMOjZvoGmvgEirn8L9l/UJLZt8cRnfmDI/6p+NWO7uXmJmk4GZhObTn3D3FWZ2D7DQ3XPD151nZiuBUuCH7r77hO5NWK1axvUjOzKsczO++8wHfGPKfK4f0YEfnJ9VI08unoiiQyXcOm0xr6/awZWDT+PucT01ByrH1Cczjb9fN4TZa3fxzJsfcMmIPgzr3JzUOnqxeZAN7dSM124ZwR/eXMvt90f3M1H9Rrj7dGB6hcvuKve9A98Pf8VUz9aNefXmEfxy+kr+Omcj89bt5vdX9ovL5yfG0uaPD3D90wtZW1jE3eN68s2h7XS0JVExM87qmo4XpDAqxi9ok5rryLLJ26McnxCHifVSkrj3ot789ZsD2b7vIBf8YS7/++6HRDoZXFMt2PQx4x+dR8Gez5h6zSCuHtZexS4iMZUQ5X7E6B4tmHHLCAa1b8pPX1rO9U8v4uP9xfGOdVxyFmzmqr++S1q92rx003BGdEmPdyQRCaCEKncIva3tU9cM5qdf7c7sNTs5/6HZzFm7M96xIiotc+59dSW3P780/IEOw0/6hSkiIkeTcOUOoZOt143oyIs3DaNxvdpMmPIe9766kkMlpfGOVql9Bw9z7dQFPD53IxOHtefJiYNoXD8x3uZVRBJTQpb7ET1bN+aVyWcy4Yx2PD53Ixc9+jZrd3wa71hfsGnXfi5+dB7z1u3iVxf35ufjeuo9O0SkyiV8y9RLSeIXF/ViytUD2RE+2fq3dzbViJOtb6/fxUV/nMfu/cX87VtDuGrIafGOJCKniIQv9yPO6d6CGbeOYEjHZvz3yyu4/umF7C46FLc8//vuh3xzynukp9Yh96YzGdqpWeQfEhGJkcCUO4TetGrqxEHcdUEPZq/ZxfkPzeGtNdV7svVwaRl3vbycn760nJFd03nhxmGc1qz6PvFcRAQCVu4QOtl67ZkdeHnycJo2qM3VT7zHPa+s5ODhqj/ZuudAMROffI+n3/mQSSM78tdvDqRhDfnAaRE5tQSu3I/o3qoRuZPP5Oqh7Xhi3kYuenQea6rwZOu6wiIuenQeCzZ+woOX9eHHX+ke149LE5FTW2DLHUIv1717fC+emDiQnZ8e4sI/zOWpt2N/svWtNTu5+I/zKDpUwj+uH8LlA9tG/iERkSoU6HI/4uxuLZhx60iGdmrGz3JX8K2nFrIrBidb3Z0n5m7kmiffI7NJfV66aTgD2zeNQWIRkZNzSpQ7QHrDOjw5cRA/u7AHc9ftYsxDc8jLj/hpgEdVXFLGnS8s455XVzK6Rwueu2EomU104lREaoZTptwh9G561wzvQO7k4TRrkMLEJxfw89wVx32y9eP9xXxjynymLdjM5OzO/OnrA2igt1wVkRrklCr3I7q1bMTLk4czcVh7pr69ifGPzCN/e3QnW/O3f8q4R+ayePMeHr6iHz84P4taOnEqIjXMKVnuEDrZ+vNxPXnymkHs3n+ICx+Zy9R5G495svX1lTu45I/zKC4pI+fbQxnfr001JhYRid4pW+5HZGdlMOPWkQzv1Iyfv7KSa6YuYOenXzzZ6u78+a31XP+3hXRMTyV38pn0a5sWp8QiIpGd8uUO0Dy1Dk9MHMTd43ry9vrdjHloNm+u3gFAcalz27NLuO+11Xy1dytyvj2Ulo3rxjmxiMix6SxgmJlx9bD2nNGxGbdM+4Brpy5kwhnteGf1Qdbt2cr3R3fl5rM76xOTRCQhqNwryGrZkJduGs79M1bz5LxNpCTBn75+OmN7t4p3NBGRqKncK1G3dhI/u7AnF/RpRf6yxSp2EUk4Uc25m9kYM8s3s3Vmdkcl1080s51mtjj8dV3so1a/Ae2a0jpVpyVEJPFEPHI3syTgUWA0sAVYYGa57r6ywtB/uvvkKsgoIiLHKZrD0sHAOnff4O7FwDRgfNXGEhGRk2GR3iHRzC4Dxrj7deHtCcCQ8kfpZjYR+DWwE1gDfM/dN1eyr0nAJID09PQBOTk5MbobVaeoqIjU1NR4x4hIOWMnETKCcsZaouTMzs5e5O4DIw5092N+AZcDj5fbngD8ocKYZkCd8Pc3AG9G2m/Xrl09EcyaNSveEaKinLGTCBndlTPWEiUnsNAj9Ku7RzUtswUo/wblmUBBhQeI3e5+5GWdfwUGRLFfERGpItGU+wKgi5l1MLMU4Aogt/wAMyu/VnAcsCp2EUVE5HhFXC3j7iVmNhmYCSQBT7j7CjO7h9DTg1zgu2Y2DigBPgYmVmFmERGJIKoXMbn7dGB6hcvuKvf9ncCdsY0mIiInSq/QEREJIJW7iEgAqdxFRAJI5S4iEkAqdxGRAFK5i4gEkMpdRCSAVO4iIgGkchcRCSCVu4hIAKncRUQCSOUuIhJAKncRkQBSuYuIBJDKXUQkgFTuIiIBpHIXEQkglbuISACp3EVEAkjlLiISQFGVu5mNMbN8M1tnZnccY9xlZuZmNjB2EUVE5HhFLHczSwIeBcYCPYArzaxHJeMaAt8F5sc6pIiIHJ9ojtwHA+vcfYO7FwPTgPGVjPsF8ABwMIb5RETkBJi7H3uA2WXAGHe/Lrw9ARji7pPLjekP/NTdLzWzPOAH7r6wkn1NAiYBpKenD8jJyYnZHakqRUVFpKamxjtGRMoZO4mQEZQz1hIlZ3Z29iJ3jzj1nRzFvqySyz5/RDCzWsDvgImRduTujwGPAWRlZfmoUaOiuPn4ysvLQzljJxFyJkJGUM5YS5Sc0YpmWmYL0LbcdiZQUG67IdALyDOzTcAZQK5OqoqIxE805b4A6GJmHcwsBbgCyD1ypbvvdffm7t7e3dsD7wLjKpuWERGR6hGx3N29BJgMzARWATnuvsLM7jGzcVUdUEREjl80c+64+3RgeoXL7jrK2FEnH0tERE6GXqEqIhJAKncRkQBSuYuIBJDKXUQkgFTuIiIBpHIXEQkglbuISACp3EVEAkjlLiISQCp3EZEAUrmLiASQyl1EJIBU7iIiAaRyFxEJIJW7iEgAqdxFRAJI5S4iEkAqdxGRAFK5i4gEkMpdRCSAoip3MxtjZvlmts7M7qjk+hvMbJmZLTazuWbWI/ZRRUQkWhHL3cySgEeBsUAP4MpKyvsf7t7b3fsBDwC/jXlSERGJWjRH7oOBde6+wd2LgWnA+PID3H1fuc0GgMcuooiIHC9zP3YPm9llwBh3vy68PQEY4u6TK4y7Cfg+kAKc7e5rK9nXJGASQHp6+oCcnJyY3ImqVFRURGpqarxjRKScsZMIGUE5Yy1RcmZnZy9y94ERB7r7Mb+Ay4HHy21PAP5wjPFXAU9F2m/Xrl09EcyaNSveEaKinLGTCBndlTPWEiUnsNAj9Ku7RzUtswVoW247Eyg4xvhpwEVR7FdERKpINOW+AOhiZh3MLAW4AsgtP8DMupTb/CrwpSkZERGpPsmRBrh7iZlNBmYCScAT7r7CzO4h9PQgF5hsZucCh4FPgKurMrSIiBxbxHIHcPfpwPQKl91V7vtbYpxLREROgl6hKiISQCp3EZEAUrmLiASQyl1EJIBU7iIiAaRyFxEJIJW7iEgAqdxFRAJI5S4iEkAqdxGRAFK5i4gEkMpdRCSAVO4iIgGkchcRCSCVu4hIAKncRUQCSOUuIhJAKncRkQBSuYuIBJDKXUQkgKIqdzMbY2b5ZrbOzO6o5Prvm9lKM1tqZm+YWbvYRxURkWhFLHczSwIeBcYCPYArzaxHhWEfAAPdvQ/wHPBArIOKiEj0ojlyHwysc/cN7l4MTAPGlx/g7rPc/UB4810gM7YxRUTkeJi7H3uA2WXAGHe/Lrw9ARji7pOPMv4RYLu731vJdZOASQDp6ekDcnJyTjJ+1SsqKiI1NTXeMSJSzthJhIygnLGWKDmzs7MXufvASOOSo9iXVXJZpY8IZvYNYCBwVmXXu/tjwGMAWVlZPmrUqChuPr7y8vJQzthJhJyJkBGUM9YSJWe0oin3LUDbctuZQEHFQWZ2LvAT4Cx3PxSbeCIiciKimXNfAHQxsw5mlgJcAeSWH2Bm/YG/AOPcvTD2MUVE5HhELHd3LwEmAzOBVUCOu68ws3vMbFx42INAKvCsmS02s9yj7E5ERKpBNNMyuPt0YHqFy+4q9/25Mc4lIiInQa9QFREJIJW7iEgAqdxFRAJI5S4iEkAqdxGRAFK5i4gEkMpdRCSAVO4iIgGkchcRCSCVu4hIAKncRUQCSOUuIhJAKncRkQBSuYuIBJDKXUQkgFTuIiIBpHIXEQkglbuISACp3EVEAkjlLiISQFGVu5mNMbN8M1tnZndUcv1IM3vfzErM7LLYxxQRkeMRsdzNLAl4FBgL9ACuNLMeFYZ9BEwE/hHrgCIicvySoxgzGFjn7hsAzGwaMB5YeWSAu28KX1dWBRlFROQ4mbsfe0BommWMu18X3p4ADHH3yZWMnQq86u7PHWVfk4BJAOnp6QNycnJOLn01KCoqIjU1Nd4xIlLO2EmEjKCcsZYoObOzsxe5+8BI46I5crdKLjv2I8JRuPtjwGMAWVlZPmrUqBPZTbXKy8tDOWMnEXImQkZQzlhLlJzRiuaE6hagbbntTKCgauKIiEgsRFPuC4AuZtbBzFKAK4Dcqo0lIiInI2K5u3sJMBmYCawCctx9hZndY2bjAMxskJltAS4H/mJmK6oytIiIHFs0c+64+3RgeoXL7ir3/QJC0zUiIlID6BWqIiIBpHIXEQkglbuISACp3EVEAkjlLiISQCp3EZEAUrmLiASQyl1EJIBU7iIiAaRyFxEJIJW7iEgAqdxFRAJI5S4iEkAqdxGRAFK5i4gEkMpdRCSAVBtuL9sAAAXiSURBVO4iIgGkchcRCSCVu4hIAKncRUQCKKpyN7MxZpZvZuvM7I5Krq9jZv8MXz/fzNrHOqiIiEQvYrmbWRLwKDAW6AFcaWY9Kgz7FvCJu3cGfgfcH+ugIiISvWiO3AcD69x9g7sXA9OA8RXGjAeeCn//HHCOmVnsYoqIyPEwdz/2ALPLgDHufl14ewIwxN0nlxuzPDxmS3h7fXjMrgr7mgRMAkhPTx+Qk5MTy/tSJYqKikhNTY13jIiUM3YSISMoZ6wlSs7s7OxF7j4w0rjkKPZV2RF4xUeEaMbg7o8BjwFkZWX5qFGjorj5+MrLy0M5YycRciZCRlDOWEuUnNGKZlpmC9C23HYmUHC0MWaWDDQGPo5FQBEROX7RlPsCoIuZdTCzFOAKILfCmFzg6vD3lwFveqT5HhERqTIRp2XcvcTMJgMzgSTgCXdfYWb3AAvdPReYAvzNzNYROmK/oipDi4jIsUUz5467TwemV7jsrnLfHwQuj200ERE5UXqFqohIAKncRUQCSOUuIhJAKncRkQCK+ArVKrths0+B/Ljc+PFpDuyKOCr+lDN2EiEjKGesJUrOLHdvGGlQVKtlqkh+NC+hjTczW6icsZMIORMhIyhnrCVSzmjGaVpGRCSAVO4iIgEUz3J/LI63fTyUM7YSIWciZATljLVA5YzbCVUREak6mpYREQkglbuISABVe7mb2RNmVhj+9KYayczamtksM1tlZivM7JZ4Z6qMmdU1s/fMbEk4593xznQsZpZkZh+Y2avxznI0ZrbJzJaZ2eJol5zFg5mlmdlzZrY6/Hs6NN6ZKjKzrPC/45GvfWZ2a7xzVcbMvhf+G1puZs+YWd14Z6rIzG4J51sRzb9jtc+5m9lIoAh42t17VeuNR8nMWgGt3P19M2sILAIucveVcY72BeHPqW3g7kVmVhuYC9zi7u/GOVqlzOz7wECgkbtfEO88lTGzTcDAih8RWdOY2VPAHHd/PPw5C/XdfU+8cx2NmSUBWwl9/OaH8c5Tnpm1IfS308PdPzOzHGC6u0+Nb7L/Z2a9CH1+9WCgGJgBfMfd1x7tZ6r9yN3dZ1PDP6XJ3be5+/vh7z8FVgFt4pvqyzykKLxZO/xVI8+Qm1km8FXg8XhnSXRm1ggYSehzFHD34ppc7GHnAOtrWrGXkwzUC3+SXH2+/Glz8dYdeNfdD7h7CfAWcPGxfkBz7hGYWXugPzA/vkkqF57qWAwUAv9x9xqZE3gIuB0oi3eQCBz4t5ktCn+ge03UEdgJPBme5nrczBrEO1QEVwDPxDtEZdx9K/Ab4CNgG7DX3f8d31RfshwYaWbNzKw+8BW++PGnX6JyPwYzSwWeB251933xzlMZdy91936EPtt2cPjpW41iZhcAhe6+KN5ZojDc3U8HxgI3hacRa5pk4HTgT+7eH9gP3BHfSEcXnjYaBzwb7yyVMbMmwHigA9AaaGBm34hvqi9y91XA/cB/CE3JLAFKjvUzKvejCM9hPw/83d1fiHeeSMJPy/OAMXGOUpnhwLjwfPY04Gwz+9/4RqqcuxeE/1sIvEhojrOm2QJsKfcs7TlCZV9TjQXed/cd8Q5yFOcCG919p7sfBl4AhsU505e4+xR3P93dRxKa2j7qfDuo3CsVPlE5BVjl7r+Nd56jMbN0M0sLf1+P0C/p6vim+jJ3v9PdM929PaGn52+6e406MgIwswbhE+iEpznOI/R0uEZx9+3AZjPLCl90DlCjTvZXcCU1dEom7CPgDDOrH/7bP4fQebYaxcwywv89DbiECP+m1f6ukGb2DDAKaG5mW4CfufuU6s4RwXBgArAsPJ8N8OPwZ8nWJK2Ap8IrEWoBOe5eY5cZJoAWwIuhv2+SgX+4+4z4Rjqqm4G/h6c8NgDXxDlPpcLzw6OBb8c7y9G4+3wzew54n9BUxwfUzLcieN7MmgGHgZvc/ZNjDdbbD4iIBJCmZUREAkjlLiISQCp3EZEAUrmLiASQyl1EJIBU7iIiAaRyFxEJoP8DTKtNzd/k8NIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reducedData = numpy.load('labeled_reduit_50dim.npy')\n",
    "numberOfData = reducedData.shape[0]\n",
    "dimensions = reducedData.shape[1]\n",
    "\n",
    "#print(numberOfData, dimensions)\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train_reduced = minmax_scale(reducedData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test_reduced = minmax_scale(reducedData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données\n",
    "\n",
    "compute_linear_results(X_train_reduced, X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification avec fonction d'activation elu à 9 couches: +- 82% de taux de réussite\n",
    "\n",
    "Classification avec fonction d'activation relu à 9 couches: +- 50% de taux de réussite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenir compte du fait que l'on n'a pas beaucoup de données considérant le nombre de dimensions\n",
    "\n",
    "2 et 6 layers ressortent souvent (impression?) possibilité d'avoir une genre d'interférence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Déclaration de la fonction permettant la classification par réseau de neurones profond de type convolution\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calcul du taux de réussite en classement d'un de convolution sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning pour Classification\n",
    "\n",
    "Ce notebook regroupe quelques configurations d'algorithmes de Deep Learning pour la classification de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Variable                        Possibilités             Nombre de colonne (+1)\n",
      "Spécialité de médecins\t        33 codes de spécialité\t        1-33\n",
      "Sexe de médecin \t            2\t                            34-35\n",
      "Langue de correspondance \t    2\t                            36-37\n",
      "Université de graduation \t    14 universités\t                38-51\n",
      "Plage horaire de facturation\t3 (AM, PM, toute la journée)\t52-54\n",
      "Agence de représentation\t    845 codes d’agences             55-899\n",
      "Établissements                  241 établissements\t            900-1140\n",
      "ActeMedical                     2 (Oui et Non)\t                1141-1142\n",
      "Activités médico-admin          2 (Oui et Non)\t                1143-1144\n",
      "Activités d’enseignement\t    2 (Oui et Non)\t                1145-1146\n",
      "Entente LE\t                    2 (Oui et Non)\t                1147-1148\n",
      "Activité en santé communautaire\t2 (Oui et Non)\t                1149-1150\n",
      "Activité en santé comm. CSST\t2 (Oui et Non)\t                1151-1152\n",
      "Activité en santé comm. INSP\t2 (Oui et Non)\t                1153-1154\n",
      "Année de graduation\t                                            1155\n",
      "semaine de l’année associée à la facturation\t                1156\n",
      "Nombre de jours dès la dernière facture avant le Perdiem\t    1157\n",
      "Année de naissance\t                                            1158\n",
      "Année de début de pratique\t                                    1159\n",
      "Expérience\t                                                    1160\n",
      "Salaire régulier\t                                            1161\n",
      "Salaire payé\t                                                1162\n",
      "Nombre de Perdiem pendant l’année\t                            1163\n",
      "Jour de la semaine associé à la facturation\t                    1164\n",
      "Nombre de Perdiem\t                                            1165\n",
      "Nombre d’heures facturées\t                                    1166\n",
      "Nombre de services avec bénéficiaire\t                        1167\n",
      "Nombre de services sans bénéficiaire\t                        1168\n",
      "Nombre de patients\t                                            1169\n",
      "Montant réclamé avec bénéficiaire\t                            1170\n",
      "Montant réclamé sans bénéficiaire\t                            1171\n",
      "Nombre de service total\t                                        1172\n",
      "Nombre de Perdiem facturé le jour avant\t                        1173\n",
      "Nombre d’heures facturées le jour avant\t                        1174\n",
      "Nombre de services avec bénéficiaire le jour avant\t            1175\n",
      "Nombre de services sans bénéficiaire le jour avant\t            1176\n",
      "Nombre de patients le jour avant\t                            1177\n",
      "Montant réclamé avec bénéficiaire le jour avant\t                1178\n",
      "Montant réclamé sans bénéficiaire le jour avant\t                1179\n",
      "Nombre de services total le jour avant\t                        1180\n",
      "âge de médecin le jour avant \t                                1181\n",
      "Nombre de Perdiem facturés la semaine d’avant\t                1182\n",
      "\n",
      "Target:\n",
      "Conformité\n",
      "(-1 : non contrôlé, 1 : cas conforme, 0 : cas non-conforme)\t    1183\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enregistrement du jeu de données comportant les données avec étiquettes\n",
    "Ici, le jeu de données comportant uniquement les données avec des étiquettes de classe est téléchargé. Il est ensuite séparé en jeux d'entraînement et de test en plus d'être normalisé dans toutes les dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData = numpy.load('labeled.npy')\n",
    "numberOfData = labeledData.shape[0] # 1441\n",
    "dimensions = labeledData.shape[1] # 1183\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train = minmax_scale(labeledData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test = minmax_scale(labeledData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Déclaration de fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_sampler(dataset):\n",
    "    def make_weights_for_balanced_classes(images, n_classes):                        \n",
    "        count = [0] * n_classes                                                      \n",
    "        for item in images:                                                         \n",
    "            count[int(item[1])] += 1                                                     \n",
    "        weight_per_class = [0.] * n_classes                                      \n",
    "        N = float(sum(count))                                                   \n",
    "        for i in range(n_classes):                                                   \n",
    "            weight_per_class[i] = N/float(count[i])                                 \n",
    "        weight = [0] * len(images)                                              \n",
    "        for idx, val in enumerate(images):                                          \n",
    "            weight[idx] = weight_per_class[int(val[1])]                                  \n",
    "        return weight\n",
    "\n",
    "    n_classes = numpy.unique(dataset.targets)\n",
    "    weights = make_weights_for_balanced_classes(dataset.data, len(n_classes))                                                         \n",
    "    weights = torch.DoubleTensor(weights)                 \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    return sampler\n",
    "\n",
    "def compute_accuracy(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition du Dataset sans transformation de la forme des données\n",
    "Définition de la classe RAMQDataset, une classe qui hérite de la classe abstraite torch.utils.data.Dataset. Comme mentionné dans la documentation, les méthodes __getitem__ et __len__ sont surchargées afin d'avoir un jeu de données utilisable par PyTorch. Le data accepté en paramètres est un array numpy dont la dernière dimension est la valeur de l'étiquette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Cette classe sert à définir le dataset RAMQ pour PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for elem in data:\n",
    "            if isinstance(elem, numpy.ndarray):\n",
    "                elem = elem.tolist()\n",
    "                elem_data = torch.Tensor(elem[:-1])\n",
    "                elem_target = torch.FloatTensor([elem[-1]])\n",
    "            # garde les paramètres en mémoire\n",
    "            self.data += [(elem_data, elem_target[0])]\n",
    "            \n",
    "        self.targets = numpy.array(data[:, -1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Déclaration d'un réseau de neurones de base: 1 couche - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.F1 = nn.Linear(1182, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.F1(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Déclaration d'un réseau de neurones de type linéraire: multicouches\n",
    "Ce réseau est personnalisable à souhait: il est possible d'y indiquer, en paramètres d'entrée, le nombre de dimensions en input, le nombre de couches linéaires à avoir dans le réseau en question et le type de la fonction d'activation qui y sera relié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_layers, activation = nn.ELU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        reduction_ratio = (1/n_inputs)**(1/n_layers)\n",
    "        \n",
    "        layers = []\n",
    "        layer_lenght = n_inputs\n",
    "        \n",
    "        # Calculate nb of layers and nb of neurons\n",
    "        # Builds a list (ex:[1000, 500, 250, ... until 1])\n",
    "        while layer_lenght > 1:\n",
    "            layers.append(layer_lenght)\n",
    "            layer_lenght = int(layer_lenght*reduction_ratio)\n",
    "        layers.append(1)\n",
    "        \n",
    "        # Build network layers\n",
    "        network_layers = []\n",
    "        for i, n_neurons in enumerate(layers[:-1]):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            network_layers.append(layer)\n",
    "            network_layers.append(activation)\n",
    "            network_layers.append(nn.BatchNorm1d(layers[i+1]))\n",
    "        network_layers[-1] = nn.Sigmoid()\n",
    "            \n",
    "        # Build pytorch sequential network\n",
    "        self.network = nn.Sequential(\n",
    "            *network_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Exécute l'inférence du réseau. L'ordre \n",
    "        # d'exécution ici est important.\n",
    "        x = self.network(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Déclaration de la fonction permettant l'affichage du pourcentage d'efficacité en classement selon 1 à 9 couches d'un réseau de neurones \"x\"\n",
    "Cette méthode n'a besoin, en entrées, que d'un tableau des pourcentages d'efficacité pour 0 à 9 couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(test_accu, start, end):\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(test_accu, label='Test accuracy')\n",
    "    ax.set_xlim(start, end)\n",
    "    pyplot.grid()\n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Déclaration de la fonction permettant la classification par réseau de neurones profond de type multicouches linéaire\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_results(X_train, X_test):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cpu\" #if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 250\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Définit le nombre de dimensions des données avec lesquelles on travaille (la dernière dimension étant l'étiquette)\n",
    "    dimensions = X_train.shape[1] - 1  \n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "\n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "\n",
    "    # Instancier un réseau RAMQNetLinear\n",
    "    # dans une variable nommée \"model\"\n",
    "    for i in range(1,10):\n",
    "        print(\"Je vais utiliser \" + str(i) + \" layers\")\n",
    "        model = RAMQNetLinear(dimensions, i)\n",
    "\n",
    "        # Tranfert le réseau au bon endroit\n",
    "        model.to(device)\n",
    "\n",
    "        # Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "        # et la mettre dans une variable nommée criterion\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        # Ne pas oublier de lui donner les hyperparamètres\n",
    "        # d'entraînement : learning rate et momentum!\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer\n",
    "                # une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu, 1, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calcul du taux de réussite en classement d'un SVM linéaire de base sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on all dimensions: 0.8361111111111111\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM performance with all dimensions\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train[:, : -1], X_train[:, -1])\n",
    "score = clf.score(X_test[:, : -1], X_test[:, -1])\n",
    "print(\"Score on all dimensions:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on tente de classifier les 1441 données à 1182 dimensions avec un réseau de neurones à 1 couche linéaire, on obtient un pourcentage de classement de l'ordre d'environ 82%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calcul du taux de réussite en classement d'un réseau linéaire multicouches sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.686894 in 0.03s\n",
      " [-] epoch    2/250, train loss 0.658042 in 0.03s\n",
      " [-] epoch    3/250, train loss 0.630507 in 0.02s\n",
      " [-] epoch    4/250, train loss 0.617247 in 0.03s\n",
      " [-] epoch    5/250, train loss 0.598471 in 0.03s\n",
      " [-] epoch    6/250, train loss 0.585760 in 0.03s\n",
      " [-] epoch    7/250, train loss 0.585472 in 0.03s\n",
      " [-] epoch    8/250, train loss 0.578508 in 0.03s\n",
      " [-] epoch    9/250, train loss 0.559436 in 0.03s\n",
      " [-] epoch   10/250, train loss 0.566538 in 0.03s\n",
      " [-] epoch   11/250, train loss 0.546757 in 0.03s\n",
      " [-] epoch   12/250, train loss 0.543458 in 0.03s\n",
      " [-] epoch   13/250, train loss 0.539289 in 0.03s\n",
      " [-] epoch   14/250, train loss 0.532973 in 0.02s\n",
      " [-] epoch   15/250, train loss 0.528345 in 0.03s\n",
      " [-] epoch   16/250, train loss 0.514023 in 0.03s\n",
      " [-] epoch   17/250, train loss 0.529386 in 0.03s\n",
      " [-] epoch   18/250, train loss 0.517567 in 0.04s\n",
      " [-] epoch   19/250, train loss 0.519313 in 0.03s\n",
      " [-] epoch   20/250, train loss 0.512015 in 0.03s\n",
      " [-] epoch   21/250, train loss 0.506013 in 0.03s\n",
      " [-] epoch   22/250, train loss 0.528303 in 0.03s\n",
      " [-] epoch   23/250, train loss 0.513400 in 0.03s\n",
      " [-] epoch   24/250, train loss 0.506760 in 0.02s\n",
      " [-] epoch   25/250, train loss 0.525287 in 0.04s\n",
      " [-] epoch   26/250, train loss 0.493433 in 0.03s\n",
      " [-] epoch   27/250, train loss 0.483047 in 0.02s\n",
      " [-] epoch   28/250, train loss 0.514353 in 0.04s\n",
      " [-] epoch   29/250, train loss 0.486162 in 0.03s\n",
      " [-] epoch   30/250, train loss 0.486986 in 0.02s\n",
      " [-] epoch   31/250, train loss 0.481819 in 0.04s\n",
      " [-] epoch   32/250, train loss 0.474423 in 0.03s\n",
      " [-] epoch   33/250, train loss 0.496064 in 0.03s\n",
      " [-] epoch   34/250, train loss 0.486675 in 0.03s\n",
      " [-] epoch   35/250, train loss 0.481116 in 0.03s\n",
      " [-] epoch   36/250, train loss 0.475196 in 0.03s\n",
      " [-] epoch   37/250, train loss 0.470870 in 0.03s\n",
      " [-] epoch   38/250, train loss 0.487678 in 0.03s\n",
      " [-] epoch   39/250, train loss 0.465195 in 0.03s\n",
      " [-] epoch   40/250, train loss 0.481104 in 0.04s\n",
      " [-] epoch   41/250, train loss 0.477669 in 0.03s\n",
      " [-] epoch   42/250, train loss 0.468704 in 0.03s\n",
      " [-] epoch   43/250, train loss 0.469356 in 0.03s\n",
      " [-] epoch   44/250, train loss 0.468588 in 0.03s\n",
      " [-] epoch   45/250, train loss 0.469191 in 0.03s\n",
      " [-] epoch   46/250, train loss 0.486303 in 0.03s\n",
      " [-] epoch   47/250, train loss 0.471700 in 0.03s\n",
      " [-] epoch   48/250, train loss 0.477710 in 0.03s\n",
      " [-] epoch   49/250, train loss 0.458941 in 0.03s\n",
      " [-] epoch   50/250, train loss 0.462526 in 0.03s\n",
      " [-] epoch   51/250, train loss 0.464078 in 0.03s\n",
      " [-] epoch   52/250, train loss 0.475905 in 0.03s\n",
      " [-] epoch   53/250, train loss 0.470088 in 0.03s\n",
      " [-] epoch   54/250, train loss 0.464953 in 0.03s\n",
      " [-] epoch   55/250, train loss 0.454509 in 0.03s\n",
      " [-] epoch   56/250, train loss 0.462534 in 0.03s\n",
      " [-] epoch   57/250, train loss 0.440198 in 0.03s\n",
      " [-] epoch   58/250, train loss 0.453093 in 0.03s\n",
      " [-] epoch   59/250, train loss 0.434883 in 0.03s\n",
      " [-] epoch   60/250, train loss 0.469784 in 0.03s\n",
      " [-] epoch   61/250, train loss 0.462267 in 0.03s\n",
      " [-] epoch   62/250, train loss 0.445654 in 0.03s\n",
      " [-] epoch   63/250, train loss 0.439448 in 0.03s\n",
      " [-] epoch   64/250, train loss 0.463466 in 0.03s\n",
      " [-] epoch   65/250, train loss 0.443017 in 0.03s\n",
      " [-] epoch   66/250, train loss 0.427959 in 0.03s\n",
      " [-] epoch   67/250, train loss 0.431752 in 0.03s\n",
      " [-] epoch   68/250, train loss 0.460021 in 0.03s\n",
      " [-] epoch   69/250, train loss 0.447327 in 0.03s\n",
      " [-] epoch   70/250, train loss 0.441008 in 0.03s\n",
      " [-] epoch   71/250, train loss 0.444952 in 0.03s\n",
      " [-] epoch   72/250, train loss 0.449204 in 0.03s\n",
      " [-] epoch   73/250, train loss 0.443091 in 0.03s\n",
      " [-] epoch   74/250, train loss 0.443718 in 0.03s\n",
      " [-] epoch   75/250, train loss 0.443723 in 0.03s\n",
      " [-] epoch   76/250, train loss 0.442998 in 0.03s\n",
      " [-] epoch   77/250, train loss 0.438818 in 0.02s\n",
      " [-] epoch   78/250, train loss 0.425812 in 0.03s\n",
      " [-] epoch   79/250, train loss 0.455550 in 0.03s\n",
      " [-] epoch   80/250, train loss 0.461926 in 0.03s\n",
      " [-] epoch   81/250, train loss 0.437535 in 0.03s\n",
      " [-] epoch   82/250, train loss 0.435652 in 0.02s\n",
      " [-] epoch   83/250, train loss 0.446051 in 0.03s\n",
      " [-] epoch   84/250, train loss 0.454365 in 0.03s\n",
      " [-] epoch   85/250, train loss 0.426782 in 0.03s\n",
      " [-] epoch   86/250, train loss 0.449189 in 0.03s\n",
      " [-] epoch   87/250, train loss 0.439182 in 0.03s\n",
      " [-] epoch   88/250, train loss 0.439271 in 0.03s\n",
      " [-] epoch   89/250, train loss 0.446835 in 0.03s\n",
      " [-] epoch   90/250, train loss 0.432996 in 0.03s\n",
      " [-] epoch   91/250, train loss 0.423893 in 0.04s\n",
      " [-] epoch   92/250, train loss 0.450408 in 0.03s\n",
      " [-] epoch   93/250, train loss 0.426811 in 0.03s\n",
      " [-] epoch   94/250, train loss 0.420783 in 0.03s\n",
      " [-] epoch   95/250, train loss 0.423785 in 0.03s\n",
      " [-] epoch   96/250, train loss 0.426003 in 0.03s\n",
      " [-] epoch   97/250, train loss 0.456690 in 0.03s\n",
      " [-] epoch   98/250, train loss 0.420610 in 0.03s\n",
      " [-] epoch   99/250, train loss 0.426513 in 0.03s\n",
      " [-] epoch  100/250, train loss 0.461158 in 0.03s\n",
      " [-] epoch  101/250, train loss 0.415989 in 0.03s\n",
      " [-] epoch  102/250, train loss 0.426366 in 0.03s\n",
      " [-] epoch  103/250, train loss 0.448316 in 0.03s\n",
      " [-] epoch  104/250, train loss 0.439311 in 0.03s\n",
      " [-] epoch  105/250, train loss 0.445333 in 0.03s\n",
      " [-] epoch  106/250, train loss 0.423601 in 0.03s\n",
      " [-] epoch  107/250, train loss 0.439187 in 0.03s\n",
      " [-] epoch  108/250, train loss 0.433433 in 0.03s\n",
      " [-] epoch  109/250, train loss 0.441506 in 0.03s\n",
      " [-] epoch  110/250, train loss 0.419462 in 0.03s\n",
      " [-] epoch  111/250, train loss 0.432694 in 0.03s\n",
      " [-] epoch  112/250, train loss 0.415232 in 0.03s\n",
      " [-] epoch  113/250, train loss 0.432503 in 0.03s\n",
      " [-] epoch  114/250, train loss 0.418831 in 0.03s\n",
      " [-] epoch  115/250, train loss 0.399350 in 0.03s\n",
      " [-] epoch  116/250, train loss 0.431033 in 0.03s\n",
      " [-] epoch  117/250, train loss 0.415211 in 0.03s\n",
      " [-] epoch  118/250, train loss 0.422882 in 0.03s\n",
      " [-] epoch  119/250, train loss 0.434474 in 0.03s\n",
      " [-] epoch  120/250, train loss 0.415012 in 0.03s\n",
      " [-] epoch  121/250, train loss 0.424558 in 0.03s\n",
      " [-] epoch  122/250, train loss 0.421011 in 0.03s\n",
      " [-] epoch  123/250, train loss 0.417624 in 0.02s\n",
      " [-] epoch  124/250, train loss 0.420680 in 0.03s\n",
      " [-] epoch  125/250, train loss 0.415637 in 0.03s\n",
      " [-] epoch  126/250, train loss 0.412918 in 0.03s\n",
      " [-] epoch  127/250, train loss 0.414603 in 0.03s\n",
      " [-] epoch  128/250, train loss 0.416057 in 0.03s\n",
      " [-] epoch  129/250, train loss 0.412618 in 0.03s\n",
      " [-] epoch  130/250, train loss 0.404668 in 0.03s\n",
      " [-] epoch  131/250, train loss 0.422584 in 0.03s\n",
      " [-] epoch  132/250, train loss 0.426590 in 0.03s\n",
      " [-] epoch  133/250, train loss 0.407028 in 0.03s\n",
      " [-] epoch  134/250, train loss 0.407906 in 0.03s\n",
      " [-] epoch  135/250, train loss 0.409173 in 0.03s\n",
      " [-] epoch  136/250, train loss 0.413573 in 0.03s\n",
      " [-] epoch  137/250, train loss 0.419781 in 0.03s\n",
      " [-] epoch  138/250, train loss 0.418465 in 0.03s\n",
      " [-] epoch  139/250, train loss 0.438541 in 0.03s\n",
      " [-] epoch  140/250, train loss 0.401110 in 0.03s\n",
      " [-] epoch  141/250, train loss 0.435644 in 0.02s\n",
      " [-] epoch  142/250, train loss 0.406052 in 0.03s\n",
      " [-] epoch  143/250, train loss 0.425998 in 0.03s\n",
      " [-] epoch  144/250, train loss 0.390823 in 0.03s\n",
      " [-] epoch  145/250, train loss 0.437552 in 0.03s\n",
      " [-] epoch  146/250, train loss 0.448306 in 0.03s\n",
      " [-] epoch  147/250, train loss 0.410591 in 0.03s\n",
      " [-] epoch  148/250, train loss 0.428829 in 0.03s\n",
      " [-] epoch  149/250, train loss 0.424597 in 0.03s\n",
      " [-] epoch  150/250, train loss 0.414360 in 0.03s\n",
      " [-] epoch  151/250, train loss 0.411201 in 0.03s\n",
      " [-] epoch  152/250, train loss 0.416670 in 0.03s\n",
      " [-] epoch  153/250, train loss 0.407897 in 0.03s\n",
      " [-] epoch  154/250, train loss 0.421654 in 0.03s\n",
      " [-] epoch  155/250, train loss 0.410982 in 0.03s\n",
      " [-] epoch  156/250, train loss 0.409894 in 0.03s\n",
      " [-] epoch  157/250, train loss 0.405318 in 0.03s\n",
      " [-] epoch  158/250, train loss 0.410543 in 0.03s\n",
      " [-] epoch  159/250, train loss 0.418293 in 0.03s\n",
      " [-] epoch  160/250, train loss 0.416968 in 0.03s\n",
      " [-] epoch  161/250, train loss 0.424800 in 0.03s\n",
      " [-] epoch  162/250, train loss 0.418563 in 0.03s\n",
      " [-] epoch  163/250, train loss 0.433209 in 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  164/250, train loss 0.408729 in 0.03s\n",
      " [-] epoch  165/250, train loss 0.422436 in 0.03s\n",
      " [-] epoch  166/250, train loss 0.426027 in 0.03s\n",
      " [-] epoch  167/250, train loss 0.409895 in 0.03s\n",
      " [-] epoch  168/250, train loss 0.409231 in 0.03s\n",
      " [-] epoch  169/250, train loss 0.395823 in 0.03s\n",
      " [-] epoch  170/250, train loss 0.414587 in 0.03s\n",
      " [-] epoch  171/250, train loss 0.411114 in 0.03s\n",
      " [-] epoch  172/250, train loss 0.399019 in 0.03s\n",
      " [-] epoch  173/250, train loss 0.394968 in 0.03s\n",
      " [-] epoch  174/250, train loss 0.406293 in 0.03s\n",
      " [-] epoch  175/250, train loss 0.410021 in 0.03s\n",
      " [-] epoch  176/250, train loss 0.411000 in 0.03s\n",
      " [-] epoch  177/250, train loss 0.426811 in 0.03s\n",
      " [-] epoch  178/250, train loss 0.423709 in 0.03s\n",
      " [-] epoch  179/250, train loss 0.400071 in 0.03s\n",
      " [-] epoch  180/250, train loss 0.390296 in 0.03s\n",
      " [-] epoch  181/250, train loss 0.416739 in 0.03s\n",
      " [-] epoch  182/250, train loss 0.410546 in 0.03s\n",
      " [-] epoch  183/250, train loss 0.399930 in 0.03s\n",
      " [-] epoch  184/250, train loss 0.405117 in 0.03s\n",
      " [-] epoch  185/250, train loss 0.414516 in 0.04s\n",
      " [-] epoch  186/250, train loss 0.417465 in 0.03s\n",
      " [-] epoch  187/250, train loss 0.411473 in 0.03s\n",
      " [-] epoch  188/250, train loss 0.401347 in 0.03s\n",
      " [-] epoch  189/250, train loss 0.407957 in 0.03s\n",
      " [-] epoch  190/250, train loss 0.390860 in 0.03s\n",
      " [-] epoch  191/250, train loss 0.409210 in 0.03s\n",
      " [-] epoch  192/250, train loss 0.389607 in 0.03s\n",
      " [-] epoch  193/250, train loss 0.395724 in 0.03s\n",
      " [-] epoch  194/250, train loss 0.439176 in 0.03s\n",
      " [-] epoch  195/250, train loss 0.408678 in 0.02s\n",
      " [-] epoch  196/250, train loss 0.409444 in 0.03s\n",
      " [-] epoch  197/250, train loss 0.400692 in 0.03s\n",
      " [-] epoch  198/250, train loss 0.386844 in 0.03s\n",
      " [-] epoch  199/250, train loss 0.409035 in 0.03s\n",
      " [-] epoch  200/250, train loss 0.399440 in 0.03s\n",
      " [-] epoch  201/250, train loss 0.400205 in 0.02s\n",
      " [-] epoch  202/250, train loss 0.408856 in 0.03s\n",
      " [-] epoch  203/250, train loss 0.404271 in 0.03s\n",
      " [-] epoch  204/250, train loss 0.402758 in 0.03s\n",
      " [-] epoch  205/250, train loss 0.428574 in 0.03s\n",
      " [-] epoch  206/250, train loss 0.428718 in 0.03s\n",
      " [-] epoch  207/250, train loss 0.388471 in 0.03s\n",
      " [-] epoch  208/250, train loss 0.400042 in 0.03s\n",
      " [-] epoch  209/250, train loss 0.391695 in 0.03s\n",
      " [-] epoch  210/250, train loss 0.427544 in 0.03s\n",
      " [-] epoch  211/250, train loss 0.400224 in 0.03s\n",
      " [-] epoch  212/250, train loss 0.395737 in 0.03s\n",
      " [-] epoch  213/250, train loss 0.407511 in 0.03s\n",
      " [-] epoch  214/250, train loss 0.407626 in 0.03s\n",
      " [-] epoch  215/250, train loss 0.377390 in 0.03s\n",
      " [-] epoch  216/250, train loss 0.412704 in 0.03s\n",
      " [-] epoch  217/250, train loss 0.395338 in 0.03s\n",
      " [-] epoch  218/250, train loss 0.406954 in 0.03s\n",
      " [-] epoch  219/250, train loss 0.393453 in 0.03s\n",
      " [-] epoch  220/250, train loss 0.393603 in 0.03s\n",
      " [-] epoch  221/250, train loss 0.415229 in 0.03s\n",
      " [-] epoch  222/250, train loss 0.399749 in 0.03s\n",
      " [-] epoch  223/250, train loss 0.388951 in 0.02s\n",
      " [-] epoch  224/250, train loss 0.386051 in 0.02s\n",
      " [-] epoch  225/250, train loss 0.414744 in 0.03s\n",
      " [-] epoch  226/250, train loss 0.390653 in 0.03s\n",
      " [-] epoch  227/250, train loss 0.404790 in 0.03s\n",
      " [-] epoch  228/250, train loss 0.377873 in 0.03s\n",
      " [-] epoch  229/250, train loss 0.387934 in 0.03s\n",
      " [-] epoch  230/250, train loss 0.396379 in 0.03s\n",
      " [-] epoch  231/250, train loss 0.424489 in 0.03s\n",
      " [-] epoch  232/250, train loss 0.380163 in 0.03s\n",
      " [-] epoch  233/250, train loss 0.395174 in 0.03s\n",
      " [-] epoch  234/250, train loss 0.400887 in 0.03s\n",
      " [-] epoch  235/250, train loss 0.401479 in 0.03s\n",
      " [-] epoch  236/250, train loss 0.373238 in 0.03s\n",
      " [-] epoch  237/250, train loss 0.378884 in 0.03s\n",
      " [-] epoch  238/250, train loss 0.394642 in 0.02s\n",
      " [-] epoch  239/250, train loss 0.409679 in 0.03s\n",
      " [-] epoch  240/250, train loss 0.375524 in 0.03s\n",
      " [-] epoch  241/250, train loss 0.400764 in 0.03s\n",
      " [-] epoch  242/250, train loss 0.388357 in 0.03s\n",
      " [-] epoch  243/250, train loss 0.394262 in 0.03s\n",
      " [-] epoch  244/250, train loss 0.380884 in 0.03s\n",
      " [-] epoch  245/250, train loss 0.395338 in 0.03s\n",
      " [-] epoch  246/250, train loss 0.413390 in 0.03s\n",
      " [-] epoch  247/250, train loss 0.419087 in 0.02s\n",
      " [-] epoch  248/250, train loss 0.408124 in 0.03s\n",
      " [-] epoch  249/250, train loss 0.370555 in 0.03s\n",
      " [-] epoch  250/250, train loss 0.395318 in 0.03s\n",
      " [-] test acc. 83.055556%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.589388 in 0.06s\n",
      " [-] epoch    2/250, train loss 0.459193 in 0.08s\n",
      " [-] epoch    3/250, train loss 0.472389 in 0.07s\n",
      " [-] epoch    4/250, train loss 0.453155 in 0.06s\n",
      " [-] epoch    5/250, train loss 0.421463 in 0.08s\n",
      " [-] epoch    6/250, train loss 0.393128 in 0.07s\n",
      " [-] epoch    7/250, train loss 0.428014 in 0.07s\n",
      " [-] epoch    8/250, train loss 0.410945 in 0.07s\n",
      " [-] epoch    9/250, train loss 0.375544 in 0.07s\n",
      " [-] epoch   10/250, train loss 0.402918 in 0.07s\n",
      " [-] epoch   11/250, train loss 0.408163 in 0.07s\n",
      " [-] epoch   12/250, train loss 0.363794 in 0.06s\n",
      " [-] epoch   13/250, train loss 0.403182 in 0.07s\n",
      " [-] epoch   14/250, train loss 0.376326 in 0.06s\n",
      " [-] epoch   15/250, train loss 0.365432 in 0.08s\n",
      " [-] epoch   16/250, train loss 0.371595 in 0.07s\n",
      " [-] epoch   17/250, train loss 0.391871 in 0.07s\n",
      " [-] epoch   18/250, train loss 0.355608 in 0.06s\n",
      " [-] epoch   19/250, train loss 0.370593 in 0.07s\n",
      " [-] epoch   20/250, train loss 0.394204 in 0.06s\n",
      " [-] epoch   21/250, train loss 0.362525 in 0.08s\n",
      " [-] epoch   22/250, train loss 0.362596 in 0.06s\n",
      " [-] epoch   23/250, train loss 0.346626 in 0.07s\n",
      " [-] epoch   24/250, train loss 0.376250 in 0.06s\n",
      " [-] epoch   25/250, train loss 0.363399 in 0.08s\n",
      " [-] epoch   26/250, train loss 0.372603 in 0.07s\n",
      " [-] epoch   27/250, train loss 0.371687 in 0.08s\n",
      " [-] epoch   28/250, train loss 0.342641 in 0.06s\n",
      " [-] epoch   29/250, train loss 0.354405 in 0.08s\n",
      " [-] epoch   30/250, train loss 0.357196 in 0.07s\n",
      " [-] epoch   31/250, train loss 0.368604 in 0.08s\n",
      " [-] epoch   32/250, train loss 0.339927 in 0.07s\n",
      " [-] epoch   33/250, train loss 0.365285 in 0.06s\n",
      " [-] epoch   34/250, train loss 0.347617 in 0.08s\n",
      " [-] epoch   35/250, train loss 0.345027 in 0.07s\n",
      " [-] epoch   36/250, train loss 0.369485 in 0.08s\n",
      " [-] epoch   37/250, train loss 0.369957 in 0.07s\n",
      " [-] epoch   38/250, train loss 0.377549 in 0.08s\n",
      " [-] epoch   39/250, train loss 0.350288 in 0.08s\n",
      " [-] epoch   40/250, train loss 0.354258 in 0.08s\n",
      " [-] epoch   41/250, train loss 0.374277 in 0.08s\n",
      " [-] epoch   42/250, train loss 0.360221 in 0.08s\n",
      " [-] epoch   43/250, train loss 0.298430 in 0.07s\n",
      " [-] epoch   44/250, train loss 0.360025 in 0.07s\n",
      " [-] epoch   45/250, train loss 0.306287 in 0.07s\n",
      " [-] epoch   46/250, train loss 0.318142 in 0.07s\n",
      " [-] epoch   47/250, train loss 0.336756 in 0.06s\n",
      " [-] epoch   48/250, train loss 0.322262 in 0.07s\n",
      " [-] epoch   49/250, train loss 0.353159 in 0.07s\n",
      " [-] epoch   50/250, train loss 0.342092 in 0.07s\n",
      " [-] epoch   51/250, train loss 0.346992 in 0.06s\n",
      " [-] epoch   52/250, train loss 0.359186 in 0.07s\n",
      " [-] epoch   53/250, train loss 0.337083 in 0.06s\n",
      " [-] epoch   54/250, train loss 0.336179 in 0.07s\n",
      " [-] epoch   55/250, train loss 0.354946 in 0.08s\n",
      " [-] epoch   56/250, train loss 0.340803 in 0.07s\n",
      " [-] epoch   57/250, train loss 0.339750 in 0.06s\n",
      " [-] epoch   58/250, train loss 0.331449 in 0.08s\n",
      " [-] epoch   59/250, train loss 0.334794 in 0.07s\n",
      " [-] epoch   60/250, train loss 0.372095 in 0.06s\n",
      " [-] epoch   61/250, train loss 0.354032 in 0.07s\n",
      " [-] epoch   62/250, train loss 0.339908 in 0.07s\n",
      " [-] epoch   63/250, train loss 0.329310 in 0.07s\n",
      " [-] epoch   64/250, train loss 0.345802 in 0.07s\n",
      " [-] epoch   65/250, train loss 0.321532 in 0.06s\n",
      " [-] epoch   66/250, train loss 0.335564 in 0.07s\n",
      " [-] epoch   67/250, train loss 0.328030 in 0.06s\n",
      " [-] epoch   68/250, train loss 0.329418 in 0.08s\n",
      " [-] epoch   69/250, train loss 0.344038 in 0.07s\n",
      " [-] epoch   70/250, train loss 0.340202 in 0.07s\n",
      " [-] epoch   71/250, train loss 0.331445 in 0.07s\n",
      " [-] epoch   72/250, train loss 0.366268 in 0.07s\n",
      " [-] epoch   73/250, train loss 0.332901 in 0.07s\n",
      " [-] epoch   74/250, train loss 0.371943 in 0.07s\n",
      " [-] epoch   75/250, train loss 0.326040 in 0.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.333345 in 0.08s\n",
      " [-] epoch   77/250, train loss 0.311390 in 0.07s\n",
      " [-] epoch   78/250, train loss 0.329150 in 0.07s\n",
      " [-] epoch   79/250, train loss 0.329106 in 0.07s\n",
      " [-] epoch   80/250, train loss 0.360876 in 0.07s\n",
      " [-] epoch   81/250, train loss 0.333102 in 0.06s\n",
      " [-] epoch   82/250, train loss 0.335034 in 0.08s\n",
      " [-] epoch   83/250, train loss 0.334150 in 0.08s\n",
      " [-] epoch   84/250, train loss 0.321516 in 0.07s\n",
      " [-] epoch   85/250, train loss 0.308439 in 0.07s\n",
      " [-] epoch   86/250, train loss 0.342272 in 0.08s\n",
      " [-] epoch   87/250, train loss 0.315602 in 0.07s\n",
      " [-] epoch   88/250, train loss 0.310725 in 0.07s\n",
      " [-] epoch   89/250, train loss 0.285735 in 0.07s\n",
      " [-] epoch   90/250, train loss 0.315211 in 0.07s\n",
      " [-] epoch   91/250, train loss 0.354974 in 0.07s\n",
      " [-] epoch   92/250, train loss 0.318265 in 0.07s\n",
      " [-] epoch   93/250, train loss 0.321508 in 0.07s\n",
      " [-] epoch   94/250, train loss 0.317926 in 0.07s\n",
      " [-] epoch   95/250, train loss 0.306919 in 0.08s\n",
      " [-] epoch   96/250, train loss 0.324599 in 0.07s\n",
      " [-] epoch   97/250, train loss 0.337846 in 0.06s\n",
      " [-] epoch   98/250, train loss 0.319472 in 0.07s\n",
      " [-] epoch   99/250, train loss 0.318448 in 0.07s\n",
      " [-] epoch  100/250, train loss 0.300456 in 0.07s\n",
      " [-] epoch  101/250, train loss 0.306435 in 0.07s\n",
      " [-] epoch  102/250, train loss 0.329245 in 0.07s\n",
      " [-] epoch  103/250, train loss 0.314801 in 0.07s\n",
      " [-] epoch  104/250, train loss 0.327013 in 0.07s\n",
      " [-] epoch  105/250, train loss 0.311309 in 0.07s\n",
      " [-] epoch  106/250, train loss 0.316106 in 0.07s\n",
      " [-] epoch  107/250, train loss 0.305279 in 0.06s\n",
      " [-] epoch  108/250, train loss 0.310491 in 0.07s\n",
      " [-] epoch  109/250, train loss 0.327569 in 0.08s\n",
      " [-] epoch  110/250, train loss 0.332822 in 0.07s\n",
      " [-] epoch  111/250, train loss 0.307179 in 0.08s\n",
      " [-] epoch  112/250, train loss 0.294819 in 0.08s\n",
      " [-] epoch  113/250, train loss 0.309085 in 0.07s\n",
      " [-] epoch  114/250, train loss 0.300326 in 0.07s\n",
      " [-] epoch  115/250, train loss 0.302809 in 0.07s\n",
      " [-] epoch  116/250, train loss 0.310339 in 0.07s\n",
      " [-] epoch  117/250, train loss 0.314488 in 0.07s\n",
      " [-] epoch  118/250, train loss 0.306241 in 0.07s\n",
      " [-] epoch  119/250, train loss 0.314247 in 0.07s\n",
      " [-] epoch  120/250, train loss 0.351496 in 0.07s\n",
      " [-] epoch  121/250, train loss 0.320036 in 0.07s\n",
      " [-] epoch  122/250, train loss 0.317873 in 0.06s\n",
      " [-] epoch  123/250, train loss 0.296083 in 0.07s\n",
      " [-] epoch  124/250, train loss 0.321607 in 0.06s\n",
      " [-] epoch  125/250, train loss 0.277357 in 0.08s\n",
      " [-] epoch  126/250, train loss 0.308154 in 0.07s\n",
      " [-] epoch  127/250, train loss 0.312035 in 0.06s\n",
      " [-] epoch  128/250, train loss 0.293939 in 0.08s\n",
      " [-] epoch  129/250, train loss 0.290185 in 0.07s\n",
      " [-] epoch  130/250, train loss 0.321380 in 0.08s\n",
      " [-] epoch  131/250, train loss 0.315790 in 0.07s\n",
      " [-] epoch  132/250, train loss 0.310334 in 0.07s\n",
      " [-] epoch  133/250, train loss 0.295770 in 0.07s\n",
      " [-] epoch  134/250, train loss 0.307225 in 0.07s\n",
      " [-] epoch  135/250, train loss 0.269859 in 0.08s\n",
      " [-] epoch  136/250, train loss 0.300428 in 0.07s\n",
      " [-] epoch  137/250, train loss 0.323631 in 0.07s\n",
      " [-] epoch  138/250, train loss 0.293592 in 0.07s\n",
      " [-] epoch  139/250, train loss 0.289810 in 0.07s\n",
      " [-] epoch  140/250, train loss 0.306743 in 0.07s\n",
      " [-] epoch  141/250, train loss 0.301681 in 0.06s\n",
      " [-] epoch  142/250, train loss 0.297407 in 0.08s\n",
      " [-] epoch  143/250, train loss 0.304401 in 0.07s\n",
      " [-] epoch  144/250, train loss 0.283707 in 0.08s\n",
      " [-] epoch  145/250, train loss 0.269838 in 0.07s\n",
      " [-] epoch  146/250, train loss 0.299356 in 0.08s\n",
      " [-] epoch  147/250, train loss 0.296061 in 0.07s\n",
      " [-] epoch  148/250, train loss 0.307497 in 0.07s\n",
      " [-] epoch  149/250, train loss 0.301179 in 0.08s\n",
      " [-] epoch  150/250, train loss 0.283223 in 0.07s\n",
      " [-] epoch  151/250, train loss 0.267941 in 0.07s\n",
      " [-] epoch  152/250, train loss 0.287276 in 0.07s\n",
      " [-] epoch  153/250, train loss 0.291224 in 0.07s\n",
      " [-] epoch  154/250, train loss 0.298835 in 0.08s\n",
      " [-] epoch  155/250, train loss 0.298897 in 0.08s\n",
      " [-] epoch  156/250, train loss 0.304863 in 0.07s\n",
      " [-] epoch  157/250, train loss 0.282436 in 0.07s\n",
      " [-] epoch  158/250, train loss 0.281890 in 0.07s\n",
      " [-] epoch  159/250, train loss 0.318245 in 0.07s\n",
      " [-] epoch  160/250, train loss 0.318915 in 0.07s\n",
      " [-] epoch  161/250, train loss 0.287761 in 0.07s\n",
      " [-] epoch  162/250, train loss 0.302645 in 0.07s\n",
      " [-] epoch  163/250, train loss 0.279151 in 0.08s\n",
      " [-] epoch  164/250, train loss 0.284099 in 0.07s\n",
      " [-] epoch  165/250, train loss 0.289034 in 0.07s\n",
      " [-] epoch  166/250, train loss 0.297657 in 0.07s\n",
      " [-] epoch  167/250, train loss 0.299889 in 0.08s\n",
      " [-] epoch  168/250, train loss 0.289059 in 0.08s\n",
      " [-] epoch  169/250, train loss 0.301907 in 0.07s\n",
      " [-] epoch  170/250, train loss 0.305862 in 0.07s\n",
      " [-] epoch  171/250, train loss 0.302918 in 0.07s\n",
      " [-] epoch  172/250, train loss 0.292983 in 0.07s\n",
      " [-] epoch  173/250, train loss 0.291203 in 0.08s\n",
      " [-] epoch  174/250, train loss 0.292020 in 0.07s\n",
      " [-] epoch  175/250, train loss 0.287331 in 0.07s\n",
      " [-] epoch  176/250, train loss 0.297907 in 0.07s\n",
      " [-] epoch  177/250, train loss 0.296119 in 0.07s\n",
      " [-] epoch  178/250, train loss 0.296267 in 0.07s\n",
      " [-] epoch  179/250, train loss 0.309634 in 0.08s\n",
      " [-] epoch  180/250, train loss 0.296431 in 0.07s\n",
      " [-] epoch  181/250, train loss 0.287648 in 0.07s\n",
      " [-] epoch  182/250, train loss 0.293996 in 0.08s\n",
      " [-] epoch  183/250, train loss 0.268670 in 0.08s\n",
      " [-] epoch  184/250, train loss 0.324164 in 0.08s\n",
      " [-] epoch  185/250, train loss 0.268052 in 0.07s\n",
      " [-] epoch  186/250, train loss 0.281531 in 0.07s\n",
      " [-] epoch  187/250, train loss 0.288383 in 0.07s\n",
      " [-] epoch  188/250, train loss 0.308977 in 0.07s\n",
      " [-] epoch  189/250, train loss 0.289011 in 0.08s\n",
      " [-] epoch  190/250, train loss 0.275585 in 0.08s\n",
      " [-] epoch  191/250, train loss 0.300489 in 0.07s\n",
      " [-] epoch  192/250, train loss 0.300325 in 0.07s\n",
      " [-] epoch  193/250, train loss 0.279927 in 0.07s\n",
      " [-] epoch  194/250, train loss 0.294871 in 0.07s\n",
      " [-] epoch  195/250, train loss 0.289069 in 0.08s\n",
      " [-] epoch  196/250, train loss 0.293895 in 0.08s\n",
      " [-] epoch  197/250, train loss 0.273155 in 0.07s\n",
      " [-] epoch  198/250, train loss 0.299189 in 0.07s\n",
      " [-] epoch  199/250, train loss 0.279941 in 0.07s\n",
      " [-] epoch  200/250, train loss 0.300155 in 0.07s\n",
      " [-] epoch  201/250, train loss 0.289988 in 0.07s\n",
      " [-] epoch  202/250, train loss 0.284975 in 0.07s\n",
      " [-] epoch  203/250, train loss 0.291274 in 0.07s\n",
      " [-] epoch  204/250, train loss 0.285387 in 0.07s\n",
      " [-] epoch  205/250, train loss 0.284815 in 0.06s\n",
      " [-] epoch  206/250, train loss 0.300427 in 0.07s\n",
      " [-] epoch  207/250, train loss 0.299631 in 0.07s\n",
      " [-] epoch  208/250, train loss 0.270530 in 0.07s\n",
      " [-] epoch  209/250, train loss 0.280674 in 0.07s\n",
      " [-] epoch  210/250, train loss 0.285312 in 0.06s\n",
      " [-] epoch  211/250, train loss 0.279595 in 0.09s\n",
      " [-] epoch  212/250, train loss 0.273938 in 0.08s\n",
      " [-] epoch  213/250, train loss 0.296658 in 0.08s\n",
      " [-] epoch  214/250, train loss 0.270728 in 0.07s\n",
      " [-] epoch  215/250, train loss 0.282286 in 0.07s\n",
      " [-] epoch  216/250, train loss 0.280587 in 0.08s\n",
      " [-] epoch  217/250, train loss 0.282202 in 0.07s\n",
      " [-] epoch  218/250, train loss 0.279520 in 0.07s\n",
      " [-] epoch  219/250, train loss 0.274696 in 0.07s\n",
      " [-] epoch  220/250, train loss 0.271196 in 0.08s\n",
      " [-] epoch  221/250, train loss 0.297711 in 0.07s\n",
      " [-] epoch  222/250, train loss 0.299969 in 0.07s\n",
      " [-] epoch  223/250, train loss 0.293038 in 0.07s\n",
      " [-] epoch  224/250, train loss 0.279633 in 0.07s\n",
      " [-] epoch  225/250, train loss 0.278264 in 0.07s\n",
      " [-] epoch  226/250, train loss 0.264108 in 0.07s\n",
      " [-] epoch  227/250, train loss 0.265084 in 0.07s\n",
      " [-] epoch  228/250, train loss 0.300871 in 0.07s\n",
      " [-] epoch  229/250, train loss 0.261103 in 0.07s\n",
      " [-] epoch  230/250, train loss 0.269833 in 0.07s\n",
      " [-] epoch  231/250, train loss 0.272480 in 0.06s\n",
      " [-] epoch  232/250, train loss 0.291648 in 0.07s\n",
      " [-] epoch  233/250, train loss 0.301868 in 0.07s\n",
      " [-] epoch  234/250, train loss 0.264062 in 0.07s\n",
      " [-] epoch  235/250, train loss 0.308281 in 0.07s\n",
      " [-] epoch  236/250, train loss 0.281242 in 0.07s\n",
      " [-] epoch  237/250, train loss 0.277029 in 0.07s\n",
      " [-] epoch  238/250, train loss 0.264663 in 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  239/250, train loss 0.264546 in 0.07s\n",
      " [-] epoch  240/250, train loss 0.270482 in 0.07s\n",
      " [-] epoch  241/250, train loss 0.262180 in 0.06s\n",
      " [-] epoch  242/250, train loss 0.284999 in 0.07s\n",
      " [-] epoch  243/250, train loss 0.266501 in 0.07s\n",
      " [-] epoch  244/250, train loss 0.288543 in 0.08s\n",
      " [-] epoch  245/250, train loss 0.281126 in 0.06s\n",
      " [-] epoch  246/250, train loss 0.272339 in 0.08s\n",
      " [-] epoch  247/250, train loss 0.311172 in 0.07s\n",
      " [-] epoch  248/250, train loss 0.270497 in 0.07s\n",
      " [-] epoch  249/250, train loss 0.278764 in 0.07s\n",
      " [-] epoch  250/250, train loss 0.278198 in 0.06s\n",
      " [-] test acc. 81.666667%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.548998 in 0.13s\n",
      " [-] epoch    2/250, train loss 0.481776 in 0.12s\n",
      " [-] epoch    3/250, train loss 0.429799 in 0.13s\n",
      " [-] epoch    4/250, train loss 0.413195 in 0.10s\n",
      " [-] epoch    5/250, train loss 0.406080 in 0.13s\n",
      " [-] epoch    6/250, train loss 0.389216 in 0.13s\n",
      " [-] epoch    7/250, train loss 0.394210 in 0.14s\n",
      " [-] epoch    8/250, train loss 0.353340 in 0.13s\n",
      " [-] epoch    9/250, train loss 0.377068 in 0.13s\n",
      " [-] epoch   10/250, train loss 0.359617 in 0.13s\n",
      " [-] epoch   11/250, train loss 0.357816 in 0.12s\n",
      " [-] epoch   12/250, train loss 0.348653 in 0.12s\n",
      " [-] epoch   13/250, train loss 0.351575 in 0.13s\n",
      " [-] epoch   14/250, train loss 0.366971 in 0.13s\n",
      " [-] epoch   15/250, train loss 0.351504 in 0.11s\n",
      " [-] epoch   16/250, train loss 0.323480 in 0.14s\n",
      " [-] epoch   17/250, train loss 0.319987 in 0.12s\n",
      " [-] epoch   18/250, train loss 0.340573 in 0.13s\n",
      " [-] epoch   19/250, train loss 0.326697 in 0.12s\n",
      " [-] epoch   20/250, train loss 0.309886 in 0.13s\n",
      " [-] epoch   21/250, train loss 0.330867 in 0.13s\n",
      " [-] epoch   22/250, train loss 0.305773 in 0.12s\n",
      " [-] epoch   23/250, train loss 0.324815 in 0.13s\n",
      " [-] epoch   24/250, train loss 0.298604 in 0.13s\n",
      " [-] epoch   25/250, train loss 0.307371 in 0.13s\n",
      " [-] epoch   26/250, train loss 0.306367 in 0.12s\n",
      " [-] epoch   27/250, train loss 0.299351 in 0.13s\n",
      " [-] epoch   28/250, train loss 0.309949 in 0.13s\n",
      " [-] epoch   29/250, train loss 0.327910 in 0.12s\n",
      " [-] epoch   30/250, train loss 0.314412 in 0.12s\n",
      " [-] epoch   31/250, train loss 0.334165 in 0.12s\n",
      " [-] epoch   32/250, train loss 0.294656 in 0.12s\n",
      " [-] epoch   33/250, train loss 0.316993 in 0.13s\n",
      " [-] epoch   34/250, train loss 0.311328 in 0.13s\n",
      " [-] epoch   35/250, train loss 0.299661 in 0.12s\n",
      " [-] epoch   36/250, train loss 0.303548 in 0.13s\n",
      " [-] epoch   37/250, train loss 0.303151 in 0.12s\n",
      " [-] epoch   38/250, train loss 0.271339 in 0.13s\n",
      " [-] epoch   39/250, train loss 0.294569 in 0.13s\n",
      " [-] epoch   40/250, train loss 0.274751 in 0.12s\n",
      " [-] epoch   41/250, train loss 0.282481 in 0.12s\n",
      " [-] epoch   42/250, train loss 0.281721 in 0.13s\n",
      " [-] epoch   43/250, train loss 0.280698 in 0.12s\n",
      " [-] epoch   44/250, train loss 0.280372 in 0.13s\n",
      " [-] epoch   45/250, train loss 0.269192 in 0.12s\n",
      " [-] epoch   46/250, train loss 0.263785 in 0.13s\n",
      " [-] epoch   47/250, train loss 0.256362 in 0.14s\n",
      " [-] epoch   48/250, train loss 0.298738 in 0.14s\n",
      " [-] epoch   49/250, train loss 0.293382 in 0.12s\n",
      " [-] epoch   50/250, train loss 0.281513 in 0.12s\n",
      " [-] epoch   51/250, train loss 0.264538 in 0.13s\n",
      " [-] epoch   52/250, train loss 0.260880 in 0.13s\n",
      " [-] epoch   53/250, train loss 0.287145 in 0.11s\n",
      " [-] epoch   54/250, train loss 0.275537 in 0.13s\n",
      " [-] epoch   55/250, train loss 0.274719 in 0.12s\n",
      " [-] epoch   56/250, train loss 0.281327 in 0.12s\n",
      " [-] epoch   57/250, train loss 0.271480 in 0.13s\n",
      " [-] epoch   58/250, train loss 0.266263 in 0.13s\n",
      " [-] epoch   59/250, train loss 0.298652 in 0.14s\n",
      " [-] epoch   60/250, train loss 0.286609 in 0.11s\n",
      " [-] epoch   61/250, train loss 0.293065 in 0.12s\n",
      " [-] epoch   62/250, train loss 0.282532 in 0.15s\n",
      " [-] epoch   63/250, train loss 0.268423 in 0.11s\n",
      " [-] epoch   64/250, train loss 0.284838 in 0.14s\n",
      " [-] epoch   65/250, train loss 0.275953 in 0.13s\n",
      " [-] epoch   66/250, train loss 0.274088 in 0.13s\n",
      " [-] epoch   67/250, train loss 0.261729 in 0.12s\n",
      " [-] epoch   68/250, train loss 0.268225 in 0.14s\n",
      " [-] epoch   69/250, train loss 0.267037 in 0.13s\n",
      " [-] epoch   70/250, train loss 0.273024 in 0.13s\n",
      " [-] epoch   71/250, train loss 0.258149 in 0.13s\n",
      " [-] epoch   72/250, train loss 0.259101 in 0.12s\n",
      " [-] epoch   73/250, train loss 0.277705 in 0.13s\n",
      " [-] epoch   74/250, train loss 0.264052 in 0.13s\n",
      " [-] epoch   75/250, train loss 0.273115 in 0.14s\n",
      " [-] epoch   76/250, train loss 0.268949 in 0.11s\n",
      " [-] epoch   77/250, train loss 0.254993 in 0.13s\n",
      " [-] epoch   78/250, train loss 0.265457 in 0.13s\n",
      " [-] epoch   79/250, train loss 0.272922 in 0.12s\n",
      " [-] epoch   80/250, train loss 0.253913 in 0.12s\n",
      " [-] epoch   81/250, train loss 0.270146 in 0.13s\n",
      " [-] epoch   82/250, train loss 0.249337 in 0.13s\n",
      " [-] epoch   83/250, train loss 0.262249 in 0.12s\n",
      " [-] epoch   84/250, train loss 0.244123 in 0.13s\n",
      " [-] epoch   85/250, train loss 0.279192 in 0.12s\n",
      " [-] epoch   86/250, train loss 0.258330 in 0.13s\n",
      " [-] epoch   87/250, train loss 0.261496 in 0.12s\n",
      " [-] epoch   88/250, train loss 0.248889 in 0.13s\n",
      " [-] epoch   89/250, train loss 0.255721 in 0.12s\n",
      " [-] epoch   90/250, train loss 0.273524 in 0.12s\n",
      " [-] epoch   91/250, train loss 0.253215 in 0.13s\n",
      " [-] epoch   92/250, train loss 0.262645 in 0.13s\n",
      " [-] epoch   93/250, train loss 0.270474 in 0.14s\n",
      " [-] epoch   94/250, train loss 0.241591 in 0.12s\n",
      " [-] epoch   95/250, train loss 0.245767 in 0.13s\n",
      " [-] epoch   96/250, train loss 0.266584 in 0.14s\n",
      " [-] epoch   97/250, train loss 0.258896 in 0.13s\n",
      " [-] epoch   98/250, train loss 0.236993 in 0.13s\n",
      " [-] epoch   99/250, train loss 0.244091 in 0.12s\n",
      " [-] epoch  100/250, train loss 0.244552 in 0.13s\n",
      " [-] epoch  101/250, train loss 0.252870 in 0.12s\n",
      " [-] epoch  102/250, train loss 0.253335 in 0.12s\n",
      " [-] epoch  103/250, train loss 0.247400 in 0.14s\n",
      " [-] epoch  104/250, train loss 0.258899 in 0.13s\n",
      " [-] epoch  105/250, train loss 0.244171 in 0.11s\n",
      " [-] epoch  106/250, train loss 0.236502 in 0.13s\n",
      " [-] epoch  107/250, train loss 0.252546 in 0.13s\n",
      " [-] epoch  108/250, train loss 0.263293 in 0.12s\n",
      " [-] epoch  109/250, train loss 0.247907 in 0.14s\n",
      " [-] epoch  110/250, train loss 0.263875 in 0.13s\n",
      " [-] epoch  111/250, train loss 0.256266 in 0.13s\n",
      " [-] epoch  112/250, train loss 0.251174 in 0.15s\n",
      " [-] epoch  113/250, train loss 0.233793 in 0.13s\n",
      " [-] epoch  114/250, train loss 0.246245 in 0.13s\n",
      " [-] epoch  115/250, train loss 0.254213 in 0.13s\n",
      " [-] epoch  116/250, train loss 0.248738 in 0.12s\n",
      " [-] epoch  117/250, train loss 0.246713 in 0.13s\n",
      " [-] epoch  118/250, train loss 0.268509 in 0.12s\n",
      " [-] epoch  119/250, train loss 0.246201 in 0.13s\n",
      " [-] epoch  120/250, train loss 0.234906 in 0.14s\n",
      " [-] epoch  121/250, train loss 0.237309 in 0.13s\n",
      " [-] epoch  122/250, train loss 0.244022 in 0.12s\n",
      " [-] epoch  123/250, train loss 0.271000 in 0.15s\n",
      " [-] epoch  124/250, train loss 0.248293 in 0.11s\n",
      " [-] epoch  125/250, train loss 0.272604 in 0.15s\n",
      " [-] epoch  126/250, train loss 0.236063 in 0.13s\n",
      " [-] epoch  127/250, train loss 0.249643 in 0.12s\n",
      " [-] epoch  128/250, train loss 0.225916 in 0.11s\n",
      " [-] epoch  129/250, train loss 0.271095 in 0.12s\n",
      " [-] epoch  130/250, train loss 0.243716 in 0.12s\n",
      " [-] epoch  131/250, train loss 0.243114 in 0.13s\n",
      " [-] epoch  132/250, train loss 0.244895 in 0.11s\n",
      " [-] epoch  133/250, train loss 0.254783 in 0.12s\n",
      " [-] epoch  134/250, train loss 0.254038 in 0.14s\n",
      " [-] epoch  135/250, train loss 0.224123 in 0.13s\n",
      " [-] epoch  136/250, train loss 0.249336 in 0.14s\n",
      " [-] epoch  137/250, train loss 0.242031 in 0.13s\n",
      " [-] epoch  138/250, train loss 0.228579 in 0.15s\n",
      " [-] epoch  139/250, train loss 0.260474 in 0.13s\n",
      " [-] epoch  140/250, train loss 0.256818 in 0.13s\n",
      " [-] epoch  141/250, train loss 0.231893 in 0.13s\n",
      " [-] epoch  142/250, train loss 0.245649 in 0.12s\n",
      " [-] epoch  143/250, train loss 0.234837 in 0.13s\n",
      " [-] epoch  144/250, train loss 0.256730 in 0.13s\n",
      " [-] epoch  145/250, train loss 0.225744 in 0.13s\n",
      " [-] epoch  146/250, train loss 0.242575 in 0.12s\n",
      " [-] epoch  147/250, train loss 0.255873 in 0.13s\n",
      " [-] epoch  148/250, train loss 0.242225 in 0.14s\n",
      " [-] epoch  149/250, train loss 0.235139 in 0.11s\n",
      " [-] epoch  150/250, train loss 0.225856 in 0.12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.234843 in 0.13s\n",
      " [-] epoch  152/250, train loss 0.266849 in 0.14s\n",
      " [-] epoch  153/250, train loss 0.239830 in 0.13s\n",
      " [-] epoch  154/250, train loss 0.241307 in 0.13s\n",
      " [-] epoch  155/250, train loss 0.242555 in 0.13s\n",
      " [-] epoch  156/250, train loss 0.246355 in 0.14s\n",
      " [-] epoch  157/250, train loss 0.228121 in 0.13s\n",
      " [-] epoch  158/250, train loss 0.232341 in 0.11s\n",
      " [-] epoch  159/250, train loss 0.233410 in 0.13s\n",
      " [-] epoch  160/250, train loss 0.235731 in 0.13s\n",
      " [-] epoch  161/250, train loss 0.254208 in 0.14s\n",
      " [-] epoch  162/250, train loss 0.249817 in 0.12s\n",
      " [-] epoch  163/250, train loss 0.239944 in 0.12s\n",
      " [-] epoch  164/250, train loss 0.234931 in 0.13s\n",
      " [-] epoch  165/250, train loss 0.231438 in 0.11s\n",
      " [-] epoch  166/250, train loss 0.222969 in 0.13s\n",
      " [-] epoch  167/250, train loss 0.222429 in 0.13s\n",
      " [-] epoch  168/250, train loss 0.206306 in 0.14s\n",
      " [-] epoch  169/250, train loss 0.231362 in 0.11s\n",
      " [-] epoch  170/250, train loss 0.235616 in 0.13s\n",
      " [-] epoch  171/250, train loss 0.246580 in 0.13s\n",
      " [-] epoch  172/250, train loss 0.212873 in 0.14s\n",
      " [-] epoch  173/250, train loss 0.202897 in 0.11s\n",
      " [-] epoch  174/250, train loss 0.238768 in 0.12s\n",
      " [-] epoch  175/250, train loss 0.250133 in 0.13s\n",
      " [-] epoch  176/250, train loss 0.241574 in 0.13s\n",
      " [-] epoch  177/250, train loss 0.227147 in 0.13s\n",
      " [-] epoch  178/250, train loss 0.223253 in 0.12s\n",
      " [-] epoch  179/250, train loss 0.223683 in 0.12s\n",
      " [-] epoch  180/250, train loss 0.254161 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.250416 in 0.13s\n",
      " [-] epoch  182/250, train loss 0.228742 in 0.13s\n",
      " [-] epoch  183/250, train loss 0.225841 in 0.13s\n",
      " [-] epoch  184/250, train loss 0.234085 in 0.13s\n",
      " [-] epoch  185/250, train loss 0.243198 in 0.12s\n",
      " [-] epoch  186/250, train loss 0.235568 in 0.13s\n",
      " [-] epoch  187/250, train loss 0.222218 in 0.13s\n",
      " [-] epoch  188/250, train loss 0.222604 in 0.14s\n",
      " [-] epoch  189/250, train loss 0.231385 in 0.12s\n",
      " [-] epoch  190/250, train loss 0.231110 in 0.13s\n",
      " [-] epoch  191/250, train loss 0.240156 in 0.12s\n",
      " [-] epoch  192/250, train loss 0.233919 in 0.13s\n",
      " [-] epoch  193/250, train loss 0.238423 in 0.13s\n",
      " [-] epoch  194/250, train loss 0.229160 in 0.12s\n",
      " [-] epoch  195/250, train loss 0.230515 in 0.13s\n",
      " [-] epoch  196/250, train loss 0.235833 in 0.13s\n",
      " [-] epoch  197/250, train loss 0.241754 in 0.12s\n",
      " [-] epoch  198/250, train loss 0.213245 in 0.13s\n",
      " [-] epoch  199/250, train loss 0.220915 in 0.14s\n",
      " [-] epoch  200/250, train loss 0.232894 in 0.13s\n",
      " [-] epoch  201/250, train loss 0.224880 in 0.14s\n",
      " [-] epoch  202/250, train loss 0.222788 in 0.13s\n",
      " [-] epoch  203/250, train loss 0.233985 in 0.12s\n",
      " [-] epoch  204/250, train loss 0.235830 in 0.13s\n",
      " [-] epoch  205/250, train loss 0.216534 in 0.14s\n",
      " [-] epoch  206/250, train loss 0.219374 in 0.11s\n",
      " [-] epoch  207/250, train loss 0.220148 in 0.12s\n",
      " [-] epoch  208/250, train loss 0.217893 in 0.13s\n",
      " [-] epoch  209/250, train loss 0.244591 in 0.13s\n",
      " [-] epoch  210/250, train loss 0.249782 in 0.12s\n",
      " [-] epoch  211/250, train loss 0.226870 in 0.13s\n",
      " [-] epoch  212/250, train loss 0.233950 in 0.13s\n",
      " [-] epoch  213/250, train loss 0.222219 in 0.13s\n",
      " [-] epoch  214/250, train loss 0.218724 in 0.12s\n",
      " [-] epoch  215/250, train loss 0.242135 in 0.15s\n",
      " [-] epoch  216/250, train loss 0.227638 in 0.12s\n",
      " [-] epoch  217/250, train loss 0.241059 in 0.12s\n",
      " [-] epoch  218/250, train loss 0.228109 in 0.13s\n",
      " [-] epoch  219/250, train loss 0.228997 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.209795 in 0.12s\n",
      " [-] epoch  221/250, train loss 0.237620 in 0.12s\n",
      " [-] epoch  222/250, train loss 0.217040 in 0.13s\n",
      " [-] epoch  223/250, train loss 0.239071 in 0.15s\n",
      " [-] epoch  224/250, train loss 0.214784 in 0.13s\n",
      " [-] epoch  225/250, train loss 0.226682 in 0.12s\n",
      " [-] epoch  226/250, train loss 0.213082 in 0.12s\n",
      " [-] epoch  227/250, train loss 0.270233 in 0.12s\n",
      " [-] epoch  228/250, train loss 0.216631 in 0.15s\n",
      " [-] epoch  229/250, train loss 0.217919 in 0.12s\n",
      " [-] epoch  230/250, train loss 0.229882 in 0.12s\n",
      " [-] epoch  231/250, train loss 0.227598 in 0.15s\n",
      " [-] epoch  232/250, train loss 0.230850 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.223608 in 0.13s\n",
      " [-] epoch  234/250, train loss 0.259461 in 0.13s\n",
      " [-] epoch  235/250, train loss 0.214967 in 0.12s\n",
      " [-] epoch  236/250, train loss 0.220748 in 0.13s\n",
      " [-] epoch  237/250, train loss 0.216516 in 0.13s\n",
      " [-] epoch  238/250, train loss 0.215944 in 0.14s\n",
      " [-] epoch  239/250, train loss 0.237326 in 0.12s\n",
      " [-] epoch  240/250, train loss 0.222661 in 0.14s\n",
      " [-] epoch  241/250, train loss 0.228059 in 0.13s\n",
      " [-] epoch  242/250, train loss 0.232370 in 0.13s\n",
      " [-] epoch  243/250, train loss 0.214242 in 0.13s\n",
      " [-] epoch  244/250, train loss 0.262731 in 0.14s\n",
      " [-] epoch  245/250, train loss 0.223283 in 0.13s\n",
      " [-] epoch  246/250, train loss 0.231175 in 0.13s\n",
      " [-] epoch  247/250, train loss 0.213466 in 0.14s\n",
      " [-] epoch  248/250, train loss 0.224650 in 0.11s\n",
      " [-] epoch  249/250, train loss 0.230365 in 0.14s\n",
      " [-] epoch  250/250, train loss 0.212486 in 0.14s\n",
      " [-] test acc. 75.833333%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.572536 in 0.20s\n",
      " [-] epoch    2/250, train loss 0.460830 in 0.21s\n",
      " [-] epoch    3/250, train loss 0.459472 in 0.20s\n",
      " [-] epoch    4/250, train loss 0.412910 in 0.21s\n",
      " [-] epoch    5/250, train loss 0.396901 in 0.21s\n",
      " [-] epoch    6/250, train loss 0.406658 in 0.19s\n",
      " [-] epoch    7/250, train loss 0.405068 in 0.19s\n",
      " [-] epoch    8/250, train loss 0.373955 in 0.20s\n",
      " [-] epoch    9/250, train loss 0.370191 in 0.21s\n",
      " [-] epoch   10/250, train loss 0.368443 in 0.20s\n",
      " [-] epoch   11/250, train loss 0.319609 in 0.21s\n",
      " [-] epoch   12/250, train loss 0.388943 in 0.21s\n",
      " [-] epoch   13/250, train loss 0.348861 in 0.19s\n",
      " [-] epoch   14/250, train loss 0.336172 in 0.22s\n",
      " [-] epoch   15/250, train loss 0.317303 in 0.20s\n",
      " [-] epoch   16/250, train loss 0.320564 in 0.21s\n",
      " [-] epoch   17/250, train loss 0.307452 in 0.20s\n",
      " [-] epoch   18/250, train loss 0.330408 in 0.21s\n",
      " [-] epoch   19/250, train loss 0.318575 in 0.18s\n",
      " [-] epoch   20/250, train loss 0.319807 in 0.21s\n",
      " [-] epoch   21/250, train loss 0.331253 in 0.20s\n",
      " [-] epoch   22/250, train loss 0.300432 in 0.22s\n",
      " [-] epoch   23/250, train loss 0.331274 in 0.22s\n",
      " [-] epoch   24/250, train loss 0.328785 in 0.20s\n",
      " [-] epoch   25/250, train loss 0.314998 in 0.22s\n",
      " [-] epoch   26/250, train loss 0.305745 in 0.21s\n",
      " [-] epoch   27/250, train loss 0.305001 in 0.18s\n",
      " [-] epoch   28/250, train loss 0.290372 in 0.20s\n",
      " [-] epoch   29/250, train loss 0.295855 in 0.23s\n",
      " [-] epoch   30/250, train loss 0.309256 in 0.19s\n",
      " [-] epoch   31/250, train loss 0.320750 in 0.19s\n",
      " [-] epoch   32/250, train loss 0.275873 in 0.21s\n",
      " [-] epoch   33/250, train loss 0.293191 in 0.21s\n",
      " [-] epoch   34/250, train loss 0.287080 in 0.20s\n",
      " [-] epoch   35/250, train loss 0.282889 in 0.20s\n",
      " [-] epoch   36/250, train loss 0.247381 in 0.21s\n",
      " [-] epoch   37/250, train loss 0.299133 in 0.20s\n",
      " [-] epoch   38/250, train loss 0.262154 in 0.21s\n",
      " [-] epoch   39/250, train loss 0.285471 in 0.19s\n",
      " [-] epoch   40/250, train loss 0.271809 in 0.23s\n",
      " [-] epoch   41/250, train loss 0.272481 in 0.19s\n",
      " [-] epoch   42/250, train loss 0.276730 in 0.21s\n",
      " [-] epoch   43/250, train loss 0.277207 in 0.20s\n",
      " [-] epoch   44/250, train loss 0.294034 in 0.22s\n",
      " [-] epoch   45/250, train loss 0.295843 in 0.21s\n",
      " [-] epoch   46/250, train loss 0.271388 in 0.21s\n",
      " [-] epoch   47/250, train loss 0.275502 in 0.22s\n",
      " [-] epoch   48/250, train loss 0.276516 in 0.22s\n",
      " [-] epoch   49/250, train loss 0.266488 in 0.21s\n",
      " [-] epoch   50/250, train loss 0.263982 in 0.21s\n",
      " [-] epoch   51/250, train loss 0.262155 in 0.22s\n",
      " [-] epoch   52/250, train loss 0.266975 in 0.21s\n",
      " [-] epoch   53/250, train loss 0.254053 in 0.22s\n",
      " [-] epoch   54/250, train loss 0.262670 in 0.22s\n",
      " [-] epoch   55/250, train loss 0.263575 in 0.20s\n",
      " [-] epoch   56/250, train loss 0.261415 in 0.21s\n",
      " [-] epoch   57/250, train loss 0.250957 in 0.21s\n",
      " [-] epoch   58/250, train loss 0.260746 in 0.21s\n",
      " [-] epoch   59/250, train loss 0.255542 in 0.21s\n",
      " [-] epoch   60/250, train loss 0.264430 in 0.22s\n",
      " [-] epoch   61/250, train loss 0.261399 in 0.19s\n",
      " [-] epoch   62/250, train loss 0.272335 in 0.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   63/250, train loss 0.260636 in 0.22s\n",
      " [-] epoch   64/250, train loss 0.261913 in 0.22s\n",
      " [-] epoch   65/250, train loss 0.267876 in 0.22s\n",
      " [-] epoch   66/250, train loss 0.274463 in 0.21s\n",
      " [-] epoch   67/250, train loss 0.257273 in 0.21s\n",
      " [-] epoch   68/250, train loss 0.277112 in 0.21s\n",
      " [-] epoch   69/250, train loss 0.241130 in 0.20s\n",
      " [-] epoch   70/250, train loss 0.262461 in 0.22s\n",
      " [-] epoch   71/250, train loss 0.254816 in 0.21s\n",
      " [-] epoch   72/250, train loss 0.249694 in 0.20s\n",
      " [-] epoch   73/250, train loss 0.265656 in 0.23s\n",
      " [-] epoch   74/250, train loss 0.257928 in 0.21s\n",
      " [-] epoch   75/250, train loss 0.258374 in 0.22s\n",
      " [-] epoch   76/250, train loss 0.250761 in 0.21s\n",
      " [-] epoch   77/250, train loss 0.258341 in 0.21s\n",
      " [-] epoch   78/250, train loss 0.253590 in 0.19s\n",
      " [-] epoch   79/250, train loss 0.243209 in 0.22s\n",
      " [-] epoch   80/250, train loss 0.220189 in 0.18s\n",
      " [-] epoch   81/250, train loss 0.276070 in 0.20s\n",
      " [-] epoch   82/250, train loss 0.264100 in 0.21s\n",
      " [-] epoch   83/250, train loss 0.248488 in 0.19s\n",
      " [-] epoch   84/250, train loss 0.256401 in 0.22s\n",
      " [-] epoch   85/250, train loss 0.234013 in 0.23s\n",
      " [-] epoch   86/250, train loss 0.254598 in 0.21s\n",
      " [-] epoch   87/250, train loss 0.256024 in 0.22s\n",
      " [-] epoch   88/250, train loss 0.240368 in 0.21s\n",
      " [-] epoch   89/250, train loss 0.251275 in 0.21s\n",
      " [-] epoch   90/250, train loss 0.263941 in 0.21s\n",
      " [-] epoch   91/250, train loss 0.239445 in 0.19s\n",
      " [-] epoch   92/250, train loss 0.251200 in 0.21s\n",
      " [-] epoch   93/250, train loss 0.222514 in 0.19s\n",
      " [-] epoch   94/250, train loss 0.260541 in 0.22s\n",
      " [-] epoch   95/250, train loss 0.236762 in 0.21s\n",
      " [-] epoch   96/250, train loss 0.261780 in 0.22s\n",
      " [-] epoch   97/250, train loss 0.252940 in 0.22s\n",
      " [-] epoch   98/250, train loss 0.220798 in 0.20s\n",
      " [-] epoch   99/250, train loss 0.236238 in 0.21s\n",
      " [-] epoch  100/250, train loss 0.234967 in 0.19s\n",
      " [-] epoch  101/250, train loss 0.228432 in 0.21s\n",
      " [-] epoch  102/250, train loss 0.238935 in 0.22s\n",
      " [-] epoch  103/250, train loss 0.245825 in 0.20s\n",
      " [-] epoch  104/250, train loss 0.248076 in 0.19s\n",
      " [-] epoch  105/250, train loss 0.235966 in 0.21s\n",
      " [-] epoch  106/250, train loss 0.252643 in 0.20s\n",
      " [-] epoch  107/250, train loss 0.232637 in 0.20s\n",
      " [-] epoch  108/250, train loss 0.227560 in 0.22s\n",
      " [-] epoch  109/250, train loss 0.240151 in 0.22s\n",
      " [-] epoch  110/250, train loss 0.233474 in 0.18s\n",
      " [-] epoch  111/250, train loss 0.249697 in 0.21s\n",
      " [-] epoch  112/250, train loss 0.225642 in 0.21s\n",
      " [-] epoch  113/250, train loss 0.229563 in 0.20s\n",
      " [-] epoch  114/250, train loss 0.242120 in 0.21s\n",
      " [-] epoch  115/250, train loss 0.231313 in 0.20s\n",
      " [-] epoch  116/250, train loss 0.247088 in 0.22s\n",
      " [-] epoch  117/250, train loss 0.230189 in 0.19s\n",
      " [-] epoch  118/250, train loss 0.236221 in 0.23s\n",
      " [-] epoch  119/250, train loss 0.226462 in 0.21s\n",
      " [-] epoch  120/250, train loss 0.230407 in 0.21s\n",
      " [-] epoch  121/250, train loss 0.245042 in 0.20s\n",
      " [-] epoch  122/250, train loss 0.219223 in 0.20s\n",
      " [-] epoch  123/250, train loss 0.214356 in 0.18s\n",
      " [-] epoch  124/250, train loss 0.235815 in 0.23s\n",
      " [-] epoch  125/250, train loss 0.228870 in 0.21s\n",
      " [-] epoch  126/250, train loss 0.232052 in 0.21s\n",
      " [-] epoch  127/250, train loss 0.235717 in 0.20s\n",
      " [-] epoch  128/250, train loss 0.225924 in 0.20s\n",
      " [-] epoch  129/250, train loss 0.225592 in 0.20s\n",
      " [-] epoch  130/250, train loss 0.248042 in 0.22s\n",
      " [-] epoch  131/250, train loss 0.253737 in 0.21s\n",
      " [-] epoch  132/250, train loss 0.232231 in 0.20s\n",
      " [-] epoch  133/250, train loss 0.243212 in 0.20s\n",
      " [-] epoch  134/250, train loss 0.228813 in 0.22s\n",
      " [-] epoch  135/250, train loss 0.236813 in 0.21s\n",
      " [-] epoch  136/250, train loss 0.219754 in 0.20s\n",
      " [-] epoch  137/250, train loss 0.251903 in 0.23s\n",
      " [-] epoch  138/250, train loss 0.231044 in 0.20s\n",
      " [-] epoch  139/250, train loss 0.230129 in 0.22s\n",
      " [-] epoch  140/250, train loss 0.210270 in 0.20s\n",
      " [-] epoch  141/250, train loss 0.240337 in 0.21s\n",
      " [-] epoch  142/250, train loss 0.223637 in 0.21s\n",
      " [-] epoch  143/250, train loss 0.230004 in 0.21s\n",
      " [-] epoch  144/250, train loss 0.213134 in 0.19s\n",
      " [-] epoch  145/250, train loss 0.223877 in 0.21s\n",
      " [-] epoch  146/250, train loss 0.231445 in 0.22s\n",
      " [-] epoch  147/250, train loss 0.211316 in 0.19s\n",
      " [-] epoch  148/250, train loss 0.215750 in 0.23s\n",
      " [-] epoch  149/250, train loss 0.228533 in 0.23s\n",
      " [-] epoch  150/250, train loss 0.242415 in 0.21s\n",
      " [-] epoch  151/250, train loss 0.222777 in 0.21s\n",
      " [-] epoch  152/250, train loss 0.224288 in 0.20s\n",
      " [-] epoch  153/250, train loss 0.232864 in 0.19s\n",
      " [-] epoch  154/250, train loss 0.228096 in 0.22s\n",
      " [-] epoch  155/250, train loss 0.214649 in 0.19s\n",
      " [-] epoch  156/250, train loss 0.242022 in 0.21s\n",
      " [-] epoch  157/250, train loss 0.220512 in 0.21s\n",
      " [-] epoch  158/250, train loss 0.211764 in 0.20s\n",
      " [-] epoch  159/250, train loss 0.244099 in 0.22s\n",
      " [-] epoch  160/250, train loss 0.217886 in 0.21s\n",
      " [-] epoch  161/250, train loss 0.216576 in 0.20s\n",
      " [-] epoch  162/250, train loss 0.226786 in 0.21s\n",
      " [-] epoch  163/250, train loss 0.214976 in 0.19s\n",
      " [-] epoch  164/250, train loss 0.229954 in 0.21s\n",
      " [-] epoch  165/250, train loss 0.206799 in 0.21s\n",
      " [-] epoch  166/250, train loss 0.204038 in 0.20s\n",
      " [-] epoch  167/250, train loss 0.203469 in 0.21s\n",
      " [-] epoch  168/250, train loss 0.215607 in 0.19s\n",
      " [-] epoch  169/250, train loss 0.208664 in 0.20s\n",
      " [-] epoch  170/250, train loss 0.220816 in 0.23s\n",
      " [-] epoch  171/250, train loss 0.239777 in 0.21s\n",
      " [-] epoch  172/250, train loss 0.235499 in 0.19s\n",
      " [-] epoch  173/250, train loss 0.226642 in 0.22s\n",
      " [-] epoch  174/250, train loss 0.221699 in 0.21s\n",
      " [-] epoch  175/250, train loss 0.217432 in 0.21s\n",
      " [-] epoch  176/250, train loss 0.223745 in 0.20s\n",
      " [-] epoch  177/250, train loss 0.218495 in 0.20s\n",
      " [-] epoch  178/250, train loss 0.206457 in 0.20s\n",
      " [-] epoch  179/250, train loss 0.212051 in 0.22s\n",
      " [-] epoch  180/250, train loss 0.216446 in 0.21s\n",
      " [-] epoch  181/250, train loss 0.227065 in 0.22s\n",
      " [-] epoch  182/250, train loss 0.234663 in 0.20s\n",
      " [-] epoch  183/250, train loss 0.223362 in 0.20s\n",
      " [-] epoch  184/250, train loss 0.221404 in 0.21s\n",
      " [-] epoch  185/250, train loss 0.232794 in 0.21s\n",
      " [-] epoch  186/250, train loss 0.228450 in 0.21s\n",
      " [-] epoch  187/250, train loss 0.232802 in 0.21s\n",
      " [-] epoch  188/250, train loss 0.218412 in 0.20s\n",
      " [-] epoch  189/250, train loss 0.220684 in 0.22s\n",
      " [-] epoch  190/250, train loss 0.228358 in 0.21s\n",
      " [-] epoch  191/250, train loss 0.227049 in 0.19s\n",
      " [-] epoch  192/250, train loss 0.211411 in 0.21s\n",
      " [-] epoch  193/250, train loss 0.204909 in 0.21s\n",
      " [-] epoch  194/250, train loss 0.211154 in 0.22s\n",
      " [-] epoch  195/250, train loss 0.211739 in 0.21s\n",
      " [-] epoch  196/250, train loss 0.210431 in 0.18s\n",
      " [-] epoch  197/250, train loss 0.206050 in 0.21s\n",
      " [-] epoch  198/250, train loss 0.217859 in 0.23s\n",
      " [-] epoch  199/250, train loss 0.222315 in 0.19s\n",
      " [-] epoch  200/250, train loss 0.217859 in 0.20s\n",
      " [-] epoch  201/250, train loss 0.214647 in 0.24s\n",
      " [-] epoch  202/250, train loss 0.197206 in 0.21s\n",
      " [-] epoch  203/250, train loss 0.215422 in 0.20s\n",
      " [-] epoch  204/250, train loss 0.186006 in 0.21s\n",
      " [-] epoch  205/250, train loss 0.229395 in 0.19s\n",
      " [-] epoch  206/250, train loss 0.217400 in 0.19s\n",
      " [-] epoch  207/250, train loss 0.232518 in 0.22s\n",
      " [-] epoch  208/250, train loss 0.222540 in 0.19s\n",
      " [-] epoch  209/250, train loss 0.215649 in 0.22s\n",
      " [-] epoch  210/250, train loss 0.208649 in 0.20s\n",
      " [-] epoch  211/250, train loss 0.215550 in 0.20s\n",
      " [-] epoch  212/250, train loss 0.205078 in 0.20s\n",
      " [-] epoch  213/250, train loss 0.221486 in 0.22s\n",
      " [-] epoch  214/250, train loss 0.207135 in 0.18s\n",
      " [-] epoch  215/250, train loss 0.228184 in 0.20s\n",
      " [-] epoch  216/250, train loss 0.212064 in 0.20s\n",
      " [-] epoch  217/250, train loss 0.216566 in 0.21s\n",
      " [-] epoch  218/250, train loss 0.214187 in 0.21s\n",
      " [-] epoch  219/250, train loss 0.220381 in 0.20s\n",
      " [-] epoch  220/250, train loss 0.226419 in 0.22s\n",
      " [-] epoch  221/250, train loss 0.210906 in 0.19s\n",
      " [-] epoch  222/250, train loss 0.209546 in 0.21s\n",
      " [-] epoch  223/250, train loss 0.199316 in 0.20s\n",
      " [-] epoch  224/250, train loss 0.198530 in 0.22s\n",
      " [-] epoch  225/250, train loss 0.222716 in 0.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.202808 in 0.19s\n",
      " [-] epoch  227/250, train loss 0.209161 in 0.22s\n",
      " [-] epoch  228/250, train loss 0.212249 in 0.22s\n",
      " [-] epoch  229/250, train loss 0.214586 in 0.21s\n",
      " [-] epoch  230/250, train loss 0.209930 in 0.21s\n",
      " [-] epoch  231/250, train loss 0.212893 in 0.20s\n",
      " [-] epoch  232/250, train loss 0.198827 in 0.19s\n",
      " [-] epoch  233/250, train loss 0.208853 in 0.22s\n",
      " [-] epoch  234/250, train loss 0.210772 in 0.20s\n",
      " [-] epoch  235/250, train loss 0.217120 in 0.20s\n",
      " [-] epoch  236/250, train loss 0.217848 in 0.22s\n",
      " [-] epoch  237/250, train loss 0.208374 in 0.21s\n",
      " [-] epoch  238/250, train loss 0.213925 in 0.21s\n",
      " [-] epoch  239/250, train loss 0.214252 in 0.20s\n",
      " [-] epoch  240/250, train loss 0.208597 in 0.22s\n",
      " [-] epoch  241/250, train loss 0.200781 in 0.19s\n",
      " [-] epoch  242/250, train loss 0.202604 in 0.21s\n",
      " [-] epoch  243/250, train loss 0.219412 in 0.20s\n",
      " [-] epoch  244/250, train loss 0.201550 in 0.22s\n",
      " [-] epoch  245/250, train loss 0.243404 in 0.20s\n",
      " [-] epoch  246/250, train loss 0.202552 in 0.21s\n",
      " [-] epoch  247/250, train loss 0.225751 in 0.21s\n",
      " [-] epoch  248/250, train loss 0.209562 in 0.20s\n",
      " [-] epoch  249/250, train loss 0.198260 in 0.21s\n",
      " [-] epoch  250/250, train loss 0.216227 in 0.21s\n",
      " [-] test acc. 81.111111%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.559365 in 0.30s\n",
      " [-] epoch    2/250, train loss 0.474303 in 0.30s\n",
      " [-] epoch    3/250, train loss 0.449787 in 0.30s\n",
      " [-] epoch    4/250, train loss 0.406066 in 0.29s\n",
      " [-] epoch    5/250, train loss 0.397710 in 0.28s\n",
      " [-] epoch    6/250, train loss 0.366343 in 0.29s\n",
      " [-] epoch    7/250, train loss 0.360887 in 0.31s\n",
      " [-] epoch    8/250, train loss 0.353513 in 0.32s\n",
      " [-] epoch    9/250, train loss 0.368263 in 0.32s\n",
      " [-] epoch   10/250, train loss 0.322781 in 0.30s\n",
      " [-] epoch   11/250, train loss 0.331909 in 0.30s\n",
      " [-] epoch   12/250, train loss 0.345786 in 0.32s\n",
      " [-] epoch   13/250, train loss 0.336821 in 0.29s\n",
      " [-] epoch   14/250, train loss 0.320412 in 0.34s\n",
      " [-] epoch   15/250, train loss 0.342665 in 0.29s\n",
      " [-] epoch   16/250, train loss 0.355893 in 0.30s\n",
      " [-] epoch   17/250, train loss 0.310402 in 0.31s\n",
      " [-] epoch   18/250, train loss 0.338297 in 0.32s\n",
      " [-] epoch   19/250, train loss 0.323058 in 0.32s\n",
      " [-] epoch   20/250, train loss 0.306038 in 0.29s\n",
      " [-] epoch   21/250, train loss 0.300765 in 0.28s\n",
      " [-] epoch   22/250, train loss 0.298256 in 0.32s\n",
      " [-] epoch   23/250, train loss 0.294804 in 0.31s\n",
      " [-] epoch   24/250, train loss 0.268979 in 0.29s\n",
      " [-] epoch   25/250, train loss 0.291884 in 0.32s\n",
      " [-] epoch   26/250, train loss 0.322528 in 0.31s\n",
      " [-] epoch   27/250, train loss 0.275973 in 0.29s\n",
      " [-] epoch   28/250, train loss 0.313792 in 0.30s\n",
      " [-] epoch   29/250, train loss 0.293669 in 0.31s\n",
      " [-] epoch   30/250, train loss 0.290170 in 0.30s\n",
      " [-] epoch   31/250, train loss 0.320042 in 0.30s\n",
      " [-] epoch   32/250, train loss 0.302409 in 0.29s\n",
      " [-] epoch   33/250, train loss 0.310814 in 0.30s\n",
      " [-] epoch   34/250, train loss 0.271252 in 0.30s\n",
      " [-] epoch   35/250, train loss 0.291185 in 0.32s\n",
      " [-] epoch   36/250, train loss 0.281220 in 0.31s\n",
      " [-] epoch   37/250, train loss 0.291094 in 0.29s\n",
      " [-] epoch   38/250, train loss 0.266366 in 0.35s\n",
      " [-] epoch   39/250, train loss 0.261049 in 0.31s\n",
      " [-] epoch   40/250, train loss 0.279213 in 0.31s\n",
      " [-] epoch   41/250, train loss 0.277932 in 0.30s\n",
      " [-] epoch   42/250, train loss 0.303160 in 0.31s\n",
      " [-] epoch   43/250, train loss 0.259338 in 0.30s\n",
      " [-] epoch   44/250, train loss 0.263279 in 0.29s\n",
      " [-] epoch   45/250, train loss 0.263264 in 0.29s\n",
      " [-] epoch   46/250, train loss 0.288649 in 0.29s\n",
      " [-] epoch   47/250, train loss 0.292007 in 0.33s\n",
      " [-] epoch   48/250, train loss 0.273959 in 0.31s\n",
      " [-] epoch   49/250, train loss 0.283108 in 0.30s\n",
      " [-] epoch   50/250, train loss 0.262154 in 0.30s\n",
      " [-] epoch   51/250, train loss 0.241147 in 0.31s\n",
      " [-] epoch   52/250, train loss 0.259903 in 0.30s\n",
      " [-] epoch   53/250, train loss 0.269300 in 0.31s\n",
      " [-] epoch   54/250, train loss 0.261374 in 0.31s\n",
      " [-] epoch   55/250, train loss 0.260212 in 0.31s\n",
      " [-] epoch   56/250, train loss 0.264297 in 0.31s\n",
      " [-] epoch   57/250, train loss 0.264470 in 0.32s\n",
      " [-] epoch   58/250, train loss 0.261110 in 0.30s\n",
      " [-] epoch   59/250, train loss 0.276469 in 0.31s\n",
      " [-] epoch   60/250, train loss 0.251565 in 0.29s\n",
      " [-] epoch   61/250, train loss 0.250381 in 0.27s\n",
      " [-] epoch   62/250, train loss 0.261358 in 0.31s\n",
      " [-] epoch   63/250, train loss 0.262186 in 0.31s\n",
      " [-] epoch   64/250, train loss 0.270290 in 0.33s\n",
      " [-] epoch   65/250, train loss 0.234046 in 0.31s\n",
      " [-] epoch   66/250, train loss 0.251998 in 0.33s\n",
      " [-] epoch   67/250, train loss 0.245488 in 0.32s\n",
      " [-] epoch   68/250, train loss 0.249951 in 0.32s\n",
      " [-] epoch   69/250, train loss 0.239619 in 0.31s\n",
      " [-] epoch   70/250, train loss 0.258894 in 0.31s\n",
      " [-] epoch   71/250, train loss 0.226679 in 0.31s\n",
      " [-] epoch   72/250, train loss 0.273140 in 0.30s\n",
      " [-] epoch   73/250, train loss 0.267879 in 0.30s\n",
      " [-] epoch   74/250, train loss 0.240409 in 0.31s\n",
      " [-] epoch   75/250, train loss 0.240212 in 0.29s\n",
      " [-] epoch   76/250, train loss 0.240801 in 0.33s\n",
      " [-] epoch   77/250, train loss 0.242408 in 0.32s\n",
      " [-] epoch   78/250, train loss 0.252674 in 0.30s\n",
      " [-] epoch   79/250, train loss 0.234116 in 0.30s\n",
      " [-] epoch   80/250, train loss 0.226366 in 0.31s\n",
      " [-] epoch   81/250, train loss 0.254069 in 0.31s\n",
      " [-] epoch   82/250, train loss 0.253929 in 0.29s\n",
      " [-] epoch   83/250, train loss 0.241328 in 0.32s\n",
      " [-] epoch   84/250, train loss 0.235021 in 0.31s\n",
      " [-] epoch   85/250, train loss 0.246328 in 0.30s\n",
      " [-] epoch   86/250, train loss 0.261377 in 0.32s\n",
      " [-] epoch   87/250, train loss 0.211541 in 0.31s\n",
      " [-] epoch   88/250, train loss 0.236196 in 0.32s\n",
      " [-] epoch   89/250, train loss 0.238637 in 0.29s\n",
      " [-] epoch   90/250, train loss 0.227984 in 0.30s\n",
      " [-] epoch   91/250, train loss 0.250745 in 0.28s\n",
      " [-] epoch   92/250, train loss 0.261745 in 0.34s\n",
      " [-] epoch   93/250, train loss 0.243565 in 0.29s\n",
      " [-] epoch   94/250, train loss 0.235681 in 0.31s\n",
      " [-] epoch   95/250, train loss 0.247769 in 0.31s\n",
      " [-] epoch   96/250, train loss 0.257544 in 0.28s\n",
      " [-] epoch   97/250, train loss 0.245783 in 0.29s\n",
      " [-] epoch   98/250, train loss 0.230524 in 0.34s\n",
      " [-] epoch   99/250, train loss 0.249954 in 0.30s\n",
      " [-] epoch  100/250, train loss 0.224078 in 0.32s\n",
      " [-] epoch  101/250, train loss 0.230256 in 0.32s\n",
      " [-] epoch  102/250, train loss 0.217713 in 0.30s\n",
      " [-] epoch  103/250, train loss 0.234390 in 0.31s\n",
      " [-] epoch  104/250, train loss 0.223650 in 0.31s\n",
      " [-] epoch  105/250, train loss 0.253263 in 0.32s\n",
      " [-] epoch  106/250, train loss 0.239015 in 0.31s\n",
      " [-] epoch  107/250, train loss 0.237491 in 0.32s\n",
      " [-] epoch  108/250, train loss 0.235398 in 0.32s\n",
      " [-] epoch  109/250, train loss 0.241581 in 0.32s\n",
      " [-] epoch  110/250, train loss 0.236286 in 0.32s\n",
      " [-] epoch  111/250, train loss 0.226700 in 0.31s\n",
      " [-] epoch  112/250, train loss 0.230768 in 0.28s\n",
      " [-] epoch  113/250, train loss 0.215925 in 0.31s\n",
      " [-] epoch  114/250, train loss 0.205119 in 0.31s\n",
      " [-] epoch  115/250, train loss 0.223898 in 0.33s\n",
      " [-] epoch  116/250, train loss 0.220197 in 0.31s\n",
      " [-] epoch  117/250, train loss 0.219428 in 0.31s\n",
      " [-] epoch  118/250, train loss 0.221842 in 0.30s\n",
      " [-] epoch  119/250, train loss 0.235064 in 0.31s\n",
      " [-] epoch  120/250, train loss 0.217315 in 0.31s\n",
      " [-] epoch  121/250, train loss 0.220712 in 0.31s\n",
      " [-] epoch  122/250, train loss 0.233920 in 0.31s\n",
      " [-] epoch  123/250, train loss 0.212119 in 0.33s\n",
      " [-] epoch  124/250, train loss 0.226997 in 0.31s\n",
      " [-] epoch  125/250, train loss 0.237278 in 0.33s\n",
      " [-] epoch  126/250, train loss 0.233133 in 0.30s\n",
      " [-] epoch  127/250, train loss 0.233827 in 0.31s\n",
      " [-] epoch  128/250, train loss 0.220809 in 0.30s\n",
      " [-] epoch  129/250, train loss 0.226778 in 0.30s\n",
      " [-] epoch  130/250, train loss 0.215021 in 0.29s\n",
      " [-] epoch  131/250, train loss 0.224337 in 0.32s\n",
      " [-] epoch  132/250, train loss 0.219975 in 0.29s\n",
      " [-] epoch  133/250, train loss 0.207293 in 0.32s\n",
      " [-] epoch  134/250, train loss 0.211402 in 0.30s\n",
      " [-] epoch  135/250, train loss 0.215959 in 0.31s\n",
      " [-] epoch  136/250, train loss 0.254175 in 0.31s\n",
      " [-] epoch  137/250, train loss 0.216193 in 0.32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  138/250, train loss 0.225178 in 0.30s\n",
      " [-] epoch  139/250, train loss 0.238280 in 0.30s\n",
      " [-] epoch  140/250, train loss 0.205123 in 0.31s\n",
      " [-] epoch  141/250, train loss 0.215887 in 0.30s\n",
      " [-] epoch  142/250, train loss 0.217657 in 0.30s\n",
      " [-] epoch  143/250, train loss 0.235512 in 0.32s\n",
      " [-] epoch  144/250, train loss 0.227026 in 0.28s\n",
      " [-] epoch  145/250, train loss 0.205447 in 0.28s\n",
      " [-] epoch  146/250, train loss 0.212684 in 0.32s\n",
      " [-] epoch  147/250, train loss 0.245781 in 0.31s\n",
      " [-] epoch  148/250, train loss 0.216611 in 0.32s\n",
      " [-] epoch  149/250, train loss 0.212425 in 0.31s\n",
      " [-] epoch  150/250, train loss 0.212128 in 0.30s\n",
      " [-] epoch  151/250, train loss 0.227811 in 0.30s\n",
      " [-] epoch  152/250, train loss 0.213126 in 0.31s\n",
      " [-] epoch  153/250, train loss 0.236706 in 0.33s\n",
      " [-] epoch  154/250, train loss 0.216155 in 0.30s\n",
      " [-] epoch  155/250, train loss 0.221536 in 0.31s\n",
      " [-] epoch  156/250, train loss 0.206334 in 0.28s\n",
      " [-] epoch  157/250, train loss 0.195101 in 0.30s\n",
      " [-] epoch  158/250, train loss 0.204277 in 0.32s\n",
      " [-] epoch  159/250, train loss 0.201154 in 0.30s\n",
      " [-] epoch  160/250, train loss 0.211628 in 0.30s\n",
      " [-] epoch  161/250, train loss 0.218148 in 0.31s\n",
      " [-] epoch  162/250, train loss 0.224066 in 0.29s\n",
      " [-] epoch  163/250, train loss 0.236891 in 0.31s\n",
      " [-] epoch  164/250, train loss 0.221148 in 0.29s\n",
      " [-] epoch  165/250, train loss 0.207004 in 0.31s\n",
      " [-] epoch  166/250, train loss 0.214705 in 0.29s\n",
      " [-] epoch  167/250, train loss 0.211414 in 0.33s\n",
      " [-] epoch  168/250, train loss 0.215535 in 0.31s\n",
      " [-] epoch  169/250, train loss 0.209496 in 0.33s\n",
      " [-] epoch  170/250, train loss 0.198050 in 0.30s\n",
      " [-] epoch  171/250, train loss 0.205008 in 0.29s\n",
      " [-] epoch  172/250, train loss 0.205621 in 0.31s\n",
      " [-] epoch  173/250, train loss 0.202651 in 0.33s\n",
      " [-] epoch  174/250, train loss 0.208220 in 0.32s\n",
      " [-] epoch  175/250, train loss 0.214223 in 0.30s\n",
      " [-] epoch  176/250, train loss 0.217584 in 0.30s\n",
      " [-] epoch  177/250, train loss 0.216680 in 0.29s\n",
      " [-] epoch  178/250, train loss 0.214155 in 0.31s\n",
      " [-] epoch  179/250, train loss 0.193137 in 0.30s\n",
      " [-] epoch  180/250, train loss 0.202276 in 0.32s\n",
      " [-] epoch  181/250, train loss 0.217257 in 0.30s\n",
      " [-] epoch  182/250, train loss 0.222422 in 0.31s\n",
      " [-] epoch  183/250, train loss 0.207103 in 0.31s\n",
      " [-] epoch  184/250, train loss 0.208362 in 0.30s\n",
      " [-] epoch  185/250, train loss 0.207237 in 0.31s\n",
      " [-] epoch  186/250, train loss 0.194299 in 0.30s\n",
      " [-] epoch  187/250, train loss 0.209790 in 0.29s\n",
      " [-] epoch  188/250, train loss 0.202818 in 0.31s\n",
      " [-] epoch  189/250, train loss 0.225664 in 0.30s\n",
      " [-] epoch  190/250, train loss 0.204240 in 0.31s\n",
      " [-] epoch  191/250, train loss 0.197380 in 0.32s\n",
      " [-] epoch  192/250, train loss 0.204932 in 0.33s\n",
      " [-] epoch  193/250, train loss 0.204571 in 0.30s\n",
      " [-] epoch  194/250, train loss 0.221280 in 0.32s\n",
      " [-] epoch  195/250, train loss 0.201124 in 0.28s\n",
      " [-] epoch  196/250, train loss 0.202120 in 0.30s\n",
      " [-] epoch  197/250, train loss 0.206508 in 0.30s\n",
      " [-] epoch  198/250, train loss 0.200598 in 0.31s\n",
      " [-] epoch  199/250, train loss 0.204162 in 0.31s\n",
      " [-] epoch  200/250, train loss 0.207757 in 0.31s\n",
      " [-] epoch  201/250, train loss 0.224401 in 0.30s\n",
      " [-] epoch  202/250, train loss 0.191727 in 0.32s\n",
      " [-] epoch  203/250, train loss 0.198030 in 0.32s\n",
      " [-] epoch  204/250, train loss 0.194447 in 0.30s\n",
      " [-] epoch  205/250, train loss 0.200706 in 0.27s\n",
      " [-] epoch  206/250, train loss 0.216124 in 0.30s\n",
      " [-] epoch  207/250, train loss 0.204666 in 0.31s\n",
      " [-] epoch  208/250, train loss 0.202701 in 0.31s\n",
      " [-] epoch  209/250, train loss 0.216062 in 0.31s\n",
      " [-] epoch  210/250, train loss 0.211360 in 0.32s\n",
      " [-] epoch  211/250, train loss 0.198946 in 0.31s\n",
      " [-] epoch  212/250, train loss 0.191063 in 0.31s\n",
      " [-] epoch  213/250, train loss 0.202620 in 0.32s\n",
      " [-] epoch  214/250, train loss 0.226943 in 0.31s\n",
      " [-] epoch  215/250, train loss 0.223208 in 0.31s\n",
      " [-] epoch  216/250, train loss 0.203237 in 0.32s\n",
      " [-] epoch  217/250, train loss 0.191074 in 0.32s\n",
      " [-] epoch  218/250, train loss 0.201627 in 0.31s\n",
      " [-] epoch  219/250, train loss 0.194927 in 0.28s\n",
      " [-] epoch  220/250, train loss 0.203719 in 0.31s\n",
      " [-] epoch  221/250, train loss 0.201420 in 0.31s\n",
      " [-] epoch  222/250, train loss 0.202563 in 0.30s\n",
      " [-] epoch  223/250, train loss 0.191297 in 0.33s\n",
      " [-] epoch  224/250, train loss 0.203249 in 0.28s\n",
      " [-] epoch  225/250, train loss 0.184479 in 0.32s\n",
      " [-] epoch  226/250, train loss 0.197731 in 0.30s\n",
      " [-] epoch  227/250, train loss 0.196310 in 0.30s\n",
      " [-] epoch  228/250, train loss 0.202073 in 0.30s\n",
      " [-] epoch  229/250, train loss 0.184929 in 0.30s\n",
      " [-] epoch  230/250, train loss 0.189830 in 0.29s\n",
      " [-] epoch  231/250, train loss 0.192879 in 0.31s\n",
      " [-] epoch  232/250, train loss 0.213231 in 0.31s\n",
      " [-] epoch  233/250, train loss 0.205662 in 0.30s\n",
      " [-] epoch  234/250, train loss 0.203057 in 0.30s\n",
      " [-] epoch  235/250, train loss 0.202783 in 0.31s\n",
      " [-] epoch  236/250, train loss 0.195748 in 0.31s\n",
      " [-] epoch  237/250, train loss 0.212649 in 0.33s\n",
      " [-] epoch  238/250, train loss 0.196296 in 0.30s\n",
      " [-] epoch  239/250, train loss 0.185566 in 0.30s\n",
      " [-] epoch  240/250, train loss 0.198370 in 0.32s\n",
      " [-] epoch  241/250, train loss 0.194606 in 0.31s\n",
      " [-] epoch  242/250, train loss 0.203354 in 0.29s\n",
      " [-] epoch  243/250, train loss 0.236682 in 0.30s\n",
      " [-] epoch  244/250, train loss 0.195741 in 0.30s\n",
      " [-] epoch  245/250, train loss 0.196701 in 0.31s\n",
      " [-] epoch  246/250, train loss 0.203554 in 0.31s\n",
      " [-] epoch  247/250, train loss 0.202776 in 0.31s\n",
      " [-] epoch  248/250, train loss 0.184484 in 0.31s\n",
      " [-] epoch  249/250, train loss 0.206273 in 0.31s\n",
      " [-] epoch  250/250, train loss 0.187154 in 0.34s\n",
      " [-] test acc. 80.833333%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.606700 in 0.40s\n",
      " [-] epoch    2/250, train loss 0.453514 in 0.41s\n",
      " [-] epoch    3/250, train loss 0.460443 in 0.42s\n",
      " [-] epoch    4/250, train loss 0.387659 in 0.42s\n",
      " [-] epoch    5/250, train loss 0.382538 in 0.41s\n",
      " [-] epoch    6/250, train loss 0.368306 in 0.42s\n",
      " [-] epoch    7/250, train loss 0.385961 in 0.41s\n",
      " [-] epoch    8/250, train loss 0.379175 in 0.40s\n",
      " [-] epoch    9/250, train loss 0.361280 in 0.40s\n",
      " [-] epoch   10/250, train loss 0.345835 in 0.42s\n",
      " [-] epoch   11/250, train loss 0.345948 in 0.39s\n",
      " [-] epoch   12/250, train loss 0.353230 in 0.37s\n",
      " [-] epoch   13/250, train loss 0.328064 in 0.40s\n",
      " [-] epoch   14/250, train loss 0.334069 in 0.38s\n",
      " [-] epoch   15/250, train loss 0.335272 in 0.40s\n",
      " [-] epoch   16/250, train loss 0.318465 in 0.41s\n",
      " [-] epoch   17/250, train loss 0.349641 in 0.42s\n",
      " [-] epoch   18/250, train loss 0.322776 in 0.41s\n",
      " [-] epoch   19/250, train loss 0.311687 in 0.41s\n",
      " [-] epoch   20/250, train loss 0.331588 in 0.37s\n",
      " [-] epoch   21/250, train loss 0.331169 in 0.40s\n",
      " [-] epoch   22/250, train loss 0.295651 in 0.38s\n",
      " [-] epoch   23/250, train loss 0.341105 in 0.41s\n",
      " [-] epoch   24/250, train loss 0.283923 in 0.41s\n",
      " [-] epoch   25/250, train loss 0.303558 in 0.38s\n",
      " [-] epoch   26/250, train loss 0.291925 in 0.40s\n",
      " [-] epoch   27/250, train loss 0.312684 in 0.40s\n",
      " [-] epoch   28/250, train loss 0.313453 in 0.39s\n",
      " [-] epoch   29/250, train loss 0.287105 in 0.41s\n",
      " [-] epoch   30/250, train loss 0.312255 in 0.40s\n",
      " [-] epoch   31/250, train loss 0.309914 in 0.37s\n",
      " [-] epoch   32/250, train loss 0.287069 in 0.43s\n",
      " [-] epoch   33/250, train loss 0.283529 in 0.41s\n",
      " [-] epoch   34/250, train loss 0.277173 in 0.39s\n",
      " [-] epoch   35/250, train loss 0.274375 in 0.39s\n",
      " [-] epoch   36/250, train loss 0.291746 in 0.38s\n",
      " [-] epoch   37/250, train loss 0.294398 in 0.38s\n",
      " [-] epoch   38/250, train loss 0.301046 in 0.41s\n",
      " [-] epoch   39/250, train loss 0.266659 in 0.39s\n",
      " [-] epoch   40/250, train loss 0.260513 in 0.39s\n",
      " [-] epoch   41/250, train loss 0.292940 in 0.39s\n",
      " [-] epoch   42/250, train loss 0.261621 in 0.40s\n",
      " [-] epoch   43/250, train loss 0.305818 in 0.41s\n",
      " [-] epoch   44/250, train loss 0.285822 in 0.39s\n",
      " [-] epoch   45/250, train loss 0.262707 in 0.39s\n",
      " [-] epoch   46/250, train loss 0.275028 in 0.37s\n",
      " [-] epoch   47/250, train loss 0.282441 in 0.40s\n",
      " [-] epoch   48/250, train loss 0.271743 in 0.40s\n",
      " [-] epoch   49/250, train loss 0.268962 in 0.40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.271757 in 0.41s\n",
      " [-] epoch   51/250, train loss 0.308569 in 0.43s\n",
      " [-] epoch   52/250, train loss 0.268918 in 0.43s\n",
      " [-] epoch   53/250, train loss 0.272214 in 0.40s\n",
      " [-] epoch   54/250, train loss 0.263229 in 0.41s\n",
      " [-] epoch   55/250, train loss 0.256736 in 0.41s\n",
      " [-] epoch   56/250, train loss 0.253083 in 0.38s\n",
      " [-] epoch   57/250, train loss 0.247069 in 0.42s\n",
      " [-] epoch   58/250, train loss 0.265275 in 0.44s\n",
      " [-] epoch   59/250, train loss 0.270713 in 0.38s\n",
      " [-] epoch   60/250, train loss 0.276519 in 0.40s\n",
      " [-] epoch   61/250, train loss 0.248532 in 0.41s\n",
      " [-] epoch   62/250, train loss 0.247417 in 0.42s\n",
      " [-] epoch   63/250, train loss 0.280878 in 0.40s\n",
      " [-] epoch   64/250, train loss 0.253168 in 0.44s\n",
      " [-] epoch   65/250, train loss 0.256112 in 0.42s\n",
      " [-] epoch   66/250, train loss 0.254210 in 0.40s\n",
      " [-] epoch   67/250, train loss 0.247717 in 0.41s\n",
      " [-] epoch   68/250, train loss 0.259737 in 0.38s\n",
      " [-] epoch   69/250, train loss 0.243274 in 0.40s\n",
      " [-] epoch   70/250, train loss 0.232747 in 0.44s\n",
      " [-] epoch   71/250, train loss 0.245174 in 0.39s\n",
      " [-] epoch   72/250, train loss 0.264671 in 0.40s\n",
      " [-] epoch   73/250, train loss 0.226685 in 0.41s\n",
      " [-] epoch   74/250, train loss 0.280451 in 0.40s\n",
      " [-] epoch   75/250, train loss 0.261491 in 0.39s\n",
      " [-] epoch   76/250, train loss 0.230690 in 0.40s\n",
      " [-] epoch   77/250, train loss 0.234880 in 0.41s\n",
      " [-] epoch   78/250, train loss 0.237076 in 0.39s\n",
      " [-] epoch   79/250, train loss 0.263785 in 0.38s\n",
      " [-] epoch   80/250, train loss 0.263984 in 0.41s\n",
      " [-] epoch   81/250, train loss 0.256483 in 0.42s\n",
      " [-] epoch   82/250, train loss 0.259302 in 0.41s\n",
      " [-] epoch   83/250, train loss 0.248889 in 0.40s\n",
      " [-] epoch   84/250, train loss 0.229319 in 0.42s\n",
      " [-] epoch   85/250, train loss 0.250740 in 0.42s\n",
      " [-] epoch   86/250, train loss 0.237367 in 0.42s\n",
      " [-] epoch   87/250, train loss 0.238393 in 0.40s\n",
      " [-] epoch   88/250, train loss 0.231575 in 0.42s\n",
      " [-] epoch   89/250, train loss 0.239834 in 0.39s\n",
      " [-] epoch   90/250, train loss 0.245359 in 0.38s\n",
      " [-] epoch   91/250, train loss 0.237799 in 0.40s\n",
      " [-] epoch   92/250, train loss 0.227273 in 0.39s\n",
      " [-] epoch   93/250, train loss 0.245116 in 0.40s\n",
      " [-] epoch   94/250, train loss 0.255412 in 0.35s\n",
      " [-] epoch   95/250, train loss 0.263461 in 0.38s\n",
      " [-] epoch   96/250, train loss 0.236792 in 0.39s\n",
      " [-] epoch   97/250, train loss 0.225396 in 0.37s\n",
      " [-] epoch   98/250, train loss 0.238571 in 0.41s\n",
      " [-] epoch   99/250, train loss 0.257403 in 0.40s\n",
      " [-] epoch  100/250, train loss 0.240021 in 0.40s\n",
      " [-] epoch  101/250, train loss 0.238627 in 0.39s\n",
      " [-] epoch  102/250, train loss 0.239762 in 0.40s\n",
      " [-] epoch  103/250, train loss 0.240352 in 0.41s\n",
      " [-] epoch  104/250, train loss 0.232972 in 0.39s\n",
      " [-] epoch  105/250, train loss 0.256917 in 0.40s\n",
      " [-] epoch  106/250, train loss 0.242607 in 0.40s\n",
      " [-] epoch  107/250, train loss 0.253125 in 0.38s\n",
      " [-] epoch  108/250, train loss 0.239590 in 0.40s\n",
      " [-] epoch  109/250, train loss 0.230715 in 0.40s\n",
      " [-] epoch  110/250, train loss 0.251490 in 0.40s\n",
      " [-] epoch  111/250, train loss 0.218639 in 0.40s\n",
      " [-] epoch  112/250, train loss 0.235512 in 0.38s\n",
      " [-] epoch  113/250, train loss 0.219310 in 0.38s\n",
      " [-] epoch  114/250, train loss 0.218812 in 0.39s\n",
      " [-] epoch  115/250, train loss 0.217758 in 0.40s\n",
      " [-] epoch  116/250, train loss 0.230783 in 0.39s\n",
      " [-] epoch  117/250, train loss 0.214615 in 0.40s\n",
      " [-] epoch  118/250, train loss 0.231715 in 0.38s\n",
      " [-] epoch  119/250, train loss 0.194774 in 0.38s\n",
      " [-] epoch  120/250, train loss 0.228888 in 0.41s\n",
      " [-] epoch  121/250, train loss 0.238188 in 0.40s\n",
      " [-] epoch  122/250, train loss 0.224556 in 0.40s\n",
      " [-] epoch  123/250, train loss 0.260977 in 0.41s\n",
      " [-] epoch  124/250, train loss 0.209321 in 0.41s\n",
      " [-] epoch  125/250, train loss 0.216028 in 0.40s\n",
      " [-] epoch  126/250, train loss 0.220493 in 0.42s\n",
      " [-] epoch  127/250, train loss 0.216323 in 0.44s\n",
      " [-] epoch  128/250, train loss 0.205393 in 0.42s\n",
      " [-] epoch  129/250, train loss 0.217060 in 0.44s\n",
      " [-] epoch  130/250, train loss 0.220411 in 0.38s\n",
      " [-] epoch  131/250, train loss 0.208253 in 0.39s\n",
      " [-] epoch  132/250, train loss 0.238756 in 0.39s\n",
      " [-] epoch  133/250, train loss 0.219243 in 0.40s\n",
      " [-] epoch  134/250, train loss 0.215019 in 0.39s\n",
      " [-] epoch  135/250, train loss 0.236422 in 0.37s\n",
      " [-] epoch  136/250, train loss 0.228433 in 0.41s\n",
      " [-] epoch  137/250, train loss 0.246448 in 0.43s\n",
      " [-] epoch  138/250, train loss 0.213095 in 0.42s\n",
      " [-] epoch  139/250, train loss 0.223189 in 0.41s\n",
      " [-] epoch  140/250, train loss 0.222444 in 0.41s\n",
      " [-] epoch  141/250, train loss 0.225862 in 0.39s\n",
      " [-] epoch  142/250, train loss 0.243187 in 0.41s\n",
      " [-] epoch  143/250, train loss 0.222916 in 0.39s\n",
      " [-] epoch  144/250, train loss 0.246214 in 0.40s\n",
      " [-] epoch  145/250, train loss 0.219005 in 0.39s\n",
      " [-] epoch  146/250, train loss 0.225158 in 0.43s\n",
      " [-] epoch  147/250, train loss 0.222127 in 0.41s\n",
      " [-] epoch  148/250, train loss 0.231210 in 0.41s\n",
      " [-] epoch  149/250, train loss 0.206749 in 0.39s\n",
      " [-] epoch  150/250, train loss 0.212790 in 0.40s\n",
      " [-] epoch  151/250, train loss 0.234194 in 0.42s\n",
      " [-] epoch  152/250, train loss 0.231821 in 0.41s\n",
      " [-] epoch  153/250, train loss 0.203613 in 0.42s\n",
      " [-] epoch  154/250, train loss 0.217449 in 0.40s\n",
      " [-] epoch  155/250, train loss 0.232436 in 0.40s\n",
      " [-] epoch  156/250, train loss 0.210756 in 0.42s\n",
      " [-] epoch  157/250, train loss 0.228298 in 0.42s\n",
      " [-] epoch  158/250, train loss 0.209545 in 0.41s\n",
      " [-] epoch  159/250, train loss 0.213645 in 0.42s\n",
      " [-] epoch  160/250, train loss 0.217901 in 0.42s\n",
      " [-] epoch  161/250, train loss 0.220076 in 0.40s\n",
      " [-] epoch  162/250, train loss 0.221281 in 0.41s\n",
      " [-] epoch  163/250, train loss 0.227406 in 0.41s\n",
      " [-] epoch  164/250, train loss 0.210814 in 0.38s\n",
      " [-] epoch  165/250, train loss 0.227018 in 0.37s\n",
      " [-] epoch  166/250, train loss 0.226211 in 0.41s\n",
      " [-] epoch  167/250, train loss 0.215977 in 0.38s\n",
      " [-] epoch  168/250, train loss 0.202118 in 0.39s\n",
      " [-] epoch  169/250, train loss 0.213054 in 0.43s\n",
      " [-] epoch  170/250, train loss 0.213283 in 0.40s\n",
      " [-] epoch  171/250, train loss 0.211885 in 0.42s\n",
      " [-] epoch  172/250, train loss 0.213308 in 0.41s\n",
      " [-] epoch  173/250, train loss 0.213610 in 0.41s\n",
      " [-] epoch  174/250, train loss 0.220448 in 0.39s\n",
      " [-] epoch  175/250, train loss 0.216171 in 0.41s\n",
      " [-] epoch  176/250, train loss 0.218126 in 0.40s\n",
      " [-] epoch  177/250, train loss 0.215108 in 0.40s\n",
      " [-] epoch  178/250, train loss 0.234245 in 0.39s\n",
      " [-] epoch  179/250, train loss 0.219616 in 0.42s\n",
      " [-] epoch  180/250, train loss 0.228013 in 0.42s\n",
      " [-] epoch  181/250, train loss 0.208991 in 0.40s\n",
      " [-] epoch  182/250, train loss 0.206174 in 0.39s\n",
      " [-] epoch  183/250, train loss 0.225221 in 0.38s\n",
      " [-] epoch  184/250, train loss 0.199344 in 0.43s\n",
      " [-] epoch  185/250, train loss 0.209099 in 0.37s\n",
      " [-] epoch  186/250, train loss 0.219488 in 0.39s\n",
      " [-] epoch  187/250, train loss 0.207243 in 0.39s\n",
      " [-] epoch  188/250, train loss 0.222200 in 0.39s\n",
      " [-] epoch  189/250, train loss 0.234067 in 0.40s\n",
      " [-] epoch  190/250, train loss 0.199049 in 0.39s\n",
      " [-] epoch  191/250, train loss 0.208947 in 0.39s\n",
      " [-] epoch  192/250, train loss 0.213520 in 0.41s\n",
      " [-] epoch  193/250, train loss 0.218820 in 0.39s\n",
      " [-] epoch  194/250, train loss 0.216508 in 0.40s\n",
      " [-] epoch  195/250, train loss 0.207567 in 0.41s\n",
      " [-] epoch  196/250, train loss 0.229386 in 0.43s\n",
      " [-] epoch  197/250, train loss 0.191775 in 0.43s\n",
      " [-] epoch  198/250, train loss 0.224843 in 0.40s\n",
      " [-] epoch  199/250, train loss 0.208162 in 0.41s\n",
      " [-] epoch  200/250, train loss 0.235003 in 0.40s\n",
      " [-] epoch  201/250, train loss 0.209682 in 0.39s\n",
      " [-] epoch  202/250, train loss 0.213581 in 0.40s\n",
      " [-] epoch  203/250, train loss 0.243684 in 0.41s\n",
      " [-] epoch  204/250, train loss 0.214649 in 0.41s\n",
      " [-] epoch  205/250, train loss 0.229872 in 0.40s\n",
      " [-] epoch  206/250, train loss 0.213881 in 0.40s\n",
      " [-] epoch  207/250, train loss 0.195453 in 0.40s\n",
      " [-] epoch  208/250, train loss 0.202909 in 0.40s\n",
      " [-] epoch  209/250, train loss 0.224735 in 0.40s\n",
      " [-] epoch  210/250, train loss 0.198440 in 0.40s\n",
      " [-] epoch  211/250, train loss 0.197920 in 0.40s\n",
      " [-] epoch  212/250, train loss 0.209454 in 0.40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  213/250, train loss 0.209138 in 0.40s\n",
      " [-] epoch  214/250, train loss 0.206808 in 0.40s\n",
      " [-] epoch  215/250, train loss 0.216036 in 0.39s\n",
      " [-] epoch  216/250, train loss 0.201850 in 0.39s\n",
      " [-] epoch  217/250, train loss 0.190416 in 0.42s\n",
      " [-] epoch  218/250, train loss 0.210053 in 0.40s\n",
      " [-] epoch  219/250, train loss 0.213334 in 0.40s\n",
      " [-] epoch  220/250, train loss 0.210501 in 0.41s\n",
      " [-] epoch  221/250, train loss 0.211895 in 0.41s\n",
      " [-] epoch  222/250, train loss 0.209103 in 0.41s\n",
      " [-] epoch  223/250, train loss 0.192471 in 0.40s\n",
      " [-] epoch  224/250, train loss 0.231747 in 0.41s\n",
      " [-] epoch  225/250, train loss 0.213436 in 0.42s\n",
      " [-] epoch  226/250, train loss 0.213688 in 0.42s\n",
      " [-] epoch  227/250, train loss 0.219267 in 0.42s\n",
      " [-] epoch  228/250, train loss 0.206710 in 0.40s\n",
      " [-] epoch  229/250, train loss 0.225297 in 0.39s\n",
      " [-] epoch  230/250, train loss 0.204479 in 0.43s\n",
      " [-] epoch  231/250, train loss 0.211790 in 0.41s\n",
      " [-] epoch  232/250, train loss 0.199844 in 0.40s\n",
      " [-] epoch  233/250, train loss 0.229870 in 0.42s\n",
      " [-] epoch  234/250, train loss 0.210197 in 0.41s\n",
      " [-] epoch  235/250, train loss 0.204607 in 0.42s\n",
      " [-] epoch  236/250, train loss 0.190714 in 0.42s\n",
      " [-] epoch  237/250, train loss 0.199299 in 0.40s\n",
      " [-] epoch  238/250, train loss 0.216164 in 0.41s\n",
      " [-] epoch  239/250, train loss 0.220309 in 0.39s\n",
      " [-] epoch  240/250, train loss 0.209018 in 0.40s\n",
      " [-] epoch  241/250, train loss 0.196271 in 0.40s\n",
      " [-] epoch  242/250, train loss 0.190388 in 0.40s\n",
      " [-] epoch  243/250, train loss 0.199562 in 0.40s\n",
      " [-] epoch  244/250, train loss 0.193718 in 0.40s\n",
      " [-] epoch  245/250, train loss 0.207596 in 0.41s\n",
      " [-] epoch  246/250, train loss 0.214986 in 0.41s\n",
      " [-] epoch  247/250, train loss 0.192085 in 0.39s\n",
      " [-] epoch  248/250, train loss 0.203006 in 0.40s\n",
      " [-] epoch  249/250, train loss 0.198135 in 0.39s\n",
      " [-] epoch  250/250, train loss 0.181156 in 0.39s\n",
      " [-] test acc. 84.444444%\n",
      "Je vais utiliser 7 layers\n",
      " [-] epoch    1/250, train loss 0.591937 in 0.48s\n",
      " [-] epoch    2/250, train loss 0.479185 in 0.48s\n",
      " [-] epoch    3/250, train loss 0.420637 in 0.50s\n",
      " [-] epoch    4/250, train loss 0.417657 in 0.50s\n",
      " [-] epoch    5/250, train loss 0.386232 in 0.50s\n",
      " [-] epoch    6/250, train loss 0.377837 in 0.49s\n",
      " [-] epoch    7/250, train loss 0.393516 in 0.49s\n",
      " [-] epoch    8/250, train loss 0.356557 in 0.49s\n",
      " [-] epoch    9/250, train loss 0.334191 in 0.49s\n",
      " [-] epoch   10/250, train loss 0.353813 in 0.51s\n",
      " [-] epoch   11/250, train loss 0.351998 in 0.46s\n",
      " [-] epoch   12/250, train loss 0.370767 in 0.47s\n",
      " [-] epoch   13/250, train loss 0.340188 in 0.49s\n",
      " [-] epoch   14/250, train loss 0.344200 in 0.50s\n",
      " [-] epoch   15/250, train loss 0.329723 in 0.49s\n",
      " [-] epoch   16/250, train loss 0.341649 in 0.47s\n",
      " [-] epoch   17/250, train loss 0.294993 in 0.50s\n",
      " [-] epoch   18/250, train loss 0.351132 in 0.49s\n",
      " [-] epoch   19/250, train loss 0.331827 in 0.52s\n",
      " [-] epoch   20/250, train loss 0.325866 in 0.49s\n",
      " [-] epoch   21/250, train loss 0.324567 in 0.48s\n",
      " [-] epoch   22/250, train loss 0.311482 in 0.49s\n",
      " [-] epoch   23/250, train loss 0.296660 in 0.49s\n",
      " [-] epoch   24/250, train loss 0.338996 in 0.52s\n",
      " [-] epoch   25/250, train loss 0.298048 in 0.50s\n",
      " [-] epoch   26/250, train loss 0.313347 in 0.47s\n",
      " [-] epoch   27/250, train loss 0.321584 in 0.50s\n",
      " [-] epoch   28/250, train loss 0.306441 in 0.51s\n",
      " [-] epoch   29/250, train loss 0.286516 in 0.48s\n",
      " [-] epoch   30/250, train loss 0.303972 in 0.48s\n",
      " [-] epoch   31/250, train loss 0.285196 in 0.52s\n",
      " [-] epoch   32/250, train loss 0.304406 in 0.50s\n",
      " [-] epoch   33/250, train loss 0.302084 in 0.49s\n",
      " [-] epoch   34/250, train loss 0.308401 in 0.48s\n",
      " [-] epoch   35/250, train loss 0.302933 in 0.51s\n",
      " [-] epoch   36/250, train loss 0.301210 in 0.50s\n",
      " [-] epoch   37/250, train loss 0.304352 in 0.50s\n",
      " [-] epoch   38/250, train loss 0.282934 in 0.48s\n",
      " [-] epoch   39/250, train loss 0.283927 in 0.51s\n",
      " [-] epoch   40/250, train loss 0.283809 in 0.49s\n",
      " [-] epoch   41/250, train loss 0.301537 in 0.49s\n",
      " [-] epoch   42/250, train loss 0.296064 in 0.50s\n",
      " [-] epoch   43/250, train loss 0.290242 in 0.49s\n",
      " [-] epoch   44/250, train loss 0.283893 in 0.49s\n",
      " [-] epoch   45/250, train loss 0.286689 in 0.51s\n",
      " [-] epoch   46/250, train loss 0.305901 in 0.49s\n",
      " [-] epoch   47/250, train loss 0.282119 in 0.51s\n",
      " [-] epoch   48/250, train loss 0.279309 in 0.50s\n",
      " [-] epoch   49/250, train loss 0.274727 in 0.50s\n",
      " [-] epoch   50/250, train loss 0.273823 in 0.50s\n",
      " [-] epoch   51/250, train loss 0.301153 in 0.52s\n",
      " [-] epoch   52/250, train loss 0.306574 in 0.51s\n",
      " [-] epoch   53/250, train loss 0.283610 in 0.50s\n",
      " [-] epoch   54/250, train loss 0.248850 in 0.50s\n",
      " [-] epoch   55/250, train loss 0.256658 in 0.49s\n",
      " [-] epoch   56/250, train loss 0.250672 in 0.51s\n",
      " [-] epoch   57/250, train loss 0.286213 in 0.52s\n",
      " [-] epoch   58/250, train loss 0.258981 in 0.47s\n",
      " [-] epoch   59/250, train loss 0.251278 in 0.48s\n",
      " [-] epoch   60/250, train loss 0.258270 in 0.50s\n",
      " [-] epoch   61/250, train loss 0.281365 in 0.47s\n",
      " [-] epoch   62/250, train loss 0.258540 in 0.50s\n",
      " [-] epoch   63/250, train loss 0.289785 in 0.50s\n",
      " [-] epoch   64/250, train loss 0.274304 in 0.50s\n",
      " [-] epoch   65/250, train loss 0.256370 in 0.51s\n",
      " [-] epoch   66/250, train loss 0.242608 in 0.49s\n",
      " [-] epoch   67/250, train loss 0.248337 in 0.47s\n",
      " [-] epoch   68/250, train loss 0.262131 in 0.52s\n",
      " [-] epoch   69/250, train loss 0.264429 in 0.48s\n",
      " [-] epoch   70/250, train loss 0.253881 in 0.50s\n",
      " [-] epoch   71/250, train loss 0.241341 in 0.51s\n",
      " [-] epoch   72/250, train loss 0.259194 in 0.51s\n",
      " [-] epoch   73/250, train loss 0.244279 in 0.49s\n",
      " [-] epoch   74/250, train loss 0.271213 in 0.46s\n",
      " [-] epoch   75/250, train loss 0.246655 in 0.45s\n",
      " [-] epoch   76/250, train loss 0.239493 in 0.46s\n",
      " [-] epoch   77/250, train loss 0.213061 in 0.51s\n",
      " [-] epoch   78/250, train loss 0.235117 in 0.49s\n",
      " [-] epoch   79/250, train loss 0.229936 in 0.47s\n",
      " [-] epoch   80/250, train loss 0.272065 in 0.48s\n",
      " [-] epoch   81/250, train loss 0.240168 in 0.48s\n",
      " [-] epoch   82/250, train loss 0.237052 in 0.50s\n",
      " [-] epoch   83/250, train loss 0.261930 in 0.50s\n",
      " [-] epoch   84/250, train loss 0.249265 in 0.49s\n",
      " [-] epoch   85/250, train loss 0.239507 in 0.48s\n",
      " [-] epoch   86/250, train loss 0.257148 in 0.49s\n",
      " [-] epoch   87/250, train loss 0.257462 in 0.48s\n",
      " [-] epoch   88/250, train loss 0.226485 in 0.51s\n",
      " [-] epoch   89/250, train loss 0.225596 in 0.49s\n",
      " [-] epoch   90/250, train loss 0.231730 in 0.46s\n",
      " [-] epoch   91/250, train loss 0.263489 in 0.50s\n",
      " [-] epoch   92/250, train loss 0.240061 in 0.48s\n",
      " [-] epoch   93/250, train loss 0.235708 in 0.46s\n",
      " [-] epoch   94/250, train loss 0.265646 in 0.49s\n",
      " [-] epoch   95/250, train loss 0.226092 in 0.48s\n",
      " [-] epoch   96/250, train loss 0.255318 in 0.50s\n",
      " [-] epoch   97/250, train loss 0.259138 in 0.49s\n",
      " [-] epoch   98/250, train loss 0.224987 in 0.49s\n",
      " [-] epoch   99/250, train loss 0.224230 in 0.53s\n",
      " [-] epoch  100/250, train loss 0.238999 in 0.49s\n",
      " [-] epoch  101/250, train loss 0.245427 in 0.47s\n",
      " [-] epoch  102/250, train loss 0.234186 in 0.50s\n",
      " [-] epoch  103/250, train loss 0.241411 in 0.48s\n",
      " [-] epoch  104/250, train loss 0.242729 in 0.46s\n",
      " [-] epoch  105/250, train loss 0.253305 in 0.47s\n",
      " [-] epoch  106/250, train loss 0.241339 in 0.48s\n",
      " [-] epoch  107/250, train loss 0.251111 in 0.47s\n",
      " [-] epoch  108/250, train loss 0.226365 in 0.50s\n",
      " [-] epoch  109/250, train loss 0.224902 in 0.48s\n",
      " [-] epoch  110/250, train loss 0.225639 in 0.51s\n",
      " [-] epoch  111/250, train loss 0.232258 in 0.49s\n",
      " [-] epoch  112/250, train loss 0.211369 in 0.51s\n",
      " [-] epoch  113/250, train loss 0.232703 in 0.50s\n",
      " [-] epoch  114/250, train loss 0.246934 in 0.50s\n",
      " [-] epoch  115/250, train loss 0.252784 in 0.49s\n",
      " [-] epoch  116/250, train loss 0.232642 in 0.51s\n",
      " [-] epoch  117/250, train loss 0.215730 in 0.48s\n",
      " [-] epoch  118/250, train loss 0.224764 in 0.49s\n",
      " [-] epoch  119/250, train loss 0.238410 in 0.49s\n",
      " [-] epoch  120/250, train loss 0.234945 in 0.48s\n",
      " [-] epoch  121/250, train loss 0.236579 in 0.48s\n",
      " [-] epoch  122/250, train loss 0.217316 in 0.45s\n",
      " [-] epoch  123/250, train loss 0.238877 in 0.50s\n",
      " [-] epoch  124/250, train loss 0.250522 in 0.52s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  125/250, train loss 0.239737 in 0.51s\n",
      " [-] epoch  126/250, train loss 0.222563 in 0.49s\n",
      " [-] epoch  127/250, train loss 0.212647 in 0.48s\n",
      " [-] epoch  128/250, train loss 0.228657 in 0.49s\n",
      " [-] epoch  129/250, train loss 0.220578 in 0.47s\n",
      " [-] epoch  130/250, train loss 0.223361 in 0.47s\n",
      " [-] epoch  131/250, train loss 0.209026 in 0.46s\n",
      " [-] epoch  132/250, train loss 0.206038 in 0.47s\n",
      " [-] epoch  133/250, train loss 0.210330 in 0.49s\n",
      " [-] epoch  134/250, train loss 0.207651 in 0.50s\n",
      " [-] epoch  135/250, train loss 0.206162 in 0.50s\n",
      " [-] epoch  136/250, train loss 0.204399 in 0.49s\n",
      " [-] epoch  137/250, train loss 0.207253 in 0.49s\n",
      " [-] epoch  138/250, train loss 0.195897 in 0.47s\n",
      " [-] epoch  139/250, train loss 0.251061 in 0.46s\n",
      " [-] epoch  140/250, train loss 0.219254 in 0.48s\n",
      " [-] epoch  141/250, train loss 0.212212 in 0.48s\n",
      " [-] epoch  142/250, train loss 0.206394 in 0.47s\n",
      " [-] epoch  143/250, train loss 0.214266 in 0.49s\n",
      " [-] epoch  144/250, train loss 0.216988 in 0.49s\n",
      " [-] epoch  145/250, train loss 0.202108 in 0.49s\n",
      " [-] epoch  146/250, train loss 0.225979 in 0.46s\n",
      " [-] epoch  147/250, train loss 0.216196 in 0.50s\n",
      " [-] epoch  148/250, train loss 0.226853 in 0.49s\n",
      " [-] epoch  149/250, train loss 0.218104 in 0.48s\n",
      " [-] epoch  150/250, train loss 0.224674 in 0.47s\n",
      " [-] epoch  151/250, train loss 0.228340 in 0.48s\n",
      " [-] epoch  152/250, train loss 0.207931 in 0.47s\n",
      " [-] epoch  153/250, train loss 0.201611 in 0.51s\n",
      " [-] epoch  154/250, train loss 0.220452 in 0.50s\n",
      " [-] epoch  155/250, train loss 0.236123 in 0.49s\n",
      " [-] epoch  156/250, train loss 0.209144 in 0.50s\n",
      " [-] epoch  157/250, train loss 0.208008 in 0.49s\n",
      " [-] epoch  158/250, train loss 0.232478 in 0.48s\n",
      " [-] epoch  159/250, train loss 0.210889 in 0.46s\n",
      " [-] epoch  160/250, train loss 0.208149 in 0.45s\n",
      " [-] epoch  161/250, train loss 0.218570 in 0.49s\n",
      " [-] epoch  162/250, train loss 0.203810 in 0.48s\n",
      " [-] epoch  163/250, train loss 0.201822 in 0.48s\n",
      " [-] epoch  164/250, train loss 0.215841 in 0.47s\n",
      " [-] epoch  165/250, train loss 0.199246 in 0.48s\n",
      " [-] epoch  166/250, train loss 0.201324 in 0.49s\n",
      " [-] epoch  167/250, train loss 0.204690 in 0.49s\n",
      " [-] epoch  168/250, train loss 0.212359 in 0.50s\n",
      " [-] epoch  169/250, train loss 0.213641 in 0.48s\n",
      " [-] epoch  170/250, train loss 0.207144 in 0.48s\n",
      " [-] epoch  171/250, train loss 0.194154 in 0.48s\n",
      " [-] epoch  172/250, train loss 0.213622 in 0.50s\n",
      " [-] epoch  173/250, train loss 0.206390 in 0.47s\n",
      " [-] epoch  174/250, train loss 0.224563 in 0.47s\n",
      " [-] epoch  175/250, train loss 0.232660 in 0.48s\n",
      " [-] epoch  176/250, train loss 0.200014 in 0.48s\n",
      " [-] epoch  177/250, train loss 0.199508 in 0.49s\n",
      " [-] epoch  178/250, train loss 0.217705 in 0.47s\n",
      " [-] epoch  179/250, train loss 0.219301 in 0.48s\n",
      " [-] epoch  180/250, train loss 0.214192 in 0.45s\n",
      " [-] epoch  181/250, train loss 0.192122 in 0.48s\n",
      " [-] epoch  182/250, train loss 0.207378 in 0.49s\n",
      " [-] epoch  183/250, train loss 0.199464 in 0.49s\n",
      " [-] epoch  184/250, train loss 0.194929 in 0.48s\n",
      " [-] epoch  185/250, train loss 0.200407 in 0.47s\n",
      " [-] epoch  186/250, train loss 0.204262 in 0.48s\n",
      " [-] epoch  187/250, train loss 0.203976 in 0.47s\n",
      " [-] epoch  188/250, train loss 0.200786 in 0.47s\n",
      " [-] epoch  189/250, train loss 0.185956 in 0.47s\n",
      " [-] epoch  190/250, train loss 0.188540 in 0.47s\n",
      " [-] epoch  191/250, train loss 0.198994 in 0.50s\n",
      " [-] epoch  192/250, train loss 0.209611 in 0.48s\n",
      " [-] epoch  193/250, train loss 0.202554 in 0.48s\n",
      " [-] epoch  194/250, train loss 0.218512 in 0.49s\n",
      " [-] epoch  195/250, train loss 0.216133 in 0.49s\n",
      " [-] epoch  196/250, train loss 0.208885 in 0.50s\n",
      " [-] epoch  197/250, train loss 0.216077 in 0.46s\n",
      " [-] epoch  198/250, train loss 0.209834 in 0.47s\n",
      " [-] epoch  199/250, train loss 0.204073 in 0.47s\n",
      " [-] epoch  200/250, train loss 0.190042 in 0.48s\n",
      " [-] epoch  201/250, train loss 0.215396 in 0.50s\n",
      " [-] epoch  202/250, train loss 0.216460 in 0.49s\n",
      " [-] epoch  203/250, train loss 0.206644 in 0.48s\n",
      " [-] epoch  204/250, train loss 0.212148 in 0.49s\n",
      " [-] epoch  205/250, train loss 0.199014 in 0.47s\n",
      " [-] epoch  206/250, train loss 0.200953 in 0.48s\n",
      " [-] epoch  207/250, train loss 0.204239 in 0.49s\n",
      " [-] epoch  208/250, train loss 0.193073 in 0.49s\n",
      " [-] epoch  209/250, train loss 0.205402 in 0.47s\n",
      " [-] epoch  210/250, train loss 0.191655 in 0.49s\n",
      " [-] epoch  211/250, train loss 0.187047 in 0.46s\n",
      " [-] epoch  212/250, train loss 0.199553 in 0.49s\n",
      " [-] epoch  213/250, train loss 0.193877 in 0.50s\n",
      " [-] epoch  214/250, train loss 0.189429 in 0.47s\n",
      " [-] epoch  215/250, train loss 0.196355 in 0.49s\n",
      " [-] epoch  216/250, train loss 0.202303 in 0.47s\n",
      " [-] epoch  217/250, train loss 0.194568 in 0.49s\n",
      " [-] epoch  218/250, train loss 0.197684 in 0.49s\n",
      " [-] epoch  219/250, train loss 0.195988 in 0.47s\n",
      " [-] epoch  220/250, train loss 0.182194 in 0.48s\n",
      " [-] epoch  221/250, train loss 0.204256 in 0.49s\n",
      " [-] epoch  222/250, train loss 0.198072 in 0.47s\n",
      " [-] epoch  223/250, train loss 0.211821 in 0.51s\n",
      " [-] epoch  224/250, train loss 0.179679 in 0.50s\n",
      " [-] epoch  225/250, train loss 0.179910 in 0.49s\n",
      " [-] epoch  226/250, train loss 0.188993 in 0.49s\n",
      " [-] epoch  227/250, train loss 0.201424 in 0.49s\n",
      " [-] epoch  228/250, train loss 0.217801 in 0.48s\n",
      " [-] epoch  229/250, train loss 0.201064 in 0.47s\n",
      " [-] epoch  230/250, train loss 0.201355 in 0.46s\n",
      " [-] epoch  231/250, train loss 0.191063 in 0.46s\n",
      " [-] epoch  232/250, train loss 0.191830 in 0.48s\n",
      " [-] epoch  233/250, train loss 0.181575 in 0.48s\n",
      " [-] epoch  234/250, train loss 0.190550 in 0.49s\n",
      " [-] epoch  235/250, train loss 0.188232 in 0.51s\n",
      " [-] epoch  236/250, train loss 0.218292 in 0.49s\n",
      " [-] epoch  237/250, train loss 0.214141 in 0.50s\n",
      " [-] epoch  238/250, train loss 0.200774 in 0.49s\n",
      " [-] epoch  239/250, train loss 0.195989 in 0.50s\n",
      " [-] epoch  240/250, train loss 0.201526 in 0.48s\n",
      " [-] epoch  241/250, train loss 0.177497 in 0.49s\n",
      " [-] epoch  242/250, train loss 0.191480 in 0.44s\n",
      " [-] epoch  243/250, train loss 0.200789 in 0.48s\n",
      " [-] epoch  244/250, train loss 0.203483 in 0.47s\n",
      " [-] epoch  245/250, train loss 0.208318 in 0.46s\n",
      " [-] epoch  246/250, train loss 0.202241 in 0.49s\n",
      " [-] epoch  247/250, train loss 0.199686 in 0.47s\n",
      " [-] epoch  248/250, train loss 0.179736 in 0.49s\n",
      " [-] epoch  249/250, train loss 0.201283 in 0.50s\n",
      " [-] epoch  250/250, train loss 0.199451 in 0.51s\n",
      " [-] test acc. 81.388889%\n",
      "Je vais utiliser 8 layers\n",
      " [-] epoch    1/250, train loss 0.616213 in 0.55s\n",
      " [-] epoch    2/250, train loss 0.483413 in 0.55s\n",
      " [-] epoch    3/250, train loss 0.408916 in 0.58s\n",
      " [-] epoch    4/250, train loss 0.407015 in 0.59s\n",
      " [-] epoch    5/250, train loss 0.396657 in 0.57s\n",
      " [-] epoch    6/250, train loss 0.366113 in 0.56s\n",
      " [-] epoch    7/250, train loss 0.383451 in 0.56s\n",
      " [-] epoch    8/250, train loss 0.376036 in 0.58s\n",
      " [-] epoch    9/250, train loss 0.340128 in 0.57s\n",
      " [-] epoch   10/250, train loss 0.335437 in 0.59s\n",
      " [-] epoch   11/250, train loss 0.354910 in 0.57s\n",
      " [-] epoch   12/250, train loss 0.315041 in 0.56s\n",
      " [-] epoch   13/250, train loss 0.364426 in 0.58s\n",
      " [-] epoch   14/250, train loss 0.344044 in 0.57s\n",
      " [-] epoch   15/250, train loss 0.341144 in 0.58s\n",
      " [-] epoch   16/250, train loss 0.338323 in 0.60s\n",
      " [-] epoch   17/250, train loss 0.292721 in 0.58s\n",
      " [-] epoch   18/250, train loss 0.328544 in 0.57s\n",
      " [-] epoch   19/250, train loss 0.342875 in 0.58s\n",
      " [-] epoch   20/250, train loss 0.300572 in 0.55s\n",
      " [-] epoch   21/250, train loss 0.301667 in 0.57s\n",
      " [-] epoch   22/250, train loss 0.320062 in 0.59s\n",
      " [-] epoch   23/250, train loss 0.337105 in 0.57s\n",
      " [-] epoch   24/250, train loss 0.310677 in 0.56s\n",
      " [-] epoch   25/250, train loss 0.295013 in 0.59s\n",
      " [-] epoch   26/250, train loss 0.295951 in 0.59s\n",
      " [-] epoch   27/250, train loss 0.312645 in 0.58s\n",
      " [-] epoch   28/250, train loss 0.323268 in 0.57s\n",
      " [-] epoch   29/250, train loss 0.314625 in 0.58s\n",
      " [-] epoch   30/250, train loss 0.280283 in 0.57s\n",
      " [-] epoch   31/250, train loss 0.333539 in 0.58s\n",
      " [-] epoch   32/250, train loss 0.289253 in 0.58s\n",
      " [-] epoch   33/250, train loss 0.290666 in 0.57s\n",
      " [-] epoch   34/250, train loss 0.299020 in 0.55s\n",
      " [-] epoch   35/250, train loss 0.300239 in 0.57s\n",
      " [-] epoch   36/250, train loss 0.284712 in 0.56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   37/250, train loss 0.299779 in 0.61s\n",
      " [-] epoch   38/250, train loss 0.291516 in 0.56s\n",
      " [-] epoch   39/250, train loss 0.269598 in 0.57s\n",
      " [-] epoch   40/250, train loss 0.314528 in 0.55s\n",
      " [-] epoch   41/250, train loss 0.321698 in 0.58s\n",
      " [-] epoch   42/250, train loss 0.282191 in 0.56s\n",
      " [-] epoch   43/250, train loss 0.273541 in 0.58s\n",
      " [-] epoch   44/250, train loss 0.266670 in 0.55s\n",
      " [-] epoch   45/250, train loss 0.286236 in 0.59s\n",
      " [-] epoch   46/250, train loss 0.299021 in 0.60s\n",
      " [-] epoch   47/250, train loss 0.259978 in 0.59s\n",
      " [-] epoch   48/250, train loss 0.316587 in 0.54s\n",
      " [-] epoch   49/250, train loss 0.266233 in 0.56s\n",
      " [-] epoch   50/250, train loss 0.290170 in 0.59s\n",
      " [-] epoch   51/250, train loss 0.277379 in 0.57s\n",
      " [-] epoch   52/250, train loss 0.270742 in 0.58s\n",
      " [-] epoch   53/250, train loss 0.263643 in 0.55s\n",
      " [-] epoch   54/250, train loss 0.272798 in 0.58s\n",
      " [-] epoch   55/250, train loss 0.276323 in 0.57s\n",
      " [-] epoch   56/250, train loss 0.266972 in 0.58s\n",
      " [-] epoch   57/250, train loss 0.252322 in 0.58s\n",
      " [-] epoch   58/250, train loss 0.294091 in 0.57s\n",
      " [-] epoch   59/250, train loss 0.279456 in 0.60s\n",
      " [-] epoch   60/250, train loss 0.254924 in 0.56s\n",
      " [-] epoch   61/250, train loss 0.262692 in 0.61s\n",
      " [-] epoch   62/250, train loss 0.270726 in 0.62s\n",
      " [-] epoch   63/250, train loss 0.245592 in 0.57s\n",
      " [-] epoch   64/250, train loss 0.269077 in 0.59s\n",
      " [-] epoch   65/250, train loss 0.250774 in 0.60s\n",
      " [-] epoch   66/250, train loss 0.255796 in 0.57s\n",
      " [-] epoch   67/250, train loss 0.278130 in 0.59s\n",
      " [-] epoch   68/250, train loss 0.274250 in 0.57s\n",
      " [-] epoch   69/250, train loss 0.255594 in 0.59s\n",
      " [-] epoch   70/250, train loss 0.248773 in 0.57s\n",
      " [-] epoch   71/250, train loss 0.247502 in 0.56s\n",
      " [-] epoch   72/250, train loss 0.250067 in 0.58s\n",
      " [-] epoch   73/250, train loss 0.239165 in 0.58s\n",
      " [-] epoch   74/250, train loss 0.279295 in 0.57s\n",
      " [-] epoch   75/250, train loss 0.236805 in 0.56s\n",
      " [-] epoch   76/250, train loss 0.272102 in 0.59s\n",
      " [-] epoch   77/250, train loss 0.263479 in 0.55s\n",
      " [-] epoch   78/250, train loss 0.260750 in 0.58s\n",
      " [-] epoch   79/250, train loss 0.271498 in 0.60s\n",
      " [-] epoch   80/250, train loss 0.272093 in 0.58s\n",
      " [-] epoch   81/250, train loss 0.230526 in 0.55s\n",
      " [-] epoch   82/250, train loss 0.249283 in 0.57s\n",
      " [-] epoch   83/250, train loss 0.268851 in 0.56s\n",
      " [-] epoch   84/250, train loss 0.267701 in 0.59s\n",
      " [-] epoch   85/250, train loss 0.242505 in 0.61s\n",
      " [-] epoch   86/250, train loss 0.238967 in 0.56s\n",
      " [-] epoch   87/250, train loss 0.251073 in 0.58s\n",
      " [-] epoch   88/250, train loss 0.270773 in 0.56s\n",
      " [-] epoch   89/250, train loss 0.254323 in 0.59s\n",
      " [-] epoch   90/250, train loss 0.241496 in 0.57s\n",
      " [-] epoch   91/250, train loss 0.232942 in 0.56s\n",
      " [-] epoch   92/250, train loss 0.271167 in 0.59s\n",
      " [-] epoch   93/250, train loss 0.220678 in 0.60s\n",
      " [-] epoch   94/250, train loss 0.230052 in 0.58s\n",
      " [-] epoch   95/250, train loss 0.230887 in 0.58s\n",
      " [-] epoch   96/250, train loss 0.267939 in 0.59s\n",
      " [-] epoch   97/250, train loss 0.248116 in 0.58s\n",
      " [-] epoch   98/250, train loss 0.259179 in 0.58s\n",
      " [-] epoch   99/250, train loss 0.249922 in 0.56s\n",
      " [-] epoch  100/250, train loss 0.235528 in 0.57s\n",
      " [-] epoch  101/250, train loss 0.227025 in 0.56s\n",
      " [-] epoch  102/250, train loss 0.234813 in 0.58s\n",
      " [-] epoch  103/250, train loss 0.254326 in 0.58s\n",
      " [-] epoch  104/250, train loss 0.244222 in 0.62s\n",
      " [-] epoch  105/250, train loss 0.234947 in 0.59s\n",
      " [-] epoch  106/250, train loss 0.216742 in 0.59s\n",
      " [-] epoch  107/250, train loss 0.226034 in 0.58s\n",
      " [-] epoch  108/250, train loss 0.251198 in 0.61s\n",
      " [-] epoch  109/250, train loss 0.261218 in 0.60s\n",
      " [-] epoch  110/250, train loss 0.250654 in 0.60s\n",
      " [-] epoch  111/250, train loss 0.252785 in 0.59s\n",
      " [-] epoch  112/250, train loss 0.245189 in 0.58s\n",
      " [-] epoch  113/250, train loss 0.223775 in 0.57s\n",
      " [-] epoch  114/250, train loss 0.224828 in 0.57s\n",
      " [-] epoch  115/250, train loss 0.277755 in 0.58s\n",
      " [-] epoch  116/250, train loss 0.240072 in 0.60s\n",
      " [-] epoch  117/250, train loss 0.240581 in 0.58s\n",
      " [-] epoch  118/250, train loss 0.224391 in 0.58s\n",
      " [-] epoch  119/250, train loss 0.214005 in 0.56s\n",
      " [-] epoch  120/250, train loss 0.225255 in 0.55s\n",
      " [-] epoch  121/250, train loss 0.245933 in 0.57s\n",
      " [-] epoch  122/250, train loss 0.254401 in 0.58s\n",
      " [-] epoch  123/250, train loss 0.236581 in 0.56s\n",
      " [-] epoch  124/250, train loss 0.220929 in 0.59s\n",
      " [-] epoch  125/250, train loss 0.257823 in 0.58s\n",
      " [-] epoch  126/250, train loss 0.242898 in 0.61s\n",
      " [-] epoch  127/250, train loss 0.242448 in 0.56s\n",
      " [-] epoch  128/250, train loss 0.230927 in 0.58s\n",
      " [-] epoch  129/250, train loss 0.219465 in 0.56s\n",
      " [-] epoch  130/250, train loss 0.246433 in 0.57s\n",
      " [-] epoch  131/250, train loss 0.228713 in 0.58s\n",
      " [-] epoch  132/250, train loss 0.226956 in 0.59s\n",
      " [-] epoch  133/250, train loss 0.226488 in 0.59s\n",
      " [-] epoch  134/250, train loss 0.226709 in 0.57s\n",
      " [-] epoch  135/250, train loss 0.250415 in 0.59s\n",
      " [-] epoch  136/250, train loss 0.229784 in 0.56s\n",
      " [-] epoch  137/250, train loss 0.223749 in 0.58s\n",
      " [-] epoch  138/250, train loss 0.215539 in 0.58s\n",
      " [-] epoch  139/250, train loss 0.258058 in 0.59s\n",
      " [-] epoch  140/250, train loss 0.219164 in 0.56s\n",
      " [-] epoch  141/250, train loss 0.220361 in 0.55s\n",
      " [-] epoch  142/250, train loss 0.207597 in 0.56s\n",
      " [-] epoch  143/250, train loss 0.214276 in 0.60s\n",
      " [-] epoch  144/250, train loss 0.218826 in 0.58s\n",
      " [-] epoch  145/250, train loss 0.224957 in 0.60s\n",
      " [-] epoch  146/250, train loss 0.198736 in 0.61s\n",
      " [-] epoch  147/250, train loss 0.208145 in 0.59s\n",
      " [-] epoch  148/250, train loss 0.227875 in 1.40s\n",
      " [-] epoch  149/250, train loss 0.224663 in 0.57s\n",
      " [-] epoch  150/250, train loss 0.223615 in 0.64s\n",
      " [-] epoch  151/250, train loss 0.231538 in 0.61s\n",
      " [-] epoch  152/250, train loss 0.224113 in 0.64s\n",
      " [-] epoch  153/250, train loss 0.238493 in 0.63s\n",
      " [-] epoch  154/250, train loss 0.229699 in 0.60s\n",
      " [-] epoch  155/250, train loss 0.217996 in 0.58s\n",
      " [-] epoch  156/250, train loss 0.224237 in 0.59s\n",
      " [-] epoch  157/250, train loss 0.216829 in 0.61s\n",
      " [-] epoch  158/250, train loss 0.208842 in 0.61s\n",
      " [-] epoch  159/250, train loss 0.217542 in 0.62s\n",
      " [-] epoch  160/250, train loss 0.224661 in 0.60s\n",
      " [-] epoch  161/250, train loss 0.209661 in 0.63s\n",
      " [-] epoch  162/250, train loss 0.215434 in 0.60s\n",
      " [-] epoch  163/250, train loss 0.226331 in 0.62s\n",
      " [-] epoch  164/250, train loss 0.227830 in 0.63s\n",
      " [-] epoch  165/250, train loss 0.222442 in 0.60s\n",
      " [-] epoch  166/250, train loss 0.215356 in 0.60s\n",
      " [-] epoch  167/250, train loss 0.249449 in 0.58s\n",
      " [-] epoch  168/250, train loss 0.219558 in 0.59s\n",
      " [-] epoch  169/250, train loss 0.210925 in 0.62s\n",
      " [-] epoch  170/250, train loss 0.223666 in 0.58s\n",
      " [-] epoch  171/250, train loss 0.217438 in 0.57s\n",
      " [-] epoch  172/250, train loss 0.221340 in 0.63s\n",
      " [-] epoch  173/250, train loss 0.218945 in 0.62s\n",
      " [-] epoch  174/250, train loss 0.211500 in 0.59s\n",
      " [-] epoch  175/250, train loss 0.209791 in 0.57s\n",
      " [-] epoch  176/250, train loss 0.237698 in 0.56s\n",
      " [-] epoch  177/250, train loss 0.214638 in 0.57s\n",
      " [-] epoch  178/250, train loss 0.212401 in 0.57s\n",
      " [-] epoch  179/250, train loss 0.210982 in 0.57s\n",
      " [-] epoch  180/250, train loss 0.231963 in 0.60s\n",
      " [-] epoch  181/250, train loss 0.222586 in 0.59s\n",
      " [-] epoch  182/250, train loss 0.207480 in 0.58s\n",
      " [-] epoch  183/250, train loss 0.208328 in 0.57s\n",
      " [-] epoch  184/250, train loss 0.228910 in 0.57s\n",
      " [-] epoch  185/250, train loss 0.218615 in 0.55s\n",
      " [-] epoch  186/250, train loss 0.202587 in 0.55s\n",
      " [-] epoch  187/250, train loss 0.235842 in 0.58s\n",
      " [-] epoch  188/250, train loss 0.203604 in 0.57s\n",
      " [-] epoch  189/250, train loss 0.216416 in 0.55s\n",
      " [-] epoch  190/250, train loss 0.207159 in 0.56s\n",
      " [-] epoch  191/250, train loss 0.213399 in 0.57s\n",
      " [-] epoch  192/250, train loss 0.198932 in 0.57s\n",
      " [-] epoch  193/250, train loss 0.207996 in 0.57s\n",
      " [-] epoch  194/250, train loss 0.221641 in 0.58s\n",
      " [-] epoch  195/250, train loss 0.217739 in 0.55s\n",
      " [-] epoch  196/250, train loss 0.215844 in 0.58s\n",
      " [-] epoch  197/250, train loss 0.226283 in 0.60s\n",
      " [-] epoch  198/250, train loss 0.214398 in 0.56s\n",
      " [-] epoch  199/250, train loss 0.198907 in 0.57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  200/250, train loss 0.214673 in 0.58s\n",
      " [-] epoch  201/250, train loss 0.214800 in 0.58s\n",
      " [-] epoch  202/250, train loss 0.208075 in 0.59s\n",
      " [-] epoch  203/250, train loss 0.226570 in 0.58s\n",
      " [-] epoch  204/250, train loss 0.221069 in 0.61s\n",
      " [-] epoch  205/250, train loss 0.213942 in 0.59s\n",
      " [-] epoch  206/250, train loss 0.215976 in 0.60s\n",
      " [-] epoch  207/250, train loss 0.208525 in 0.61s\n",
      " [-] epoch  208/250, train loss 0.237495 in 0.59s\n",
      " [-] epoch  209/250, train loss 0.211913 in 0.57s\n",
      " [-] epoch  210/250, train loss 0.239292 in 0.59s\n",
      " [-] epoch  211/250, train loss 0.200678 in 0.59s\n",
      " [-] epoch  212/250, train loss 0.224072 in 0.59s\n",
      " [-] epoch  213/250, train loss 0.227767 in 0.60s\n",
      " [-] epoch  214/250, train loss 0.202910 in 0.56s\n",
      " [-] epoch  215/250, train loss 0.209129 in 0.61s\n",
      " [-] epoch  216/250, train loss 0.206841 in 0.58s\n",
      " [-] epoch  217/250, train loss 0.210480 in 0.59s\n",
      " [-] epoch  218/250, train loss 0.203098 in 0.57s\n",
      " [-] epoch  219/250, train loss 0.192173 in 0.59s\n",
      " [-] epoch  220/250, train loss 0.217930 in 0.59s\n",
      " [-] epoch  221/250, train loss 0.229103 in 0.57s\n",
      " [-] epoch  222/250, train loss 0.215128 in 0.58s\n",
      " [-] epoch  223/250, train loss 0.196934 in 0.58s\n",
      " [-] epoch  224/250, train loss 0.197197 in 0.54s\n",
      " [-] epoch  225/250, train loss 0.203478 in 0.58s\n",
      " [-] epoch  226/250, train loss 0.194193 in 0.61s\n",
      " [-] epoch  227/250, train loss 0.214508 in 0.57s\n",
      " [-] epoch  228/250, train loss 0.198409 in 0.58s\n",
      " [-] epoch  229/250, train loss 0.202089 in 0.57s\n",
      " [-] epoch  230/250, train loss 0.228223 in 0.59s\n",
      " [-] epoch  231/250, train loss 0.203344 in 0.60s\n",
      " [-] epoch  232/250, train loss 0.208016 in 0.57s\n",
      " [-] epoch  233/250, train loss 0.205473 in 0.57s\n",
      " [-] epoch  234/250, train loss 0.217558 in 0.58s\n",
      " [-] epoch  235/250, train loss 0.228977 in 0.58s\n",
      " [-] epoch  236/250, train loss 0.186410 in 0.61s\n",
      " [-] epoch  237/250, train loss 0.186610 in 0.61s\n",
      " [-] epoch  238/250, train loss 0.215958 in 0.57s\n",
      " [-] epoch  239/250, train loss 0.203208 in 0.58s\n",
      " [-] epoch  240/250, train loss 0.198448 in 0.57s\n",
      " [-] epoch  241/250, train loss 0.215309 in 0.59s\n",
      " [-] epoch  242/250, train loss 0.197670 in 0.59s\n",
      " [-] epoch  243/250, train loss 0.189917 in 0.57s\n",
      " [-] epoch  244/250, train loss 0.211341 in 0.61s\n",
      " [-] epoch  245/250, train loss 0.204141 in 0.60s\n",
      " [-] epoch  246/250, train loss 0.211105 in 0.58s\n",
      " [-] epoch  247/250, train loss 0.219167 in 0.55s\n",
      " [-] epoch  248/250, train loss 0.214459 in 0.57s\n",
      " [-] epoch  249/250, train loss 0.213074 in 0.61s\n",
      " [-] epoch  250/250, train loss 0.227424 in 0.61s\n",
      " [-] test acc. 82.777778%\n",
      "Je vais utiliser 9 layers\n",
      " [-] epoch    1/250, train loss 0.546251 in 0.64s\n",
      " [-] epoch    2/250, train loss 0.459414 in 0.66s\n",
      " [-] epoch    3/250, train loss 0.424939 in 0.65s\n",
      " [-] epoch    4/250, train loss 0.417108 in 0.63s\n",
      " [-] epoch    5/250, train loss 0.414335 in 0.67s\n",
      " [-] epoch    6/250, train loss 0.397302 in 0.66s\n",
      " [-] epoch    7/250, train loss 0.348281 in 0.65s\n",
      " [-] epoch    8/250, train loss 0.379900 in 0.65s\n",
      " [-] epoch    9/250, train loss 0.372328 in 0.65s\n",
      " [-] epoch   10/250, train loss 0.374787 in 0.66s\n",
      " [-] epoch   11/250, train loss 0.374740 in 0.66s\n",
      " [-] epoch   12/250, train loss 0.342352 in 0.65s\n",
      " [-] epoch   13/250, train loss 0.328286 in 0.64s\n",
      " [-] epoch   14/250, train loss 0.328217 in 0.67s\n",
      " [-] epoch   15/250, train loss 0.311887 in 0.66s\n",
      " [-] epoch   16/250, train loss 0.342111 in 0.63s\n",
      " [-] epoch   17/250, train loss 0.338922 in 0.67s\n",
      " [-] epoch   18/250, train loss 0.349616 in 0.64s\n",
      " [-] epoch   19/250, train loss 0.319776 in 0.64s\n",
      " [-] epoch   20/250, train loss 0.325586 in 0.64s\n",
      " [-] epoch   21/250, train loss 0.310156 in 0.65s\n",
      " [-] epoch   22/250, train loss 0.333342 in 0.65s\n",
      " [-] epoch   23/250, train loss 0.330995 in 0.65s\n",
      " [-] epoch   24/250, train loss 0.307687 in 0.65s\n",
      " [-] epoch   25/250, train loss 0.298159 in 0.65s\n",
      " [-] epoch   26/250, train loss 0.299943 in 0.65s\n",
      " [-] epoch   27/250, train loss 0.310173 in 0.65s\n",
      " [-] epoch   28/250, train loss 0.305567 in 0.65s\n",
      " [-] epoch   29/250, train loss 0.311666 in 0.63s\n",
      " [-] epoch   30/250, train loss 0.305794 in 0.66s\n",
      " [-] epoch   31/250, train loss 0.301834 in 0.65s\n",
      " [-] epoch   32/250, train loss 0.305430 in 0.64s\n",
      " [-] epoch   33/250, train loss 0.294749 in 0.63s\n",
      " [-] epoch   34/250, train loss 0.313157 in 0.64s\n",
      " [-] epoch   35/250, train loss 0.297973 in 0.64s\n",
      " [-] epoch   36/250, train loss 0.271468 in 0.65s\n",
      " [-] epoch   37/250, train loss 0.265160 in 0.66s\n",
      " [-] epoch   38/250, train loss 0.264115 in 0.64s\n",
      " [-] epoch   39/250, train loss 0.267407 in 0.63s\n",
      " [-] epoch   40/250, train loss 0.294956 in 0.64s\n",
      " [-] epoch   41/250, train loss 0.293474 in 0.63s\n",
      " [-] epoch   42/250, train loss 0.285384 in 0.64s\n",
      " [-] epoch   43/250, train loss 0.273016 in 0.62s\n",
      " [-] epoch   44/250, train loss 0.282820 in 0.65s\n",
      " [-] epoch   45/250, train loss 0.286726 in 0.65s\n",
      " [-] epoch   46/250, train loss 0.284823 in 0.65s\n",
      " [-] epoch   47/250, train loss 0.255793 in 0.62s\n",
      " [-] epoch   48/250, train loss 0.284713 in 0.65s\n",
      " [-] epoch   49/250, train loss 0.268988 in 0.66s\n",
      " [-] epoch   50/250, train loss 0.258425 in 0.63s\n",
      " [-] epoch   51/250, train loss 0.283257 in 0.65s\n",
      " [-] epoch   52/250, train loss 0.271468 in 0.64s\n",
      " [-] epoch   53/250, train loss 0.259428 in 0.67s\n",
      " [-] epoch   54/250, train loss 0.279385 in 0.65s\n",
      " [-] epoch   55/250, train loss 0.272660 in 0.66s\n",
      " [-] epoch   56/250, train loss 0.261790 in 0.66s\n",
      " [-] epoch   57/250, train loss 0.291125 in 0.69s\n",
      " [-] epoch   58/250, train loss 0.280818 in 0.68s\n",
      " [-] epoch   59/250, train loss 0.263964 in 0.66s\n",
      " [-] epoch   60/250, train loss 0.275688 in 0.67s\n",
      " [-] epoch   61/250, train loss 0.255807 in 0.67s\n",
      " [-] epoch   62/250, train loss 0.258189 in 0.63s\n",
      " [-] epoch   63/250, train loss 0.263859 in 0.65s\n",
      " [-] epoch   64/250, train loss 0.295290 in 0.68s\n",
      " [-] epoch   65/250, train loss 0.273281 in 0.66s\n",
      " [-] epoch   66/250, train loss 0.249287 in 0.66s\n",
      " [-] epoch   67/250, train loss 0.248453 in 0.66s\n",
      " [-] epoch   68/250, train loss 0.267408 in 0.66s\n",
      " [-] epoch   69/250, train loss 0.260972 in 0.64s\n",
      " [-] epoch   70/250, train loss 0.272227 in 0.69s\n",
      " [-] epoch   71/250, train loss 0.273932 in 0.67s\n",
      " [-] epoch   72/250, train loss 0.280199 in 0.66s\n",
      " [-] epoch   73/250, train loss 0.251397 in 0.64s\n",
      " [-] epoch   74/250, train loss 0.254109 in 0.66s\n",
      " [-] epoch   75/250, train loss 0.263793 in 0.67s\n",
      " [-] epoch   76/250, train loss 0.243663 in 0.63s\n",
      " [-] epoch   77/250, train loss 0.277482 in 0.67s\n",
      " [-] epoch   78/250, train loss 0.254269 in 0.66s\n",
      " [-] epoch   79/250, train loss 0.229119 in 0.67s\n",
      " [-] epoch   80/250, train loss 0.241030 in 0.62s\n",
      " [-] epoch   81/250, train loss 0.232231 in 0.64s\n",
      " [-] epoch   82/250, train loss 0.249952 in 0.68s\n",
      " [-] epoch   83/250, train loss 0.255646 in 0.68s\n",
      " [-] epoch   84/250, train loss 0.266183 in 0.65s\n",
      " [-] epoch   85/250, train loss 0.254743 in 0.65s\n",
      " [-] epoch   86/250, train loss 0.261809 in 0.64s\n",
      " [-] epoch   87/250, train loss 0.233894 in 0.63s\n",
      " [-] epoch   88/250, train loss 0.234176 in 0.66s\n",
      " [-] epoch   89/250, train loss 0.234443 in 0.66s\n",
      " [-] epoch   90/250, train loss 0.249661 in 0.65s\n",
      " [-] epoch   91/250, train loss 0.248524 in 0.65s\n",
      " [-] epoch   92/250, train loss 0.214799 in 0.65s\n",
      " [-] epoch   93/250, train loss 0.246609 in 0.68s\n",
      " [-] epoch   94/250, train loss 0.235516 in 0.68s\n",
      " [-] epoch   95/250, train loss 0.245852 in 0.66s\n",
      " [-] epoch   96/250, train loss 0.239470 in 0.64s\n",
      " [-] epoch   97/250, train loss 0.242883 in 0.68s\n",
      " [-] epoch   98/250, train loss 0.248428 in 0.68s\n",
      " [-] epoch   99/250, train loss 0.239207 in 0.65s\n",
      " [-] epoch  100/250, train loss 0.236659 in 0.64s\n",
      " [-] epoch  101/250, train loss 0.235656 in 0.66s\n",
      " [-] epoch  102/250, train loss 0.234036 in 0.65s\n",
      " [-] epoch  103/250, train loss 0.225030 in 0.65s\n",
      " [-] epoch  104/250, train loss 0.222554 in 0.63s\n",
      " [-] epoch  105/250, train loss 0.238939 in 0.62s\n",
      " [-] epoch  106/250, train loss 0.270166 in 0.67s\n",
      " [-] epoch  107/250, train loss 0.250537 in 0.66s\n",
      " [-] epoch  108/250, train loss 0.247351 in 0.68s\n",
      " [-] epoch  109/250, train loss 0.226334 in 0.68s\n",
      " [-] epoch  110/250, train loss 0.225956 in 0.67s\n",
      " [-] epoch  111/250, train loss 0.208791 in 0.65s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  112/250, train loss 0.219947 in 0.67s\n",
      " [-] epoch  113/250, train loss 0.222286 in 0.63s\n",
      " [-] epoch  114/250, train loss 0.217177 in 0.64s\n",
      " [-] epoch  115/250, train loss 0.250263 in 0.66s\n",
      " [-] epoch  116/250, train loss 0.261881 in 0.65s\n",
      " [-] epoch  117/250, train loss 0.244741 in 0.65s\n",
      " [-] epoch  118/250, train loss 0.227767 in 0.64s\n",
      " [-] epoch  119/250, train loss 0.229019 in 0.65s\n",
      " [-] epoch  120/250, train loss 0.229182 in 0.66s\n",
      " [-] epoch  121/250, train loss 0.221093 in 0.64s\n",
      " [-] epoch  122/250, train loss 0.212851 in 0.68s\n",
      " [-] epoch  123/250, train loss 0.224749 in 0.66s\n",
      " [-] epoch  124/250, train loss 0.218725 in 0.66s\n",
      " [-] epoch  125/250, train loss 0.219165 in 0.66s\n",
      " [-] epoch  126/250, train loss 0.222087 in 0.63s\n",
      " [-] epoch  127/250, train loss 0.242980 in 0.66s\n",
      " [-] epoch  128/250, train loss 0.236165 in 0.65s\n",
      " [-] epoch  129/250, train loss 0.254408 in 0.67s\n",
      " [-] epoch  130/250, train loss 0.221239 in 0.65s\n",
      " [-] epoch  131/250, train loss 0.244963 in 0.66s\n",
      " [-] epoch  132/250, train loss 0.225788 in 0.66s\n",
      " [-] epoch  133/250, train loss 0.254189 in 0.67s\n",
      " [-] epoch  134/250, train loss 0.221333 in 0.65s\n",
      " [-] epoch  135/250, train loss 0.223956 in 0.67s\n",
      " [-] epoch  136/250, train loss 0.214772 in 0.67s\n",
      " [-] epoch  137/250, train loss 0.218113 in 0.65s\n",
      " [-] epoch  138/250, train loss 0.212121 in 0.67s\n",
      " [-] epoch  139/250, train loss 0.218068 in 0.65s\n",
      " [-] epoch  140/250, train loss 0.229265 in 0.65s\n",
      " [-] epoch  141/250, train loss 0.227914 in 0.61s\n",
      " [-] epoch  142/250, train loss 0.214215 in 0.67s\n",
      " [-] epoch  143/250, train loss 0.217437 in 0.67s\n",
      " [-] epoch  144/250, train loss 0.205202 in 0.65s\n",
      " [-] epoch  145/250, train loss 0.216867 in 0.63s\n",
      " [-] epoch  146/250, train loss 0.207274 in 0.64s\n",
      " [-] epoch  147/250, train loss 0.218029 in 0.66s\n",
      " [-] epoch  148/250, train loss 0.234817 in 0.64s\n",
      " [-] epoch  149/250, train loss 0.228869 in 0.67s\n",
      " [-] epoch  150/250, train loss 0.217003 in 0.65s\n",
      " [-] epoch  151/250, train loss 0.225089 in 0.63s\n",
      " [-] epoch  152/250, train loss 0.192140 in 0.64s\n",
      " [-] epoch  153/250, train loss 0.242755 in 0.66s\n",
      " [-] epoch  154/250, train loss 0.230665 in 0.68s\n",
      " [-] epoch  155/250, train loss 0.209902 in 0.64s\n",
      " [-] epoch  156/250, train loss 0.223983 in 0.66s\n",
      " [-] epoch  157/250, train loss 0.221927 in 0.66s\n",
      " [-] epoch  158/250, train loss 0.211966 in 0.66s\n",
      " [-] epoch  159/250, train loss 0.229507 in 0.66s\n",
      " [-] epoch  160/250, train loss 0.230010 in 0.66s\n",
      " [-] epoch  161/250, train loss 0.221764 in 0.67s\n",
      " [-] epoch  162/250, train loss 0.238751 in 0.66s\n",
      " [-] epoch  163/250, train loss 0.214824 in 0.67s\n",
      " [-] epoch  164/250, train loss 0.224800 in 0.68s\n",
      " [-] epoch  165/250, train loss 0.210375 in 0.67s\n",
      " [-] epoch  166/250, train loss 0.212587 in 0.66s\n",
      " [-] epoch  167/250, train loss 0.221106 in 0.65s\n",
      " [-] epoch  168/250, train loss 0.199001 in 0.66s\n",
      " [-] epoch  169/250, train loss 0.206457 in 0.69s\n",
      " [-] epoch  170/250, train loss 0.192192 in 0.65s\n",
      " [-] epoch  171/250, train loss 0.212904 in 0.65s\n",
      " [-] epoch  172/250, train loss 0.204371 in 0.66s\n",
      " [-] epoch  173/250, train loss 0.222085 in 0.64s\n",
      " [-] epoch  174/250, train loss 0.216415 in 0.66s\n",
      " [-] epoch  175/250, train loss 0.206479 in 0.65s\n",
      " [-] epoch  176/250, train loss 0.208424 in 0.65s\n",
      " [-] epoch  177/250, train loss 0.202903 in 0.66s\n",
      " [-] epoch  178/250, train loss 0.216646 in 0.66s\n",
      " [-] epoch  179/250, train loss 0.205825 in 0.65s\n",
      " [-] epoch  180/250, train loss 0.229041 in 0.67s\n",
      " [-] epoch  181/250, train loss 0.193595 in 0.63s\n",
      " [-] epoch  182/250, train loss 0.211190 in 0.61s\n",
      " [-] epoch  183/250, train loss 0.234175 in 0.65s\n",
      " [-] epoch  184/250, train loss 0.203041 in 0.64s\n",
      " [-] epoch  185/250, train loss 0.207518 in 0.63s\n",
      " [-] epoch  186/250, train loss 0.205973 in 0.65s\n",
      " [-] epoch  187/250, train loss 0.210523 in 0.64s\n",
      " [-] epoch  188/250, train loss 0.191772 in 0.63s\n",
      " [-] epoch  189/250, train loss 0.202828 in 0.65s\n",
      " [-] epoch  190/250, train loss 0.188556 in 0.66s\n",
      " [-] epoch  191/250, train loss 0.204347 in 0.64s\n",
      " [-] epoch  192/250, train loss 0.203378 in 0.66s\n",
      " [-] epoch  193/250, train loss 0.198037 in 0.67s\n",
      " [-] epoch  194/250, train loss 0.212428 in 0.70s\n",
      " [-] epoch  195/250, train loss 0.214295 in 0.71s\n",
      " [-] epoch  196/250, train loss 0.208756 in 0.67s\n",
      " [-] epoch  197/250, train loss 0.200137 in 0.67s\n",
      " [-] epoch  198/250, train loss 0.229618 in 0.65s\n",
      " [-] epoch  199/250, train loss 0.207485 in 0.67s\n",
      " [-] epoch  200/250, train loss 0.229762 in 0.66s\n",
      " [-] epoch  201/250, train loss 0.199144 in 0.68s\n",
      " [-] epoch  202/250, train loss 0.221109 in 0.65s\n",
      " [-] epoch  203/250, train loss 0.224342 in 0.63s\n",
      " [-] epoch  204/250, train loss 0.198324 in 0.64s\n",
      " [-] epoch  205/250, train loss 0.202090 in 0.63s\n",
      " [-] epoch  206/250, train loss 0.209954 in 0.61s\n",
      " [-] epoch  207/250, train loss 0.219596 in 0.66s\n",
      " [-] epoch  208/250, train loss 0.207544 in 0.69s\n",
      " [-] epoch  209/250, train loss 0.211461 in 0.68s\n",
      " [-] epoch  210/250, train loss 0.204275 in 0.65s\n",
      " [-] epoch  211/250, train loss 0.203228 in 0.62s\n",
      " [-] epoch  212/250, train loss 0.203832 in 0.66s\n",
      " [-] epoch  213/250, train loss 0.191327 in 0.65s\n",
      " [-] epoch  214/250, train loss 0.194773 in 0.64s\n",
      " [-] epoch  215/250, train loss 0.185193 in 0.63s\n",
      " [-] epoch  216/250, train loss 0.204839 in 0.63s\n",
      " [-] epoch  217/250, train loss 0.216171 in 0.63s\n",
      " [-] epoch  218/250, train loss 0.223021 in 0.62s\n",
      " [-] epoch  219/250, train loss 0.207417 in 0.66s\n",
      " [-] epoch  220/250, train loss 0.196738 in 0.65s\n",
      " [-] epoch  221/250, train loss 0.201172 in 0.65s\n",
      " [-] epoch  222/250, train loss 0.210403 in 0.65s\n",
      " [-] epoch  223/250, train loss 0.201821 in 0.64s\n",
      " [-] epoch  224/250, train loss 0.200721 in 0.65s\n",
      " [-] epoch  225/250, train loss 0.212590 in 0.63s\n",
      " [-] epoch  226/250, train loss 0.215002 in 0.67s\n",
      " [-] epoch  227/250, train loss 0.210116 in 0.63s\n",
      " [-] epoch  228/250, train loss 0.215797 in 0.64s\n",
      " [-] epoch  229/250, train loss 0.211706 in 0.66s\n",
      " [-] epoch  230/250, train loss 0.205333 in 0.65s\n",
      " [-] epoch  231/250, train loss 0.229323 in 0.65s\n",
      " [-] epoch  232/250, train loss 0.191890 in 0.64s\n",
      " [-] epoch  233/250, train loss 0.204198 in 0.64s\n",
      " [-] epoch  234/250, train loss 0.193622 in 0.65s\n",
      " [-] epoch  235/250, train loss 0.202900 in 0.64s\n",
      " [-] epoch  236/250, train loss 0.203300 in 0.64s\n",
      " [-] epoch  237/250, train loss 0.226142 in 0.64s\n",
      " [-] epoch  238/250, train loss 0.211688 in 0.66s\n",
      " [-] epoch  239/250, train loss 0.202726 in 0.66s\n",
      " [-] epoch  240/250, train loss 0.199189 in 0.67s\n",
      " [-] epoch  241/250, train loss 0.193989 in 0.66s\n",
      " [-] epoch  242/250, train loss 0.208169 in 0.64s\n",
      " [-] epoch  243/250, train loss 0.208205 in 0.67s\n",
      " [-] epoch  244/250, train loss 0.197655 in 0.67s\n",
      " [-] epoch  245/250, train loss 0.215348 in 0.67s\n",
      " [-] epoch  246/250, train loss 0.215167 in 0.66s\n",
      " [-] epoch  247/250, train loss 0.214619 in 0.65s\n",
      " [-] epoch  248/250, train loss 0.192809 in 0.65s\n",
      " [-] epoch  249/250, train loss 0.195971 in 0.65s\n",
      " [-] epoch  250/250, train loss 0.191923 in 0.67s\n",
      " [-] test acc. 84.166667%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_linear_results(X_train, X_test) # all dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calcul du taux de réussite en classement d'un réseau linéaire multicouches sur les données réduites\n",
    "Cette deuxième étape consiste à avoir les résultats sur les dimensions réduites engendrées par la réduction de dimensionnalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.697702 in 0.02s\n",
      " [-] epoch    2/250, train loss 0.688243 in 0.02s\n",
      " [-] epoch    3/250, train loss 0.682683 in 0.02s\n",
      " [-] epoch    4/250, train loss 0.682285 in 0.02s\n",
      " [-] epoch    5/250, train loss 0.678832 in 0.02s\n",
      " [-] epoch    6/250, train loss 0.675675 in 0.02s\n",
      " [-] epoch    7/250, train loss 0.684544 in 0.02s\n",
      " [-] epoch    8/250, train loss 0.673033 in 0.02s\n",
      " [-] epoch    9/250, train loss 0.671916 in 0.03s\n",
      " [-] epoch   10/250, train loss 0.672226 in 0.02s\n",
      " [-] epoch   11/250, train loss 0.670583 in 0.02s\n",
      " [-] epoch   12/250, train loss 0.666326 in 0.02s\n",
      " [-] epoch   13/250, train loss 0.667649 in 0.02s\n",
      " [-] epoch   14/250, train loss 0.662418 in 0.02s\n",
      " [-] epoch   15/250, train loss 0.653127 in 0.02s\n",
      " [-] epoch   16/250, train loss 0.656079 in 0.02s\n",
      " [-] epoch   17/250, train loss 0.661458 in 0.02s\n",
      " [-] epoch   18/250, train loss 0.672926 in 0.02s\n",
      " [-] epoch   19/250, train loss 0.659211 in 0.02s\n",
      " [-] epoch   20/250, train loss 0.662108 in 0.02s\n",
      " [-] epoch   21/250, train loss 0.657357 in 0.02s\n",
      " [-] epoch   22/250, train loss 0.665424 in 0.02s\n",
      " [-] epoch   23/250, train loss 0.652172 in 0.02s\n",
      " [-] epoch   24/250, train loss 0.655005 in 0.02s\n",
      " [-] epoch   25/250, train loss 0.655103 in 0.02s\n",
      " [-] epoch   26/250, train loss 0.655179 in 0.02s\n",
      " [-] epoch   27/250, train loss 0.650348 in 0.02s\n",
      " [-] epoch   28/250, train loss 0.653154 in 0.02s\n",
      " [-] epoch   29/250, train loss 0.653612 in 0.02s\n",
      " [-] epoch   30/250, train loss 0.642553 in 0.02s\n",
      " [-] epoch   31/250, train loss 0.641150 in 0.02s\n",
      " [-] epoch   32/250, train loss 0.651485 in 0.02s\n",
      " [-] epoch   33/250, train loss 0.648176 in 0.02s\n",
      " [-] epoch   34/250, train loss 0.645878 in 0.02s\n",
      " [-] epoch   35/250, train loss 0.642970 in 0.02s\n",
      " [-] epoch   36/250, train loss 0.644507 in 0.02s\n",
      " [-] epoch   37/250, train loss 0.655370 in 0.02s\n",
      " [-] epoch   38/250, train loss 0.659727 in 0.02s\n",
      " [-] epoch   39/250, train loss 0.645076 in 0.02s\n",
      " [-] epoch   40/250, train loss 0.647120 in 0.02s\n",
      " [-] epoch   41/250, train loss 0.643982 in 0.02s\n",
      " [-] epoch   42/250, train loss 0.642512 in 0.02s\n",
      " [-] epoch   43/250, train loss 0.644743 in 0.02s\n",
      " [-] epoch   44/250, train loss 0.647643 in 0.02s\n",
      " [-] epoch   45/250, train loss 0.653962 in 0.02s\n",
      " [-] epoch   46/250, train loss 0.643269 in 0.02s\n",
      " [-] epoch   47/250, train loss 0.638945 in 0.02s\n",
      " [-] epoch   48/250, train loss 0.632724 in 0.02s\n",
      " [-] epoch   49/250, train loss 0.642291 in 0.02s\n",
      " [-] epoch   50/250, train loss 0.629301 in 0.02s\n",
      " [-] epoch   51/250, train loss 0.635942 in 0.02s\n",
      " [-] epoch   52/250, train loss 0.638753 in 0.02s\n",
      " [-] epoch   53/250, train loss 0.632411 in 0.02s\n",
      " [-] epoch   54/250, train loss 0.641163 in 0.02s\n",
      " [-] epoch   55/250, train loss 0.625262 in 0.02s\n",
      " [-] epoch   56/250, train loss 0.640891 in 0.02s\n",
      " [-] epoch   57/250, train loss 0.626306 in 0.02s\n",
      " [-] epoch   58/250, train loss 0.640864 in 0.02s\n",
      " [-] epoch   59/250, train loss 0.629486 in 0.02s\n",
      " [-] epoch   60/250, train loss 0.636776 in 0.02s\n",
      " [-] epoch   61/250, train loss 0.635473 in 0.02s\n",
      " [-] epoch   62/250, train loss 0.633801 in 0.02s\n",
      " [-] epoch   63/250, train loss 0.627677 in 0.02s\n",
      " [-] epoch   64/250, train loss 0.630089 in 0.02s\n",
      " [-] epoch   65/250, train loss 0.630457 in 0.02s\n",
      " [-] epoch   66/250, train loss 0.628239 in 0.02s\n",
      " [-] epoch   67/250, train loss 0.627188 in 0.02s\n",
      " [-] epoch   68/250, train loss 0.626658 in 0.02s\n",
      " [-] epoch   69/250, train loss 0.622204 in 0.02s\n",
      " [-] epoch   70/250, train loss 0.625979 in 0.02s\n",
      " [-] epoch   71/250, train loss 0.635072 in 0.02s\n",
      " [-] epoch   72/250, train loss 0.631967 in 0.02s\n",
      " [-] epoch   73/250, train loss 0.625637 in 0.02s\n",
      " [-] epoch   74/250, train loss 0.637093 in 0.02s\n",
      " [-] epoch   75/250, train loss 0.639067 in 0.02s\n",
      " [-] epoch   76/250, train loss 0.631734 in 0.02s\n",
      " [-] epoch   77/250, train loss 0.637048 in 0.02s\n",
      " [-] epoch   78/250, train loss 0.617752 in 0.02s\n",
      " [-] epoch   79/250, train loss 0.624061 in 0.02s\n",
      " [-] epoch   80/250, train loss 0.621276 in 0.02s\n",
      " [-] epoch   81/250, train loss 0.629128 in 0.02s\n",
      " [-] epoch   82/250, train loss 0.625482 in 0.02s\n",
      " [-] epoch   83/250, train loss 0.641799 in 0.02s\n",
      " [-] epoch   84/250, train loss 0.633778 in 0.02s\n",
      " [-] epoch   85/250, train loss 0.624500 in 0.02s\n",
      " [-] epoch   86/250, train loss 0.623431 in 0.02s\n",
      " [-] epoch   87/250, train loss 0.618440 in 0.02s\n",
      " [-] epoch   88/250, train loss 0.615634 in 0.02s\n",
      " [-] epoch   89/250, train loss 0.626371 in 0.02s\n",
      " [-] epoch   90/250, train loss 0.623197 in 0.02s\n",
      " [-] epoch   91/250, train loss 0.614664 in 0.02s\n",
      " [-] epoch   92/250, train loss 0.625110 in 0.02s\n",
      " [-] epoch   93/250, train loss 0.617492 in 0.02s\n",
      " [-] epoch   94/250, train loss 0.625245 in 0.02s\n",
      " [-] epoch   95/250, train loss 0.618170 in 0.02s\n",
      " [-] epoch   96/250, train loss 0.615973 in 0.02s\n",
      " [-] epoch   97/250, train loss 0.622180 in 0.02s\n",
      " [-] epoch   98/250, train loss 0.621510 in 0.02s\n",
      " [-] epoch   99/250, train loss 0.618543 in 0.02s\n",
      " [-] epoch  100/250, train loss 0.622996 in 0.02s\n",
      " [-] epoch  101/250, train loss 0.627152 in 0.02s\n",
      " [-] epoch  102/250, train loss 0.630437 in 0.03s\n",
      " [-] epoch  103/250, train loss 0.621870 in 0.02s\n",
      " [-] epoch  104/250, train loss 0.630938 in 0.02s\n",
      " [-] epoch  105/250, train loss 0.623294 in 0.02s\n",
      " [-] epoch  106/250, train loss 0.609822 in 0.02s\n",
      " [-] epoch  107/250, train loss 0.625027 in 0.02s\n",
      " [-] epoch  108/250, train loss 0.622505 in 0.02s\n",
      " [-] epoch  109/250, train loss 0.614569 in 0.02s\n",
      " [-] epoch  110/250, train loss 0.635125 in 0.02s\n",
      " [-] epoch  111/250, train loss 0.620496 in 0.02s\n",
      " [-] epoch  112/250, train loss 0.631274 in 0.02s\n",
      " [-] epoch  113/250, train loss 0.618250 in 0.02s\n",
      " [-] epoch  114/250, train loss 0.612892 in 0.02s\n",
      " [-] epoch  115/250, train loss 0.622023 in 0.02s\n",
      " [-] epoch  116/250, train loss 0.612189 in 0.02s\n",
      " [-] epoch  117/250, train loss 0.629667 in 0.02s\n",
      " [-] epoch  118/250, train loss 0.612779 in 0.02s\n",
      " [-] epoch  119/250, train loss 0.619875 in 0.02s\n",
      " [-] epoch  120/250, train loss 0.614346 in 0.02s\n",
      " [-] epoch  121/250, train loss 0.609280 in 0.02s\n",
      " [-] epoch  122/250, train loss 0.611628 in 0.02s\n",
      " [-] epoch  123/250, train loss 0.620411 in 0.02s\n",
      " [-] epoch  124/250, train loss 0.620041 in 0.02s\n",
      " [-] epoch  125/250, train loss 0.621673 in 0.02s\n",
      " [-] epoch  126/250, train loss 0.622570 in 0.02s\n",
      " [-] epoch  127/250, train loss 0.612367 in 0.02s\n",
      " [-] epoch  128/250, train loss 0.603456 in 0.02s\n",
      " [-] epoch  129/250, train loss 0.607331 in 0.02s\n",
      " [-] epoch  130/250, train loss 0.614530 in 0.02s\n",
      " [-] epoch  131/250, train loss 0.618066 in 0.02s\n",
      " [-] epoch  132/250, train loss 0.614534 in 0.02s\n",
      " [-] epoch  133/250, train loss 0.604109 in 0.02s\n",
      " [-] epoch  134/250, train loss 0.605589 in 0.02s\n",
      " [-] epoch  135/250, train loss 0.611696 in 0.02s\n",
      " [-] epoch  136/250, train loss 0.618055 in 0.02s\n",
      " [-] epoch  137/250, train loss 0.616143 in 0.02s\n",
      " [-] epoch  138/250, train loss 0.610456 in 0.02s\n",
      " [-] epoch  139/250, train loss 0.627299 in 0.02s\n",
      " [-] epoch  140/250, train loss 0.610033 in 0.02s\n",
      " [-] epoch  141/250, train loss 0.613868 in 0.02s\n",
      " [-] epoch  142/250, train loss 0.604122 in 0.02s\n",
      " [-] epoch  143/250, train loss 0.618514 in 0.02s\n",
      " [-] epoch  144/250, train loss 0.615148 in 0.02s\n",
      " [-] epoch  145/250, train loss 0.619467 in 0.02s\n",
      " [-] epoch  146/250, train loss 0.610670 in 0.02s\n",
      " [-] epoch  147/250, train loss 0.625962 in 0.02s\n",
      " [-] epoch  148/250, train loss 0.607975 in 0.02s\n",
      " [-] epoch  149/250, train loss 0.603200 in 0.02s\n",
      " [-] epoch  150/250, train loss 0.619029 in 0.02s\n",
      " [-] epoch  151/250, train loss 0.619470 in 0.02s\n",
      " [-] epoch  152/250, train loss 0.601642 in 0.02s\n",
      " [-] epoch  153/250, train loss 0.620462 in 0.02s\n",
      " [-] epoch  154/250, train loss 0.609501 in 0.02s\n",
      " [-] epoch  155/250, train loss 0.620010 in 0.02s\n",
      " [-] epoch  156/250, train loss 0.604384 in 0.02s\n",
      " [-] epoch  157/250, train loss 0.612322 in 0.02s\n",
      " [-] epoch  158/250, train loss 0.603467 in 0.02s\n",
      " [-] epoch  159/250, train loss 0.606821 in 0.02s\n",
      " [-] epoch  160/250, train loss 0.612314 in 0.02s\n",
      " [-] epoch  161/250, train loss 0.612165 in 0.02s\n",
      " [-] epoch  162/250, train loss 0.621839 in 0.02s\n",
      " [-] epoch  163/250, train loss 0.603472 in 0.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  164/250, train loss 0.611660 in 0.02s\n",
      " [-] epoch  165/250, train loss 0.590119 in 0.02s\n",
      " [-] epoch  166/250, train loss 0.612122 in 0.02s\n",
      " [-] epoch  167/250, train loss 0.609085 in 0.02s\n",
      " [-] epoch  168/250, train loss 0.622743 in 0.02s\n",
      " [-] epoch  169/250, train loss 0.610379 in 0.02s\n",
      " [-] epoch  170/250, train loss 0.598508 in 0.02s\n",
      " [-] epoch  171/250, train loss 0.597239 in 0.02s\n",
      " [-] epoch  172/250, train loss 0.607763 in 0.02s\n",
      " [-] epoch  173/250, train loss 0.602076 in 0.02s\n",
      " [-] epoch  174/250, train loss 0.612861 in 0.02s\n",
      " [-] epoch  175/250, train loss 0.602792 in 0.02s\n",
      " [-] epoch  176/250, train loss 0.604683 in 0.02s\n",
      " [-] epoch  177/250, train loss 0.607398 in 0.02s\n",
      " [-] epoch  178/250, train loss 0.609389 in 0.02s\n",
      " [-] epoch  179/250, train loss 0.593213 in 0.02s\n",
      " [-] epoch  180/250, train loss 0.619192 in 0.02s\n",
      " [-] epoch  181/250, train loss 0.604371 in 0.02s\n",
      " [-] epoch  182/250, train loss 0.609115 in 0.02s\n",
      " [-] epoch  183/250, train loss 0.598304 in 0.02s\n",
      " [-] epoch  184/250, train loss 0.612692 in 0.02s\n",
      " [-] epoch  185/250, train loss 0.608764 in 0.02s\n",
      " [-] epoch  186/250, train loss 0.608240 in 0.02s\n",
      " [-] epoch  187/250, train loss 0.606772 in 0.02s\n",
      " [-] epoch  188/250, train loss 0.609258 in 0.02s\n",
      " [-] epoch  189/250, train loss 0.605091 in 0.02s\n",
      " [-] epoch  190/250, train loss 0.597604 in 0.02s\n",
      " [-] epoch  191/250, train loss 0.611278 in 0.02s\n",
      " [-] epoch  192/250, train loss 0.610680 in 0.02s\n",
      " [-] epoch  193/250, train loss 0.602119 in 0.02s\n",
      " [-] epoch  194/250, train loss 0.594784 in 0.02s\n",
      " [-] epoch  195/250, train loss 0.614725 in 0.02s\n",
      " [-] epoch  196/250, train loss 0.600790 in 0.02s\n",
      " [-] epoch  197/250, train loss 0.614392 in 0.02s\n",
      " [-] epoch  198/250, train loss 0.592863 in 0.02s\n",
      " [-] epoch  199/250, train loss 0.601405 in 0.02s\n",
      " [-] epoch  200/250, train loss 0.602479 in 0.01s\n",
      " [-] epoch  201/250, train loss 0.586770 in 0.02s\n",
      " [-] epoch  202/250, train loss 0.603104 in 0.02s\n",
      " [-] epoch  203/250, train loss 0.591018 in 0.02s\n",
      " [-] epoch  204/250, train loss 0.597652 in 0.02s\n",
      " [-] epoch  205/250, train loss 0.616475 in 0.02s\n",
      " [-] epoch  206/250, train loss 0.598052 in 0.02s\n",
      " [-] epoch  207/250, train loss 0.607000 in 0.02s\n",
      " [-] epoch  208/250, train loss 0.614931 in 0.02s\n",
      " [-] epoch  209/250, train loss 0.611094 in 0.02s\n",
      " [-] epoch  210/250, train loss 0.599723 in 0.02s\n",
      " [-] epoch  211/250, train loss 0.611995 in 0.02s\n",
      " [-] epoch  212/250, train loss 0.594746 in 0.02s\n",
      " [-] epoch  213/250, train loss 0.607661 in 0.02s\n",
      " [-] epoch  214/250, train loss 0.609740 in 0.02s\n",
      " [-] epoch  215/250, train loss 0.585944 in 0.02s\n",
      " [-] epoch  216/250, train loss 0.589436 in 0.02s\n",
      " [-] epoch  217/250, train loss 0.591096 in 0.02s\n",
      " [-] epoch  218/250, train loss 0.586149 in 0.02s\n",
      " [-] epoch  219/250, train loss 0.601135 in 0.02s\n",
      " [-] epoch  220/250, train loss 0.599188 in 0.02s\n",
      " [-] epoch  221/250, train loss 0.599673 in 0.02s\n",
      " [-] epoch  222/250, train loss 0.597168 in 0.02s\n",
      " [-] epoch  223/250, train loss 0.605382 in 0.02s\n",
      " [-] epoch  224/250, train loss 0.606619 in 0.02s\n",
      " [-] epoch  225/250, train loss 0.602617 in 0.02s\n",
      " [-] epoch  226/250, train loss 0.617519 in 0.02s\n",
      " [-] epoch  227/250, train loss 0.601022 in 0.02s\n",
      " [-] epoch  228/250, train loss 0.603186 in 0.02s\n",
      " [-] epoch  229/250, train loss 0.594133 in 0.02s\n",
      " [-] epoch  230/250, train loss 0.603056 in 0.02s\n",
      " [-] epoch  231/250, train loss 0.601449 in 0.02s\n",
      " [-] epoch  232/250, train loss 0.611313 in 0.02s\n",
      " [-] epoch  233/250, train loss 0.595918 in 0.02s\n",
      " [-] epoch  234/250, train loss 0.613708 in 0.02s\n",
      " [-] epoch  235/250, train loss 0.607890 in 0.02s\n",
      " [-] epoch  236/250, train loss 0.596932 in 0.02s\n",
      " [-] epoch  237/250, train loss 0.584457 in 0.02s\n",
      " [-] epoch  238/250, train loss 0.596876 in 0.02s\n",
      " [-] epoch  239/250, train loss 0.609270 in 0.02s\n",
      " [-] epoch  240/250, train loss 0.583292 in 0.02s\n",
      " [-] epoch  241/250, train loss 0.613537 in 0.02s\n",
      " [-] epoch  242/250, train loss 0.599347 in 0.02s\n",
      " [-] epoch  243/250, train loss 0.590110 in 0.02s\n",
      " [-] epoch  244/250, train loss 0.600380 in 0.02s\n",
      " [-] epoch  245/250, train loss 0.601081 in 0.02s\n",
      " [-] epoch  246/250, train loss 0.598517 in 0.02s\n",
      " [-] epoch  247/250, train loss 0.592265 in 0.02s\n",
      " [-] epoch  248/250, train loss 0.580833 in 0.02s\n",
      " [-] epoch  249/250, train loss 0.590899 in 0.02s\n",
      " [-] epoch  250/250, train loss 0.600450 in 0.02s\n",
      " [-] test acc. 70.833333%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.673831 in 0.06s\n",
      " [-] epoch    2/250, train loss 0.634164 in 0.05s\n",
      " [-] epoch    3/250, train loss 0.627235 in 0.05s\n",
      " [-] epoch    4/250, train loss 0.617256 in 0.05s\n",
      " [-] epoch    5/250, train loss 0.607934 in 0.06s\n",
      " [-] epoch    6/250, train loss 0.595678 in 0.06s\n",
      " [-] epoch    7/250, train loss 0.596658 in 0.05s\n",
      " [-] epoch    8/250, train loss 0.597775 in 0.06s\n",
      " [-] epoch    9/250, train loss 0.587624 in 0.05s\n",
      " [-] epoch   10/250, train loss 0.561052 in 0.05s\n",
      " [-] epoch   11/250, train loss 0.580298 in 0.05s\n",
      " [-] epoch   12/250, train loss 0.556422 in 0.05s\n",
      " [-] epoch   13/250, train loss 0.574551 in 0.05s\n",
      " [-] epoch   14/250, train loss 0.580695 in 0.05s\n",
      " [-] epoch   15/250, train loss 0.557518 in 0.05s\n",
      " [-] epoch   16/250, train loss 0.550277 in 0.05s\n",
      " [-] epoch   17/250, train loss 0.571762 in 0.05s\n",
      " [-] epoch   18/250, train loss 0.565608 in 0.06s\n",
      " [-] epoch   19/250, train loss 0.562435 in 0.06s\n",
      " [-] epoch   20/250, train loss 0.565545 in 0.06s\n",
      " [-] epoch   21/250, train loss 0.576568 in 0.06s\n",
      " [-] epoch   22/250, train loss 0.549691 in 0.06s\n",
      " [-] epoch   23/250, train loss 0.562315 in 0.06s\n",
      " [-] epoch   24/250, train loss 0.559098 in 0.05s\n",
      " [-] epoch   25/250, train loss 0.548170 in 0.05s\n",
      " [-] epoch   26/250, train loss 0.529263 in 0.06s\n",
      " [-] epoch   27/250, train loss 0.524424 in 0.04s\n",
      " [-] epoch   28/250, train loss 0.546603 in 0.06s\n",
      " [-] epoch   29/250, train loss 0.551617 in 0.05s\n",
      " [-] epoch   30/250, train loss 0.558883 in 0.05s\n",
      " [-] epoch   31/250, train loss 0.536383 in 0.05s\n",
      " [-] epoch   32/250, train loss 0.542554 in 0.05s\n",
      " [-] epoch   33/250, train loss 0.539219 in 0.04s\n",
      " [-] epoch   34/250, train loss 0.540707 in 0.05s\n",
      " [-] epoch   35/250, train loss 0.551772 in 0.05s\n",
      " [-] epoch   36/250, train loss 0.554329 in 0.05s\n",
      " [-] epoch   37/250, train loss 0.551159 in 0.06s\n",
      " [-] epoch   38/250, train loss 0.527463 in 0.05s\n",
      " [-] epoch   39/250, train loss 0.572535 in 0.06s\n",
      " [-] epoch   40/250, train loss 0.541238 in 0.05s\n",
      " [-] epoch   41/250, train loss 0.534063 in 0.05s\n",
      " [-] epoch   42/250, train loss 0.551319 in 0.05s\n",
      " [-] epoch   43/250, train loss 0.540556 in 0.05s\n",
      " [-] epoch   44/250, train loss 0.522681 in 0.06s\n",
      " [-] epoch   45/250, train loss 0.549466 in 0.05s\n",
      " [-] epoch   46/250, train loss 0.542364 in 0.05s\n",
      " [-] epoch   47/250, train loss 0.556075 in 0.06s\n",
      " [-] epoch   48/250, train loss 0.557613 in 0.06s\n",
      " [-] epoch   49/250, train loss 0.533145 in 0.05s\n",
      " [-] epoch   50/250, train loss 0.523303 in 0.06s\n",
      " [-] epoch   51/250, train loss 0.524072 in 0.05s\n",
      " [-] epoch   52/250, train loss 0.544828 in 0.04s\n",
      " [-] epoch   53/250, train loss 0.547070 in 0.05s\n",
      " [-] epoch   54/250, train loss 0.527238 in 0.05s\n",
      " [-] epoch   55/250, train loss 0.551221 in 0.06s\n",
      " [-] epoch   56/250, train loss 0.541014 in 0.05s\n",
      " [-] epoch   57/250, train loss 0.551803 in 0.06s\n",
      " [-] epoch   58/250, train loss 0.532778 in 0.06s\n",
      " [-] epoch   59/250, train loss 0.516012 in 0.05s\n",
      " [-] epoch   60/250, train loss 0.514837 in 0.06s\n",
      " [-] epoch   61/250, train loss 0.526805 in 0.06s\n",
      " [-] epoch   62/250, train loss 0.530123 in 0.04s\n",
      " [-] epoch   63/250, train loss 0.513825 in 0.06s\n",
      " [-] epoch   64/250, train loss 0.529149 in 0.06s\n",
      " [-] epoch   65/250, train loss 0.546966 in 0.05s\n",
      " [-] epoch   66/250, train loss 0.522696 in 0.05s\n",
      " [-] epoch   67/250, train loss 0.526784 in 0.05s\n",
      " [-] epoch   68/250, train loss 0.528682 in 0.06s\n",
      " [-] epoch   69/250, train loss 0.544708 in 0.06s\n",
      " [-] epoch   70/250, train loss 0.519254 in 0.06s\n",
      " [-] epoch   71/250, train loss 0.521336 in 0.05s\n",
      " [-] epoch   72/250, train loss 0.539845 in 0.06s\n",
      " [-] epoch   73/250, train loss 0.530343 in 0.06s\n",
      " [-] epoch   74/250, train loss 0.532937 in 0.05s\n",
      " [-] epoch   75/250, train loss 0.528551 in 0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.523835 in 0.06s\n",
      " [-] epoch   77/250, train loss 0.516735 in 0.06s\n",
      " [-] epoch   78/250, train loss 0.526421 in 0.06s\n",
      " [-] epoch   79/250, train loss 0.541742 in 0.04s\n",
      " [-] epoch   80/250, train loss 0.511512 in 0.06s\n",
      " [-] epoch   81/250, train loss 0.536095 in 0.06s\n",
      " [-] epoch   82/250, train loss 0.535388 in 0.06s\n",
      " [-] epoch   83/250, train loss 0.575144 in 0.06s\n",
      " [-] epoch   84/250, train loss 0.526186 in 0.06s\n",
      " [-] epoch   85/250, train loss 0.520304 in 0.05s\n",
      " [-] epoch   86/250, train loss 0.484221 in 0.06s\n",
      " [-] epoch   87/250, train loss 0.518739 in 0.05s\n",
      " [-] epoch   88/250, train loss 0.502358 in 0.05s\n",
      " [-] epoch   89/250, train loss 0.491337 in 0.05s\n",
      " [-] epoch   90/250, train loss 0.531261 in 0.05s\n",
      " [-] epoch   91/250, train loss 0.502947 in 0.05s\n",
      " [-] epoch   92/250, train loss 0.517982 in 0.05s\n",
      " [-] epoch   93/250, train loss 0.537151 in 0.04s\n",
      " [-] epoch   94/250, train loss 0.505020 in 0.05s\n",
      " [-] epoch   95/250, train loss 0.485264 in 0.06s\n",
      " [-] epoch   96/250, train loss 0.525669 in 0.05s\n",
      " [-] epoch   97/250, train loss 0.508469 in 0.06s\n",
      " [-] epoch   98/250, train loss 0.502480 in 0.05s\n",
      " [-] epoch   99/250, train loss 0.531710 in 0.05s\n",
      " [-] epoch  100/250, train loss 0.507632 in 0.05s\n",
      " [-] epoch  101/250, train loss 0.486377 in 0.04s\n",
      " [-] epoch  102/250, train loss 0.511227 in 0.05s\n",
      " [-] epoch  103/250, train loss 0.525027 in 0.05s\n",
      " [-] epoch  104/250, train loss 0.500667 in 0.05s\n",
      " [-] epoch  105/250, train loss 0.495107 in 0.05s\n",
      " [-] epoch  106/250, train loss 0.510192 in 0.05s\n",
      " [-] epoch  107/250, train loss 0.535549 in 0.05s\n",
      " [-] epoch  108/250, train loss 0.502118 in 0.04s\n",
      " [-] epoch  109/250, train loss 0.498519 in 0.04s\n",
      " [-] epoch  110/250, train loss 0.490729 in 0.06s\n",
      " [-] epoch  111/250, train loss 0.512424 in 0.05s\n",
      " [-] epoch  112/250, train loss 0.509511 in 0.06s\n",
      " [-] epoch  113/250, train loss 0.485983 in 0.05s\n",
      " [-] epoch  114/250, train loss 0.493273 in 0.05s\n",
      " [-] epoch  115/250, train loss 0.512154 in 0.04s\n",
      " [-] epoch  116/250, train loss 0.492191 in 0.05s\n",
      " [-] epoch  117/250, train loss 0.518836 in 0.05s\n",
      " [-] epoch  118/250, train loss 0.517970 in 0.05s\n",
      " [-] epoch  119/250, train loss 0.510875 in 0.05s\n",
      " [-] epoch  120/250, train loss 0.473831 in 0.05s\n",
      " [-] epoch  121/250, train loss 0.493425 in 0.06s\n",
      " [-] epoch  122/250, train loss 0.498152 in 0.06s\n",
      " [-] epoch  123/250, train loss 0.503988 in 0.05s\n",
      " [-] epoch  124/250, train loss 0.513925 in 0.05s\n",
      " [-] epoch  125/250, train loss 0.507021 in 0.05s\n",
      " [-] epoch  126/250, train loss 0.491595 in 0.05s\n",
      " [-] epoch  127/250, train loss 0.515410 in 0.04s\n",
      " [-] epoch  128/250, train loss 0.480545 in 0.06s\n",
      " [-] epoch  129/250, train loss 0.492541 in 0.04s\n",
      " [-] epoch  130/250, train loss 0.488769 in 0.05s\n",
      " [-] epoch  131/250, train loss 0.501582 in 0.05s\n",
      " [-] epoch  132/250, train loss 0.523789 in 0.06s\n",
      " [-] epoch  133/250, train loss 0.511717 in 0.05s\n",
      " [-] epoch  134/250, train loss 0.506623 in 0.05s\n",
      " [-] epoch  135/250, train loss 0.477946 in 0.05s\n",
      " [-] epoch  136/250, train loss 0.483962 in 0.06s\n",
      " [-] epoch  137/250, train loss 0.489986 in 0.05s\n",
      " [-] epoch  138/250, train loss 0.498118 in 0.05s\n",
      " [-] epoch  139/250, train loss 0.489573 in 0.05s\n",
      " [-] epoch  140/250, train loss 0.501268 in 0.05s\n",
      " [-] epoch  141/250, train loss 0.473153 in 0.05s\n",
      " [-] epoch  142/250, train loss 0.482626 in 0.06s\n",
      " [-] epoch  143/250, train loss 0.472436 in 0.06s\n",
      " [-] epoch  144/250, train loss 0.499516 in 0.05s\n",
      " [-] epoch  145/250, train loss 0.528403 in 0.05s\n",
      " [-] epoch  146/250, train loss 0.510998 in 0.05s\n",
      " [-] epoch  147/250, train loss 0.509788 in 0.05s\n",
      " [-] epoch  148/250, train loss 0.510378 in 0.05s\n",
      " [-] epoch  149/250, train loss 0.472281 in 0.05s\n",
      " [-] epoch  150/250, train loss 0.489542 in 0.05s\n",
      " [-] epoch  151/250, train loss 0.483773 in 0.05s\n",
      " [-] epoch  152/250, train loss 0.469707 in 0.05s\n",
      " [-] epoch  153/250, train loss 0.513474 in 0.04s\n",
      " [-] epoch  154/250, train loss 0.512605 in 0.05s\n",
      " [-] epoch  155/250, train loss 0.505597 in 0.05s\n",
      " [-] epoch  156/250, train loss 0.498662 in 0.05s\n",
      " [-] epoch  157/250, train loss 0.508704 in 0.04s\n",
      " [-] epoch  158/250, train loss 0.466867 in 0.05s\n",
      " [-] epoch  159/250, train loss 0.477137 in 0.05s\n",
      " [-] epoch  160/250, train loss 0.512026 in 0.05s\n",
      " [-] epoch  161/250, train loss 0.492664 in 0.06s\n",
      " [-] epoch  162/250, train loss 0.517708 in 0.05s\n",
      " [-] epoch  163/250, train loss 0.486601 in 0.06s\n",
      " [-] epoch  164/250, train loss 0.497255 in 0.06s\n",
      " [-] epoch  165/250, train loss 0.481942 in 0.05s\n",
      " [-] epoch  166/250, train loss 0.490947 in 0.05s\n",
      " [-] epoch  167/250, train loss 0.481612 in 0.05s\n",
      " [-] epoch  168/250, train loss 0.469460 in 0.05s\n",
      " [-] epoch  169/250, train loss 0.492300 in 0.05s\n",
      " [-] epoch  170/250, train loss 0.473358 in 0.05s\n",
      " [-] epoch  171/250, train loss 0.477514 in 0.05s\n",
      " [-] epoch  172/250, train loss 0.482827 in 0.06s\n",
      " [-] epoch  173/250, train loss 0.468117 in 0.05s\n",
      " [-] epoch  174/250, train loss 0.501124 in 0.05s\n",
      " [-] epoch  175/250, train loss 0.504386 in 0.05s\n",
      " [-] epoch  176/250, train loss 0.479334 in 0.05s\n",
      " [-] epoch  177/250, train loss 0.502986 in 0.05s\n",
      " [-] epoch  178/250, train loss 0.462224 in 0.05s\n",
      " [-] epoch  179/250, train loss 0.475420 in 0.06s\n",
      " [-] epoch  180/250, train loss 0.474510 in 0.06s\n",
      " [-] epoch  181/250, train loss 0.461171 in 0.05s\n",
      " [-] epoch  182/250, train loss 0.499679 in 0.06s\n",
      " [-] epoch  183/250, train loss 0.492731 in 0.05s\n",
      " [-] epoch  184/250, train loss 0.493145 in 0.05s\n",
      " [-] epoch  185/250, train loss 0.499212 in 0.06s\n",
      " [-] epoch  186/250, train loss 0.475997 in 0.05s\n",
      " [-] epoch  187/250, train loss 0.471300 in 0.04s\n",
      " [-] epoch  188/250, train loss 0.465609 in 0.05s\n",
      " [-] epoch  189/250, train loss 0.469064 in 0.04s\n",
      " [-] epoch  190/250, train loss 0.513164 in 0.05s\n",
      " [-] epoch  191/250, train loss 0.499461 in 0.06s\n",
      " [-] epoch  192/250, train loss 0.467828 in 0.05s\n",
      " [-] epoch  193/250, train loss 0.471349 in 0.05s\n",
      " [-] epoch  194/250, train loss 0.486878 in 0.05s\n",
      " [-] epoch  195/250, train loss 0.451409 in 0.05s\n",
      " [-] epoch  196/250, train loss 0.498421 in 0.05s\n",
      " [-] epoch  197/250, train loss 0.509831 in 0.05s\n",
      " [-] epoch  198/250, train loss 0.481326 in 0.05s\n",
      " [-] epoch  199/250, train loss 0.466957 in 0.06s\n",
      " [-] epoch  200/250, train loss 0.472495 in 0.06s\n",
      " [-] epoch  201/250, train loss 0.481214 in 0.05s\n",
      " [-] epoch  202/250, train loss 0.503612 in 0.05s\n",
      " [-] epoch  203/250, train loss 0.476228 in 0.05s\n",
      " [-] epoch  204/250, train loss 0.493569 in 0.05s\n",
      " [-] epoch  205/250, train loss 0.495430 in 0.06s\n",
      " [-] epoch  206/250, train loss 0.470105 in 0.06s\n",
      " [-] epoch  207/250, train loss 0.487012 in 0.04s\n",
      " [-] epoch  208/250, train loss 0.466409 in 0.05s\n",
      " [-] epoch  209/250, train loss 0.489197 in 0.05s\n",
      " [-] epoch  210/250, train loss 0.490656 in 0.06s\n",
      " [-] epoch  211/250, train loss 0.483386 in 0.06s\n",
      " [-] epoch  212/250, train loss 0.488519 in 0.05s\n",
      " [-] epoch  213/250, train loss 0.471435 in 0.04s\n",
      " [-] epoch  214/250, train loss 0.457300 in 0.05s\n",
      " [-] epoch  215/250, train loss 0.491810 in 0.04s\n",
      " [-] epoch  216/250, train loss 0.482051 in 0.06s\n",
      " [-] epoch  217/250, train loss 0.484409 in 0.05s\n",
      " [-] epoch  218/250, train loss 0.485167 in 0.04s\n",
      " [-] epoch  219/250, train loss 0.468484 in 0.06s\n",
      " [-] epoch  220/250, train loss 0.492651 in 0.05s\n",
      " [-] epoch  221/250, train loss 0.481578 in 0.05s\n",
      " [-] epoch  222/250, train loss 0.456374 in 0.05s\n",
      " [-] epoch  223/250, train loss 0.455882 in 0.06s\n",
      " [-] epoch  224/250, train loss 0.446301 in 0.05s\n",
      " [-] epoch  225/250, train loss 0.490245 in 0.06s\n",
      " [-] epoch  226/250, train loss 0.485616 in 0.05s\n",
      " [-] epoch  227/250, train loss 0.475503 in 0.05s\n",
      " [-] epoch  228/250, train loss 0.469438 in 0.05s\n",
      " [-] epoch  229/250, train loss 0.492448 in 0.05s\n",
      " [-] epoch  230/250, train loss 0.478335 in 0.05s\n",
      " [-] epoch  231/250, train loss 0.461108 in 0.05s\n",
      " [-] epoch  232/250, train loss 0.467107 in 0.05s\n",
      " [-] epoch  233/250, train loss 0.475490 in 0.05s\n",
      " [-] epoch  234/250, train loss 0.490174 in 0.05s\n",
      " [-] epoch  235/250, train loss 0.453348 in 0.05s\n",
      " [-] epoch  236/250, train loss 0.455594 in 0.06s\n",
      " [-] epoch  237/250, train loss 0.487351 in 0.06s\n",
      " [-] epoch  238/250, train loss 0.463490 in 0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  239/250, train loss 0.464823 in 0.05s\n",
      " [-] epoch  240/250, train loss 0.475442 in 0.06s\n",
      " [-] epoch  241/250, train loss 0.455468 in 0.05s\n",
      " [-] epoch  242/250, train loss 0.455678 in 0.04s\n",
      " [-] epoch  243/250, train loss 0.495266 in 0.05s\n",
      " [-] epoch  244/250, train loss 0.470142 in 0.05s\n",
      " [-] epoch  245/250, train loss 0.466683 in 0.06s\n",
      " [-] epoch  246/250, train loss 0.466054 in 0.05s\n",
      " [-] epoch  247/250, train loss 0.478222 in 0.04s\n",
      " [-] epoch  248/250, train loss 0.471159 in 0.06s\n",
      " [-] epoch  249/250, train loss 0.486024 in 0.05s\n",
      " [-] epoch  250/250, train loss 0.454737 in 0.06s\n",
      " [-] test acc. 55.000000%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.670905 in 0.08s\n",
      " [-] epoch    2/250, train loss 0.632618 in 0.07s\n",
      " [-] epoch    3/250, train loss 0.604878 in 0.08s\n",
      " [-] epoch    4/250, train loss 0.605270 in 0.08s\n",
      " [-] epoch    5/250, train loss 0.600224 in 0.08s\n",
      " [-] epoch    6/250, train loss 0.581979 in 0.08s\n",
      " [-] epoch    7/250, train loss 0.560503 in 0.08s\n",
      " [-] epoch    8/250, train loss 0.557711 in 0.07s\n",
      " [-] epoch    9/250, train loss 0.554557 in 0.06s\n",
      " [-] epoch   10/250, train loss 0.566345 in 0.08s\n",
      " [-] epoch   11/250, train loss 0.571181 in 0.08s\n",
      " [-] epoch   12/250, train loss 0.530678 in 0.07s\n",
      " [-] epoch   13/250, train loss 0.531623 in 0.08s\n",
      " [-] epoch   14/250, train loss 0.550399 in 0.08s\n",
      " [-] epoch   15/250, train loss 0.533057 in 0.07s\n",
      " [-] epoch   16/250, train loss 0.569640 in 0.08s\n",
      " [-] epoch   17/250, train loss 0.551080 in 0.08s\n",
      " [-] epoch   18/250, train loss 0.536093 in 0.07s\n",
      " [-] epoch   19/250, train loss 0.545500 in 0.08s\n",
      " [-] epoch   20/250, train loss 0.523647 in 0.07s\n",
      " [-] epoch   21/250, train loss 0.528635 in 0.08s\n",
      " [-] epoch   22/250, train loss 0.517783 in 0.08s\n",
      " [-] epoch   23/250, train loss 0.532697 in 0.07s\n",
      " [-] epoch   24/250, train loss 0.522873 in 0.08s\n",
      " [-] epoch   25/250, train loss 0.518372 in 0.08s\n",
      " [-] epoch   26/250, train loss 0.520253 in 0.08s\n",
      " [-] epoch   27/250, train loss 0.506815 in 0.06s\n",
      " [-] epoch   28/250, train loss 0.502220 in 0.08s\n",
      " [-] epoch   29/250, train loss 0.476589 in 0.07s\n",
      " [-] epoch   30/250, train loss 0.517205 in 0.06s\n",
      " [-] epoch   31/250, train loss 0.501547 in 0.09s\n",
      " [-] epoch   32/250, train loss 0.495856 in 0.07s\n",
      " [-] epoch   33/250, train loss 0.497809 in 0.08s\n",
      " [-] epoch   34/250, train loss 0.512809 in 0.08s\n",
      " [-] epoch   35/250, train loss 0.504573 in 0.06s\n",
      " [-] epoch   36/250, train loss 0.488459 in 0.07s\n",
      " [-] epoch   37/250, train loss 0.461448 in 0.07s\n",
      " [-] epoch   38/250, train loss 0.511267 in 0.08s\n",
      " [-] epoch   39/250, train loss 0.491299 in 0.07s\n",
      " [-] epoch   40/250, train loss 0.512373 in 0.07s\n",
      " [-] epoch   41/250, train loss 0.495273 in 0.08s\n",
      " [-] epoch   42/250, train loss 0.483040 in 0.07s\n",
      " [-] epoch   43/250, train loss 0.492888 in 0.08s\n",
      " [-] epoch   44/250, train loss 0.501199 in 0.08s\n",
      " [-] epoch   45/250, train loss 0.487276 in 0.07s\n",
      " [-] epoch   46/250, train loss 0.486088 in 0.08s\n",
      " [-] epoch   47/250, train loss 0.480557 in 0.07s\n",
      " [-] epoch   48/250, train loss 0.491701 in 0.07s\n",
      " [-] epoch   49/250, train loss 0.486029 in 0.09s\n",
      " [-] epoch   50/250, train loss 0.487398 in 0.08s\n",
      " [-] epoch   51/250, train loss 0.463457 in 0.07s\n",
      " [-] epoch   52/250, train loss 0.500255 in 0.07s\n",
      " [-] epoch   53/250, train loss 0.495723 in 0.08s\n",
      " [-] epoch   54/250, train loss 0.476069 in 0.08s\n",
      " [-] epoch   55/250, train loss 0.501869 in 0.08s\n",
      " [-] epoch   56/250, train loss 0.494393 in 0.07s\n",
      " [-] epoch   57/250, train loss 0.474884 in 0.08s\n",
      " [-] epoch   58/250, train loss 0.494427 in 0.08s\n",
      " [-] epoch   59/250, train loss 0.471197 in 0.07s\n",
      " [-] epoch   60/250, train loss 0.521429 in 0.08s\n",
      " [-] epoch   61/250, train loss 0.460496 in 0.08s\n",
      " [-] epoch   62/250, train loss 0.470052 in 0.08s\n",
      " [-] epoch   63/250, train loss 0.471891 in 0.07s\n",
      " [-] epoch   64/250, train loss 0.476123 in 0.08s\n",
      " [-] epoch   65/250, train loss 0.457713 in 0.07s\n",
      " [-] epoch   66/250, train loss 0.473084 in 0.08s\n",
      " [-] epoch   67/250, train loss 0.445464 in 0.08s\n",
      " [-] epoch   68/250, train loss 0.455155 in 0.07s\n",
      " [-] epoch   69/250, train loss 0.447961 in 0.08s\n",
      " [-] epoch   70/250, train loss 0.469770 in 0.08s\n",
      " [-] epoch   71/250, train loss 0.501099 in 0.09s\n",
      " [-] epoch   72/250, train loss 0.463933 in 0.08s\n",
      " [-] epoch   73/250, train loss 0.484510 in 0.07s\n",
      " [-] epoch   74/250, train loss 0.468541 in 0.07s\n",
      " [-] epoch   75/250, train loss 0.481510 in 0.08s\n",
      " [-] epoch   76/250, train loss 0.461092 in 0.08s\n",
      " [-] epoch   77/250, train loss 0.472165 in 0.08s\n",
      " [-] epoch   78/250, train loss 0.462505 in 0.08s\n",
      " [-] epoch   79/250, train loss 0.456995 in 0.08s\n",
      " [-] epoch   80/250, train loss 0.479685 in 0.08s\n",
      " [-] epoch   81/250, train loss 0.468964 in 0.08s\n",
      " [-] epoch   82/250, train loss 0.461404 in 0.08s\n",
      " [-] epoch   83/250, train loss 0.454662 in 0.08s\n",
      " [-] epoch   84/250, train loss 0.461062 in 0.07s\n",
      " [-] epoch   85/250, train loss 0.447085 in 0.07s\n",
      " [-] epoch   86/250, train loss 0.459384 in 0.08s\n",
      " [-] epoch   87/250, train loss 0.429797 in 0.08s\n",
      " [-] epoch   88/250, train loss 0.472872 in 0.08s\n",
      " [-] epoch   89/250, train loss 0.464212 in 0.08s\n",
      " [-] epoch   90/250, train loss 0.460957 in 0.08s\n",
      " [-] epoch   91/250, train loss 0.482066 in 0.08s\n",
      " [-] epoch   92/250, train loss 0.465617 in 0.08s\n",
      " [-] epoch   93/250, train loss 0.447171 in 0.07s\n",
      " [-] epoch   94/250, train loss 0.427971 in 0.06s\n",
      " [-] epoch   95/250, train loss 0.452713 in 0.07s\n",
      " [-] epoch   96/250, train loss 0.460963 in 0.08s\n",
      " [-] epoch   97/250, train loss 0.475630 in 0.08s\n",
      " [-] epoch   98/250, train loss 0.459023 in 0.08s\n",
      " [-] epoch   99/250, train loss 0.433445 in 0.08s\n",
      " [-] epoch  100/250, train loss 0.443855 in 0.07s\n",
      " [-] epoch  101/250, train loss 0.452967 in 0.06s\n",
      " [-] epoch  102/250, train loss 0.428954 in 0.09s\n",
      " [-] epoch  103/250, train loss 0.450094 in 0.07s\n",
      " [-] epoch  104/250, train loss 0.448011 in 0.08s\n",
      " [-] epoch  105/250, train loss 0.443010 in 0.07s\n",
      " [-] epoch  106/250, train loss 0.446589 in 0.08s\n",
      " [-] epoch  107/250, train loss 0.420827 in 0.10s\n",
      " [-] epoch  108/250, train loss 0.437590 in 0.08s\n",
      " [-] epoch  109/250, train loss 0.441825 in 0.07s\n",
      " [-] epoch  110/250, train loss 0.441828 in 0.07s\n",
      " [-] epoch  111/250, train loss 0.426984 in 0.10s\n",
      " [-] epoch  112/250, train loss 0.463212 in 0.07s\n",
      " [-] epoch  113/250, train loss 0.435120 in 0.06s\n",
      " [-] epoch  114/250, train loss 0.426568 in 0.08s\n",
      " [-] epoch  115/250, train loss 0.469055 in 0.08s\n",
      " [-] epoch  116/250, train loss 0.462006 in 0.08s\n",
      " [-] epoch  117/250, train loss 0.432333 in 0.08s\n",
      " [-] epoch  118/250, train loss 0.434035 in 0.07s\n",
      " [-] epoch  119/250, train loss 0.459960 in 0.06s\n",
      " [-] epoch  120/250, train loss 0.454563 in 0.10s\n",
      " [-] epoch  121/250, train loss 0.424635 in 0.07s\n",
      " [-] epoch  122/250, train loss 0.428318 in 0.07s\n",
      " [-] epoch  123/250, train loss 0.415827 in 0.08s\n",
      " [-] epoch  124/250, train loss 0.420151 in 0.08s\n",
      " [-] epoch  125/250, train loss 0.424757 in 0.07s\n",
      " [-] epoch  126/250, train loss 0.422030 in 0.08s\n",
      " [-] epoch  127/250, train loss 0.408228 in 0.07s\n",
      " [-] epoch  128/250, train loss 0.476655 in 0.08s\n",
      " [-] epoch  129/250, train loss 0.437597 in 0.09s\n",
      " [-] epoch  130/250, train loss 0.451084 in 0.07s\n",
      " [-] epoch  131/250, train loss 0.438724 in 0.08s\n",
      " [-] epoch  132/250, train loss 0.449375 in 0.08s\n",
      " [-] epoch  133/250, train loss 0.443636 in 0.08s\n",
      " [-] epoch  134/250, train loss 0.435662 in 0.08s\n",
      " [-] epoch  135/250, train loss 0.423375 in 0.08s\n",
      " [-] epoch  136/250, train loss 0.451577 in 0.07s\n",
      " [-] epoch  137/250, train loss 0.420860 in 0.08s\n",
      " [-] epoch  138/250, train loss 0.428313 in 0.08s\n",
      " [-] epoch  139/250, train loss 0.450797 in 0.07s\n",
      " [-] epoch  140/250, train loss 0.427396 in 0.06s\n",
      " [-] epoch  141/250, train loss 0.444224 in 0.07s\n",
      " [-] epoch  142/250, train loss 0.468561 in 0.08s\n",
      " [-] epoch  143/250, train loss 0.447270 in 0.08s\n",
      " [-] epoch  144/250, train loss 0.428955 in 0.07s\n",
      " [-] epoch  145/250, train loss 0.427773 in 0.07s\n",
      " [-] epoch  146/250, train loss 0.432084 in 0.08s\n",
      " [-] epoch  147/250, train loss 0.422819 in 0.07s\n",
      " [-] epoch  148/250, train loss 0.439080 in 0.08s\n",
      " [-] epoch  149/250, train loss 0.432287 in 0.08s\n",
      " [-] epoch  150/250, train loss 0.446840 in 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.439348 in 0.08s\n",
      " [-] epoch  152/250, train loss 0.432837 in 0.08s\n",
      " [-] epoch  153/250, train loss 0.424067 in 0.07s\n",
      " [-] epoch  154/250, train loss 0.444982 in 0.08s\n",
      " [-] epoch  155/250, train loss 0.416156 in 0.08s\n",
      " [-] epoch  156/250, train loss 0.435993 in 0.07s\n",
      " [-] epoch  157/250, train loss 0.462967 in 0.06s\n",
      " [-] epoch  158/250, train loss 0.437847 in 0.08s\n",
      " [-] epoch  159/250, train loss 0.439482 in 0.08s\n",
      " [-] epoch  160/250, train loss 0.434838 in 0.07s\n",
      " [-] epoch  161/250, train loss 0.412501 in 0.08s\n",
      " [-] epoch  162/250, train loss 0.413036 in 0.08s\n",
      " [-] epoch  163/250, train loss 0.422953 in 0.08s\n",
      " [-] epoch  164/250, train loss 0.403227 in 0.07s\n",
      " [-] epoch  165/250, train loss 0.409287 in 0.08s\n",
      " [-] epoch  166/250, train loss 0.416533 in 0.08s\n",
      " [-] epoch  167/250, train loss 0.433503 in 0.08s\n",
      " [-] epoch  168/250, train loss 0.421801 in 0.07s\n",
      " [-] epoch  169/250, train loss 0.403704 in 0.07s\n",
      " [-] epoch  170/250, train loss 0.445718 in 0.07s\n",
      " [-] epoch  171/250, train loss 0.432406 in 0.08s\n",
      " [-] epoch  172/250, train loss 0.426552 in 0.08s\n",
      " [-] epoch  173/250, train loss 0.408564 in 0.08s\n",
      " [-] epoch  174/250, train loss 0.429764 in 0.08s\n",
      " [-] epoch  175/250, train loss 0.412286 in 0.08s\n",
      " [-] epoch  176/250, train loss 0.423274 in 0.08s\n",
      " [-] epoch  177/250, train loss 0.431141 in 0.08s\n",
      " [-] epoch  178/250, train loss 0.400857 in 0.08s\n",
      " [-] epoch  179/250, train loss 0.414048 in 0.08s\n",
      " [-] epoch  180/250, train loss 0.454427 in 0.08s\n",
      " [-] epoch  181/250, train loss 0.429293 in 0.08s\n",
      " [-] epoch  182/250, train loss 0.396471 in 0.08s\n",
      " [-] epoch  183/250, train loss 0.415315 in 0.08s\n",
      " [-] epoch  184/250, train loss 0.453485 in 0.07s\n",
      " [-] epoch  185/250, train loss 0.422354 in 0.07s\n",
      " [-] epoch  186/250, train loss 0.452240 in 0.07s\n",
      " [-] epoch  187/250, train loss 0.423154 in 0.08s\n",
      " [-] epoch  188/250, train loss 0.438727 in 0.08s\n",
      " [-] epoch  189/250, train loss 0.425559 in 0.08s\n",
      " [-] epoch  190/250, train loss 0.416621 in 0.07s\n",
      " [-] epoch  191/250, train loss 0.438365 in 0.08s\n",
      " [-] epoch  192/250, train loss 0.403195 in 0.08s\n",
      " [-] epoch  193/250, train loss 0.432654 in 0.08s\n",
      " [-] epoch  194/250, train loss 0.398874 in 0.08s\n",
      " [-] epoch  195/250, train loss 0.403985 in 0.07s\n",
      " [-] epoch  196/250, train loss 0.450992 in 0.08s\n",
      " [-] epoch  197/250, train loss 0.436145 in 0.08s\n",
      " [-] epoch  198/250, train loss 0.414887 in 0.08s\n",
      " [-] epoch  199/250, train loss 0.399635 in 0.07s\n",
      " [-] epoch  200/250, train loss 0.394551 in 0.07s\n",
      " [-] epoch  201/250, train loss 0.416030 in 0.08s\n",
      " [-] epoch  202/250, train loss 0.391364 in 0.08s\n",
      " [-] epoch  203/250, train loss 0.422350 in 0.09s\n",
      " [-] epoch  204/250, train loss 0.409906 in 0.07s\n",
      " [-] epoch  205/250, train loss 0.416767 in 0.08s\n",
      " [-] epoch  206/250, train loss 0.413458 in 0.07s\n",
      " [-] epoch  207/250, train loss 0.401677 in 0.07s\n",
      " [-] epoch  208/250, train loss 0.415978 in 0.08s\n",
      " [-] epoch  209/250, train loss 0.413903 in 0.08s\n",
      " [-] epoch  210/250, train loss 0.427685 in 0.08s\n",
      " [-] epoch  211/250, train loss 0.411294 in 0.08s\n",
      " [-] epoch  212/250, train loss 0.412075 in 0.06s\n",
      " [-] epoch  213/250, train loss 0.419745 in 0.07s\n",
      " [-] epoch  214/250, train loss 0.408398 in 0.08s\n",
      " [-] epoch  215/250, train loss 0.414549 in 0.07s\n",
      " [-] epoch  216/250, train loss 0.408127 in 0.08s\n",
      " [-] epoch  217/250, train loss 0.412597 in 0.07s\n",
      " [-] epoch  218/250, train loss 0.428758 in 0.08s\n",
      " [-] epoch  219/250, train loss 0.415009 in 0.08s\n",
      " [-] epoch  220/250, train loss 0.405962 in 0.06s\n",
      " [-] epoch  221/250, train loss 0.417698 in 0.08s\n",
      " [-] epoch  222/250, train loss 0.406033 in 0.07s\n",
      " [-] epoch  223/250, train loss 0.410363 in 0.07s\n",
      " [-] epoch  224/250, train loss 0.399506 in 0.07s\n",
      " [-] epoch  225/250, train loss 0.378424 in 0.08s\n",
      " [-] epoch  226/250, train loss 0.409814 in 0.08s\n",
      " [-] epoch  227/250, train loss 0.397057 in 0.08s\n",
      " [-] epoch  228/250, train loss 0.424737 in 0.08s\n",
      " [-] epoch  229/250, train loss 0.406234 in 0.07s\n",
      " [-] epoch  230/250, train loss 0.449189 in 0.09s\n",
      " [-] epoch  231/250, train loss 0.426052 in 0.08s\n",
      " [-] epoch  232/250, train loss 0.417216 in 0.08s\n",
      " [-] epoch  233/250, train loss 0.424666 in 0.08s\n",
      " [-] epoch  234/250, train loss 0.400470 in 0.07s\n",
      " [-] epoch  235/250, train loss 0.418410 in 0.08s\n",
      " [-] epoch  236/250, train loss 0.440763 in 0.08s\n",
      " [-] epoch  237/250, train loss 0.414113 in 0.08s\n",
      " [-] epoch  238/250, train loss 0.405283 in 0.08s\n",
      " [-] epoch  239/250, train loss 0.412915 in 0.08s\n",
      " [-] epoch  240/250, train loss 0.387425 in 0.08s\n",
      " [-] epoch  241/250, train loss 0.412677 in 0.08s\n",
      " [-] epoch  242/250, train loss 0.406795 in 0.08s\n",
      " [-] epoch  243/250, train loss 0.405331 in 0.08s\n",
      " [-] epoch  244/250, train loss 0.439030 in 0.08s\n",
      " [-] epoch  245/250, train loss 0.411573 in 0.07s\n",
      " [-] epoch  246/250, train loss 0.438980 in 0.08s\n",
      " [-] epoch  247/250, train loss 0.397359 in 0.08s\n",
      " [-] epoch  248/250, train loss 0.391737 in 0.08s\n",
      " [-] epoch  249/250, train loss 0.416272 in 0.07s\n",
      " [-] epoch  250/250, train loss 0.389673 in 0.07s\n",
      " [-] test acc. 60.000000%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.662105 in 0.08s\n",
      " [-] epoch    2/250, train loss 0.619735 in 0.10s\n",
      " [-] epoch    3/250, train loss 0.590477 in 0.12s\n",
      " [-] epoch    4/250, train loss 0.583699 in 0.08s\n",
      " [-] epoch    5/250, train loss 0.574395 in 0.10s\n",
      " [-] epoch    6/250, train loss 0.560110 in 0.10s\n",
      " [-] epoch    7/250, train loss 0.572787 in 0.10s\n",
      " [-] epoch    8/250, train loss 0.577528 in 0.10s\n",
      " [-] epoch    9/250, train loss 0.571430 in 0.10s\n",
      " [-] epoch   10/250, train loss 0.516602 in 0.10s\n",
      " [-] epoch   11/250, train loss 0.537257 in 0.11s\n",
      " [-] epoch   12/250, train loss 0.532040 in 0.10s\n",
      " [-] epoch   13/250, train loss 0.537958 in 0.09s\n",
      " [-] epoch   14/250, train loss 0.537775 in 0.11s\n",
      " [-] epoch   15/250, train loss 0.524506 in 0.10s\n",
      " [-] epoch   16/250, train loss 0.542357 in 0.10s\n",
      " [-] epoch   17/250, train loss 0.538662 in 0.10s\n",
      " [-] epoch   18/250, train loss 0.549656 in 0.10s\n",
      " [-] epoch   19/250, train loss 0.540399 in 0.10s\n",
      " [-] epoch   20/250, train loss 0.504665 in 0.10s\n",
      " [-] epoch   21/250, train loss 0.489667 in 0.10s\n",
      " [-] epoch   22/250, train loss 0.485534 in 0.10s\n",
      " [-] epoch   23/250, train loss 0.500374 in 0.11s\n",
      " [-] epoch   24/250, train loss 0.503371 in 0.10s\n",
      " [-] epoch   25/250, train loss 0.475244 in 0.12s\n",
      " [-] epoch   26/250, train loss 0.471880 in 0.09s\n",
      " [-] epoch   27/250, train loss 0.488134 in 0.10s\n",
      " [-] epoch   28/250, train loss 0.502998 in 0.09s\n",
      " [-] epoch   29/250, train loss 0.481125 in 0.11s\n",
      " [-] epoch   30/250, train loss 0.500223 in 0.10s\n",
      " [-] epoch   31/250, train loss 0.495336 in 0.10s\n",
      " [-] epoch   32/250, train loss 0.466275 in 0.10s\n",
      " [-] epoch   33/250, train loss 0.497550 in 0.09s\n",
      " [-] epoch   34/250, train loss 0.492240 in 0.08s\n",
      " [-] epoch   35/250, train loss 0.472005 in 0.10s\n",
      " [-] epoch   36/250, train loss 0.485415 in 0.12s\n",
      " [-] epoch   37/250, train loss 0.476580 in 0.10s\n",
      " [-] epoch   38/250, train loss 0.470073 in 0.12s\n",
      " [-] epoch   39/250, train loss 0.487602 in 0.10s\n",
      " [-] epoch   40/250, train loss 0.495184 in 0.10s\n",
      " [-] epoch   41/250, train loss 0.491157 in 0.10s\n",
      " [-] epoch   42/250, train loss 0.471826 in 0.10s\n",
      " [-] epoch   43/250, train loss 0.494166 in 0.12s\n",
      " [-] epoch   44/250, train loss 0.454855 in 0.10s\n",
      " [-] epoch   45/250, train loss 0.475319 in 0.10s\n",
      " [-] epoch   46/250, train loss 0.460655 in 0.11s\n",
      " [-] epoch   47/250, train loss 0.488932 in 0.10s\n",
      " [-] epoch   48/250, train loss 0.471649 in 0.12s\n",
      " [-] epoch   49/250, train loss 0.493147 in 0.10s\n",
      " [-] epoch   50/250, train loss 0.436754 in 0.12s\n",
      " [-] epoch   51/250, train loss 0.468019 in 0.09s\n",
      " [-] epoch   52/250, train loss 0.474776 in 0.10s\n",
      " [-] epoch   53/250, train loss 0.446063 in 0.10s\n",
      " [-] epoch   54/250, train loss 0.448660 in 0.08s\n",
      " [-] epoch   55/250, train loss 0.462660 in 0.10s\n",
      " [-] epoch   56/250, train loss 0.424712 in 0.09s\n",
      " [-] epoch   57/250, train loss 0.450786 in 0.10s\n",
      " [-] epoch   58/250, train loss 0.470204 in 0.10s\n",
      " [-] epoch   59/250, train loss 0.454448 in 0.10s\n",
      " [-] epoch   60/250, train loss 0.446425 in 0.11s\n",
      " [-] epoch   61/250, train loss 0.455278 in 0.10s\n",
      " [-] epoch   62/250, train loss 0.468364 in 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   63/250, train loss 0.442928 in 0.10s\n",
      " [-] epoch   64/250, train loss 0.478056 in 0.10s\n",
      " [-] epoch   65/250, train loss 0.441860 in 0.11s\n",
      " [-] epoch   66/250, train loss 0.452658 in 0.10s\n",
      " [-] epoch   67/250, train loss 0.470496 in 0.10s\n",
      " [-] epoch   68/250, train loss 0.415058 in 0.09s\n",
      " [-] epoch   69/250, train loss 0.439415 in 0.11s\n",
      " [-] epoch   70/250, train loss 0.450365 in 0.07s\n",
      " [-] epoch   71/250, train loss 0.428636 in 0.12s\n",
      " [-] epoch   72/250, train loss 0.451562 in 0.10s\n",
      " [-] epoch   73/250, train loss 0.437038 in 0.10s\n",
      " [-] epoch   74/250, train loss 0.466768 in 0.08s\n",
      " [-] epoch   75/250, train loss 0.429558 in 0.12s\n",
      " [-] epoch   76/250, train loss 0.462830 in 0.09s\n",
      " [-] epoch   77/250, train loss 0.466094 in 0.10s\n",
      " [-] epoch   78/250, train loss 0.419938 in 0.12s\n",
      " [-] epoch   79/250, train loss 0.460999 in 0.09s\n",
      " [-] epoch   80/250, train loss 0.464561 in 0.08s\n",
      " [-] epoch   81/250, train loss 0.455472 in 0.10s\n",
      " [-] epoch   82/250, train loss 0.408868 in 0.10s\n",
      " [-] epoch   83/250, train loss 0.435043 in 0.10s\n",
      " [-] epoch   84/250, train loss 0.469828 in 0.10s\n",
      " [-] epoch   85/250, train loss 0.448095 in 0.09s\n",
      " [-] epoch   86/250, train loss 0.449363 in 0.10s\n",
      " [-] epoch   87/250, train loss 0.438970 in 0.10s\n",
      " [-] epoch   88/250, train loss 0.407932 in 0.10s\n",
      " [-] epoch   89/250, train loss 0.458389 in 0.10s\n",
      " [-] epoch   90/250, train loss 0.463845 in 0.10s\n",
      " [-] epoch   91/250, train loss 0.452448 in 0.10s\n",
      " [-] epoch   92/250, train loss 0.417429 in 0.10s\n",
      " [-] epoch   93/250, train loss 0.434034 in 0.10s\n",
      " [-] epoch   94/250, train loss 0.453756 in 0.10s\n",
      " [-] epoch   95/250, train loss 0.424353 in 0.11s\n",
      " [-] epoch   96/250, train loss 0.432364 in 0.10s\n",
      " [-] epoch   97/250, train loss 0.401676 in 0.12s\n",
      " [-] epoch   98/250, train loss 0.441823 in 0.10s\n",
      " [-] epoch   99/250, train loss 0.419784 in 0.10s\n",
      " [-] epoch  100/250, train loss 0.426942 in 0.10s\n",
      " [-] epoch  101/250, train loss 0.451137 in 0.10s\n",
      " [-] epoch  102/250, train loss 0.423333 in 0.10s\n",
      " [-] epoch  103/250, train loss 0.395726 in 0.10s\n",
      " [-] epoch  104/250, train loss 0.442107 in 0.10s\n",
      " [-] epoch  105/250, train loss 0.462313 in 0.10s\n",
      " [-] epoch  106/250, train loss 0.452132 in 0.10s\n",
      " [-] epoch  107/250, train loss 0.395369 in 0.10s\n",
      " [-] epoch  108/250, train loss 0.414157 in 0.10s\n",
      " [-] epoch  109/250, train loss 0.448952 in 0.10s\n",
      " [-] epoch  110/250, train loss 0.424345 in 0.10s\n",
      " [-] epoch  111/250, train loss 0.446052 in 0.10s\n",
      " [-] epoch  112/250, train loss 0.412066 in 0.10s\n",
      " [-] epoch  113/250, train loss 0.423501 in 0.10s\n",
      " [-] epoch  114/250, train loss 0.410083 in 0.10s\n",
      " [-] epoch  115/250, train loss 0.430310 in 0.10s\n",
      " [-] epoch  116/250, train loss 0.429614 in 0.09s\n",
      " [-] epoch  117/250, train loss 0.406950 in 0.10s\n",
      " [-] epoch  118/250, train loss 0.434178 in 0.10s\n",
      " [-] epoch  119/250, train loss 0.410094 in 0.10s\n",
      " [-] epoch  120/250, train loss 0.408512 in 0.12s\n",
      " [-] epoch  121/250, train loss 0.421080 in 0.08s\n",
      " [-] epoch  122/250, train loss 0.470939 in 0.10s\n",
      " [-] epoch  123/250, train loss 0.438718 in 0.12s\n",
      " [-] epoch  124/250, train loss 0.432962 in 0.10s\n",
      " [-] epoch  125/250, train loss 0.402845 in 0.10s\n",
      " [-] epoch  126/250, train loss 0.440729 in 0.10s\n",
      " [-] epoch  127/250, train loss 0.421819 in 0.12s\n",
      " [-] epoch  128/250, train loss 0.432973 in 0.08s\n",
      " [-] epoch  129/250, train loss 0.427608 in 0.10s\n",
      " [-] epoch  130/250, train loss 0.406727 in 0.10s\n",
      " [-] epoch  131/250, train loss 0.416766 in 0.12s\n",
      " [-] epoch  132/250, train loss 0.411290 in 0.09s\n",
      " [-] epoch  133/250, train loss 0.406499 in 0.10s\n",
      " [-] epoch  134/250, train loss 0.382983 in 0.10s\n",
      " [-] epoch  135/250, train loss 0.393388 in 0.10s\n",
      " [-] epoch  136/250, train loss 0.425729 in 0.10s\n",
      " [-] epoch  137/250, train loss 0.393324 in 0.12s\n",
      " [-] epoch  138/250, train loss 0.385447 in 0.10s\n",
      " [-] epoch  139/250, train loss 0.432801 in 0.10s\n",
      " [-] epoch  140/250, train loss 0.472111 in 0.10s\n",
      " [-] epoch  141/250, train loss 0.391695 in 0.12s\n",
      " [-] epoch  142/250, train loss 0.428763 in 0.10s\n",
      " [-] epoch  143/250, train loss 0.407016 in 0.10s\n",
      " [-] epoch  144/250, train loss 0.409232 in 0.10s\n",
      " [-] epoch  145/250, train loss 0.435181 in 0.10s\n",
      " [-] epoch  146/250, train loss 0.399112 in 0.12s\n",
      " [-] epoch  147/250, train loss 0.399358 in 0.10s\n",
      " [-] epoch  148/250, train loss 0.406096 in 0.10s\n",
      " [-] epoch  149/250, train loss 0.412540 in 0.10s\n",
      " [-] epoch  150/250, train loss 0.415653 in 0.10s\n",
      " [-] epoch  151/250, train loss 0.396426 in 0.10s\n",
      " [-] epoch  152/250, train loss 0.400975 in 0.10s\n",
      " [-] epoch  153/250, train loss 0.389756 in 0.12s\n",
      " [-] epoch  154/250, train loss 0.401108 in 0.10s\n",
      " [-] epoch  155/250, train loss 0.397330 in 0.12s\n",
      " [-] epoch  156/250, train loss 0.386443 in 0.09s\n",
      " [-] epoch  157/250, train loss 0.406350 in 0.11s\n",
      " [-] epoch  158/250, train loss 0.418164 in 0.09s\n",
      " [-] epoch  159/250, train loss 0.424400 in 0.10s\n",
      " [-] epoch  160/250, train loss 0.393149 in 0.12s\n",
      " [-] epoch  161/250, train loss 0.382384 in 0.10s\n",
      " [-] epoch  162/250, train loss 0.369747 in 0.10s\n",
      " [-] epoch  163/250, train loss 0.402443 in 0.10s\n",
      " [-] epoch  164/250, train loss 0.420917 in 0.10s\n",
      " [-] epoch  165/250, train loss 0.413583 in 0.10s\n",
      " [-] epoch  166/250, train loss 0.404912 in 0.10s\n",
      " [-] epoch  167/250, train loss 0.382719 in 0.12s\n",
      " [-] epoch  168/250, train loss 0.380199 in 0.10s\n",
      " [-] epoch  169/250, train loss 0.389666 in 0.10s\n",
      " [-] epoch  170/250, train loss 0.400092 in 0.10s\n",
      " [-] epoch  171/250, train loss 0.394258 in 0.10s\n",
      " [-] epoch  172/250, train loss 0.414525 in 0.10s\n",
      " [-] epoch  173/250, train loss 0.396683 in 0.10s\n",
      " [-] epoch  174/250, train loss 0.417338 in 0.10s\n",
      " [-] epoch  175/250, train loss 0.398613 in 0.08s\n",
      " [-] epoch  176/250, train loss 0.404084 in 0.12s\n",
      " [-] epoch  177/250, train loss 0.386525 in 0.10s\n",
      " [-] epoch  178/250, train loss 0.410666 in 0.10s\n",
      " [-] epoch  179/250, train loss 0.409516 in 0.12s\n",
      " [-] epoch  180/250, train loss 0.409944 in 0.10s\n",
      " [-] epoch  181/250, train loss 0.363348 in 0.09s\n",
      " [-] epoch  182/250, train loss 0.407112 in 0.11s\n",
      " [-] epoch  183/250, train loss 0.392566 in 0.09s\n",
      " [-] epoch  184/250, train loss 0.375018 in 0.10s\n",
      " [-] epoch  185/250, train loss 0.393433 in 0.12s\n",
      " [-] epoch  186/250, train loss 0.373877 in 0.12s\n",
      " [-] epoch  187/250, train loss 0.399458 in 0.09s\n",
      " [-] epoch  188/250, train loss 0.381504 in 0.10s\n",
      " [-] epoch  189/250, train loss 0.371683 in 0.12s\n",
      " [-] epoch  190/250, train loss 0.393950 in 0.10s\n",
      " [-] epoch  191/250, train loss 0.393999 in 0.12s\n",
      " [-] epoch  192/250, train loss 0.369380 in 0.10s\n",
      " [-] epoch  193/250, train loss 0.347346 in 0.12s\n",
      " [-] epoch  194/250, train loss 0.386617 in 0.10s\n",
      " [-] epoch  195/250, train loss 0.372598 in 0.08s\n",
      " [-] epoch  196/250, train loss 0.399916 in 0.10s\n",
      " [-] epoch  197/250, train loss 0.397371 in 0.11s\n",
      " [-] epoch  198/250, train loss 0.392521 in 0.10s\n",
      " [-] epoch  199/250, train loss 0.376803 in 0.10s\n",
      " [-] epoch  200/250, train loss 0.406730 in 0.10s\n",
      " [-] epoch  201/250, train loss 0.392827 in 0.10s\n",
      " [-] epoch  202/250, train loss 0.385641 in 0.09s\n",
      " [-] epoch  203/250, train loss 0.370650 in 0.10s\n",
      " [-] epoch  204/250, train loss 0.381006 in 0.12s\n",
      " [-] epoch  205/250, train loss 0.380681 in 0.10s\n",
      " [-] epoch  206/250, train loss 0.378871 in 0.10s\n",
      " [-] epoch  207/250, train loss 0.384159 in 0.08s\n",
      " [-] epoch  208/250, train loss 0.387026 in 0.10s\n",
      " [-] epoch  209/250, train loss 0.391346 in 0.10s\n",
      " [-] epoch  210/250, train loss 0.396329 in 0.10s\n",
      " [-] epoch  211/250, train loss 0.379049 in 0.12s\n",
      " [-] epoch  212/250, train loss 0.373236 in 0.10s\n",
      " [-] epoch  213/250, train loss 0.422447 in 0.10s\n",
      " [-] epoch  214/250, train loss 0.377928 in 0.10s\n",
      " [-] epoch  215/250, train loss 0.396559 in 0.12s\n",
      " [-] epoch  216/250, train loss 0.364873 in 0.10s\n",
      " [-] epoch  217/250, train loss 0.387891 in 0.08s\n",
      " [-] epoch  218/250, train loss 0.399856 in 0.10s\n",
      " [-] epoch  219/250, train loss 0.390366 in 0.10s\n",
      " [-] epoch  220/250, train loss 0.369223 in 0.10s\n",
      " [-] epoch  221/250, train loss 0.377268 in 0.10s\n",
      " [-] epoch  222/250, train loss 0.372376 in 0.10s\n",
      " [-] epoch  223/250, train loss 0.377972 in 0.10s\n",
      " [-] epoch  224/250, train loss 0.388948 in 0.12s\n",
      " [-] epoch  225/250, train loss 0.383893 in 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.356430 in 0.09s\n",
      " [-] epoch  227/250, train loss 0.369163 in 0.10s\n",
      " [-] epoch  228/250, train loss 0.365190 in 0.10s\n",
      " [-] epoch  229/250, train loss 0.366757 in 0.10s\n",
      " [-] epoch  230/250, train loss 0.381251 in 0.10s\n",
      " [-] epoch  231/250, train loss 0.379529 in 0.10s\n",
      " [-] epoch  232/250, train loss 0.371153 in 0.10s\n",
      " [-] epoch  233/250, train loss 0.390877 in 0.10s\n",
      " [-] epoch  234/250, train loss 0.420987 in 0.10s\n",
      " [-] epoch  235/250, train loss 0.353959 in 0.10s\n",
      " [-] epoch  236/250, train loss 0.386336 in 0.10s\n",
      " [-] epoch  237/250, train loss 0.376918 in 0.08s\n",
      " [-] epoch  238/250, train loss 0.375999 in 0.10s\n",
      " [-] epoch  239/250, train loss 0.362319 in 0.10s\n",
      " [-] epoch  240/250, train loss 0.395247 in 0.08s\n",
      " [-] epoch  241/250, train loss 0.382386 in 0.10s\n",
      " [-] epoch  242/250, train loss 0.364389 in 0.10s\n",
      " [-] epoch  243/250, train loss 0.357010 in 0.08s\n",
      " [-] epoch  244/250, train loss 0.356128 in 0.10s\n",
      " [-] epoch  245/250, train loss 0.380358 in 0.11s\n",
      " [-] epoch  246/250, train loss 0.369812 in 0.10s\n",
      " [-] epoch  247/250, train loss 0.350828 in 0.12s\n",
      " [-] epoch  248/250, train loss 0.380638 in 0.08s\n",
      " [-] epoch  249/250, train loss 0.371734 in 0.10s\n",
      " [-] epoch  250/250, train loss 0.378084 in 0.10s\n",
      " [-] test acc. 67.777778%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.675728 in 0.09s\n",
      " [-] epoch    2/250, train loss 0.610299 in 0.12s\n",
      " [-] epoch    3/250, train loss 0.594076 in 0.10s\n",
      " [-] epoch    4/250, train loss 0.580070 in 0.10s\n",
      " [-] epoch    5/250, train loss 0.566956 in 0.11s\n",
      " [-] epoch    6/250, train loss 0.558914 in 0.10s\n",
      " [-] epoch    7/250, train loss 0.543626 in 0.12s\n",
      " [-] epoch    8/250, train loss 0.553870 in 0.12s\n",
      " [-] epoch    9/250, train loss 0.530755 in 0.10s\n",
      " [-] epoch   10/250, train loss 0.536753 in 0.12s\n",
      " [-] epoch   11/250, train loss 0.533222 in 0.10s\n",
      " [-] epoch   12/250, train loss 0.528510 in 0.10s\n",
      " [-] epoch   13/250, train loss 0.513517 in 0.10s\n",
      " [-] epoch   14/250, train loss 0.510861 in 0.10s\n",
      " [-] epoch   15/250, train loss 0.533884 in 0.12s\n",
      " [-] epoch   16/250, train loss 0.498877 in 0.10s\n",
      " [-] epoch   17/250, train loss 0.488429 in 0.12s\n",
      " [-] epoch   18/250, train loss 0.522799 in 0.12s\n",
      " [-] epoch   19/250, train loss 0.499112 in 0.10s\n",
      " [-] epoch   20/250, train loss 0.511899 in 0.12s\n",
      " [-] epoch   21/250, train loss 0.513920 in 0.10s\n",
      " [-] epoch   22/250, train loss 0.497953 in 0.10s\n",
      " [-] epoch   23/250, train loss 0.468398 in 0.10s\n",
      " [-] epoch   24/250, train loss 0.488239 in 0.12s\n",
      " [-] epoch   25/250, train loss 0.526633 in 0.12s\n",
      " [-] epoch   26/250, train loss 0.487708 in 0.10s\n",
      " [-] epoch   27/250, train loss 0.479536 in 0.12s\n",
      " [-] epoch   28/250, train loss 0.476585 in 0.12s\n",
      " [-] epoch   29/250, train loss 0.482957 in 0.10s\n",
      " [-] epoch   30/250, train loss 0.485617 in 0.12s\n",
      " [-] epoch   31/250, train loss 0.499027 in 0.12s\n",
      " [-] epoch   32/250, train loss 0.478965 in 0.10s\n",
      " [-] epoch   33/250, train loss 0.458366 in 0.10s\n",
      " [-] epoch   34/250, train loss 0.453700 in 0.10s\n",
      " [-] epoch   35/250, train loss 0.458035 in 0.12s\n",
      " [-] epoch   36/250, train loss 0.490967 in 0.10s\n",
      " [-] epoch   37/250, train loss 0.467686 in 0.10s\n",
      " [-] epoch   38/250, train loss 0.438734 in 0.12s\n",
      " [-] epoch   39/250, train loss 0.469139 in 0.09s\n",
      " [-] epoch   40/250, train loss 0.480748 in 0.11s\n",
      " [-] epoch   41/250, train loss 0.454897 in 0.12s\n",
      " [-] epoch   42/250, train loss 0.476342 in 0.10s\n",
      " [-] epoch   43/250, train loss 0.449595 in 0.12s\n",
      " [-] epoch   44/250, train loss 0.446602 in 0.10s\n",
      " [-] epoch   45/250, train loss 0.455629 in 0.12s\n",
      " [-] epoch   46/250, train loss 0.453429 in 0.10s\n",
      " [-] epoch   47/250, train loss 0.451046 in 0.10s\n",
      " [-] epoch   48/250, train loss 0.429979 in 0.10s\n",
      " [-] epoch   49/250, train loss 0.434059 in 0.10s\n",
      " [-] epoch   50/250, train loss 0.437016 in 0.11s\n",
      " [-] epoch   51/250, train loss 0.486484 in 0.10s\n",
      " [-] epoch   52/250, train loss 0.470395 in 0.11s\n",
      " [-] epoch   53/250, train loss 0.440742 in 0.11s\n",
      " [-] epoch   54/250, train loss 0.441045 in 0.11s\n",
      " [-] epoch   55/250, train loss 0.449131 in 0.10s\n",
      " [-] epoch   56/250, train loss 0.424765 in 0.11s\n",
      " [-] epoch   57/250, train loss 0.443809 in 0.12s\n",
      " [-] epoch   58/250, train loss 0.454618 in 0.12s\n",
      " [-] epoch   59/250, train loss 0.447470 in 0.11s\n",
      " [-] epoch   60/250, train loss 0.434203 in 0.11s\n",
      " [-] epoch   61/250, train loss 0.445585 in 0.11s\n",
      " [-] epoch   62/250, train loss 0.435058 in 0.09s\n",
      " [-] epoch   63/250, train loss 0.439174 in 0.10s\n",
      " [-] epoch   64/250, train loss 0.429349 in 0.12s\n",
      " [-] epoch   65/250, train loss 0.404544 in 0.12s\n",
      " [-] epoch   66/250, train loss 0.419276 in 0.10s\n",
      " [-] epoch   67/250, train loss 0.423254 in 0.12s\n",
      " [-] epoch   68/250, train loss 0.437641 in 0.10s\n",
      " [-] epoch   69/250, train loss 0.461309 in 0.12s\n",
      " [-] epoch   70/250, train loss 0.430245 in 0.12s\n",
      " [-] epoch   71/250, train loss 0.408915 in 0.10s\n",
      " [-] epoch   72/250, train loss 0.476359 in 0.12s\n",
      " [-] epoch   73/250, train loss 0.448584 in 0.10s\n",
      " [-] epoch   74/250, train loss 0.430039 in 0.12s\n",
      " [-] epoch   75/250, train loss 0.421965 in 0.10s\n",
      " [-] epoch   76/250, train loss 0.422017 in 0.10s\n",
      " [-] epoch   77/250, train loss 0.423247 in 0.12s\n",
      " [-] epoch   78/250, train loss 0.395594 in 0.12s\n",
      " [-] epoch   79/250, train loss 0.401282 in 0.12s\n",
      " [-] epoch   80/250, train loss 0.406227 in 0.10s\n",
      " [-] epoch   81/250, train loss 0.432507 in 0.11s\n",
      " [-] epoch   82/250, train loss 0.446385 in 0.10s\n",
      " [-] epoch   83/250, train loss 0.407552 in 0.12s\n",
      " [-] epoch   84/250, train loss 0.412814 in 0.12s\n",
      " [-] epoch   85/250, train loss 0.407970 in 0.10s\n",
      " [-] epoch   86/250, train loss 0.444888 in 0.10s\n",
      " [-] epoch   87/250, train loss 0.444605 in 0.10s\n",
      " [-] epoch   88/250, train loss 0.404024 in 0.10s\n",
      " [-] epoch   89/250, train loss 0.410534 in 0.10s\n",
      " [-] epoch   90/250, train loss 0.427321 in 0.08s\n",
      " [-] epoch   91/250, train loss 0.391939 in 0.11s\n",
      " [-] epoch   92/250, train loss 0.426292 in 0.10s\n",
      " [-] epoch   93/250, train loss 0.399605 in 0.10s\n",
      " [-] epoch   94/250, train loss 0.380631 in 0.12s\n",
      " [-] epoch   95/250, train loss 0.424717 in 0.12s\n",
      " [-] epoch   96/250, train loss 0.421356 in 0.12s\n",
      " [-] epoch   97/250, train loss 0.426341 in 0.10s\n",
      " [-] epoch   98/250, train loss 0.408414 in 0.10s\n",
      " [-] epoch   99/250, train loss 0.425799 in 0.10s\n",
      " [-] epoch  100/250, train loss 0.410257 in 0.12s\n",
      " [-] epoch  101/250, train loss 0.403058 in 0.12s\n",
      " [-] epoch  102/250, train loss 0.398433 in 0.12s\n",
      " [-] epoch  103/250, train loss 0.385650 in 0.10s\n",
      " [-] epoch  104/250, train loss 0.410512 in 0.10s\n",
      " [-] epoch  105/250, train loss 0.385566 in 0.10s\n",
      " [-] epoch  106/250, train loss 0.411643 in 0.10s\n",
      " [-] epoch  107/250, train loss 0.403195 in 0.12s\n",
      " [-] epoch  108/250, train loss 0.445164 in 0.12s\n",
      " [-] epoch  109/250, train loss 0.386016 in 0.10s\n",
      " [-] epoch  110/250, train loss 0.376715 in 0.08s\n",
      " [-] epoch  111/250, train loss 0.423243 in 0.10s\n",
      " [-] epoch  112/250, train loss 0.379654 in 0.12s\n",
      " [-] epoch  113/250, train loss 0.423482 in 0.12s\n",
      " [-] epoch  114/250, train loss 0.438985 in 0.10s\n",
      " [-] epoch  115/250, train loss 0.392254 in 0.11s\n",
      " [-] epoch  116/250, train loss 0.372945 in 0.12s\n",
      " [-] epoch  117/250, train loss 0.357823 in 0.12s\n",
      " [-] epoch  118/250, train loss 0.402252 in 0.10s\n",
      " [-] epoch  119/250, train loss 0.374721 in 0.10s\n",
      " [-] epoch  120/250, train loss 0.386216 in 0.10s\n",
      " [-] epoch  121/250, train loss 0.384759 in 0.12s\n",
      " [-] epoch  122/250, train loss 0.424209 in 0.10s\n",
      " [-] epoch  123/250, train loss 0.379600 in 0.10s\n",
      " [-] epoch  124/250, train loss 0.382955 in 0.08s\n",
      " [-] epoch  125/250, train loss 0.379109 in 0.12s\n",
      " [-] epoch  126/250, train loss 0.397707 in 0.12s\n",
      " [-] epoch  127/250, train loss 0.387699 in 0.09s\n",
      " [-] epoch  128/250, train loss 0.410690 in 0.10s\n",
      " [-] epoch  129/250, train loss 0.385196 in 0.10s\n",
      " [-] epoch  130/250, train loss 0.385199 in 0.12s\n",
      " [-] epoch  131/250, train loss 0.382955 in 0.12s\n",
      " [-] epoch  132/250, train loss 0.389941 in 0.10s\n",
      " [-] epoch  133/250, train loss 0.398109 in 0.11s\n",
      " [-] epoch  134/250, train loss 0.382485 in 0.12s\n",
      " [-] epoch  135/250, train loss 0.376573 in 0.12s\n",
      " [-] epoch  136/250, train loss 0.415381 in 0.12s\n",
      " [-] epoch  137/250, train loss 0.368211 in 0.12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  138/250, train loss 0.401596 in 0.12s\n",
      " [-] epoch  139/250, train loss 0.379374 in 0.10s\n",
      " [-] epoch  140/250, train loss 0.364866 in 0.08s\n",
      " [-] epoch  141/250, train loss 0.382403 in 0.10s\n",
      " [-] epoch  142/250, train loss 0.381783 in 0.12s\n",
      " [-] epoch  143/250, train loss 0.372639 in 0.10s\n",
      " [-] epoch  144/250, train loss 0.396357 in 0.10s\n",
      " [-] epoch  145/250, train loss 0.386781 in 0.10s\n",
      " [-] epoch  146/250, train loss 0.377658 in 0.10s\n",
      " [-] epoch  147/250, train loss 0.372313 in 0.12s\n",
      " [-] epoch  148/250, train loss 0.379541 in 0.11s\n",
      " [-] epoch  149/250, train loss 0.379702 in 0.10s\n",
      " [-] epoch  150/250, train loss 0.390544 in 0.10s\n",
      " [-] epoch  151/250, train loss 0.370317 in 0.10s\n",
      " [-] epoch  152/250, train loss 0.347087 in 0.10s\n",
      " [-] epoch  153/250, train loss 0.357866 in 0.10s\n",
      " [-] epoch  154/250, train loss 0.368521 in 0.10s\n",
      " [-] epoch  155/250, train loss 0.348949 in 0.11s\n",
      " [-] epoch  156/250, train loss 0.365238 in 0.12s\n",
      " [-] epoch  157/250, train loss 0.357464 in 0.10s\n",
      " [-] epoch  158/250, train loss 0.379779 in 0.12s\n",
      " [-] epoch  159/250, train loss 0.387327 in 0.09s\n",
      " [-] epoch  160/250, train loss 0.398615 in 0.12s\n",
      " [-] epoch  161/250, train loss 0.373480 in 0.12s\n",
      " [-] epoch  162/250, train loss 0.373901 in 0.10s\n",
      " [-] epoch  163/250, train loss 0.373543 in 0.09s\n",
      " [-] epoch  164/250, train loss 0.389828 in 0.10s\n",
      " [-] epoch  165/250, train loss 0.373138 in 0.12s\n",
      " [-] epoch  166/250, train loss 0.362963 in 0.10s\n",
      " [-] epoch  167/250, train loss 0.352043 in 0.09s\n",
      " [-] epoch  168/250, train loss 0.381854 in 0.11s\n",
      " [-] epoch  169/250, train loss 0.384813 in 0.09s\n",
      " [-] epoch  170/250, train loss 0.356508 in 0.10s\n",
      " [-] epoch  171/250, train loss 0.388166 in 0.11s\n",
      " [-] epoch  172/250, train loss 0.402748 in 0.12s\n",
      " [-] epoch  173/250, train loss 0.373305 in 0.12s\n",
      " [-] epoch  174/250, train loss 0.359335 in 0.10s\n",
      " [-] epoch  175/250, train loss 0.328837 in 0.09s\n",
      " [-] epoch  176/250, train loss 0.361711 in 0.12s\n",
      " [-] epoch  177/250, train loss 0.358453 in 0.12s\n",
      " [-] epoch  178/250, train loss 0.364358 in 0.12s\n",
      " [-] epoch  179/250, train loss 0.348716 in 0.10s\n",
      " [-] epoch  180/250, train loss 0.359396 in 0.12s\n",
      " [-] epoch  181/250, train loss 0.378105 in 0.08s\n",
      " [-] epoch  182/250, train loss 0.391651 in 0.09s\n",
      " [-] epoch  183/250, train loss 0.382809 in 0.12s\n",
      " [-] epoch  184/250, train loss 0.366884 in 0.12s\n",
      " [-] epoch  185/250, train loss 0.372256 in 0.12s\n",
      " [-] epoch  186/250, train loss 0.385170 in 0.12s\n",
      " [-] epoch  187/250, train loss 0.378148 in 0.10s\n",
      " [-] epoch  188/250, train loss 0.353807 in 0.12s\n",
      " [-] epoch  189/250, train loss 0.350014 in 0.12s\n",
      " [-] epoch  190/250, train loss 0.329725 in 0.12s\n",
      " [-] epoch  191/250, train loss 0.345030 in 0.12s\n",
      " [-] epoch  192/250, train loss 0.342435 in 0.09s\n",
      " [-] epoch  193/250, train loss 0.372414 in 0.10s\n",
      " [-] epoch  194/250, train loss 0.348646 in 0.08s\n",
      " [-] epoch  195/250, train loss 0.366715 in 0.10s\n",
      " [-] epoch  196/250, train loss 0.349889 in 0.10s\n",
      " [-] epoch  197/250, train loss 0.372402 in 0.12s\n",
      " [-] epoch  198/250, train loss 0.376373 in 0.09s\n",
      " [-] epoch  199/250, train loss 0.375287 in 0.12s\n",
      " [-] epoch  200/250, train loss 0.366848 in 0.10s\n",
      " [-] epoch  201/250, train loss 0.389619 in 0.10s\n",
      " [-] epoch  202/250, train loss 0.343016 in 0.10s\n",
      " [-] epoch  203/250, train loss 0.369042 in 0.12s\n",
      " [-] epoch  204/250, train loss 0.330681 in 0.12s\n",
      " [-] epoch  205/250, train loss 0.357666 in 0.10s\n",
      " [-] epoch  206/250, train loss 0.358665 in 0.10s\n",
      " [-] epoch  207/250, train loss 0.337802 in 0.10s\n",
      " [-] epoch  208/250, train loss 0.375017 in 0.12s\n",
      " [-] epoch  209/250, train loss 0.372409 in 0.10s\n",
      " [-] epoch  210/250, train loss 0.401812 in 0.10s\n",
      " [-] epoch  211/250, train loss 0.358385 in 0.10s\n",
      " [-] epoch  212/250, train loss 0.334804 in 0.10s\n",
      " [-] epoch  213/250, train loss 0.356060 in 0.12s\n",
      " [-] epoch  214/250, train loss 0.341297 in 0.12s\n",
      " [-] epoch  215/250, train loss 0.334334 in 0.10s\n",
      " [-] epoch  216/250, train loss 0.344362 in 0.12s\n",
      " [-] epoch  217/250, train loss 0.331554 in 0.10s\n",
      " [-] epoch  218/250, train loss 0.362871 in 0.12s\n",
      " [-] epoch  219/250, train loss 0.350422 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.357700 in 0.10s\n",
      " [-] epoch  221/250, train loss 0.353952 in 0.12s\n",
      " [-] epoch  222/250, train loss 0.335369 in 0.11s\n",
      " [-] epoch  223/250, train loss 0.374868 in 0.11s\n",
      " [-] epoch  224/250, train loss 0.319339 in 0.11s\n",
      " [-] epoch  225/250, train loss 0.338787 in 0.10s\n",
      " [-] epoch  226/250, train loss 0.349019 in 0.11s\n",
      " [-] epoch  227/250, train loss 0.361141 in 0.11s\n",
      " [-] epoch  228/250, train loss 0.341908 in 0.12s\n",
      " [-] epoch  229/250, train loss 0.340297 in 0.11s\n",
      " [-] epoch  230/250, train loss 0.362510 in 0.11s\n",
      " [-] epoch  231/250, train loss 0.350946 in 0.11s\n",
      " [-] epoch  232/250, train loss 0.375958 in 0.11s\n",
      " [-] epoch  233/250, train loss 0.364915 in 0.12s\n",
      " [-] epoch  234/250, train loss 0.362613 in 0.11s\n",
      " [-] epoch  235/250, train loss 0.350164 in 0.12s\n",
      " [-] epoch  236/250, train loss 0.325071 in 0.11s\n",
      " [-] epoch  237/250, train loss 0.332361 in 0.10s\n",
      " [-] epoch  238/250, train loss 0.370970 in 0.11s\n",
      " [-] epoch  239/250, train loss 0.339416 in 0.11s\n",
      " [-] epoch  240/250, train loss 0.348031 in 0.12s\n",
      " [-] epoch  241/250, train loss 0.342388 in 0.10s\n",
      " [-] epoch  242/250, train loss 0.334227 in 0.11s\n",
      " [-] epoch  243/250, train loss 0.369046 in 0.12s\n",
      " [-] epoch  244/250, train loss 0.355747 in 0.12s\n",
      " [-] epoch  245/250, train loss 0.327414 in 0.11s\n",
      " [-] epoch  246/250, train loss 0.356227 in 0.10s\n",
      " [-] epoch  247/250, train loss 0.340939 in 0.11s\n",
      " [-] epoch  248/250, train loss 0.328591 in 0.12s\n",
      " [-] epoch  249/250, train loss 0.365327 in 0.11s\n",
      " [-] epoch  250/250, train loss 0.341336 in 0.11s\n",
      " [-] test acc. 63.888889%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.651054 in 0.12s\n",
      " [-] epoch    2/250, train loss 0.618170 in 0.14s\n",
      " [-] epoch    3/250, train loss 0.585431 in 0.14s\n",
      " [-] epoch    4/250, train loss 0.571768 in 0.14s\n",
      " [-] epoch    5/250, train loss 0.580078 in 0.13s\n",
      " [-] epoch    6/250, train loss 0.559252 in 0.14s\n",
      " [-] epoch    7/250, train loss 0.537988 in 0.14s\n",
      " [-] epoch    8/250, train loss 0.527827 in 0.14s\n",
      " [-] epoch    9/250, train loss 0.518669 in 0.14s\n",
      " [-] epoch   10/250, train loss 0.519223 in 0.13s\n",
      " [-] epoch   11/250, train loss 0.529429 in 0.14s\n",
      " [-] epoch   12/250, train loss 0.518577 in 0.15s\n",
      " [-] epoch   13/250, train loss 0.497364 in 0.13s\n",
      " [-] epoch   14/250, train loss 0.511910 in 0.15s\n",
      " [-] epoch   15/250, train loss 0.539694 in 0.13s\n",
      " [-] epoch   16/250, train loss 0.510635 in 0.13s\n",
      " [-] epoch   17/250, train loss 0.500589 in 0.13s\n",
      " [-] epoch   18/250, train loss 0.487469 in 0.12s\n",
      " [-] epoch   19/250, train loss 0.499690 in 0.13s\n",
      " [-] epoch   20/250, train loss 0.485887 in 0.14s\n",
      " [-] epoch   21/250, train loss 0.477144 in 0.13s\n",
      " [-] epoch   22/250, train loss 0.476151 in 0.14s\n",
      " [-] epoch   23/250, train loss 0.460618 in 0.13s\n",
      " [-] epoch   24/250, train loss 0.468006 in 0.15s\n",
      " [-] epoch   25/250, train loss 0.495251 in 0.13s\n",
      " [-] epoch   26/250, train loss 0.468826 in 0.13s\n",
      " [-] epoch   27/250, train loss 0.467059 in 0.12s\n",
      " [-] epoch   28/250, train loss 0.473697 in 0.15s\n",
      " [-] epoch   29/250, train loss 0.463124 in 0.15s\n",
      " [-] epoch   30/250, train loss 0.466042 in 0.13s\n",
      " [-] epoch   31/250, train loss 0.469891 in 0.12s\n",
      " [-] epoch   32/250, train loss 0.456745 in 0.13s\n",
      " [-] epoch   33/250, train loss 0.465833 in 0.15s\n",
      " [-] epoch   34/250, train loss 0.442906 in 0.12s\n",
      " [-] epoch   35/250, train loss 0.440887 in 0.15s\n",
      " [-] epoch   36/250, train loss 0.449348 in 0.14s\n",
      " [-] epoch   37/250, train loss 0.450087 in 0.13s\n",
      " [-] epoch   38/250, train loss 0.469949 in 0.15s\n",
      " [-] epoch   39/250, train loss 0.416800 in 0.13s\n",
      " [-] epoch   40/250, train loss 0.460208 in 0.12s\n",
      " [-] epoch   41/250, train loss 0.431692 in 0.13s\n",
      " [-] epoch   42/250, train loss 0.449938 in 0.13s\n",
      " [-] epoch   43/250, train loss 0.442401 in 0.15s\n",
      " [-] epoch   44/250, train loss 0.418981 in 0.12s\n",
      " [-] epoch   45/250, train loss 0.453192 in 0.15s\n",
      " [-] epoch   46/250, train loss 0.444901 in 0.12s\n",
      " [-] epoch   47/250, train loss 0.430175 in 0.13s\n",
      " [-] epoch   48/250, train loss 0.452437 in 0.13s\n",
      " [-] epoch   49/250, train loss 0.443799 in 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.447575 in 0.15s\n",
      " [-] epoch   51/250, train loss 0.444593 in 0.13s\n",
      " [-] epoch   52/250, train loss 0.436091 in 0.14s\n",
      " [-] epoch   53/250, train loss 0.458339 in 0.13s\n",
      " [-] epoch   54/250, train loss 0.424010 in 0.13s\n",
      " [-] epoch   55/250, train loss 0.435138 in 0.12s\n",
      " [-] epoch   56/250, train loss 0.414060 in 0.12s\n",
      " [-] epoch   57/250, train loss 0.392465 in 0.13s\n",
      " [-] epoch   58/250, train loss 0.415878 in 0.13s\n",
      " [-] epoch   59/250, train loss 0.437655 in 0.13s\n",
      " [-] epoch   60/250, train loss 0.414555 in 0.13s\n",
      " [-] epoch   61/250, train loss 0.427001 in 0.13s\n",
      " [-] epoch   62/250, train loss 0.401383 in 0.15s\n",
      " [-] epoch   63/250, train loss 0.424540 in 0.13s\n",
      " [-] epoch   64/250, train loss 0.422269 in 0.13s\n",
      " [-] epoch   65/250, train loss 0.413465 in 0.13s\n",
      " [-] epoch   66/250, train loss 0.417428 in 0.13s\n",
      " [-] epoch   67/250, train loss 0.409264 in 0.13s\n",
      " [-] epoch   68/250, train loss 0.432765 in 0.15s\n",
      " [-] epoch   69/250, train loss 0.447049 in 0.13s\n",
      " [-] epoch   70/250, train loss 0.414171 in 0.14s\n",
      " [-] epoch   71/250, train loss 0.385827 in 0.13s\n",
      " [-] epoch   72/250, train loss 0.420481 in 0.15s\n",
      " [-] epoch   73/250, train loss 0.418380 in 0.15s\n",
      " [-] epoch   74/250, train loss 0.399349 in 0.13s\n",
      " [-] epoch   75/250, train loss 0.417222 in 0.13s\n",
      " [-] epoch   76/250, train loss 0.388977 in 0.15s\n",
      " [-] epoch   77/250, train loss 0.392345 in 0.13s\n",
      " [-] epoch   78/250, train loss 0.394672 in 0.14s\n",
      " [-] epoch   79/250, train loss 0.391465 in 0.15s\n",
      " [-] epoch   80/250, train loss 0.424839 in 0.13s\n",
      " [-] epoch   81/250, train loss 0.400169 in 0.13s\n",
      " [-] epoch   82/250, train loss 0.426026 in 0.15s\n",
      " [-] epoch   83/250, train loss 0.428774 in 0.13s\n",
      " [-] epoch   84/250, train loss 0.406917 in 0.15s\n",
      " [-] epoch   85/250, train loss 0.426934 in 0.13s\n",
      " [-] epoch   86/250, train loss 0.380511 in 0.13s\n",
      " [-] epoch   87/250, train loss 0.386756 in 0.10s\n",
      " [-] epoch   88/250, train loss 0.431635 in 0.13s\n",
      " [-] epoch   89/250, train loss 0.413220 in 0.15s\n",
      " [-] epoch   90/250, train loss 0.400278 in 0.13s\n",
      " [-] epoch   91/250, train loss 0.390810 in 0.15s\n",
      " [-] epoch   92/250, train loss 0.385836 in 0.14s\n",
      " [-] epoch   93/250, train loss 0.390067 in 0.14s\n",
      " [-] epoch   94/250, train loss 0.405952 in 0.14s\n",
      " [-] epoch   95/250, train loss 0.390098 in 0.14s\n",
      " [-] epoch   96/250, train loss 0.404360 in 0.12s\n",
      " [-] epoch   97/250, train loss 0.395276 in 0.14s\n",
      " [-] epoch   98/250, train loss 0.405502 in 0.13s\n",
      " [-] epoch   99/250, train loss 0.405197 in 0.12s\n",
      " [-] epoch  100/250, train loss 0.388806 in 0.13s\n",
      " [-] epoch  101/250, train loss 0.368608 in 0.13s\n",
      " [-] epoch  102/250, train loss 0.400854 in 0.15s\n",
      " [-] epoch  103/250, train loss 0.411423 in 0.13s\n",
      " [-] epoch  104/250, train loss 0.392134 in 0.15s\n",
      " [-] epoch  105/250, train loss 0.361180 in 0.14s\n",
      " [-] epoch  106/250, train loss 0.393854 in 0.12s\n",
      " [-] epoch  107/250, train loss 0.397923 in 0.13s\n",
      " [-] epoch  108/250, train loss 0.392520 in 0.13s\n",
      " [-] epoch  109/250, train loss 0.364107 in 0.13s\n",
      " [-] epoch  110/250, train loss 0.377685 in 0.14s\n",
      " [-] epoch  111/250, train loss 0.385801 in 0.14s\n",
      " [-] epoch  112/250, train loss 0.372965 in 0.14s\n",
      " [-] epoch  113/250, train loss 0.371146 in 0.12s\n",
      " [-] epoch  114/250, train loss 0.403242 in 0.15s\n",
      " [-] epoch  115/250, train loss 0.387552 in 0.11s\n",
      " [-] epoch  116/250, train loss 0.379149 in 0.14s\n",
      " [-] epoch  117/250, train loss 0.352996 in 0.15s\n",
      " [-] epoch  118/250, train loss 0.391737 in 0.13s\n",
      " [-] epoch  119/250, train loss 0.372197 in 0.13s\n",
      " [-] epoch  120/250, train loss 0.381834 in 0.13s\n",
      " [-] epoch  121/250, train loss 0.391812 in 0.12s\n",
      " [-] epoch  122/250, train loss 0.373947 in 0.13s\n",
      " [-] epoch  123/250, train loss 0.400461 in 0.15s\n",
      " [-] epoch  124/250, train loss 0.374309 in 0.13s\n",
      " [-] epoch  125/250, train loss 0.376010 in 0.13s\n",
      " [-] epoch  126/250, train loss 0.392110 in 0.13s\n",
      " [-] epoch  127/250, train loss 0.394370 in 0.13s\n",
      " [-] epoch  128/250, train loss 0.383836 in 0.14s\n",
      " [-] epoch  129/250, train loss 0.365687 in 0.15s\n",
      " [-] epoch  130/250, train loss 0.378239 in 0.13s\n",
      " [-] epoch  131/250, train loss 0.349366 in 0.13s\n",
      " [-] epoch  132/250, train loss 0.382304 in 0.13s\n",
      " [-] epoch  133/250, train loss 0.394944 in 0.13s\n",
      " [-] epoch  134/250, train loss 0.359366 in 0.15s\n",
      " [-] epoch  135/250, train loss 0.363266 in 0.13s\n",
      " [-] epoch  136/250, train loss 0.367632 in 0.13s\n",
      " [-] epoch  137/250, train loss 0.358697 in 0.15s\n",
      " [-] epoch  138/250, train loss 0.395992 in 0.13s\n",
      " [-] epoch  139/250, train loss 0.384580 in 0.15s\n",
      " [-] epoch  140/250, train loss 0.410329 in 0.13s\n",
      " [-] epoch  141/250, train loss 0.369581 in 0.13s\n",
      " [-] epoch  142/250, train loss 0.368840 in 0.13s\n",
      " [-] epoch  143/250, train loss 0.369633 in 0.15s\n",
      " [-] epoch  144/250, train loss 0.351833 in 0.13s\n",
      " [-] epoch  145/250, train loss 0.362846 in 0.13s\n",
      " [-] epoch  146/250, train loss 0.353545 in 0.13s\n",
      " [-] epoch  147/250, train loss 0.377633 in 0.15s\n",
      " [-] epoch  148/250, train loss 0.345206 in 0.14s\n",
      " [-] epoch  149/250, train loss 0.409641 in 0.13s\n",
      " [-] epoch  150/250, train loss 0.410120 in 0.15s\n",
      " [-] epoch  151/250, train loss 0.361000 in 0.12s\n",
      " [-] epoch  152/250, train loss 0.365430 in 0.13s\n",
      " [-] epoch  153/250, train loss 0.372313 in 0.15s\n",
      " [-] epoch  154/250, train loss 0.380785 in 0.15s\n",
      " [-] epoch  155/250, train loss 0.349044 in 0.13s\n",
      " [-] epoch  156/250, train loss 0.375283 in 0.15s\n",
      " [-] epoch  157/250, train loss 0.337283 in 0.12s\n",
      " [-] epoch  158/250, train loss 0.366473 in 0.13s\n",
      " [-] epoch  159/250, train loss 0.362065 in 0.10s\n",
      " [-] epoch  160/250, train loss 0.362541 in 0.12s\n",
      " [-] epoch  161/250, train loss 0.380128 in 0.13s\n",
      " [-] epoch  162/250, train loss 0.342453 in 0.13s\n",
      " [-] epoch  163/250, train loss 0.371886 in 0.12s\n",
      " [-] epoch  164/250, train loss 0.348557 in 0.16s\n",
      " [-] epoch  165/250, train loss 0.367191 in 0.12s\n",
      " [-] epoch  166/250, train loss 0.373433 in 0.13s\n",
      " [-] epoch  167/250, train loss 0.346995 in 0.17s\n",
      " [-] epoch  168/250, train loss 0.337112 in 0.11s\n",
      " [-] epoch  169/250, train loss 0.356560 in 0.15s\n",
      " [-] epoch  170/250, train loss 0.372621 in 0.13s\n",
      " [-] epoch  171/250, train loss 0.345915 in 0.13s\n",
      " [-] epoch  172/250, train loss 0.346939 in 0.15s\n",
      " [-] epoch  173/250, train loss 0.363485 in 0.13s\n",
      " [-] epoch  174/250, train loss 0.363846 in 0.13s\n",
      " [-] epoch  175/250, train loss 0.337095 in 0.15s\n",
      " [-] epoch  176/250, train loss 0.325413 in 0.13s\n",
      " [-] epoch  177/250, train loss 0.386903 in 0.13s\n",
      " [-] epoch  178/250, train loss 0.387834 in 0.12s\n",
      " [-] epoch  179/250, train loss 0.365604 in 0.15s\n",
      " [-] epoch  180/250, train loss 0.362029 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.345842 in 0.13s\n",
      " [-] epoch  182/250, train loss 0.345859 in 0.14s\n",
      " [-] epoch  183/250, train loss 0.351244 in 0.12s\n",
      " [-] epoch  184/250, train loss 0.331610 in 0.12s\n",
      " [-] epoch  185/250, train loss 0.326937 in 0.13s\n",
      " [-] epoch  186/250, train loss 0.326520 in 0.15s\n",
      " [-] epoch  187/250, train loss 0.354069 in 0.15s\n",
      " [-] epoch  188/250, train loss 0.362447 in 0.13s\n",
      " [-] epoch  189/250, train loss 0.351957 in 0.15s\n",
      " [-] epoch  190/250, train loss 0.333968 in 0.13s\n",
      " [-] epoch  191/250, train loss 0.349211 in 0.13s\n",
      " [-] epoch  192/250, train loss 0.344492 in 0.15s\n",
      " [-] epoch  193/250, train loss 0.349830 in 0.13s\n",
      " [-] epoch  194/250, train loss 0.346639 in 0.15s\n",
      " [-] epoch  195/250, train loss 0.321565 in 0.13s\n",
      " [-] epoch  196/250, train loss 0.350842 in 0.15s\n",
      " [-] epoch  197/250, train loss 0.335706 in 0.14s\n",
      " [-] epoch  198/250, train loss 0.322318 in 0.14s\n",
      " [-] epoch  199/250, train loss 0.365150 in 0.13s\n",
      " [-] epoch  200/250, train loss 0.341744 in 0.15s\n",
      " [-] epoch  201/250, train loss 0.338372 in 0.13s\n",
      " [-] epoch  202/250, train loss 0.362351 in 0.15s\n",
      " [-] epoch  203/250, train loss 0.342096 in 0.15s\n",
      " [-] epoch  204/250, train loss 0.340122 in 0.12s\n",
      " [-] epoch  205/250, train loss 0.352763 in 0.12s\n",
      " [-] epoch  206/250, train loss 0.337597 in 0.13s\n",
      " [-] epoch  207/250, train loss 0.315633 in 0.13s\n",
      " [-] epoch  208/250, train loss 0.329520 in 0.13s\n",
      " [-] epoch  209/250, train loss 0.350212 in 0.13s\n",
      " [-] epoch  210/250, train loss 0.322167 in 0.15s\n",
      " [-] epoch  211/250, train loss 0.342643 in 0.16s\n",
      " [-] epoch  212/250, train loss 0.339708 in 0.14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  213/250, train loss 0.349905 in 0.15s\n",
      " [-] epoch  214/250, train loss 0.336420 in 0.14s\n",
      " [-] epoch  215/250, train loss 0.354095 in 0.14s\n",
      " [-] epoch  216/250, train loss 0.337685 in 0.15s\n",
      " [-] epoch  217/250, train loss 0.334239 in 0.13s\n",
      " [-] epoch  218/250, train loss 0.329252 in 0.14s\n",
      " [-] epoch  219/250, train loss 0.333678 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.343972 in 0.13s\n",
      " [-] epoch  221/250, train loss 0.347037 in 0.14s\n",
      " [-] epoch  222/250, train loss 0.348786 in 0.13s\n",
      " [-] epoch  223/250, train loss 0.329283 in 0.13s\n",
      " [-] epoch  224/250, train loss 0.331613 in 0.12s\n",
      " [-] epoch  225/250, train loss 0.330871 in 0.14s\n",
      " [-] epoch  226/250, train loss 0.344931 in 0.13s\n",
      " [-] epoch  227/250, train loss 0.341413 in 0.14s\n",
      " [-] epoch  228/250, train loss 0.316250 in 0.12s\n",
      " [-] epoch  229/250, train loss 0.327911 in 0.13s\n",
      " [-] epoch  230/250, train loss 0.339134 in 0.13s\n",
      " [-] epoch  231/250, train loss 0.325137 in 0.13s\n",
      " [-] epoch  232/250, train loss 0.347706 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.339495 in 0.13s\n",
      " [-] epoch  234/250, train loss 0.333270 in 0.13s\n",
      " [-] epoch  235/250, train loss 0.322690 in 0.12s\n",
      " [-] epoch  236/250, train loss 0.321839 in 0.13s\n",
      " [-] epoch  237/250, train loss 0.346507 in 0.13s\n",
      " [-] epoch  238/250, train loss 0.335358 in 0.13s\n",
      " [-] epoch  239/250, train loss 0.326622 in 0.13s\n",
      " [-] epoch  240/250, train loss 0.324465 in 0.14s\n",
      " [-] epoch  241/250, train loss 0.299737 in 0.13s\n",
      " [-] epoch  242/250, train loss 0.337790 in 0.13s\n",
      " [-] epoch  243/250, train loss 0.309387 in 0.14s\n",
      " [-] epoch  244/250, train loss 0.349058 in 0.13s\n",
      " [-] epoch  245/250, train loss 0.309179 in 0.13s\n",
      " [-] epoch  246/250, train loss 0.299948 in 0.14s\n",
      " [-] epoch  247/250, train loss 0.345471 in 0.14s\n",
      " [-] epoch  248/250, train loss 0.317516 in 0.14s\n",
      " [-] epoch  249/250, train loss 0.318044 in 0.13s\n",
      " [-] epoch  250/250, train loss 0.343642 in 0.13s\n",
      " [-] test acc. 65.277778%\n",
      "Je vais utiliser 7 layers\n",
      " [-] epoch    1/250, train loss 0.646287 in 0.16s\n",
      " [-] epoch    2/250, train loss 0.601074 in 0.16s\n",
      " [-] epoch    3/250, train loss 0.608055 in 0.16s\n",
      " [-] epoch    4/250, train loss 0.586155 in 0.16s\n",
      " [-] epoch    5/250, train loss 0.549785 in 0.16s\n",
      " [-] epoch    6/250, train loss 0.574854 in 0.15s\n",
      " [-] epoch    7/250, train loss 0.544009 in 0.15s\n",
      " [-] epoch    8/250, train loss 0.539194 in 0.15s\n",
      " [-] epoch    9/250, train loss 0.518024 in 0.17s\n",
      " [-] epoch   10/250, train loss 0.520004 in 0.16s\n",
      " [-] epoch   11/250, train loss 0.506552 in 0.15s\n",
      " [-] epoch   12/250, train loss 0.537659 in 0.15s\n",
      " [-] epoch   13/250, train loss 0.518991 in 0.14s\n",
      " [-] epoch   14/250, train loss 0.514038 in 0.15s\n",
      " [-] epoch   15/250, train loss 0.514569 in 0.16s\n",
      " [-] epoch   16/250, train loss 0.525614 in 0.15s\n",
      " [-] epoch   17/250, train loss 0.503589 in 0.16s\n",
      " [-] epoch   18/250, train loss 0.535214 in 0.16s\n",
      " [-] epoch   19/250, train loss 0.474081 in 0.15s\n",
      " [-] epoch   20/250, train loss 0.490521 in 0.16s\n",
      " [-] epoch   21/250, train loss 0.475646 in 0.15s\n",
      " [-] epoch   22/250, train loss 0.482149 in 0.15s\n",
      " [-] epoch   23/250, train loss 0.487804 in 0.17s\n",
      " [-] epoch   24/250, train loss 0.476482 in 0.18s\n",
      " [-] epoch   25/250, train loss 0.478667 in 0.16s\n",
      " [-] epoch   26/250, train loss 0.464425 in 0.17s\n",
      " [-] epoch   27/250, train loss 0.473341 in 0.17s\n",
      " [-] epoch   28/250, train loss 0.481053 in 0.17s\n",
      " [-] epoch   29/250, train loss 0.474240 in 0.14s\n",
      " [-] epoch   30/250, train loss 0.474383 in 0.16s\n",
      " [-] epoch   31/250, train loss 0.469705 in 0.15s\n",
      " [-] epoch   32/250, train loss 0.491339 in 0.17s\n",
      " [-] epoch   33/250, train loss 0.442990 in 0.16s\n",
      " [-] epoch   34/250, train loss 0.450731 in 0.16s\n",
      " [-] epoch   35/250, train loss 0.509106 in 0.16s\n",
      " [-] epoch   36/250, train loss 0.465301 in 0.15s\n",
      " [-] epoch   37/250, train loss 0.477239 in 0.17s\n",
      " [-] epoch   38/250, train loss 0.458996 in 0.15s\n",
      " [-] epoch   39/250, train loss 0.455261 in 0.16s\n",
      " [-] epoch   40/250, train loss 0.459055 in 0.14s\n",
      " [-] epoch   41/250, train loss 0.422409 in 0.16s\n",
      " [-] epoch   42/250, train loss 0.454296 in 0.17s\n",
      " [-] epoch   43/250, train loss 0.460669 in 0.17s\n",
      " [-] epoch   44/250, train loss 0.466762 in 0.14s\n",
      " [-] epoch   45/250, train loss 0.456816 in 0.16s\n",
      " [-] epoch   46/250, train loss 0.437326 in 0.16s\n",
      " [-] epoch   47/250, train loss 0.429320 in 0.15s\n",
      " [-] epoch   48/250, train loss 0.502481 in 0.16s\n",
      " [-] epoch   49/250, train loss 0.467767 in 0.15s\n",
      " [-] epoch   50/250, train loss 0.459960 in 0.16s\n",
      " [-] epoch   51/250, train loss 0.418220 in 0.17s\n",
      " [-] epoch   52/250, train loss 0.445899 in 0.16s\n",
      " [-] epoch   53/250, train loss 0.437078 in 0.17s\n",
      " [-] epoch   54/250, train loss 0.464994 in 0.19s\n",
      " [-] epoch   55/250, train loss 0.413528 in 0.19s\n",
      " [-] epoch   56/250, train loss 0.420161 in 0.17s\n",
      " [-] epoch   57/250, train loss 0.435327 in 0.16s\n",
      " [-] epoch   58/250, train loss 0.422142 in 0.15s\n",
      " [-] epoch   59/250, train loss 0.418967 in 0.17s\n",
      " [-] epoch   60/250, train loss 0.425998 in 0.17s\n",
      " [-] epoch   61/250, train loss 0.421667 in 0.17s\n",
      " [-] epoch   62/250, train loss 0.410722 in 0.16s\n",
      " [-] epoch   63/250, train loss 0.432115 in 0.17s\n",
      " [-] epoch   64/250, train loss 0.395604 in 0.16s\n",
      " [-] epoch   65/250, train loss 0.465066 in 0.16s\n",
      " [-] epoch   66/250, train loss 0.437603 in 0.16s\n",
      " [-] epoch   67/250, train loss 0.403221 in 0.15s\n",
      " [-] epoch   68/250, train loss 0.405595 in 0.16s\n",
      " [-] epoch   69/250, train loss 0.401632 in 0.16s\n",
      " [-] epoch   70/250, train loss 0.418721 in 0.16s\n",
      " [-] epoch   71/250, train loss 0.402883 in 0.15s\n",
      " [-] epoch   72/250, train loss 0.431197 in 0.17s\n",
      " [-] epoch   73/250, train loss 0.435470 in 0.17s\n",
      " [-] epoch   74/250, train loss 0.397646 in 0.16s\n",
      " [-] epoch   75/250, train loss 0.380927 in 0.16s\n",
      " [-] epoch   76/250, train loss 0.398108 in 0.16s\n",
      " [-] epoch   77/250, train loss 0.388480 in 0.17s\n",
      " [-] epoch   78/250, train loss 0.425135 in 0.16s\n",
      " [-] epoch   79/250, train loss 0.428949 in 0.16s\n",
      " [-] epoch   80/250, train loss 0.430474 in 0.15s\n",
      " [-] epoch   81/250, train loss 0.409564 in 0.16s\n",
      " [-] epoch   82/250, train loss 0.385393 in 0.17s\n",
      " [-] epoch   83/250, train loss 0.411547 in 0.17s\n",
      " [-] epoch   84/250, train loss 0.385036 in 0.17s\n",
      " [-] epoch   85/250, train loss 0.363744 in 0.15s\n",
      " [-] epoch   86/250, train loss 0.387095 in 0.16s\n",
      " [-] epoch   87/250, train loss 0.406432 in 0.17s\n",
      " [-] epoch   88/250, train loss 0.426767 in 0.17s\n",
      " [-] epoch   89/250, train loss 0.374444 in 0.17s\n",
      " [-] epoch   90/250, train loss 0.370688 in 0.16s\n",
      " [-] epoch   91/250, train loss 0.363907 in 0.15s\n",
      " [-] epoch   92/250, train loss 0.396396 in 0.16s\n",
      " [-] epoch   93/250, train loss 0.438081 in 0.15s\n",
      " [-] epoch   94/250, train loss 0.372875 in 0.17s\n",
      " [-] epoch   95/250, train loss 0.430842 in 0.16s\n",
      " [-] epoch   96/250, train loss 0.401374 in 0.17s\n",
      " [-] epoch   97/250, train loss 0.412180 in 0.17s\n",
      " [-] epoch   98/250, train loss 0.392044 in 0.16s\n",
      " [-] epoch   99/250, train loss 0.389176 in 0.17s\n",
      " [-] epoch  100/250, train loss 0.388642 in 0.15s\n",
      " [-] epoch  101/250, train loss 0.392004 in 0.15s\n",
      " [-] epoch  102/250, train loss 0.376843 in 0.16s\n",
      " [-] epoch  103/250, train loss 0.419978 in 0.16s\n",
      " [-] epoch  104/250, train loss 0.363043 in 0.17s\n",
      " [-] epoch  105/250, train loss 0.383903 in 0.14s\n",
      " [-] epoch  106/250, train loss 0.399492 in 0.18s\n",
      " [-] epoch  107/250, train loss 0.387182 in 0.15s\n",
      " [-] epoch  108/250, train loss 0.371300 in 0.18s\n",
      " [-] epoch  109/250, train loss 0.386586 in 0.16s\n",
      " [-] epoch  110/250, train loss 0.373457 in 0.17s\n",
      " [-] epoch  111/250, train loss 0.388150 in 0.13s\n",
      " [-] epoch  112/250, train loss 0.373255 in 0.18s\n",
      " [-] epoch  113/250, train loss 0.368228 in 0.15s\n",
      " [-] epoch  114/250, train loss 0.375314 in 0.18s\n",
      " [-] epoch  115/250, train loss 0.380829 in 0.17s\n",
      " [-] epoch  116/250, train loss 0.399320 in 0.15s\n",
      " [-] epoch  117/250, train loss 0.373625 in 0.17s\n",
      " [-] epoch  118/250, train loss 0.367798 in 0.14s\n",
      " [-] epoch  119/250, train loss 0.381399 in 0.16s\n",
      " [-] epoch  120/250, train loss 0.386040 in 0.16s\n",
      " [-] epoch  121/250, train loss 0.376670 in 0.17s\n",
      " [-] epoch  122/250, train loss 0.384842 in 0.16s\n",
      " [-] epoch  123/250, train loss 0.380752 in 0.14s\n",
      " [-] epoch  124/250, train loss 0.361145 in 0.16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  125/250, train loss 0.328268 in 0.15s\n",
      " [-] epoch  126/250, train loss 0.354890 in 0.16s\n",
      " [-] epoch  127/250, train loss 0.344814 in 0.15s\n",
      " [-] epoch  128/250, train loss 0.359939 in 0.17s\n",
      " [-] epoch  129/250, train loss 0.404072 in 0.16s\n",
      " [-] epoch  130/250, train loss 0.375927 in 0.14s\n",
      " [-] epoch  131/250, train loss 0.388019 in 0.17s\n",
      " [-] epoch  132/250, train loss 0.336940 in 0.17s\n",
      " [-] epoch  133/250, train loss 0.355364 in 0.18s\n",
      " [-] epoch  134/250, train loss 0.372775 in 0.19s\n",
      " [-] epoch  135/250, train loss 0.408522 in 0.18s\n",
      " [-] epoch  136/250, train loss 0.357025 in 0.18s\n",
      " [-] epoch  137/250, train loss 0.365336 in 0.17s\n",
      " [-] epoch  138/250, train loss 0.389966 in 0.17s\n",
      " [-] epoch  139/250, train loss 0.360733 in 0.13s\n",
      " [-] epoch  140/250, train loss 0.346851 in 0.17s\n",
      " [-] epoch  141/250, train loss 0.360804 in 0.17s\n",
      " [-] epoch  142/250, train loss 0.368629 in 0.17s\n",
      " [-] epoch  143/250, train loss 0.357418 in 0.16s\n",
      " [-] epoch  144/250, train loss 0.362502 in 0.16s\n",
      " [-] epoch  145/250, train loss 0.348735 in 0.15s\n",
      " [-] epoch  146/250, train loss 0.346798 in 0.13s\n",
      " [-] epoch  147/250, train loss 0.380340 in 0.15s\n",
      " [-] epoch  148/250, train loss 0.370763 in 0.15s\n",
      " [-] epoch  149/250, train loss 0.354152 in 0.16s\n",
      " [-] epoch  150/250, train loss 0.364402 in 0.14s\n",
      " [-] epoch  151/250, train loss 0.365130 in 0.17s\n",
      " [-] epoch  152/250, train loss 0.353504 in 0.18s\n",
      " [-] epoch  153/250, train loss 0.381247 in 0.14s\n",
      " [-] epoch  154/250, train loss 0.375712 in 0.17s\n",
      " [-] epoch  155/250, train loss 0.376459 in 0.14s\n",
      " [-] epoch  156/250, train loss 0.362125 in 0.18s\n",
      " [-] epoch  157/250, train loss 0.374959 in 0.15s\n",
      " [-] epoch  158/250, train loss 0.363828 in 0.16s\n",
      " [-] epoch  159/250, train loss 0.334346 in 0.14s\n",
      " [-] epoch  160/250, train loss 0.391354 in 0.16s\n",
      " [-] epoch  161/250, train loss 0.377551 in 0.15s\n",
      " [-] epoch  162/250, train loss 0.349951 in 0.15s\n",
      " [-] epoch  163/250, train loss 0.325200 in 0.13s\n",
      " [-] epoch  164/250, train loss 0.349456 in 0.16s\n",
      " [-] epoch  165/250, train loss 0.360997 in 0.15s\n",
      " [-] epoch  166/250, train loss 0.375978 in 0.17s\n",
      " [-] epoch  167/250, train loss 0.355131 in 0.16s\n",
      " [-] epoch  168/250, train loss 0.361542 in 0.17s\n",
      " [-] epoch  169/250, train loss 0.340205 in 0.15s\n",
      " [-] epoch  170/250, train loss 0.332569 in 0.16s\n",
      " [-] epoch  171/250, train loss 0.340187 in 0.13s\n",
      " [-] epoch  172/250, train loss 0.341875 in 0.15s\n",
      " [-] epoch  173/250, train loss 0.351585 in 0.17s\n",
      " [-] epoch  174/250, train loss 0.356577 in 0.15s\n",
      " [-] epoch  175/250, train loss 0.349429 in 0.17s\n",
      " [-] epoch  176/250, train loss 0.350406 in 0.17s\n",
      " [-] epoch  177/250, train loss 0.332182 in 0.21s\n",
      " [-] epoch  178/250, train loss 0.340075 in 0.16s\n",
      " [-] epoch  179/250, train loss 0.332714 in 0.15s\n",
      " [-] epoch  180/250, train loss 0.333397 in 0.14s\n",
      " [-] epoch  181/250, train loss 0.363631 in 0.16s\n",
      " [-] epoch  182/250, train loss 0.325100 in 0.16s\n",
      " [-] epoch  183/250, train loss 0.349232 in 0.14s\n",
      " [-] epoch  184/250, train loss 0.336490 in 0.18s\n",
      " [-] epoch  185/250, train loss 0.326434 in 0.15s\n",
      " [-] epoch  186/250, train loss 0.374540 in 0.15s\n",
      " [-] epoch  187/250, train loss 0.328571 in 0.15s\n",
      " [-] epoch  188/250, train loss 0.346269 in 0.15s\n",
      " [-] epoch  189/250, train loss 0.327482 in 0.14s\n",
      " [-] epoch  190/250, train loss 0.358675 in 0.16s\n",
      " [-] epoch  191/250, train loss 0.358022 in 0.17s\n",
      " [-] epoch  192/250, train loss 0.328915 in 0.17s\n",
      " [-] epoch  193/250, train loss 0.323946 in 0.14s\n",
      " [-] epoch  194/250, train loss 0.340508 in 0.15s\n",
      " [-] epoch  195/250, train loss 0.330480 in 0.16s\n",
      " [-] epoch  196/250, train loss 0.336541 in 0.15s\n",
      " [-] epoch  197/250, train loss 0.351553 in 0.16s\n",
      " [-] epoch  198/250, train loss 0.343188 in 0.13s\n",
      " [-] epoch  199/250, train loss 0.322336 in 0.16s\n",
      " [-] epoch  200/250, train loss 0.324776 in 0.16s\n",
      " [-] epoch  201/250, train loss 0.331144 in 0.13s\n",
      " [-] epoch  202/250, train loss 0.362202 in 0.16s\n",
      " [-] epoch  203/250, train loss 0.332911 in 0.14s\n",
      " [-] epoch  204/250, train loss 0.334074 in 0.16s\n",
      " [-] epoch  205/250, train loss 0.364627 in 0.14s\n",
      " [-] epoch  206/250, train loss 0.348330 in 0.14s\n",
      " [-] epoch  207/250, train loss 0.344428 in 0.16s\n",
      " [-] epoch  208/250, train loss 0.317739 in 0.13s\n",
      " [-] epoch  209/250, train loss 0.314318 in 0.17s\n",
      " [-] epoch  210/250, train loss 0.311518 in 0.17s\n",
      " [-] epoch  211/250, train loss 0.336176 in 0.15s\n",
      " [-] epoch  212/250, train loss 0.309725 in 0.16s\n",
      " [-] epoch  213/250, train loss 0.327734 in 0.17s\n",
      " [-] epoch  214/250, train loss 0.377553 in 0.15s\n",
      " [-] epoch  215/250, train loss 0.322622 in 0.17s\n",
      " [-] epoch  216/250, train loss 0.350372 in 0.16s\n",
      " [-] epoch  217/250, train loss 0.321590 in 0.17s\n",
      " [-] epoch  218/250, train loss 0.359331 in 0.17s\n",
      " [-] epoch  219/250, train loss 0.343510 in 0.14s\n",
      " [-] epoch  220/250, train loss 0.336533 in 0.15s\n",
      " [-] epoch  221/250, train loss 0.351413 in 0.13s\n",
      " [-] epoch  222/250, train loss 0.323092 in 0.14s\n",
      " [-] epoch  223/250, train loss 0.323870 in 0.15s\n",
      " [-] epoch  224/250, train loss 0.325326 in 0.15s\n",
      " [-] epoch  225/250, train loss 0.345852 in 0.14s\n",
      " [-] epoch  226/250, train loss 0.302065 in 0.16s\n",
      " [-] epoch  227/250, train loss 0.334763 in 0.15s\n",
      " [-] epoch  228/250, train loss 0.333558 in 0.17s\n",
      " [-] epoch  229/250, train loss 0.337210 in 0.17s\n",
      " [-] epoch  230/250, train loss 0.322321 in 0.23s\n",
      " [-] epoch  231/250, train loss 0.355750 in 0.17s\n",
      " [-] epoch  232/250, train loss 0.344791 in 0.13s\n",
      " [-] epoch  233/250, train loss 0.326803 in 0.16s\n",
      " [-] epoch  234/250, train loss 0.327930 in 0.17s\n",
      " [-] epoch  235/250, train loss 0.319114 in 0.17s\n",
      " [-] epoch  236/250, train loss 0.307844 in 0.16s\n",
      " [-] epoch  237/250, train loss 0.352836 in 0.15s\n",
      " [-] epoch  238/250, train loss 0.321254 in 0.18s\n",
      " [-] epoch  239/250, train loss 0.313559 in 0.16s\n",
      " [-] epoch  240/250, train loss 0.298257 in 0.16s\n",
      " [-] epoch  241/250, train loss 0.316712 in 0.14s\n",
      " [-] epoch  242/250, train loss 0.317190 in 0.15s\n",
      " [-] epoch  243/250, train loss 0.331603 in 0.16s\n",
      " [-] epoch  244/250, train loss 0.322588 in 0.17s\n",
      " [-] epoch  245/250, train loss 0.335889 in 0.16s\n",
      " [-] epoch  246/250, train loss 0.300907 in 0.14s\n",
      " [-] epoch  247/250, train loss 0.351218 in 0.14s\n",
      " [-] epoch  248/250, train loss 0.313433 in 0.16s\n",
      " [-] epoch  249/250, train loss 0.316519 in 0.15s\n",
      " [-] epoch  250/250, train loss 0.313651 in 0.17s\n",
      " [-] test acc. 68.333333%\n",
      "Je vais utiliser 8 layers\n",
      " [-] epoch    1/250, train loss 0.654222 in 0.19s\n",
      " [-] epoch    2/250, train loss 0.587546 in 0.16s\n",
      " [-] epoch    3/250, train loss 0.576182 in 0.18s\n",
      " [-] epoch    4/250, train loss 0.573919 in 0.19s\n",
      " [-] epoch    5/250, train loss 0.553629 in 0.19s\n",
      " [-] epoch    6/250, train loss 0.542403 in 0.19s\n",
      " [-] epoch    7/250, train loss 0.541220 in 0.18s\n",
      " [-] epoch    8/250, train loss 0.543463 in 0.20s\n",
      " [-] epoch    9/250, train loss 0.519618 in 0.18s\n",
      " [-] epoch   10/250, train loss 0.568964 in 0.21s\n",
      " [-] epoch   11/250, train loss 0.533096 in 0.19s\n",
      " [-] epoch   12/250, train loss 0.511815 in 0.18s\n",
      " [-] epoch   13/250, train loss 0.518316 in 0.17s\n",
      " [-] epoch   14/250, train loss 0.512630 in 0.20s\n",
      " [-] epoch   15/250, train loss 0.517006 in 0.20s\n",
      " [-] epoch   16/250, train loss 0.552065 in 0.20s\n",
      " [-] epoch   17/250, train loss 0.512190 in 0.18s\n",
      " [-] epoch   18/250, train loss 0.495835 in 0.18s\n",
      " [-] epoch   19/250, train loss 0.495055 in 0.18s\n",
      " [-] epoch   20/250, train loss 0.508597 in 0.18s\n",
      " [-] epoch   21/250, train loss 0.476973 in 0.16s\n",
      " [-] epoch   22/250, train loss 0.485274 in 0.20s\n",
      " [-] epoch   23/250, train loss 0.481576 in 0.25s\n",
      " [-] epoch   24/250, train loss 0.475623 in 0.15s\n",
      " [-] epoch   25/250, train loss 0.472479 in 0.18s\n",
      " [-] epoch   26/250, train loss 0.437827 in 0.21s\n",
      " [-] epoch   27/250, train loss 0.493473 in 0.19s\n",
      " [-] epoch   28/250, train loss 0.470624 in 0.17s\n",
      " [-] epoch   29/250, train loss 0.467183 in 0.19s\n",
      " [-] epoch   30/250, train loss 0.450888 in 0.19s\n",
      " [-] epoch   31/250, train loss 0.461205 in 0.19s\n",
      " [-] epoch   32/250, train loss 0.469198 in 0.19s\n",
      " [-] epoch   33/250, train loss 0.459260 in 0.17s\n",
      " [-] epoch   34/250, train loss 0.467292 in 0.18s\n",
      " [-] epoch   35/250, train loss 0.473934 in 0.20s\n",
      " [-] epoch   36/250, train loss 0.452404 in 0.19s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   37/250, train loss 0.450032 in 0.20s\n",
      " [-] epoch   38/250, train loss 0.469687 in 0.18s\n",
      " [-] epoch   39/250, train loss 0.466558 in 0.19s\n",
      " [-] epoch   40/250, train loss 0.451477 in 0.19s\n",
      " [-] epoch   41/250, train loss 0.478750 in 0.19s\n",
      " [-] epoch   42/250, train loss 0.444049 in 0.17s\n",
      " [-] epoch   43/250, train loss 0.461275 in 0.20s\n",
      " [-] epoch   44/250, train loss 0.445139 in 0.16s\n",
      " [-] epoch   45/250, train loss 0.458899 in 0.20s\n",
      " [-] epoch   46/250, train loss 0.459635 in 0.18s\n",
      " [-] epoch   47/250, train loss 0.454233 in 0.18s\n",
      " [-] epoch   48/250, train loss 0.471299 in 0.17s\n",
      " [-] epoch   49/250, train loss 0.482080 in 0.18s\n",
      " [-] epoch   50/250, train loss 0.451435 in 0.20s\n",
      " [-] epoch   51/250, train loss 0.464272 in 0.18s\n",
      " [-] epoch   52/250, train loss 0.450978 in 0.21s\n",
      " [-] epoch   53/250, train loss 0.415904 in 0.17s\n",
      " [-] epoch   54/250, train loss 0.411231 in 0.19s\n",
      " [-] epoch   55/250, train loss 0.457359 in 0.19s\n",
      " [-] epoch   56/250, train loss 0.457218 in 0.19s\n",
      " [-] epoch   57/250, train loss 0.442156 in 0.19s\n",
      " [-] epoch   58/250, train loss 0.429949 in 0.18s\n",
      " [-] epoch   59/250, train loss 0.449892 in 0.19s\n",
      " [-] epoch   60/250, train loss 0.445425 in 0.14s\n",
      " [-] epoch   61/250, train loss 0.444006 in 0.21s\n",
      " [-] epoch   62/250, train loss 0.458849 in 0.19s\n",
      " [-] epoch   63/250, train loss 0.416822 in 0.19s\n",
      " [-] epoch   64/250, train loss 0.410224 in 0.19s\n",
      " [-] epoch   65/250, train loss 0.427670 in 0.18s\n",
      " [-] epoch   66/250, train loss 0.422417 in 0.18s\n",
      " [-] epoch   67/250, train loss 0.384374 in 0.20s\n",
      " [-] epoch   68/250, train loss 0.450276 in 0.19s\n",
      " [-] epoch   69/250, train loss 0.429564 in 0.18s\n",
      " [-] epoch   70/250, train loss 0.406215 in 0.19s\n",
      " [-] epoch   71/250, train loss 0.425699 in 0.18s\n",
      " [-] epoch   72/250, train loss 0.424457 in 0.18s\n",
      " [-] epoch   73/250, train loss 0.394514 in 0.17s\n",
      " [-] epoch   74/250, train loss 0.440558 in 0.18s\n",
      " [-] epoch   75/250, train loss 0.406086 in 0.18s\n",
      " [-] epoch   76/250, train loss 0.415803 in 0.20s\n",
      " [-] epoch   77/250, train loss 0.437304 in 0.18s\n",
      " [-] epoch   78/250, train loss 0.409861 in 0.19s\n",
      " [-] epoch   79/250, train loss 0.379337 in 0.19s\n",
      " [-] epoch   80/250, train loss 0.449094 in 0.21s\n",
      " [-] epoch   81/250, train loss 0.392122 in 0.17s\n",
      " [-] epoch   82/250, train loss 0.431866 in 0.18s\n",
      " [-] epoch   83/250, train loss 0.402829 in 0.16s\n",
      " [-] epoch   84/250, train loss 0.410391 in 0.18s\n",
      " [-] epoch   85/250, train loss 0.398501 in 0.17s\n",
      " [-] epoch   86/250, train loss 0.393313 in 0.21s\n",
      " [-] epoch   87/250, train loss 0.400346 in 0.18s\n",
      " [-] epoch   88/250, train loss 0.407773 in 0.16s\n",
      " [-] epoch   89/250, train loss 0.392171 in 0.20s\n",
      " [-] epoch   90/250, train loss 0.402962 in 0.18s\n",
      " [-] epoch   91/250, train loss 0.425590 in 0.17s\n",
      " [-] epoch   92/250, train loss 0.418692 in 0.19s\n",
      " [-] epoch   93/250, train loss 0.389202 in 0.14s\n",
      " [-] epoch   94/250, train loss 0.387242 in 0.19s\n",
      " [-] epoch   95/250, train loss 0.375023 in 0.19s\n",
      " [-] epoch   96/250, train loss 0.393722 in 0.18s\n",
      " [-] epoch   97/250, train loss 0.396289 in 0.17s\n",
      " [-] epoch   98/250, train loss 0.413579 in 0.17s\n",
      " [-] epoch   99/250, train loss 0.398042 in 0.18s\n",
      " [-] epoch  100/250, train loss 0.379611 in 0.17s\n",
      " [-] epoch  101/250, train loss 0.441436 in 0.20s\n",
      " [-] epoch  102/250, train loss 0.388072 in 0.17s\n",
      " [-] epoch  103/250, train loss 0.368088 in 0.20s\n",
      " [-] epoch  104/250, train loss 0.413856 in 0.19s\n",
      " [-] epoch  105/250, train loss 0.383036 in 0.17s\n",
      " [-] epoch  106/250, train loss 0.376414 in 0.20s\n",
      " [-] epoch  107/250, train loss 0.398934 in 0.18s\n",
      " [-] epoch  108/250, train loss 0.391276 in 0.18s\n",
      " [-] epoch  109/250, train loss 0.411371 in 0.19s\n",
      " [-] epoch  110/250, train loss 0.380810 in 0.17s\n",
      " [-] epoch  111/250, train loss 0.387540 in 0.20s\n",
      " [-] epoch  112/250, train loss 0.380763 in 0.17s\n",
      " [-] epoch  113/250, train loss 0.388500 in 0.19s\n",
      " [-] epoch  114/250, train loss 0.409612 in 0.18s\n",
      " [-] epoch  115/250, train loss 0.360494 in 0.20s\n",
      " [-] epoch  116/250, train loss 0.386283 in 0.18s\n",
      " [-] epoch  117/250, train loss 0.392416 in 0.18s\n",
      " [-] epoch  118/250, train loss 0.396787 in 0.17s\n",
      " [-] epoch  119/250, train loss 0.380789 in 0.18s\n",
      " [-] epoch  120/250, train loss 0.356552 in 0.19s\n",
      " [-] epoch  121/250, train loss 0.383270 in 0.17s\n",
      " [-] epoch  122/250, train loss 0.428482 in 0.21s\n",
      " [-] epoch  123/250, train loss 0.375573 in 0.20s\n",
      " [-] epoch  124/250, train loss 0.373438 in 0.18s\n",
      " [-] epoch  125/250, train loss 0.396164 in 0.19s\n",
      " [-] epoch  126/250, train loss 0.382249 in 0.20s\n",
      " [-] epoch  127/250, train loss 0.359139 in 0.19s\n",
      " [-] epoch  128/250, train loss 0.349532 in 0.20s\n",
      " [-] epoch  129/250, train loss 0.377317 in 0.16s\n",
      " [-] epoch  130/250, train loss 0.329035 in 0.17s\n",
      " [-] epoch  131/250, train loss 0.387238 in 0.20s\n",
      " [-] epoch  132/250, train loss 0.350817 in 0.21s\n",
      " [-] epoch  133/250, train loss 0.401676 in 0.19s\n",
      " [-] epoch  134/250, train loss 0.353050 in 0.16s\n",
      " [-] epoch  135/250, train loss 0.369922 in 0.17s\n",
      " [-] epoch  136/250, train loss 0.366695 in 0.21s\n",
      " [-] epoch  137/250, train loss 0.357170 in 0.18s\n",
      " [-] epoch  138/250, train loss 0.341846 in 0.20s\n",
      " [-] epoch  139/250, train loss 0.385929 in 0.19s\n",
      " [-] epoch  140/250, train loss 0.378690 in 0.17s\n",
      " [-] epoch  141/250, train loss 0.371559 in 0.16s\n",
      " [-] epoch  142/250, train loss 0.369608 in 0.19s\n",
      " [-] epoch  143/250, train loss 0.334782 in 0.20s\n",
      " [-] epoch  144/250, train loss 0.361027 in 0.18s\n",
      " [-] epoch  145/250, train loss 0.344734 in 0.18s\n",
      " [-] epoch  146/250, train loss 0.387122 in 0.17s\n",
      " [-] epoch  147/250, train loss 0.366347 in 0.19s\n",
      " [-] epoch  148/250, train loss 0.350145 in 0.19s\n",
      " [-] epoch  149/250, train loss 0.379194 in 0.18s\n",
      " [-] epoch  150/250, train loss 0.366615 in 0.20s\n",
      " [-] epoch  151/250, train loss 0.362411 in 0.18s\n",
      " [-] epoch  152/250, train loss 0.355750 in 0.16s\n",
      " [-] epoch  153/250, train loss 0.368935 in 0.16s\n",
      " [-] epoch  154/250, train loss 0.376919 in 0.18s\n",
      " [-] epoch  155/250, train loss 0.336067 in 0.16s\n",
      " [-] epoch  156/250, train loss 0.360048 in 0.19s\n",
      " [-] epoch  157/250, train loss 0.357704 in 0.17s\n",
      " [-] epoch  158/250, train loss 0.361625 in 0.16s\n",
      " [-] epoch  159/250, train loss 0.348554 in 0.20s\n",
      " [-] epoch  160/250, train loss 0.346045 in 0.16s\n",
      " [-] epoch  161/250, train loss 0.365792 in 0.19s\n",
      " [-] epoch  162/250, train loss 0.350643 in 0.19s\n",
      " [-] epoch  163/250, train loss 0.371691 in 0.18s\n",
      " [-] epoch  164/250, train loss 0.354068 in 0.20s\n",
      " [-] epoch  165/250, train loss 0.359138 in 0.19s\n",
      " [-] epoch  166/250, train loss 0.332510 in 0.17s\n",
      " [-] epoch  167/250, train loss 0.339402 in 0.18s\n",
      " [-] epoch  168/250, train loss 0.341832 in 0.18s\n",
      " [-] epoch  169/250, train loss 0.357891 in 0.24s\n",
      " [-] epoch  170/250, train loss 0.341177 in 0.21s\n",
      " [-] epoch  171/250, train loss 0.327163 in 0.20s\n",
      " [-] epoch  172/250, train loss 0.357046 in 0.19s\n",
      " [-] epoch  173/250, train loss 0.345914 in 0.19s\n",
      " [-] epoch  174/250, train loss 0.379365 in 0.17s\n",
      " [-] epoch  175/250, train loss 0.345113 in 0.18s\n",
      " [-] epoch  176/250, train loss 0.329875 in 0.18s\n",
      " [-] epoch  177/250, train loss 0.359534 in 0.20s\n",
      " [-] epoch  178/250, train loss 0.356521 in 0.19s\n",
      " [-] epoch  179/250, train loss 0.330083 in 0.18s\n",
      " [-] epoch  180/250, train loss 0.354719 in 0.18s\n",
      " [-] epoch  181/250, train loss 0.329191 in 0.19s\n",
      " [-] epoch  182/250, train loss 0.357725 in 0.18s\n",
      " [-] epoch  183/250, train loss 0.335374 in 0.19s\n",
      " [-] epoch  184/250, train loss 0.365816 in 0.18s\n",
      " [-] epoch  185/250, train loss 0.334204 in 0.20s\n",
      " [-] epoch  186/250, train loss 0.316682 in 0.20s\n",
      " [-] epoch  187/250, train loss 0.341557 in 0.21s\n",
      " [-] epoch  188/250, train loss 0.355060 in 0.18s\n",
      " [-] epoch  189/250, train loss 0.327313 in 0.17s\n",
      " [-] epoch  190/250, train loss 0.354428 in 0.20s\n",
      " [-] epoch  191/250, train loss 0.338251 in 0.21s\n",
      " [-] epoch  192/250, train loss 0.322835 in 0.20s\n",
      " [-] epoch  193/250, train loss 0.321721 in 0.17s\n",
      " [-] epoch  194/250, train loss 0.309643 in 0.17s\n",
      " [-] epoch  195/250, train loss 0.338630 in 0.18s\n",
      " [-] epoch  196/250, train loss 0.340272 in 0.18s\n",
      " [-] epoch  197/250, train loss 0.358199 in 0.16s\n",
      " [-] epoch  198/250, train loss 0.329506 in 0.19s\n",
      " [-] epoch  199/250, train loss 0.324904 in 0.18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  200/250, train loss 0.308165 in 0.18s\n",
      " [-] epoch  201/250, train loss 0.324503 in 0.17s\n",
      " [-] epoch  202/250, train loss 0.306606 in 0.17s\n",
      " [-] epoch  203/250, train loss 0.309257 in 0.19s\n",
      " [-] epoch  204/250, train loss 0.324206 in 0.19s\n",
      " [-] epoch  205/250, train loss 0.336946 in 0.18s\n",
      " [-] epoch  206/250, train loss 0.330888 in 0.20s\n",
      " [-] epoch  207/250, train loss 0.351181 in 0.20s\n",
      " [-] epoch  208/250, train loss 0.331965 in 0.22s\n",
      " [-] epoch  209/250, train loss 0.313603 in 0.22s\n",
      " [-] epoch  210/250, train loss 0.315036 in 0.22s\n",
      " [-] epoch  211/250, train loss 0.321512 in 0.22s\n",
      " [-] epoch  212/250, train loss 0.345591 in 0.20s\n",
      " [-] epoch  213/250, train loss 0.308887 in 0.23s\n",
      " [-] epoch  214/250, train loss 0.322091 in 0.20s\n",
      " [-] epoch  215/250, train loss 0.310733 in 0.21s\n",
      " [-] epoch  216/250, train loss 0.305200 in 0.22s\n",
      " [-] epoch  217/250, train loss 0.302342 in 0.19s\n",
      " [-] epoch  218/250, train loss 0.322403 in 0.19s\n",
      " [-] epoch  219/250, train loss 0.338511 in 0.18s\n",
      " [-] epoch  220/250, train loss 0.330683 in 0.18s\n",
      " [-] epoch  221/250, train loss 0.318118 in 0.20s\n",
      " [-] epoch  222/250, train loss 0.322804 in 0.20s\n",
      " [-] epoch  223/250, train loss 0.313703 in 0.20s\n",
      " [-] epoch  224/250, train loss 0.318542 in 0.19s\n",
      " [-] epoch  225/250, train loss 0.318233 in 0.19s\n",
      " [-] epoch  226/250, train loss 0.316852 in 0.17s\n",
      " [-] epoch  227/250, train loss 0.340879 in 0.20s\n",
      " [-] epoch  228/250, train loss 0.336697 in 0.18s\n",
      " [-] epoch  229/250, train loss 0.354591 in 0.20s\n",
      " [-] epoch  230/250, train loss 0.308954 in 0.19s\n",
      " [-] epoch  231/250, train loss 0.300467 in 0.19s\n",
      " [-] epoch  232/250, train loss 0.302052 in 0.18s\n",
      " [-] epoch  233/250, train loss 0.313226 in 0.19s\n",
      " [-] epoch  234/250, train loss 0.323567 in 0.19s\n",
      " [-] epoch  235/250, train loss 0.328561 in 0.20s\n",
      " [-] epoch  236/250, train loss 0.307489 in 0.20s\n",
      " [-] epoch  237/250, train loss 0.332270 in 0.20s\n",
      " [-] epoch  238/250, train loss 0.327609 in 0.20s\n",
      " [-] epoch  239/250, train loss 0.312749 in 0.21s\n",
      " [-] epoch  240/250, train loss 0.342452 in 0.20s\n",
      " [-] epoch  241/250, train loss 0.319954 in 0.20s\n",
      " [-] epoch  242/250, train loss 0.328404 in 0.19s\n",
      " [-] epoch  243/250, train loss 0.313067 in 0.19s\n",
      " [-] epoch  244/250, train loss 0.326464 in 0.20s\n",
      " [-] epoch  245/250, train loss 0.311926 in 0.19s\n",
      " [-] epoch  246/250, train loss 0.316610 in 0.19s\n",
      " [-] epoch  247/250, train loss 0.303901 in 0.18s\n",
      " [-] epoch  248/250, train loss 0.304679 in 0.19s\n",
      " [-] epoch  249/250, train loss 0.293993 in 0.19s\n",
      " [-] epoch  250/250, train loss 0.305089 in 0.19s\n",
      " [-] test acc. 71.666667%\n",
      "Je vais utiliser 9 layers\n",
      " [-] epoch    1/250, train loss 0.667107 in 0.19s\n",
      " [-] epoch    2/250, train loss 0.609267 in 0.20s\n",
      " [-] epoch    3/250, train loss 0.568142 in 0.19s\n",
      " [-] epoch    4/250, train loss 0.538049 in 0.21s\n",
      " [-] epoch    5/250, train loss 0.570587 in 0.21s\n",
      " [-] epoch    6/250, train loss 0.552324 in 0.20s\n",
      " [-] epoch    7/250, train loss 0.538899 in 0.18s\n",
      " [-] epoch    8/250, train loss 0.515836 in 0.21s\n",
      " [-] epoch    9/250, train loss 0.521632 in 0.19s\n",
      " [-] epoch   10/250, train loss 0.563737 in 0.20s\n",
      " [-] epoch   11/250, train loss 0.539395 in 0.19s\n",
      " [-] epoch   12/250, train loss 0.517099 in 0.21s\n",
      " [-] epoch   13/250, train loss 0.528256 in 0.20s\n",
      " [-] epoch   14/250, train loss 0.520183 in 0.21s\n",
      " [-] epoch   15/250, train loss 0.514929 in 0.19s\n",
      " [-] epoch   16/250, train loss 0.468505 in 0.19s\n",
      " [-] epoch   17/250, train loss 0.490651 in 0.19s\n",
      " [-] epoch   18/250, train loss 0.474098 in 0.19s\n",
      " [-] epoch   19/250, train loss 0.507894 in 0.20s\n",
      " [-] epoch   20/250, train loss 0.492652 in 0.19s\n",
      " [-] epoch   21/250, train loss 0.490859 in 0.20s\n",
      " [-] epoch   22/250, train loss 0.488449 in 0.17s\n",
      " [-] epoch   23/250, train loss 0.505815 in 0.19s\n",
      " [-] epoch   24/250, train loss 0.466905 in 0.19s\n",
      " [-] epoch   25/250, train loss 0.493850 in 0.19s\n",
      " [-] epoch   26/250, train loss 0.479537 in 0.20s\n",
      " [-] epoch   27/250, train loss 0.485162 in 0.17s\n",
      " [-] epoch   28/250, train loss 0.471570 in 0.19s\n",
      " [-] epoch   29/250, train loss 0.465142 in 0.19s\n",
      " [-] epoch   30/250, train loss 0.479826 in 0.18s\n",
      " [-] epoch   31/250, train loss 0.459092 in 0.18s\n",
      " [-] epoch   32/250, train loss 0.479408 in 0.19s\n",
      " [-] epoch   33/250, train loss 0.456095 in 0.19s\n",
      " [-] epoch   34/250, train loss 0.473772 in 0.19s\n",
      " [-] epoch   35/250, train loss 0.456392 in 0.20s\n",
      " [-] epoch   36/250, train loss 0.459469 in 0.19s\n",
      " [-] epoch   37/250, train loss 0.471671 in 0.20s\n",
      " [-] epoch   38/250, train loss 0.467821 in 0.19s\n",
      " [-] epoch   39/250, train loss 0.451443 in 0.20s\n",
      " [-] epoch   40/250, train loss 0.447295 in 0.20s\n",
      " [-] epoch   41/250, train loss 0.444816 in 0.19s\n",
      " [-] epoch   42/250, train loss 0.442077 in 0.19s\n",
      " [-] epoch   43/250, train loss 0.435320 in 0.20s\n",
      " [-] epoch   44/250, train loss 0.443976 in 0.17s\n",
      " [-] epoch   45/250, train loss 0.446666 in 0.19s\n",
      " [-] epoch   46/250, train loss 0.450575 in 0.21s\n",
      " [-] epoch   47/250, train loss 0.414201 in 0.18s\n",
      " [-] epoch   48/250, train loss 0.430911 in 0.20s\n",
      " [-] epoch   49/250, train loss 0.441189 in 0.21s\n",
      " [-] epoch   50/250, train loss 0.430899 in 0.21s\n",
      " [-] epoch   51/250, train loss 0.444569 in 0.19s\n",
      " [-] epoch   52/250, train loss 0.427194 in 0.20s\n",
      " [-] epoch   53/250, train loss 0.437593 in 0.20s\n",
      " [-] epoch   54/250, train loss 0.423036 in 0.17s\n",
      " [-] epoch   55/250, train loss 0.464431 in 0.19s\n",
      " [-] epoch   56/250, train loss 0.440069 in 0.21s\n",
      " [-] epoch   57/250, train loss 0.431163 in 0.19s\n",
      " [-] epoch   58/250, train loss 0.430816 in 0.20s\n",
      " [-] epoch   59/250, train loss 0.452932 in 0.20s\n",
      " [-] epoch   60/250, train loss 0.427784 in 0.18s\n",
      " [-] epoch   61/250, train loss 0.405788 in 0.19s\n",
      " [-] epoch   62/250, train loss 0.401387 in 0.22s\n",
      " [-] epoch   63/250, train loss 0.414354 in 0.17s\n",
      " [-] epoch   64/250, train loss 0.404838 in 0.19s\n",
      " [-] epoch   65/250, train loss 0.423087 in 0.19s\n",
      " [-] epoch   66/250, train loss 0.421766 in 0.20s\n",
      " [-] epoch   67/250, train loss 0.422574 in 0.18s\n",
      " [-] epoch   68/250, train loss 0.438804 in 0.20s\n",
      " [-] epoch   69/250, train loss 0.419071 in 0.18s\n",
      " [-] epoch   70/250, train loss 0.432211 in 0.19s\n",
      " [-] epoch   71/250, train loss 0.407439 in 0.20s\n",
      " [-] epoch   72/250, train loss 0.432355 in 0.19s\n",
      " [-] epoch   73/250, train loss 0.414713 in 0.18s\n",
      " [-] epoch   74/250, train loss 0.395002 in 0.20s\n",
      " [-] epoch   75/250, train loss 0.400751 in 0.20s\n",
      " [-] epoch   76/250, train loss 0.419349 in 0.20s\n",
      " [-] epoch   77/250, train loss 0.441497 in 0.19s\n",
      " [-] epoch   78/250, train loss 0.381826 in 0.18s\n",
      " [-] epoch   79/250, train loss 0.391313 in 0.17s\n",
      " [-] epoch   80/250, train loss 0.423518 in 0.19s\n",
      " [-] epoch   81/250, train loss 0.425837 in 0.19s\n",
      " [-] epoch   82/250, train loss 0.425074 in 0.19s\n",
      " [-] epoch   83/250, train loss 0.377331 in 0.21s\n",
      " [-] epoch   84/250, train loss 0.388346 in 0.17s\n",
      " [-] epoch   85/250, train loss 0.409552 in 0.17s\n",
      " [-] epoch   86/250, train loss 0.397560 in 0.20s\n",
      " [-] epoch   87/250, train loss 0.391118 in 0.21s\n",
      " [-] epoch   88/250, train loss 0.367025 in 0.19s\n",
      " [-] epoch   89/250, train loss 0.387149 in 0.19s\n",
      " [-] epoch   90/250, train loss 0.388394 in 0.19s\n",
      " [-] epoch   91/250, train loss 0.401772 in 0.22s\n",
      " [-] epoch   92/250, train loss 0.413653 in 0.19s\n",
      " [-] epoch   93/250, train loss 0.384859 in 0.20s\n",
      " [-] epoch   94/250, train loss 0.377133 in 0.18s\n",
      " [-] epoch   95/250, train loss 0.391487 in 0.17s\n",
      " [-] epoch   96/250, train loss 0.381173 in 0.19s\n",
      " [-] epoch   97/250, train loss 0.389271 in 0.20s\n",
      " [-] epoch   98/250, train loss 0.406128 in 0.16s\n",
      " [-] epoch   99/250, train loss 0.388794 in 0.17s\n",
      " [-] epoch  100/250, train loss 0.380162 in 0.19s\n",
      " [-] epoch  101/250, train loss 0.366815 in 0.20s\n",
      " [-] epoch  102/250, train loss 0.386913 in 0.20s\n",
      " [-] epoch  103/250, train loss 0.397319 in 0.17s\n",
      " [-] epoch  104/250, train loss 0.403156 in 0.19s\n",
      " [-] epoch  105/250, train loss 0.377583 in 0.20s\n",
      " [-] epoch  106/250, train loss 0.365686 in 0.16s\n",
      " [-] epoch  107/250, train loss 0.384721 in 0.20s\n",
      " [-] epoch  108/250, train loss 0.392065 in 0.19s\n",
      " [-] epoch  109/250, train loss 0.366399 in 0.21s\n",
      " [-] epoch  110/250, train loss 0.376062 in 0.18s\n",
      " [-] epoch  111/250, train loss 0.390035 in 0.19s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  112/250, train loss 0.371662 in 0.20s\n",
      " [-] epoch  113/250, train loss 0.348443 in 0.18s\n",
      " [-] epoch  114/250, train loss 0.375728 in 0.19s\n",
      " [-] epoch  115/250, train loss 0.395075 in 0.20s\n",
      " [-] epoch  116/250, train loss 0.368912 in 0.16s\n",
      " [-] epoch  117/250, train loss 0.387833 in 0.18s\n",
      " [-] epoch  118/250, train loss 0.364877 in 0.20s\n",
      " [-] epoch  119/250, train loss 0.360762 in 0.18s\n",
      " [-] epoch  120/250, train loss 0.404586 in 0.17s\n",
      " [-] epoch  121/250, train loss 0.371948 in 0.20s\n",
      " [-] epoch  122/250, train loss 0.329183 in 0.18s\n",
      " [-] epoch  123/250, train loss 0.372901 in 0.19s\n",
      " [-] epoch  124/250, train loss 0.393295 in 0.18s\n",
      " [-] epoch  125/250, train loss 0.381983 in 0.20s\n",
      " [-] epoch  126/250, train loss 0.362267 in 0.18s\n",
      " [-] epoch  127/250, train loss 0.378907 in 0.20s\n",
      " [-] epoch  128/250, train loss 0.394929 in 0.18s\n",
      " [-] epoch  129/250, train loss 0.348155 in 0.19s\n",
      " [-] epoch  130/250, train loss 0.332321 in 0.18s\n",
      " [-] epoch  131/250, train loss 0.400163 in 0.18s\n",
      " [-] epoch  132/250, train loss 0.374155 in 0.17s\n",
      " [-] epoch  133/250, train loss 0.336518 in 0.19s\n",
      " [-] epoch  134/250, train loss 0.369900 in 0.20s\n",
      " [-] epoch  135/250, train loss 0.344890 in 0.19s\n",
      " [-] epoch  136/250, train loss 0.335434 in 0.21s\n",
      " [-] epoch  137/250, train loss 0.371781 in 0.21s\n",
      " [-] epoch  138/250, train loss 0.379437 in 0.17s\n",
      " [-] epoch  139/250, train loss 0.387616 in 0.21s\n",
      " [-] epoch  140/250, train loss 0.352117 in 0.17s\n",
      " [-] epoch  141/250, train loss 0.358987 in 0.19s\n",
      " [-] epoch  142/250, train loss 0.363784 in 0.18s\n",
      " [-] epoch  143/250, train loss 0.357189 in 0.19s\n",
      " [-] epoch  144/250, train loss 0.341866 in 0.19s\n",
      " [-] epoch  145/250, train loss 0.324206 in 0.19s\n",
      " [-] epoch  146/250, train loss 0.329326 in 0.21s\n",
      " [-] epoch  147/250, train loss 0.346982 in 0.19s\n",
      " [-] epoch  148/250, train loss 0.371171 in 0.19s\n",
      " [-] epoch  149/250, train loss 0.363265 in 0.18s\n",
      " [-] epoch  150/250, train loss 0.313468 in 0.20s\n",
      " [-] epoch  151/250, train loss 0.338967 in 0.19s\n",
      " [-] epoch  152/250, train loss 0.341177 in 0.20s\n",
      " [-] epoch  153/250, train loss 0.339992 in 0.18s\n",
      " [-] epoch  154/250, train loss 0.371667 in 0.20s\n",
      " [-] epoch  155/250, train loss 0.340513 in 0.19s\n",
      " [-] epoch  156/250, train loss 0.311557 in 0.20s\n",
      " [-] epoch  157/250, train loss 0.336570 in 0.18s\n",
      " [-] epoch  158/250, train loss 0.340595 in 0.19s\n",
      " [-] epoch  159/250, train loss 0.355283 in 0.20s\n",
      " [-] epoch  160/250, train loss 0.356635 in 0.20s\n",
      " [-] epoch  161/250, train loss 0.358355 in 0.22s\n",
      " [-] epoch  162/250, train loss 0.339816 in 0.17s\n",
      " [-] epoch  163/250, train loss 0.325694 in 0.17s\n",
      " [-] epoch  164/250, train loss 0.359134 in 0.20s\n",
      " [-] epoch  165/250, train loss 0.322821 in 0.18s\n",
      " [-] epoch  166/250, train loss 0.326030 in 0.21s\n",
      " [-] epoch  167/250, train loss 0.326628 in 0.19s\n",
      " [-] epoch  168/250, train loss 0.376456 in 0.19s\n",
      " [-] epoch  169/250, train loss 0.338945 in 0.17s\n",
      " [-] epoch  170/250, train loss 0.355659 in 0.17s\n",
      " [-] epoch  171/250, train loss 0.337410 in 0.17s\n",
      " [-] epoch  172/250, train loss 0.320627 in 0.20s\n",
      " [-] epoch  173/250, train loss 0.317686 in 0.20s\n",
      " [-] epoch  174/250, train loss 0.348157 in 0.19s\n",
      " [-] epoch  175/250, train loss 0.341814 in 0.18s\n",
      " [-] epoch  176/250, train loss 0.335152 in 0.19s\n",
      " [-] epoch  177/250, train loss 0.340615 in 0.20s\n",
      " [-] epoch  178/250, train loss 0.345227 in 0.18s\n",
      " [-] epoch  179/250, train loss 0.349630 in 0.20s\n",
      " [-] epoch  180/250, train loss 0.338743 in 0.19s\n",
      " [-] epoch  181/250, train loss 0.314345 in 0.20s\n",
      " [-] epoch  182/250, train loss 0.326183 in 0.17s\n",
      " [-] epoch  183/250, train loss 0.320979 in 0.19s\n",
      " [-] epoch  184/250, train loss 0.329906 in 0.19s\n",
      " [-] epoch  185/250, train loss 0.324966 in 0.20s\n",
      " [-] epoch  186/250, train loss 0.346321 in 0.18s\n",
      " [-] epoch  187/250, train loss 0.307561 in 0.20s\n",
      " [-] epoch  188/250, train loss 0.329752 in 0.20s\n",
      " [-] epoch  189/250, train loss 0.331576 in 0.21s\n",
      " [-] epoch  190/250, train loss 0.338499 in 0.18s\n",
      " [-] epoch  191/250, train loss 0.347386 in 0.19s\n",
      " [-] epoch  192/250, train loss 0.349407 in 0.17s\n",
      " [-] epoch  193/250, train loss 0.346469 in 0.20s\n",
      " [-] epoch  194/250, train loss 0.334037 in 0.19s\n",
      " [-] epoch  195/250, train loss 0.334773 in 0.16s\n",
      " [-] epoch  196/250, train loss 0.328383 in 0.19s\n",
      " [-] epoch  197/250, train loss 0.338927 in 0.17s\n",
      " [-] epoch  198/250, train loss 0.351342 in 0.18s\n",
      " [-] epoch  199/250, train loss 0.291904 in 0.20s\n",
      " [-] epoch  200/250, train loss 0.339431 in 0.21s\n",
      " [-] epoch  201/250, train loss 0.325911 in 0.19s\n",
      " [-] epoch  202/250, train loss 0.316689 in 0.20s\n",
      " [-] epoch  203/250, train loss 0.319058 in 0.19s\n",
      " [-] epoch  204/250, train loss 0.318923 in 0.19s\n",
      " [-] epoch  205/250, train loss 0.315664 in 0.19s\n",
      " [-] epoch  206/250, train loss 0.335975 in 0.19s\n",
      " [-] epoch  207/250, train loss 0.339483 in 0.19s\n",
      " [-] epoch  208/250, train loss 0.343748 in 0.18s\n",
      " [-] epoch  209/250, train loss 0.305658 in 0.19s\n",
      " [-] epoch  210/250, train loss 0.346691 in 0.18s\n",
      " [-] epoch  211/250, train loss 0.313068 in 0.20s\n",
      " [-] epoch  212/250, train loss 0.345463 in 0.18s\n",
      " [-] epoch  213/250, train loss 0.325504 in 0.21s\n",
      " [-] epoch  214/250, train loss 0.330399 in 0.21s\n",
      " [-] epoch  215/250, train loss 0.342436 in 0.17s\n",
      " [-] epoch  216/250, train loss 0.324414 in 0.19s\n",
      " [-] epoch  217/250, train loss 0.320215 in 0.18s\n",
      " [-] epoch  218/250, train loss 0.323198 in 0.20s\n",
      " [-] epoch  219/250, train loss 0.333997 in 0.19s\n",
      " [-] epoch  220/250, train loss 0.318520 in 0.19s\n",
      " [-] epoch  221/250, train loss 0.299113 in 0.20s\n",
      " [-] epoch  222/250, train loss 0.312654 in 0.17s\n",
      " [-] epoch  223/250, train loss 0.319153 in 0.20s\n",
      " [-] epoch  224/250, train loss 0.311480 in 0.19s\n",
      " [-] epoch  225/250, train loss 0.303420 in 0.19s\n",
      " [-] epoch  226/250, train loss 0.344242 in 0.19s\n",
      " [-] epoch  227/250, train loss 0.301848 in 0.19s\n",
      " [-] epoch  228/250, train loss 0.300653 in 0.18s\n",
      " [-] epoch  229/250, train loss 0.334211 in 0.17s\n",
      " [-] epoch  230/250, train loss 0.291090 in 0.19s\n",
      " [-] epoch  231/250, train loss 0.320837 in 0.20s\n",
      " [-] epoch  232/250, train loss 0.323483 in 0.18s\n",
      " [-] epoch  233/250, train loss 0.315509 in 0.20s\n",
      " [-] epoch  234/250, train loss 0.316606 in 0.18s\n",
      " [-] epoch  235/250, train loss 0.318180 in 0.19s\n",
      " [-] epoch  236/250, train loss 0.312081 in 0.18s\n",
      " [-] epoch  237/250, train loss 0.309964 in 0.17s\n",
      " [-] epoch  238/250, train loss 0.287673 in 0.20s\n",
      " [-] epoch  239/250, train loss 0.316496 in 0.20s\n",
      " [-] epoch  240/250, train loss 0.298694 in 0.18s\n",
      " [-] epoch  241/250, train loss 0.298693 in 0.22s\n",
      " [-] epoch  242/250, train loss 0.301679 in 0.18s\n",
      " [-] epoch  243/250, train loss 0.294112 in 0.20s\n",
      " [-] epoch  244/250, train loss 0.305429 in 0.19s\n",
      " [-] epoch  245/250, train loss 0.336362 in 0.17s\n",
      " [-] epoch  246/250, train loss 0.303546 in 0.20s\n",
      " [-] epoch  247/250, train loss 0.313809 in 0.18s\n",
      " [-] epoch  248/250, train loss 0.299754 in 0.19s\n",
      " [-] epoch  249/250, train loss 0.321484 in 0.20s\n",
      " [-] epoch  250/250, train loss 0.317003 in 0.19s\n",
      " [-] test acc. 69.722222%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reducedData = numpy.load('labeled_reduit_100dim.npy')\n",
    "numberOfData = reducedData.shape[0]\n",
    "dimensions = reducedData.shape[1]\n",
    "\n",
    "#print(numberOfData, dimensions)\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train_reduced = minmax_scale(reducedData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test_reduced = minmax_scale(reducedData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données\n",
    "\n",
    "compute_linear_results(X_train_reduced, X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification avec fonction d'activation elu à 9 couches: +- 70% de taux de réussite\n",
    "\n",
    "Classification avec fonction d'activation relu à 9 couches: +- 50% de taux de réussite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenir compte du fait que l'on n'a pas beaucoup de données considérant le nombre de dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Déclaration d'un réseau de neurones de type convolution\n",
    "Ce réseau est personnalisable à souhait: il est possible d'y indiquer, en paramètres d'entrée, le nombre de dimensions en input, le nombre de convolution à avoir dans le réseau en question et le type de la fonction d'activation qui y sera relié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau par convolution permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numberOfFilters):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.numberOfFilters = numberOfFilters\n",
    "        \n",
    "        self.C1 = nn.Conv1d(1, 32, kernel_size=251, stride=2, padding=125, bias=False)\n",
    "        self.B1 = nn.BatchNorm1d(32)\n",
    "        self.C2 = nn.Conv1d(32, 64, kernel_size=175, stride=2, padding=87, bias=False)\n",
    "        \n",
    "        self.B = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.C = nn.Conv1d(64, 64, kernel_size=175, stride=2, padding=87, bias=False)\n",
    "        \n",
    "        self.activation = nn.ELU()\n",
    "        self.F1 = nn.Linear(64, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Sélectionne la taille batch à l'entrée\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.activation(self.B1(self.C1(x)))\n",
    "        x = self.activation(self.B(self.C2(x)))\n",
    "        \n",
    "        for i in range(self.numberOfFilters):\n",
    "            x = self.activation(self.B(self.C(x)))\n",
    "        \n",
    "        # Fait un average pooling sur les caractéristiques\n",
    "        # de chaque filtre\n",
    "        x = x.view(batch_size, 64, -1).mean(dim=2)\n",
    "        \n",
    "        # Couche lineaire et sigmoide\n",
    "        x = self.F1(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Déclaration de la fonction permettant la classification par réseau de neurones profond de type convolution\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_for_convolution(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        values, targets = batch\n",
    "        values = values.unsqueeze(1)\n",
    "        values = values.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(values)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()\n",
    "\n",
    "def compute_convolution_results(X_train, X_test):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 10\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "    \n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    \n",
    "    for i in range(6, 10):\n",
    "        print(\"Je vais ajouter \" + str(i) + \" filters\")\n",
    "        \n",
    "        # Instancier un réseau RAMQNetConvolution\n",
    "        # dans une variable nommée \"model\"\n",
    "        model = RAMQNetConvolution(i)\n",
    "\n",
    "        # Tranfert le réseau au bon endroit\n",
    "        model.to(device)\n",
    "\n",
    "        # Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "        # et la mettre dans une variable nommée criterion\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        # Ne pas oublier de lui donner les hyperparamètres\n",
    "        # d'entraînement : learning rate et momentum!\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "\n",
    "                values = values.unsqueeze(1)\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer\n",
    "                # une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy_for_convolution(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu, 6, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calcul du taux de réussite en classement d'un réseau de convolution sur les données non réduites - référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais ajouter 6 filters\n",
      " [-] epoch    1/10, train loss 0.484058 in 4.02s\n",
      " [-] epoch    2/10, train loss 0.381153 in 3.96s\n",
      " [-] epoch    3/10, train loss 0.364269 in 4.00s\n",
      " [-] epoch    4/10, train loss 0.306918 in 4.01s\n",
      " [-] epoch    5/10, train loss 0.278638 in 4.04s\n",
      " [-] epoch    6/10, train loss 0.272154 in 4.03s\n",
      " [-] epoch    7/10, train loss 0.286477 in 4.03s\n",
      " [-] epoch    8/10, train loss 0.253794 in 4.01s\n",
      " [-] epoch    9/10, train loss 0.275955 in 4.03s\n",
      " [-] epoch   10/10, train loss 0.255494 in 4.04s\n",
      " [-] test acc. 82.777778%\n",
      "Je vais ajouter 7 filters\n",
      " [-] epoch    1/10, train loss 0.524414 in 4.28s\n",
      " [-] epoch    2/10, train loss 0.406604 in 4.29s\n",
      " [-] epoch    3/10, train loss 0.385529 in 4.30s\n",
      " [-] epoch    4/10, train loss 0.335149 in 4.31s\n",
      " [-] epoch    5/10, train loss 0.304158 in 4.33s\n",
      " [-] epoch    6/10, train loss 0.285415 in 4.32s\n",
      " [-] epoch    7/10, train loss 0.303692 in 4.32s\n",
      " [-] epoch    8/10, train loss 0.293430 in 4.35s\n",
      " [-] epoch    9/10, train loss 0.240349 in 4.32s\n",
      " [-] epoch   10/10, train loss 0.239113 in 4.29s\n",
      " [-] test acc. 85.000000%\n",
      "Je vais ajouter 8 filters\n",
      " [-] epoch    1/10, train loss 0.540620 in 4.50s\n",
      " [-] epoch    2/10, train loss 0.439046 in 4.51s\n",
      " [-] epoch    3/10, train loss 0.355697 in 4.55s\n",
      " [-] epoch    4/10, train loss 0.323742 in 4.61s\n",
      " [-] epoch    5/10, train loss 0.269040 in 4.57s\n",
      " [-] epoch    6/10, train loss 0.294889 in 4.57s\n",
      " [-] epoch    7/10, train loss 0.290408 in 4.62s\n",
      " [-] epoch    8/10, train loss 0.242971 in 4.59s\n",
      " [-] epoch    9/10, train loss 0.240234 in 4.59s\n",
      " [-] epoch   10/10, train loss 0.258803 in 4.63s\n",
      " [-] test acc. 83.888889%\n",
      "Je vais ajouter 9 filters\n",
      " [-] epoch    1/10, train loss 0.538614 in 4.86s\n",
      " [-] epoch    2/10, train loss 0.390530 in 4.88s\n",
      " [-] epoch    3/10, train loss 0.341876 in 4.83s\n",
      " [-] epoch    4/10, train loss 0.312966 in 4.88s\n",
      " [-] epoch    5/10, train loss 0.325115 in 4.92s\n",
      " [-] epoch    6/10, train loss 0.305183 in 4.82s\n",
      " [-] epoch    7/10, train loss 0.266637 in 4.81s\n",
      " [-] epoch    8/10, train loss 0.259080 in 4.83s\n",
      " [-] epoch    9/10, train loss 0.261704 in 4.83s\n",
      " [-] epoch   10/10, train loss 0.206695 in 4.88s\n",
      " [-] test acc. 81.944444%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_convolution_results(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Calcul du taux de réussite en classement d'un réseau de convolution sur les données réduites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais ajouter 6 filters\n",
      " [-] epoch    1/10, train loss 0.672189 in 1.76s\n",
      " [-] epoch    2/10, train loss 0.604475 in 1.72s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-514482a43d85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_convolution_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_reduced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_reduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# ne pas oublier de set les kernel size et padding de la façon suivante: 29-14-19-9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-154-3ef711ab09cb>\u001b[0m in \u001b[0;36mcompute_convolution_results\u001b[1;34m(X_train, X_test)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[1;31m# Ajoute le loss de la batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m                 \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compute_convolution_results(X_train_reduced, X_test_reduced) \n",
    "# ne pas oublier de set les kernel size et padding de la façon suivante: 29-14-19-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

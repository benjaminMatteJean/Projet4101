{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning pour Classification\n",
    "\n",
    "Ce notebook regroupe quelques configurations d'algorithmes de Deep Learning pour la classification de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Variable                        Possibilités             Nombre de colonne (+1)\n",
      "Spécialité de médecins\t        33 codes de spécialité\t        1-33\n",
      "Sexe de médecin \t            2\t                            34-35\n",
      "Langue de correspondance \t    2\t                            36-37\n",
      "Université de graduation \t    14 universités\t                38-51\n",
      "Plage horaire de facturation\t3 (AM, PM, toute la journée)\t52-54\n",
      "Agence de représentation\t    845 codes d’agences             55-899\n",
      "Établissements                  241 établissements\t            900-1140\n",
      "ActeMedical                     2 (Oui et Non)\t                1141-1142\n",
      "Activités médico-admin          2 (Oui et Non)\t                1143-1144\n",
      "Activités d’enseignement\t    2 (Oui et Non)\t                1145-1146\n",
      "Entente LE\t                    2 (Oui et Non)\t                1147-1148\n",
      "Activité en santé communautaire\t2 (Oui et Non)\t                1149-1150\n",
      "Activité en santé comm. CSST\t2 (Oui et Non)\t                1151-1152\n",
      "Activité en santé comm. INSP\t2 (Oui et Non)\t                1153-1154\n",
      "Année de graduation\t                                            1155\n",
      "semaine de l’année associée à la facturation\t                1156\n",
      "Nombre de jours dès la dernière facture avant le Perdiem\t    1157\n",
      "Année de naissance\t                                            1158\n",
      "Année de début de pratique\t                                    1159\n",
      "Expérience\t                                                    1160\n",
      "Salaire régulier\t                                            1161\n",
      "Salaire payé\t                                                1162\n",
      "Nombre de Perdiem pendant l’année\t                            1163\n",
      "Jour de la semaine associé à la facturation\t                    1164\n",
      "Nombre de Perdiem\t                                            1165\n",
      "Nombre d’heures facturées\t                                    1166\n",
      "Nombre de services avec bénéficiaire\t                        1167\n",
      "Nombre de services sans bénéficiaire\t                        1168\n",
      "Nombre de patients\t                                            1169\n",
      "Montant réclamé avec bénéficiaire\t                            1170\n",
      "Montant réclamé sans bénéficiaire\t                            1171\n",
      "Nombre de service total\t                                        1172\n",
      "Nombre de Perdiem facturé le jour avant\t                        1173\n",
      "Nombre d’heures facturées le jour avant\t                        1174\n",
      "Nombre de services avec bénéficiaire le jour avant\t            1175\n",
      "Nombre de services sans bénéficiaire le jour avant\t            1176\n",
      "Nombre de patients le jour avant\t                            1177\n",
      "Montant réclamé avec bénéficiaire le jour avant\t                1178\n",
      "Montant réclamé sans bénéficiaire le jour avant\t                1179\n",
      "Nombre de services total le jour avant\t                        1180\n",
      "âge de médecin le jour avant \t                                1181\n",
      "Nombre de Perdiem facturés la semaine d’avant\t                1182\n",
      "\n",
      "Target:\n",
      "Conformité\n",
      "(-1 : non contrôlé, 1 : cas conforme, 0 : cas non-conforme)\t    1183\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enregistrement des jeux de données comportant les données avec étiquettes\n",
    "Ici, le jeu de données comportant uniquement les données avec des étiquettes de classe est téléchargé (dans sa forme initiale). Il est ensuite séparé en jeux d'entraînement et de test en plus d'être normalisé dans toutes les dimensions. Il en va de même pour le jeu de données dont les dimensions ont été réduites suite à l'étape de réduction de dimensionnalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData = numpy.load('labeled.npy')\n",
    "numberOfData = labeledData.shape[0] # 1441\n",
    "dimensions = labeledData.shape[1] # 1183\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train = minmax_scale(labeledData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test = minmax_scale(labeledData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données\n",
    "\n",
    "reducedData = numpy.load('labeled_reduit_100dim.npy')\n",
    "numberOfData = reducedData.shape[0]\n",
    "dimensions = reducedData.shape[1]\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train_reduced = minmax_scale(reducedData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test_reduced = minmax_scale(reducedData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Déclaration de fonctions utilitaires (utilisées dans les TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_sampler(dataset):\n",
    "    def make_weights_for_balanced_classes(images, n_classes):                        \n",
    "        count = [0] * n_classes                                                      \n",
    "        for item in images:                                                         \n",
    "            count[int(item[1])] += 1                                                     \n",
    "        weight_per_class = [0.] * n_classes                                      \n",
    "        N = float(sum(count))                                                   \n",
    "        for i in range(n_classes):                                                   \n",
    "            weight_per_class[i] = N/float(count[i])                                 \n",
    "        weight = [0] * len(images)                                              \n",
    "        for idx, val in enumerate(images):                                          \n",
    "            weight[idx] = weight_per_class[int(val[1])]                                  \n",
    "        return weight\n",
    "\n",
    "    n_classes = numpy.unique(dataset.targets)\n",
    "    weights = make_weights_for_balanced_classes(dataset.data, len(n_classes))                                                         \n",
    "    weights = torch.DoubleTensor(weights)                 \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    return sampler\n",
    "\n",
    "def compute_accuracy(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition du Dataset\n",
    "Définition de la classe RAMQDataset, une classe qui hérite de la classe abstraite torch.utils.data.Dataset. Comme mentionné dans la documentation, les méthodes __getitem__ et __len__ sont surchargées afin d'avoir un jeu de données utilisable par PyTorch. Le data accepté en paramètres est un array numpy dont la dernière dimension est la valeur de l'étiquette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Cette classe sert à définir le dataset RAMQ pour PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for elem in data:\n",
    "            if isinstance(elem, numpy.ndarray):\n",
    "                elem = elem.tolist()\n",
    "                elem_data = torch.Tensor(elem[:-1])\n",
    "                elem_target = torch.FloatTensor([elem[-1]])\n",
    "            # garde les paramètres en mémoire\n",
    "            self.data += [(elem_data, elem_target[0])]\n",
    "            \n",
    "        self.targets = numpy.array(data[:, -1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Déclaration d'un réseau de neurones de base: 1 couche linéaire - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.F1 = nn.Linear(1182, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.F1(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on tente de classifier les 1441 données à 1182 dimensions avec un réseau de neurones à 1 couche linéaire, on obtient un pourcentage de classement de l'ordre d'environ 82%. Le calcul ne sera pas présenté ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calcul du taux de réussite en classement d'un SVM linéaire de base sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on all dimensions: 0.8361111111111111\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM performance with all dimensions\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train[:, : -1], X_train[:, -1])\n",
    "score = clf.score(X_test[:, : -1], X_test[:, -1])\n",
    "print(\"Score on all dimensions:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Déclaration d'un réseau de neurones de type linéaire: multicouches\n",
    "Ce réseau est personnalisable à souhait: il est possible d'y indiquer, en paramètres d'entrée, le nombre de dimensions en input, le nombre de couches linéaires à avoir dans le réseau en question et le type de la fonction d'activation qui y sera relié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_layers, activation = nn.ELU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        reduction_ratio = (1/n_inputs)**(1/n_layers)\n",
    "        \n",
    "        layers = []\n",
    "        layer_lenght = n_inputs\n",
    "        \n",
    "        # Calculate nb of layers and nb of neurons\n",
    "        # Builds a list (ex:[1000, 500, 250, ... until 1])\n",
    "        while layer_lenght > 1:\n",
    "            layers.append(layer_lenght)\n",
    "            layer_lenght = int(layer_lenght*reduction_ratio)\n",
    "        layers.append(1)\n",
    "        \n",
    "        # Build network layers\n",
    "        network_layers = []\n",
    "        for i, n_neurons in enumerate(layers[:-1]):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            network_layers.append(layer)\n",
    "            network_layers.append(activation)\n",
    "            network_layers.append(nn.BatchNorm1d(layers[i+1]))\n",
    "        network_layers[-1] = nn.Sigmoid()\n",
    "            \n",
    "        # Build pytorch sequential network\n",
    "        self.network = nn.Sequential(\n",
    "            *network_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Exécute l'inférence du réseau. L'ordre \n",
    "        # d'exécution ici est important.\n",
    "        x = self.network(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Déclaration de la fonction permettant l'affichage du pourcentage d'efficacité en classement selon le nombre de couches d'un réseau de neurones \"x\"\n",
    "Cette méthode n'a besoin, en entrées, que d'un tableau des pourcentages d'efficacité pour le nombre de couches désiré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(test_accu, start, end):\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(test_accu, label='Test accuracy')\n",
    "    ax.set_xlim(start, end)\n",
    "    pyplot.grid()\n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Déclaration de la fonction permettant la classification par réseau de neurones profond de type multicouches linéaire\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_results(X_train, X_test):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cpu\" \n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 250\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Définit le nombre de dimensions des données avec lesquelles on travaille (la dernière dimension étant l'étiquette)\n",
    "    dimensions = X_train.shape[1] - 1  \n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "\n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "\n",
    "    # Instancier un réseau RAMQNetLinear\n",
    "    # dans une variable nommée \"model\"\n",
    "    for i in range(1,10):\n",
    "        print(\"Je vais utiliser \" + str(i) + \" layers\")\n",
    "        model = RAMQNetLinear(dimensions, i)\n",
    "\n",
    "        # Tranfert le réseau au bon endroit\n",
    "        model.to(device)\n",
    "\n",
    "        # Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "        # et la mettre dans une variable nommée criterion\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        # Ne pas oublier de lui donner les hyperparamètres\n",
    "        # d'entraînement : learning rate et momentum!\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer\n",
    "                # une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu, 1, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calcul du taux de réussite en classement d'un réseau linéaire multicouches sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.685915 in 0.98s\n",
      " [-] epoch    2/250, train loss 0.657282 in 0.04s\n",
      " [-] epoch    3/250, train loss 0.631358 in 0.03s\n",
      " [-] epoch    4/250, train loss 0.618534 in 0.04s\n",
      " [-] epoch    5/250, train loss 0.610932 in 0.04s\n",
      " [-] epoch    6/250, train loss 0.586163 in 0.04s\n",
      " [-] epoch    7/250, train loss 0.582521 in 0.04s\n",
      " [-] epoch    8/250, train loss 0.580246 in 0.04s\n",
      " [-] epoch    9/250, train loss 0.571302 in 0.03s\n",
      " [-] epoch   10/250, train loss 0.563058 in 0.04s\n",
      " [-] epoch   11/250, train loss 0.550458 in 0.03s\n",
      " [-] epoch   12/250, train loss 0.547995 in 0.04s\n",
      " [-] epoch   13/250, train loss 0.523392 in 0.04s\n",
      " [-] epoch   14/250, train loss 0.548445 in 0.03s\n",
      " [-] epoch   15/250, train loss 0.540680 in 0.03s\n",
      " [-] epoch   16/250, train loss 0.526839 in 0.04s\n",
      " [-] epoch   17/250, train loss 0.530012 in 0.03s\n",
      " [-] epoch   18/250, train loss 0.511358 in 0.03s\n",
      " [-] epoch   19/250, train loss 0.518176 in 0.04s\n",
      " [-] epoch   20/250, train loss 0.498379 in 0.04s\n",
      " [-] epoch   21/250, train loss 0.511509 in 0.03s\n",
      " [-] epoch   22/250, train loss 0.510200 in 0.03s\n",
      " [-] epoch   23/250, train loss 0.510158 in 0.04s\n",
      " [-] epoch   24/250, train loss 0.499524 in 0.03s\n",
      " [-] epoch   25/250, train loss 0.504578 in 0.04s\n",
      " [-] epoch   26/250, train loss 0.493598 in 0.03s\n",
      " [-] epoch   27/250, train loss 0.500381 in 0.03s\n",
      " [-] epoch   28/250, train loss 0.493276 in 0.03s\n",
      " [-] epoch   29/250, train loss 0.488740 in 0.03s\n",
      " [-] epoch   30/250, train loss 0.492109 in 0.04s\n",
      " [-] epoch   31/250, train loss 0.492195 in 0.03s\n",
      " [-] epoch   32/250, train loss 0.472864 in 0.04s\n",
      " [-] epoch   33/250, train loss 0.491634 in 0.03s\n",
      " [-] epoch   34/250, train loss 0.470162 in 0.04s\n",
      " [-] epoch   35/250, train loss 0.475652 in 0.03s\n",
      " [-] epoch   36/250, train loss 0.475267 in 0.03s\n",
      " [-] epoch   37/250, train loss 0.468860 in 0.04s\n",
      " [-] epoch   38/250, train loss 0.474938 in 0.03s\n",
      " [-] epoch   39/250, train loss 0.492723 in 0.03s\n",
      " [-] epoch   40/250, train loss 0.475233 in 0.04s\n",
      " [-] epoch   41/250, train loss 0.453226 in 0.03s\n",
      " [-] epoch   42/250, train loss 0.490242 in 0.03s\n",
      " [-] epoch   43/250, train loss 0.475678 in 0.04s\n",
      " [-] epoch   44/250, train loss 0.477998 in 0.04s\n",
      " [-] epoch   45/250, train loss 0.478682 in 0.04s\n",
      " [-] epoch   46/250, train loss 0.460803 in 0.03s\n",
      " [-] epoch   47/250, train loss 0.469375 in 0.03s\n",
      " [-] epoch   48/250, train loss 0.459145 in 0.03s\n",
      " [-] epoch   49/250, train loss 0.468498 in 0.04s\n",
      " [-] epoch   50/250, train loss 0.466955 in 0.03s\n",
      " [-] epoch   51/250, train loss 0.466928 in 0.03s\n",
      " [-] epoch   52/250, train loss 0.471648 in 0.03s\n",
      " [-] epoch   53/250, train loss 0.453324 in 0.04s\n",
      " [-] epoch   54/250, train loss 0.466094 in 0.03s\n",
      " [-] epoch   55/250, train loss 0.463003 in 0.03s\n",
      " [-] epoch   56/250, train loss 0.481419 in 0.03s\n",
      " [-] epoch   57/250, train loss 0.452382 in 0.04s\n",
      " [-] epoch   58/250, train loss 0.461742 in 0.03s\n",
      " [-] epoch   59/250, train loss 0.449131 in 0.03s\n",
      " [-] epoch   60/250, train loss 0.463012 in 0.03s\n",
      " [-] epoch   61/250, train loss 0.452635 in 0.03s\n",
      " [-] epoch   62/250, train loss 0.452698 in 0.03s\n",
      " [-] epoch   63/250, train loss 0.471533 in 0.03s\n",
      " [-] epoch   64/250, train loss 0.438650 in 0.03s\n",
      " [-] epoch   65/250, train loss 0.450308 in 0.03s\n",
      " [-] epoch   66/250, train loss 0.439755 in 0.04s\n",
      " [-] epoch   67/250, train loss 0.446674 in 0.03s\n",
      " [-] epoch   68/250, train loss 0.451853 in 0.03s\n",
      " [-] epoch   69/250, train loss 0.479934 in 0.03s\n",
      " [-] epoch   70/250, train loss 0.455578 in 0.04s\n",
      " [-] epoch   71/250, train loss 0.435056 in 0.03s\n",
      " [-] epoch   72/250, train loss 0.436002 in 0.03s\n",
      " [-] epoch   73/250, train loss 0.456062 in 0.03s\n",
      " [-] epoch   74/250, train loss 0.437422 in 0.04s\n",
      " [-] epoch   75/250, train loss 0.438888 in 0.03s\n",
      " [-] epoch   76/250, train loss 0.452374 in 0.03s\n",
      " [-] epoch   77/250, train loss 0.444022 in 0.03s\n",
      " [-] epoch   78/250, train loss 0.453103 in 0.03s\n",
      " [-] epoch   79/250, train loss 0.427798 in 0.03s\n",
      " [-] epoch   80/250, train loss 0.438003 in 0.04s\n",
      " [-] epoch   81/250, train loss 0.440315 in 0.03s\n",
      " [-] epoch   82/250, train loss 0.425541 in 0.03s\n",
      " [-] epoch   83/250, train loss 0.448942 in 0.03s\n",
      " [-] epoch   84/250, train loss 0.448585 in 0.03s\n",
      " [-] epoch   85/250, train loss 0.443185 in 0.03s\n",
      " [-] epoch   86/250, train loss 0.438799 in 0.03s\n",
      " [-] epoch   87/250, train loss 0.461042 in 0.04s\n",
      " [-] epoch   88/250, train loss 0.443803 in 0.03s\n",
      " [-] epoch   89/250, train loss 0.436791 in 0.03s\n",
      " [-] epoch   90/250, train loss 0.440930 in 0.03s\n",
      " [-] epoch   91/250, train loss 0.427706 in 0.03s\n",
      " [-] epoch   92/250, train loss 0.442717 in 0.03s\n",
      " [-] epoch   93/250, train loss 0.440284 in 0.03s\n",
      " [-] epoch   94/250, train loss 0.432228 in 0.03s\n",
      " [-] epoch   95/250, train loss 0.420540 in 0.03s\n",
      " [-] epoch   96/250, train loss 0.411790 in 0.03s\n",
      " [-] epoch   97/250, train loss 0.444920 in 0.03s\n",
      " [-] epoch   98/250, train loss 0.427655 in 0.03s\n",
      " [-] epoch   99/250, train loss 0.426560 in 0.03s\n",
      " [-] epoch  100/250, train loss 0.434870 in 0.03s\n",
      " [-] epoch  101/250, train loss 0.438816 in 0.03s\n",
      " [-] epoch  102/250, train loss 0.433351 in 0.04s\n",
      " [-] epoch  103/250, train loss 0.422081 in 0.03s\n",
      " [-] epoch  104/250, train loss 0.429683 in 0.03s\n",
      " [-] epoch  105/250, train loss 0.433988 in 0.03s\n",
      " [-] epoch  106/250, train loss 0.426744 in 0.04s\n",
      " [-] epoch  107/250, train loss 0.416904 in 0.03s\n",
      " [-] epoch  108/250, train loss 0.440878 in 0.03s\n",
      " [-] epoch  109/250, train loss 0.436659 in 0.03s\n",
      " [-] epoch  110/250, train loss 0.414261 in 0.03s\n",
      " [-] epoch  111/250, train loss 0.417993 in 0.03s\n",
      " [-] epoch  112/250, train loss 0.421110 in 0.04s\n",
      " [-] epoch  113/250, train loss 0.421869 in 0.03s\n",
      " [-] epoch  114/250, train loss 0.420142 in 0.03s\n",
      " [-] epoch  115/250, train loss 0.417828 in 0.03s\n",
      " [-] epoch  116/250, train loss 0.411737 in 0.03s\n",
      " [-] epoch  117/250, train loss 0.444858 in 0.03s\n",
      " [-] epoch  118/250, train loss 0.442230 in 0.03s\n",
      " [-] epoch  119/250, train loss 0.440367 in 0.03s\n",
      " [-] epoch  120/250, train loss 0.418303 in 0.03s\n",
      " [-] epoch  121/250, train loss 0.434677 in 0.03s\n",
      " [-] epoch  122/250, train loss 0.410743 in 0.03s\n",
      " [-] epoch  123/250, train loss 0.421667 in 0.03s\n",
      " [-] epoch  124/250, train loss 0.430521 in 0.03s\n",
      " [-] epoch  125/250, train loss 0.416167 in 0.03s\n",
      " [-] epoch  126/250, train loss 0.400596 in 0.03s\n",
      " [-] epoch  127/250, train loss 0.422568 in 0.03s\n",
      " [-] epoch  128/250, train loss 0.432618 in 0.03s\n",
      " [-] epoch  129/250, train loss 0.422522 in 0.03s\n",
      " [-] epoch  130/250, train loss 0.420617 in 0.03s\n",
      " [-] epoch  131/250, train loss 0.427530 in 0.03s\n",
      " [-] epoch  132/250, train loss 0.430010 in 0.03s\n",
      " [-] epoch  133/250, train loss 0.404448 in 0.03s\n",
      " [-] epoch  134/250, train loss 0.426401 in 0.04s\n",
      " [-] epoch  135/250, train loss 0.389448 in 0.03s\n",
      " [-] epoch  136/250, train loss 0.410944 in 0.03s\n",
      " [-] epoch  137/250, train loss 0.427795 in 0.03s\n",
      " [-] epoch  138/250, train loss 0.427832 in 0.03s\n",
      " [-] epoch  139/250, train loss 0.411183 in 0.03s\n",
      " [-] epoch  140/250, train loss 0.420438 in 0.03s\n",
      " [-] epoch  141/250, train loss 0.420567 in 0.03s\n",
      " [-] epoch  142/250, train loss 0.402071 in 0.04s\n",
      " [-] epoch  143/250, train loss 0.426361 in 0.03s\n",
      " [-] epoch  144/250, train loss 0.413566 in 0.03s\n",
      " [-] epoch  145/250, train loss 0.426572 in 0.03s\n",
      " [-] epoch  146/250, train loss 0.423531 in 0.03s\n",
      " [-] epoch  147/250, train loss 0.418140 in 0.03s\n",
      " [-] epoch  148/250, train loss 0.405461 in 0.03s\n",
      " [-] epoch  149/250, train loss 0.425310 in 0.03s\n",
      " [-] epoch  150/250, train loss 0.404321 in 0.03s\n",
      " [-] epoch  151/250, train loss 0.424867 in 0.03s\n",
      " [-] epoch  152/250, train loss 0.412630 in 0.04s\n",
      " [-] epoch  153/250, train loss 0.409495 in 0.03s\n",
      " [-] epoch  154/250, train loss 0.418203 in 0.03s\n",
      " [-] epoch  155/250, train loss 0.422409 in 0.03s\n",
      " [-] epoch  156/250, train loss 0.423545 in 0.04s\n",
      " [-] epoch  157/250, train loss 0.402719 in 0.04s\n",
      " [-] epoch  158/250, train loss 0.405047 in 0.03s\n",
      " [-] epoch  159/250, train loss 0.424049 in 0.03s\n",
      " [-] epoch  160/250, train loss 0.407691 in 0.03s\n",
      " [-] epoch  161/250, train loss 0.411040 in 0.04s\n",
      " [-] epoch  162/250, train loss 0.417801 in 0.03s\n",
      " [-] epoch  163/250, train loss 0.418453 in 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  164/250, train loss 0.403088 in 0.03s\n",
      " [-] epoch  165/250, train loss 0.421958 in 0.03s\n",
      " [-] epoch  166/250, train loss 0.392303 in 0.03s\n",
      " [-] epoch  167/250, train loss 0.404900 in 0.04s\n",
      " [-] epoch  168/250, train loss 0.423312 in 0.04s\n",
      " [-] epoch  169/250, train loss 0.406624 in 0.04s\n",
      " [-] epoch  170/250, train loss 0.422674 in 0.04s\n",
      " [-] epoch  171/250, train loss 0.419781 in 0.04s\n",
      " [-] epoch  172/250, train loss 0.402896 in 0.04s\n",
      " [-] epoch  173/250, train loss 0.407468 in 0.05s\n",
      " [-] epoch  174/250, train loss 0.400910 in 0.05s\n",
      " [-] epoch  175/250, train loss 0.407586 in 0.03s\n",
      " [-] epoch  176/250, train loss 0.406937 in 0.04s\n",
      " [-] epoch  177/250, train loss 0.433540 in 0.05s\n",
      " [-] epoch  178/250, train loss 0.425574 in 0.04s\n",
      " [-] epoch  179/250, train loss 0.405207 in 0.03s\n",
      " [-] epoch  180/250, train loss 0.403396 in 0.04s\n",
      " [-] epoch  181/250, train loss 0.387099 in 0.04s\n",
      " [-] epoch  182/250, train loss 0.407921 in 0.04s\n",
      " [-] epoch  183/250, train loss 0.424566 in 0.03s\n",
      " [-] epoch  184/250, train loss 0.385124 in 0.04s\n",
      " [-] epoch  185/250, train loss 0.404609 in 0.04s\n",
      " [-] epoch  186/250, train loss 0.406086 in 0.04s\n",
      " [-] epoch  187/250, train loss 0.419817 in 0.03s\n",
      " [-] epoch  188/250, train loss 0.418154 in 0.04s\n",
      " [-] epoch  189/250, train loss 0.403542 in 0.04s\n",
      " [-] epoch  190/250, train loss 0.406216 in 0.04s\n",
      " [-] epoch  191/250, train loss 0.401860 in 0.04s\n",
      " [-] epoch  192/250, train loss 0.405118 in 0.03s\n",
      " [-] epoch  193/250, train loss 0.395444 in 0.04s\n",
      " [-] epoch  194/250, train loss 0.410456 in 0.05s\n",
      " [-] epoch  195/250, train loss 0.400622 in 0.04s\n",
      " [-] epoch  196/250, train loss 0.431656 in 0.04s\n",
      " [-] epoch  197/250, train loss 0.395146 in 0.04s\n",
      " [-] epoch  198/250, train loss 0.408726 in 0.04s\n",
      " [-] epoch  199/250, train loss 0.414754 in 0.04s\n",
      " [-] epoch  200/250, train loss 0.412419 in 0.04s\n",
      " [-] epoch  201/250, train loss 0.405092 in 0.04s\n",
      " [-] epoch  202/250, train loss 0.397736 in 0.04s\n",
      " [-] epoch  203/250, train loss 0.375699 in 0.04s\n",
      " [-] epoch  204/250, train loss 0.407292 in 0.04s\n",
      " [-] epoch  205/250, train loss 0.397354 in 0.04s\n",
      " [-] epoch  206/250, train loss 0.396589 in 0.03s\n",
      " [-] epoch  207/250, train loss 0.408801 in 0.03s\n",
      " [-] epoch  208/250, train loss 0.370735 in 0.04s\n",
      " [-] epoch  209/250, train loss 0.402814 in 0.05s\n",
      " [-] epoch  210/250, train loss 0.396277 in 0.03s\n",
      " [-] epoch  211/250, train loss 0.393464 in 0.05s\n",
      " [-] epoch  212/250, train loss 0.403584 in 0.04s\n",
      " [-] epoch  213/250, train loss 0.402396 in 0.04s\n",
      " [-] epoch  214/250, train loss 0.413725 in 0.03s\n",
      " [-] epoch  215/250, train loss 0.376118 in 0.04s\n",
      " [-] epoch  216/250, train loss 0.401216 in 0.04s\n",
      " [-] epoch  217/250, train loss 0.395369 in 0.04s\n",
      " [-] epoch  218/250, train loss 0.391051 in 0.03s\n",
      " [-] epoch  219/250, train loss 0.396772 in 0.04s\n",
      " [-] epoch  220/250, train loss 0.381370 in 0.04s\n",
      " [-] epoch  221/250, train loss 0.401909 in 0.05s\n",
      " [-] epoch  222/250, train loss 0.404486 in 0.03s\n",
      " [-] epoch  223/250, train loss 0.398169 in 0.04s\n",
      " [-] epoch  224/250, train loss 0.410319 in 0.04s\n",
      " [-] epoch  225/250, train loss 0.409702 in 0.04s\n",
      " [-] epoch  226/250, train loss 0.403551 in 0.04s\n",
      " [-] epoch  227/250, train loss 0.412865 in 0.04s\n",
      " [-] epoch  228/250, train loss 0.402467 in 0.04s\n",
      " [-] epoch  229/250, train loss 0.394436 in 0.05s\n",
      " [-] epoch  230/250, train loss 0.393968 in 0.04s\n",
      " [-] epoch  231/250, train loss 0.389393 in 0.04s\n",
      " [-] epoch  232/250, train loss 0.408876 in 0.04s\n",
      " [-] epoch  233/250, train loss 0.395526 in 0.03s\n",
      " [-] epoch  234/250, train loss 0.386909 in 0.04s\n",
      " [-] epoch  235/250, train loss 0.391612 in 0.03s\n",
      " [-] epoch  236/250, train loss 0.406901 in 0.04s\n",
      " [-] epoch  237/250, train loss 0.416069 in 0.04s\n",
      " [-] epoch  238/250, train loss 0.394595 in 0.04s\n",
      " [-] epoch  239/250, train loss 0.385789 in 0.04s\n",
      " [-] epoch  240/250, train loss 0.397039 in 0.03s\n",
      " [-] epoch  241/250, train loss 0.412137 in 0.04s\n",
      " [-] epoch  242/250, train loss 0.400888 in 0.04s\n",
      " [-] epoch  243/250, train loss 0.399948 in 0.03s\n",
      " [-] epoch  244/250, train loss 0.391659 in 0.04s\n",
      " [-] epoch  245/250, train loss 0.408044 in 0.04s\n",
      " [-] epoch  246/250, train loss 0.388706 in 0.05s\n",
      " [-] epoch  247/250, train loss 0.396649 in 0.04s\n",
      " [-] epoch  248/250, train loss 0.390798 in 0.04s\n",
      " [-] epoch  249/250, train loss 0.390331 in 0.05s\n",
      " [-] epoch  250/250, train loss 0.394586 in 0.04s\n",
      " [-] test acc. 83.333333%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.554152 in 0.10s\n",
      " [-] epoch    2/250, train loss 0.496921 in 0.11s\n",
      " [-] epoch    3/250, train loss 0.452064 in 0.10s\n",
      " [-] epoch    4/250, train loss 0.457112 in 0.10s\n",
      " [-] epoch    5/250, train loss 0.397766 in 0.11s\n",
      " [-] epoch    6/250, train loss 0.432788 in 0.10s\n",
      " [-] epoch    7/250, train loss 0.431321 in 0.09s\n",
      " [-] epoch    8/250, train loss 0.391605 in 0.09s\n",
      " [-] epoch    9/250, train loss 0.407082 in 0.10s\n",
      " [-] epoch   10/250, train loss 0.419575 in 0.12s\n",
      " [-] epoch   11/250, train loss 0.381532 in 0.10s\n",
      " [-] epoch   12/250, train loss 0.394217 in 0.10s\n",
      " [-] epoch   13/250, train loss 0.387165 in 0.10s\n",
      " [-] epoch   14/250, train loss 0.361213 in 0.10s\n",
      " [-] epoch   15/250, train loss 0.365004 in 0.12s\n",
      " [-] epoch   16/250, train loss 0.373228 in 0.12s\n",
      " [-] epoch   17/250, train loss 0.375607 in 0.12s\n",
      " [-] epoch   18/250, train loss 0.371532 in 0.12s\n",
      " [-] epoch   19/250, train loss 0.339399 in 0.11s\n",
      " [-] epoch   20/250, train loss 0.343847 in 0.12s\n",
      " [-] epoch   21/250, train loss 0.346087 in 0.11s\n",
      " [-] epoch   22/250, train loss 0.369263 in 0.10s\n",
      " [-] epoch   23/250, train loss 0.350049 in 0.10s\n",
      " [-] epoch   24/250, train loss 0.357661 in 0.12s\n",
      " [-] epoch   25/250, train loss 0.358527 in 0.12s\n",
      " [-] epoch   26/250, train loss 0.364113 in 0.12s\n",
      " [-] epoch   27/250, train loss 0.374037 in 0.09s\n",
      " [-] epoch   28/250, train loss 0.318529 in 0.12s\n",
      " [-] epoch   29/250, train loss 0.344479 in 0.11s\n",
      " [-] epoch   30/250, train loss 0.382235 in 0.10s\n",
      " [-] epoch   31/250, train loss 0.349711 in 0.11s\n",
      " [-] epoch   32/250, train loss 0.324097 in 0.10s\n",
      " [-] epoch   33/250, train loss 0.375505 in 0.08s\n",
      " [-] epoch   34/250, train loss 0.348217 in 0.11s\n",
      " [-] epoch   35/250, train loss 0.366111 in 0.12s\n",
      " [-] epoch   36/250, train loss 0.379303 in 0.10s\n",
      " [-] epoch   37/250, train loss 0.341688 in 0.10s\n",
      " [-] epoch   38/250, train loss 0.360057 in 0.10s\n",
      " [-] epoch   39/250, train loss 0.328082 in 0.10s\n",
      " [-] epoch   40/250, train loss 0.381983 in 0.11s\n",
      " [-] epoch   41/250, train loss 0.326584 in 0.08s\n",
      " [-] epoch   42/250, train loss 0.337469 in 0.09s\n",
      " [-] epoch   43/250, train loss 0.377275 in 0.10s\n",
      " [-] epoch   44/250, train loss 0.332208 in 0.09s\n",
      " [-] epoch   45/250, train loss 0.333652 in 0.11s\n",
      " [-] epoch   46/250, train loss 0.324055 in 0.11s\n",
      " [-] epoch   47/250, train loss 0.340208 in 0.08s\n",
      " [-] epoch   48/250, train loss 0.330625 in 0.09s\n",
      " [-] epoch   49/250, train loss 0.349478 in 0.09s\n",
      " [-] epoch   50/250, train loss 0.328087 in 0.10s\n",
      " [-] epoch   51/250, train loss 0.325348 in 0.09s\n",
      " [-] epoch   52/250, train loss 0.361834 in 0.08s\n",
      " [-] epoch   53/250, train loss 0.352572 in 0.09s\n",
      " [-] epoch   54/250, train loss 0.360272 in 0.10s\n",
      " [-] epoch   55/250, train loss 0.357067 in 0.11s\n",
      " [-] epoch   56/250, train loss 0.325374 in 0.10s\n",
      " [-] epoch   57/250, train loss 0.350981 in 0.12s\n",
      " [-] epoch   58/250, train loss 0.329701 in 0.10s\n",
      " [-] epoch   59/250, train loss 0.329377 in 0.12s\n",
      " [-] epoch   60/250, train loss 0.342472 in 0.10s\n",
      " [-] epoch   61/250, train loss 0.344355 in 0.10s\n",
      " [-] epoch   62/250, train loss 0.334536 in 0.10s\n",
      " [-] epoch   63/250, train loss 0.326334 in 0.10s\n",
      " [-] epoch   64/250, train loss 0.332312 in 0.08s\n",
      " [-] epoch   65/250, train loss 0.325360 in 0.08s\n",
      " [-] epoch   66/250, train loss 0.338469 in 0.10s\n",
      " [-] epoch   67/250, train loss 0.338704 in 0.10s\n",
      " [-] epoch   68/250, train loss 0.338808 in 0.10s\n",
      " [-] epoch   69/250, train loss 0.325905 in 0.10s\n",
      " [-] epoch   70/250, train loss 0.325558 in 0.10s\n",
      " [-] epoch   71/250, train loss 0.326433 in 0.10s\n",
      " [-] epoch   72/250, train loss 0.310060 in 0.09s\n",
      " [-] epoch   73/250, train loss 0.312421 in 0.10s\n",
      " [-] epoch   74/250, train loss 0.350688 in 0.11s\n",
      " [-] epoch   75/250, train loss 0.339952 in 0.09s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.303871 in 0.09s\n",
      " [-] epoch   77/250, train loss 0.322097 in 0.12s\n",
      " [-] epoch   78/250, train loss 0.321703 in 0.12s\n",
      " [-] epoch   79/250, train loss 0.325162 in 0.12s\n",
      " [-] epoch   80/250, train loss 0.335867 in 0.12s\n",
      " [-] epoch   81/250, train loss 0.329176 in 0.12s\n",
      " [-] epoch   82/250, train loss 0.305898 in 0.13s\n",
      " [-] epoch   83/250, train loss 0.335387 in 0.13s\n",
      " [-] epoch   84/250, train loss 0.344781 in 0.10s\n",
      " [-] epoch   85/250, train loss 0.318464 in 0.12s\n",
      " [-] epoch   86/250, train loss 0.300321 in 0.12s\n",
      " [-] epoch   87/250, train loss 0.312597 in 0.12s\n",
      " [-] epoch   88/250, train loss 0.322259 in 0.10s\n",
      " [-] epoch   89/250, train loss 0.339639 in 0.13s\n",
      " [-] epoch   90/250, train loss 0.306656 in 0.13s\n",
      " [-] epoch   91/250, train loss 0.319685 in 0.12s\n",
      " [-] epoch   92/250, train loss 0.328456 in 0.11s\n",
      " [-] epoch   93/250, train loss 0.303767 in 0.12s\n",
      " [-] epoch   94/250, train loss 0.323494 in 0.12s\n",
      " [-] epoch   95/250, train loss 0.313957 in 0.11s\n",
      " [-] epoch   96/250, train loss 0.309462 in 0.13s\n",
      " [-] epoch   97/250, train loss 0.341983 in 0.13s\n",
      " [-] epoch   98/250, train loss 0.295514 in 0.12s\n",
      " [-] epoch   99/250, train loss 0.304323 in 0.12s\n",
      " [-] epoch  100/250, train loss 0.329632 in 0.10s\n",
      " [-] epoch  101/250, train loss 0.316985 in 0.11s\n",
      " [-] epoch  102/250, train loss 0.314043 in 0.11s\n",
      " [-] epoch  103/250, train loss 0.326156 in 0.12s\n",
      " [-] epoch  104/250, train loss 0.300888 in 0.12s\n",
      " [-] epoch  105/250, train loss 0.310698 in 0.12s\n",
      " [-] epoch  106/250, train loss 0.317796 in 0.13s\n",
      " [-] epoch  107/250, train loss 0.308635 in 0.13s\n",
      " [-] epoch  108/250, train loss 0.332073 in 0.12s\n",
      " [-] epoch  109/250, train loss 0.315199 in 0.12s\n",
      " [-] epoch  110/250, train loss 0.346373 in 0.12s\n",
      " [-] epoch  111/250, train loss 0.312722 in 0.11s\n",
      " [-] epoch  112/250, train loss 0.324711 in 0.12s\n",
      " [-] epoch  113/250, train loss 0.322888 in 0.10s\n",
      " [-] epoch  114/250, train loss 0.300714 in 0.13s\n",
      " [-] epoch  115/250, train loss 0.299855 in 0.14s\n",
      " [-] epoch  116/250, train loss 0.319683 in 0.11s\n",
      " [-] epoch  117/250, train loss 0.302395 in 0.12s\n",
      " [-] epoch  118/250, train loss 0.320721 in 0.09s\n",
      " [-] epoch  119/250, train loss 0.310118 in 0.12s\n",
      " [-] epoch  120/250, train loss 0.297197 in 0.13s\n",
      " [-] epoch  121/250, train loss 0.321069 in 0.12s\n",
      " [-] epoch  122/250, train loss 0.286633 in 0.12s\n",
      " [-] epoch  123/250, train loss 0.314343 in 0.12s\n",
      " [-] epoch  124/250, train loss 0.304354 in 0.10s\n",
      " [-] epoch  125/250, train loss 0.318606 in 0.12s\n",
      " [-] epoch  126/250, train loss 0.312544 in 0.11s\n",
      " [-] epoch  127/250, train loss 0.321477 in 0.11s\n",
      " [-] epoch  128/250, train loss 0.332879 in 0.10s\n",
      " [-] epoch  129/250, train loss 0.308002 in 0.12s\n",
      " [-] epoch  130/250, train loss 0.307432 in 0.10s\n",
      " [-] epoch  131/250, train loss 0.282512 in 0.10s\n",
      " [-] epoch  132/250, train loss 0.304427 in 0.13s\n",
      " [-] epoch  133/250, train loss 0.300456 in 0.10s\n",
      " [-] epoch  134/250, train loss 0.289091 in 0.11s\n",
      " [-] epoch  135/250, train loss 0.270891 in 0.10s\n",
      " [-] epoch  136/250, train loss 0.316052 in 0.12s\n",
      " [-] epoch  137/250, train loss 0.300093 in 0.12s\n",
      " [-] epoch  138/250, train loss 0.306921 in 0.10s\n",
      " [-] epoch  139/250, train loss 0.295916 in 0.10s\n",
      " [-] epoch  140/250, train loss 0.264181 in 0.11s\n",
      " [-] epoch  141/250, train loss 0.293694 in 0.12s\n",
      " [-] epoch  142/250, train loss 0.295106 in 0.10s\n",
      " [-] epoch  143/250, train loss 0.299089 in 0.12s\n",
      " [-] epoch  144/250, train loss 0.302266 in 0.12s\n",
      " [-] epoch  145/250, train loss 0.310432 in 0.12s\n",
      " [-] epoch  146/250, train loss 0.276691 in 0.10s\n",
      " [-] epoch  147/250, train loss 0.291295 in 0.11s\n",
      " [-] epoch  148/250, train loss 0.280199 in 0.12s\n",
      " [-] epoch  149/250, train loss 0.301154 in 0.11s\n",
      " [-] epoch  150/250, train loss 0.317858 in 0.12s\n",
      " [-] epoch  151/250, train loss 0.295546 in 0.13s\n",
      " [-] epoch  152/250, train loss 0.283986 in 0.12s\n",
      " [-] epoch  153/250, train loss 0.282567 in 0.13s\n",
      " [-] epoch  154/250, train loss 0.300643 in 0.11s\n",
      " [-] epoch  155/250, train loss 0.289300 in 0.12s\n",
      " [-] epoch  156/250, train loss 0.291853 in 0.12s\n",
      " [-] epoch  157/250, train loss 0.296544 in 0.13s\n",
      " [-] epoch  158/250, train loss 0.287867 in 0.12s\n",
      " [-] epoch  159/250, train loss 0.294996 in 0.10s\n",
      " [-] epoch  160/250, train loss 0.272299 in 0.10s\n",
      " [-] epoch  161/250, train loss 0.286820 in 0.10s\n",
      " [-] epoch  162/250, train loss 0.290747 in 0.12s\n",
      " [-] epoch  163/250, train loss 0.304941 in 0.10s\n",
      " [-] epoch  164/250, train loss 0.282768 in 0.08s\n",
      " [-] epoch  165/250, train loss 0.302577 in 0.12s\n",
      " [-] epoch  166/250, train loss 0.299114 in 0.12s\n",
      " [-] epoch  167/250, train loss 0.277434 in 0.12s\n",
      " [-] epoch  168/250, train loss 0.283779 in 0.12s\n",
      " [-] epoch  169/250, train loss 0.303248 in 0.12s\n",
      " [-] epoch  170/250, train loss 0.287414 in 0.13s\n",
      " [-] epoch  171/250, train loss 0.290870 in 0.12s\n",
      " [-] epoch  172/250, train loss 0.302662 in 0.12s\n",
      " [-] epoch  173/250, train loss 0.283677 in 0.10s\n",
      " [-] epoch  174/250, train loss 0.282517 in 0.12s\n",
      " [-] epoch  175/250, train loss 0.311669 in 0.11s\n",
      " [-] epoch  176/250, train loss 0.275935 in 0.12s\n",
      " [-] epoch  177/250, train loss 0.310742 in 0.10s\n",
      " [-] epoch  178/250, train loss 0.297942 in 0.10s\n",
      " [-] epoch  179/250, train loss 0.279388 in 0.10s\n",
      " [-] epoch  180/250, train loss 0.317507 in 0.09s\n",
      " [-] epoch  181/250, train loss 0.318554 in 0.10s\n",
      " [-] epoch  182/250, train loss 0.311155 in 0.10s\n",
      " [-] epoch  183/250, train loss 0.286101 in 0.12s\n",
      " [-] epoch  184/250, train loss 0.310393 in 0.10s\n",
      " [-] epoch  185/250, train loss 0.283610 in 0.10s\n",
      " [-] epoch  186/250, train loss 0.294756 in 0.11s\n",
      " [-] epoch  187/250, train loss 0.296918 in 0.12s\n",
      " [-] epoch  188/250, train loss 0.291233 in 0.10s\n",
      " [-] epoch  189/250, train loss 0.283201 in 0.11s\n",
      " [-] epoch  190/250, train loss 0.275786 in 0.11s\n",
      " [-] epoch  191/250, train loss 0.274997 in 0.10s\n",
      " [-] epoch  192/250, train loss 0.274884 in 0.11s\n",
      " [-] epoch  193/250, train loss 0.304122 in 0.12s\n",
      " [-] epoch  194/250, train loss 0.275436 in 0.08s\n",
      " [-] epoch  195/250, train loss 0.272092 in 0.07s\n",
      " [-] epoch  196/250, train loss 0.276918 in 0.07s\n",
      " [-] epoch  197/250, train loss 0.292896 in 0.08s\n",
      " [-] epoch  198/250, train loss 0.284373 in 0.07s\n",
      " [-] epoch  199/250, train loss 0.268278 in 0.07s\n",
      " [-] epoch  200/250, train loss 0.280212 in 0.09s\n",
      " [-] epoch  201/250, train loss 0.278657 in 0.09s\n",
      " [-] epoch  202/250, train loss 0.277427 in 0.12s\n",
      " [-] epoch  203/250, train loss 0.289301 in 0.11s\n",
      " [-] epoch  204/250, train loss 0.280183 in 0.11s\n",
      " [-] epoch  205/250, train loss 0.280957 in 0.10s\n",
      " [-] epoch  206/250, train loss 0.310578 in 0.10s\n",
      " [-] epoch  207/250, train loss 0.287502 in 0.11s\n",
      " [-] epoch  208/250, train loss 0.295985 in 0.11s\n",
      " [-] epoch  209/250, train loss 0.270503 in 0.09s\n",
      " [-] epoch  210/250, train loss 0.262538 in 0.12s\n",
      " [-] epoch  211/250, train loss 0.294404 in 0.10s\n",
      " [-] epoch  212/250, train loss 0.259413 in 0.11s\n",
      " [-] epoch  213/250, train loss 0.277928 in 0.13s\n",
      " [-] epoch  214/250, train loss 0.302904 in 0.09s\n",
      " [-] epoch  215/250, train loss 0.270949 in 0.09s\n",
      " [-] epoch  216/250, train loss 0.276472 in 0.09s\n",
      " [-] epoch  217/250, train loss 0.271101 in 0.10s\n",
      " [-] epoch  218/250, train loss 0.267175 in 0.09s\n",
      " [-] epoch  219/250, train loss 0.299967 in 0.09s\n",
      " [-] epoch  220/250, train loss 0.279807 in 0.11s\n",
      " [-] epoch  221/250, train loss 0.293702 in 0.10s\n",
      " [-] epoch  222/250, train loss 0.283435 in 0.09s\n",
      " [-] epoch  223/250, train loss 0.274299 in 0.09s\n",
      " [-] epoch  224/250, train loss 0.262546 in 0.10s\n",
      " [-] epoch  225/250, train loss 0.287023 in 0.09s\n",
      " [-] epoch  226/250, train loss 0.266766 in 0.09s\n",
      " [-] epoch  227/250, train loss 0.282085 in 0.08s\n",
      " [-] epoch  228/250, train loss 0.258058 in 0.10s\n",
      " [-] epoch  229/250, train loss 0.277629 in 0.08s\n",
      " [-] epoch  230/250, train loss 0.272650 in 0.10s\n",
      " [-] epoch  231/250, train loss 0.291048 in 0.12s\n",
      " [-] epoch  232/250, train loss 0.274210 in 0.12s\n",
      " [-] epoch  233/250, train loss 0.276489 in 0.13s\n",
      " [-] epoch  234/250, train loss 0.285320 in 0.10s\n",
      " [-] epoch  235/250, train loss 0.283814 in 0.10s\n",
      " [-] epoch  236/250, train loss 0.264979 in 0.10s\n",
      " [-] epoch  237/250, train loss 0.263538 in 0.10s\n",
      " [-] epoch  238/250, train loss 0.260088 in 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  239/250, train loss 0.279401 in 0.11s\n",
      " [-] epoch  240/250, train loss 0.296675 in 0.09s\n",
      " [-] epoch  241/250, train loss 0.273559 in 0.11s\n",
      " [-] epoch  242/250, train loss 0.278503 in 0.11s\n",
      " [-] epoch  243/250, train loss 0.311867 in 0.13s\n",
      " [-] epoch  244/250, train loss 0.280549 in 0.14s\n",
      " [-] epoch  245/250, train loss 0.266724 in 0.12s\n",
      " [-] epoch  246/250, train loss 0.283547 in 0.11s\n",
      " [-] epoch  247/250, train loss 0.273445 in 0.13s\n",
      " [-] epoch  248/250, train loss 0.279423 in 0.13s\n",
      " [-] epoch  249/250, train loss 0.298541 in 0.12s\n",
      " [-] epoch  250/250, train loss 0.276082 in 0.10s\n",
      " [-] test acc. 76.944444%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.558034 in 0.25s\n",
      " [-] epoch    2/250, train loss 0.475303 in 0.25s\n",
      " [-] epoch    3/250, train loss 0.420772 in 0.23s\n",
      " [-] epoch    4/250, train loss 0.426509 in 0.24s\n",
      " [-] epoch    5/250, train loss 0.410973 in 0.26s\n",
      " [-] epoch    6/250, train loss 0.379989 in 0.24s\n",
      " [-] epoch    7/250, train loss 0.381423 in 0.26s\n",
      " [-] epoch    8/250, train loss 0.379637 in 0.24s\n",
      " [-] epoch    9/250, train loss 0.356582 in 0.20s\n",
      " [-] epoch   10/250, train loss 0.383719 in 0.26s\n",
      " [-] epoch   11/250, train loss 0.381349 in 0.19s\n",
      " [-] epoch   12/250, train loss 0.354404 in 0.20s\n",
      " [-] epoch   13/250, train loss 0.357558 in 0.16s\n",
      " [-] epoch   14/250, train loss 0.381322 in 0.16s\n",
      " [-] epoch   15/250, train loss 0.353295 in 0.17s\n",
      " [-] epoch   16/250, train loss 0.361855 in 0.17s\n",
      " [-] epoch   17/250, train loss 0.313085 in 0.18s\n",
      " [-] epoch   18/250, train loss 0.330552 in 0.17s\n",
      " [-] epoch   19/250, train loss 0.311364 in 0.17s\n",
      " [-] epoch   20/250, train loss 0.334484 in 0.17s\n",
      " [-] epoch   21/250, train loss 0.332980 in 0.18s\n",
      " [-] epoch   22/250, train loss 0.338010 in 0.17s\n",
      " [-] epoch   23/250, train loss 0.330017 in 0.16s\n",
      " [-] epoch   24/250, train loss 0.301029 in 0.16s\n",
      " [-] epoch   25/250, train loss 0.326844 in 0.20s\n",
      " [-] epoch   26/250, train loss 0.343231 in 0.17s\n",
      " [-] epoch   27/250, train loss 0.312016 in 0.16s\n",
      " [-] epoch   28/250, train loss 0.317024 in 0.17s\n",
      " [-] epoch   29/250, train loss 0.303705 in 0.19s\n",
      " [-] epoch   30/250, train loss 0.302334 in 0.16s\n",
      " [-] epoch   31/250, train loss 0.317348 in 0.18s\n",
      " [-] epoch   32/250, train loss 0.285961 in 0.18s\n",
      " [-] epoch   33/250, train loss 0.316412 in 0.18s\n",
      " [-] epoch   34/250, train loss 0.309894 in 0.18s\n",
      " [-] epoch   35/250, train loss 0.283953 in 0.17s\n",
      " [-] epoch   36/250, train loss 0.276997 in 0.22s\n",
      " [-] epoch   37/250, train loss 0.316400 in 0.16s\n",
      " [-] epoch   38/250, train loss 0.293159 in 0.19s\n",
      " [-] epoch   39/250, train loss 0.296051 in 0.16s\n",
      " [-] epoch   40/250, train loss 0.291919 in 0.18s\n",
      " [-] epoch   41/250, train loss 0.285764 in 0.22s\n",
      " [-] epoch   42/250, train loss 0.287362 in 0.21s\n",
      " [-] epoch   43/250, train loss 0.284895 in 0.23s\n",
      " [-] epoch   44/250, train loss 0.291814 in 0.25s\n",
      " [-] epoch   45/250, train loss 0.285200 in 0.25s\n",
      " [-] epoch   46/250, train loss 0.293444 in 0.25s\n",
      " [-] epoch   47/250, train loss 0.269483 in 0.24s\n",
      " [-] epoch   48/250, train loss 0.272194 in 0.26s\n",
      " [-] epoch   49/250, train loss 0.286336 in 0.25s\n",
      " [-] epoch   50/250, train loss 0.298909 in 0.24s\n",
      " [-] epoch   51/250, train loss 0.302475 in 0.23s\n",
      " [-] epoch   52/250, train loss 0.281511 in 0.24s\n",
      " [-] epoch   53/250, train loss 0.282104 in 0.24s\n",
      " [-] epoch   54/250, train loss 0.288603 in 0.26s\n",
      " [-] epoch   55/250, train loss 0.286935 in 0.25s\n",
      " [-] epoch   56/250, train loss 0.269109 in 0.25s\n",
      " [-] epoch   57/250, train loss 0.263097 in 0.24s\n",
      " [-] epoch   58/250, train loss 0.270614 in 0.24s\n",
      " [-] epoch   59/250, train loss 0.273986 in 0.25s\n",
      " [-] epoch   60/250, train loss 0.278301 in 0.25s\n",
      " [-] epoch   61/250, train loss 0.301097 in 0.25s\n",
      " [-] epoch   62/250, train loss 0.282322 in 0.25s\n",
      " [-] epoch   63/250, train loss 0.252564 in 0.23s\n",
      " [-] epoch   64/250, train loss 0.280518 in 0.24s\n",
      " [-] epoch   65/250, train loss 0.287458 in 0.25s\n",
      " [-] epoch   66/250, train loss 0.265626 in 0.22s\n",
      " [-] epoch   67/250, train loss 0.255857 in 0.21s\n",
      " [-] epoch   68/250, train loss 0.291120 in 0.17s\n",
      " [-] epoch   69/250, train loss 0.291387 in 0.23s\n",
      " [-] epoch   70/250, train loss 0.269384 in 0.20s\n",
      " [-] epoch   71/250, train loss 0.268869 in 0.17s\n",
      " [-] epoch   72/250, train loss 0.272271 in 0.19s\n",
      " [-] epoch   73/250, train loss 0.268316 in 0.22s\n",
      " [-] epoch   74/250, train loss 0.263790 in 0.17s\n",
      " [-] epoch   75/250, train loss 0.279226 in 0.23s\n",
      " [-] epoch   76/250, train loss 0.253210 in 0.23s\n",
      " [-] epoch   77/250, train loss 0.275638 in 0.19s\n",
      " [-] epoch   78/250, train loss 0.263425 in 0.22s\n",
      " [-] epoch   79/250, train loss 0.229200 in 0.23s\n",
      " [-] epoch   80/250, train loss 0.262984 in 0.22s\n",
      " [-] epoch   81/250, train loss 0.257140 in 0.22s\n",
      " [-] epoch   82/250, train loss 0.270137 in 0.24s\n",
      " [-] epoch   83/250, train loss 0.252184 in 0.23s\n",
      " [-] epoch   84/250, train loss 0.266453 in 0.22s\n",
      " [-] epoch   85/250, train loss 0.249575 in 0.22s\n",
      " [-] epoch   86/250, train loss 0.245494 in 0.23s\n",
      " [-] epoch   87/250, train loss 0.259440 in 0.24s\n",
      " [-] epoch   88/250, train loss 0.253707 in 0.23s\n",
      " [-] epoch   89/250, train loss 0.279545 in 0.22s\n",
      " [-] epoch   90/250, train loss 0.257241 in 0.25s\n",
      " [-] epoch   91/250, train loss 0.272682 in 0.25s\n",
      " [-] epoch   92/250, train loss 0.255714 in 0.19s\n",
      " [-] epoch   93/250, train loss 0.267776 in 0.18s\n",
      " [-] epoch   94/250, train loss 0.267379 in 0.23s\n",
      " [-] epoch   95/250, train loss 0.297736 in 0.21s\n",
      " [-] epoch   96/250, train loss 0.257355 in 0.22s\n",
      " [-] epoch   97/250, train loss 0.279674 in 0.23s\n",
      " [-] epoch   98/250, train loss 0.251179 in 0.21s\n",
      " [-] epoch   99/250, train loss 0.274847 in 0.23s\n",
      " [-] epoch  100/250, train loss 0.259519 in 0.24s\n",
      " [-] epoch  101/250, train loss 0.244679 in 0.22s\n",
      " [-] epoch  102/250, train loss 0.239847 in 0.21s\n",
      " [-] epoch  103/250, train loss 0.256003 in 0.22s\n",
      " [-] epoch  104/250, train loss 0.258067 in 0.23s\n",
      " [-] epoch  105/250, train loss 0.249392 in 0.20s\n",
      " [-] epoch  106/250, train loss 0.269644 in 0.21s\n",
      " [-] epoch  107/250, train loss 0.231914 in 0.22s\n",
      " [-] epoch  108/250, train loss 0.239612 in 0.16s\n",
      " [-] epoch  109/250, train loss 0.245732 in 0.23s\n",
      " [-] epoch  110/250, train loss 0.232034 in 0.25s\n",
      " [-] epoch  111/250, train loss 0.240645 in 0.23s\n",
      " [-] epoch  112/250, train loss 0.242463 in 0.23s\n",
      " [-] epoch  113/250, train loss 0.243946 in 0.22s\n",
      " [-] epoch  114/250, train loss 0.228738 in 0.24s\n",
      " [-] epoch  115/250, train loss 0.238310 in 0.24s\n",
      " [-] epoch  116/250, train loss 0.239201 in 0.25s\n",
      " [-] epoch  117/250, train loss 0.243748 in 0.24s\n",
      " [-] epoch  118/250, train loss 0.257928 in 0.24s\n",
      " [-] epoch  119/250, train loss 0.271254 in 0.24s\n",
      " [-] epoch  120/250, train loss 0.230997 in 0.25s\n",
      " [-] epoch  121/250, train loss 0.244515 in 0.23s\n",
      " [-] epoch  122/250, train loss 0.243029 in 0.22s\n",
      " [-] epoch  123/250, train loss 0.255286 in 0.24s\n",
      " [-] epoch  124/250, train loss 0.263470 in 0.23s\n",
      " [-] epoch  125/250, train loss 0.256028 in 0.22s\n",
      " [-] epoch  126/250, train loss 0.246316 in 0.22s\n",
      " [-] epoch  127/250, train loss 0.240194 in 0.23s\n",
      " [-] epoch  128/250, train loss 0.260504 in 0.22s\n",
      " [-] epoch  129/250, train loss 0.249256 in 0.23s\n",
      " [-] epoch  130/250, train loss 0.242819 in 0.22s\n",
      " [-] epoch  131/250, train loss 0.249865 in 0.21s\n",
      " [-] epoch  132/250, train loss 0.225173 in 0.21s\n",
      " [-] epoch  133/250, train loss 0.240720 in 0.22s\n",
      " [-] epoch  134/250, train loss 0.232142 in 0.24s\n",
      " [-] epoch  135/250, train loss 0.243419 in 0.23s\n",
      " [-] epoch  136/250, train loss 0.259764 in 0.22s\n",
      " [-] epoch  137/250, train loss 0.248296 in 0.21s\n",
      " [-] epoch  138/250, train loss 0.251268 in 0.23s\n",
      " [-] epoch  139/250, train loss 0.261308 in 0.23s\n",
      " [-] epoch  140/250, train loss 0.239615 in 0.23s\n",
      " [-] epoch  141/250, train loss 0.243057 in 0.22s\n",
      " [-] epoch  142/250, train loss 0.235052 in 0.23s\n",
      " [-] epoch  143/250, train loss 0.235896 in 0.25s\n",
      " [-] epoch  144/250, train loss 0.221862 in 0.21s\n",
      " [-] epoch  145/250, train loss 0.249255 in 0.23s\n",
      " [-] epoch  146/250, train loss 0.225719 in 0.22s\n",
      " [-] epoch  147/250, train loss 0.249930 in 0.23s\n",
      " [-] epoch  148/250, train loss 0.262847 in 0.23s\n",
      " [-] epoch  149/250, train loss 0.226138 in 0.22s\n",
      " [-] epoch  150/250, train loss 0.227294 in 0.25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.231230 in 0.23s\n",
      " [-] epoch  152/250, train loss 0.218491 in 0.22s\n",
      " [-] epoch  153/250, train loss 0.233487 in 0.25s\n",
      " [-] epoch  154/250, train loss 0.240727 in 0.26s\n",
      " [-] epoch  155/250, train loss 0.233978 in 0.26s\n",
      " [-] epoch  156/250, train loss 0.226356 in 0.24s\n",
      " [-] epoch  157/250, train loss 0.229963 in 0.25s\n",
      " [-] epoch  158/250, train loss 0.251959 in 0.25s\n",
      " [-] epoch  159/250, train loss 0.237359 in 0.26s\n",
      " [-] epoch  160/250, train loss 0.231920 in 0.27s\n",
      " [-] epoch  161/250, train loss 0.255590 in 0.24s\n",
      " [-] epoch  162/250, train loss 0.246431 in 0.26s\n",
      " [-] epoch  163/250, train loss 0.242323 in 0.24s\n",
      " [-] epoch  164/250, train loss 0.243999 in 0.24s\n",
      " [-] epoch  165/250, train loss 0.225832 in 0.25s\n",
      " [-] epoch  166/250, train loss 0.237176 in 0.26s\n",
      " [-] epoch  167/250, train loss 0.222037 in 0.26s\n",
      " [-] epoch  168/250, train loss 0.254721 in 0.26s\n",
      " [-] epoch  169/250, train loss 0.240399 in 0.25s\n",
      " [-] epoch  170/250, train loss 0.231011 in 0.25s\n",
      " [-] epoch  171/250, train loss 0.245659 in 0.27s\n",
      " [-] epoch  172/250, train loss 0.219469 in 0.25s\n",
      " [-] epoch  173/250, train loss 0.225970 in 0.27s\n",
      " [-] epoch  174/250, train loss 0.205172 in 0.25s\n",
      " [-] epoch  175/250, train loss 0.237957 in 0.27s\n",
      " [-] epoch  176/250, train loss 0.227609 in 0.27s\n",
      " [-] epoch  177/250, train loss 0.224284 in 0.24s\n",
      " [-] epoch  178/250, train loss 0.247998 in 0.23s\n",
      " [-] epoch  179/250, train loss 0.226714 in 0.22s\n",
      " [-] epoch  180/250, train loss 0.236506 in 0.23s\n",
      " [-] epoch  181/250, train loss 0.231953 in 0.26s\n",
      " [-] epoch  182/250, train loss 0.236757 in 0.25s\n",
      " [-] epoch  183/250, train loss 0.232164 in 0.23s\n",
      " [-] epoch  184/250, train loss 0.229945 in 0.25s\n",
      " [-] epoch  185/250, train loss 0.225460 in 0.20s\n",
      " [-] epoch  186/250, train loss 0.233323 in 0.19s\n",
      " [-] epoch  187/250, train loss 0.228441 in 0.18s\n",
      " [-] epoch  188/250, train loss 0.220103 in 0.18s\n",
      " [-] epoch  189/250, train loss 0.224648 in 0.18s\n",
      " [-] epoch  190/250, train loss 0.224729 in 0.16s\n",
      " [-] epoch  191/250, train loss 0.228260 in 0.20s\n",
      " [-] epoch  192/250, train loss 0.232166 in 0.23s\n",
      " [-] epoch  193/250, train loss 0.220264 in 0.20s\n",
      " [-] epoch  194/250, train loss 0.247724 in 0.16s\n",
      " [-] epoch  195/250, train loss 0.225841 in 0.17s\n",
      " [-] epoch  196/250, train loss 0.223370 in 0.16s\n",
      " [-] epoch  197/250, train loss 0.229532 in 0.16s\n",
      " [-] epoch  198/250, train loss 0.223883 in 0.16s\n",
      " [-] epoch  199/250, train loss 0.219953 in 0.16s\n",
      " [-] epoch  200/250, train loss 0.229528 in 0.16s\n",
      " [-] epoch  201/250, train loss 0.249083 in 0.18s\n",
      " [-] epoch  202/250, train loss 0.220573 in 0.20s\n",
      " [-] epoch  203/250, train loss 0.232007 in 0.16s\n",
      " [-] epoch  204/250, train loss 0.225662 in 0.18s\n",
      " [-] epoch  205/250, train loss 0.224528 in 0.24s\n",
      " [-] epoch  206/250, train loss 0.240291 in 0.23s\n",
      " [-] epoch  207/250, train loss 0.231304 in 0.20s\n",
      " [-] epoch  208/250, train loss 0.230526 in 0.26s\n",
      " [-] epoch  209/250, train loss 0.228366 in 0.19s\n",
      " [-] epoch  210/250, train loss 0.233403 in 0.17s\n",
      " [-] epoch  211/250, train loss 0.233821 in 0.20s\n",
      " [-] epoch  212/250, train loss 0.213812 in 0.25s\n",
      " [-] epoch  213/250, train loss 0.220678 in 0.16s\n",
      " [-] epoch  214/250, train loss 0.227797 in 0.18s\n",
      " [-] epoch  215/250, train loss 0.232796 in 0.23s\n",
      " [-] epoch  216/250, train loss 0.221543 in 0.21s\n",
      " [-] epoch  217/250, train loss 0.221882 in 0.21s\n",
      " [-] epoch  218/250, train loss 0.251324 in 0.20s\n",
      " [-] epoch  219/250, train loss 0.217531 in 0.20s\n",
      " [-] epoch  220/250, train loss 0.236123 in 0.25s\n",
      " [-] epoch  221/250, train loss 0.229980 in 0.22s\n",
      " [-] epoch  222/250, train loss 0.232571 in 0.21s\n",
      " [-] epoch  223/250, train loss 0.226815 in 0.22s\n",
      " [-] epoch  224/250, train loss 0.235061 in 0.26s\n",
      " [-] epoch  225/250, train loss 0.222616 in 0.26s\n",
      " [-] epoch  226/250, train loss 0.227775 in 0.27s\n",
      " [-] epoch  227/250, train loss 0.231019 in 0.25s\n",
      " [-] epoch  228/250, train loss 0.223500 in 0.26s\n",
      " [-] epoch  229/250, train loss 0.225285 in 0.25s\n",
      " [-] epoch  230/250, train loss 0.226682 in 0.24s\n",
      " [-] epoch  231/250, train loss 0.206886 in 0.22s\n",
      " [-] epoch  232/250, train loss 0.234919 in 0.21s\n",
      " [-] epoch  233/250, train loss 0.227928 in 0.25s\n",
      " [-] epoch  234/250, train loss 0.213611 in 0.23s\n",
      " [-] epoch  235/250, train loss 0.229483 in 0.21s\n",
      " [-] epoch  236/250, train loss 0.231266 in 0.23s\n",
      " [-] epoch  237/250, train loss 0.215266 in 0.23s\n",
      " [-] epoch  238/250, train loss 0.239369 in 0.22s\n",
      " [-] epoch  239/250, train loss 0.240831 in 0.24s\n",
      " [-] epoch  240/250, train loss 0.219627 in 0.23s\n",
      " [-] epoch  241/250, train loss 0.200215 in 0.22s\n",
      " [-] epoch  242/250, train loss 0.219766 in 0.23s\n",
      " [-] epoch  243/250, train loss 0.212465 in 0.23s\n",
      " [-] epoch  244/250, train loss 0.229293 in 0.25s\n",
      " [-] epoch  245/250, train loss 0.225959 in 0.23s\n",
      " [-] epoch  246/250, train loss 0.218759 in 0.21s\n",
      " [-] epoch  247/250, train loss 0.210860 in 0.24s\n",
      " [-] epoch  248/250, train loss 0.243394 in 0.22s\n",
      " [-] epoch  249/250, train loss 0.229649 in 0.22s\n",
      " [-] epoch  250/250, train loss 0.224872 in 0.21s\n",
      " [-] test acc. 83.333333%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.588551 in 0.39s\n",
      " [-] epoch    2/250, train loss 0.459161 in 0.40s\n",
      " [-] epoch    3/250, train loss 0.431052 in 0.37s\n",
      " [-] epoch    4/250, train loss 0.400633 in 0.40s\n",
      " [-] epoch    5/250, train loss 0.370813 in 0.39s\n",
      " [-] epoch    6/250, train loss 0.394653 in 0.28s\n",
      " [-] epoch    7/250, train loss 0.372834 in 0.27s\n",
      " [-] epoch    8/250, train loss 0.347218 in 0.30s\n",
      " [-] epoch    9/250, train loss 0.363509 in 0.32s\n",
      " [-] epoch   10/250, train loss 0.337873 in 0.25s\n",
      " [-] epoch   11/250, train loss 0.342274 in 0.27s\n",
      " [-] epoch   12/250, train loss 0.333812 in 0.29s\n",
      " [-] epoch   13/250, train loss 0.322756 in 0.32s\n",
      " [-] epoch   14/250, train loss 0.357135 in 0.35s\n",
      " [-] epoch   15/250, train loss 0.336413 in 0.38s\n",
      " [-] epoch   16/250, train loss 0.332420 in 0.39s\n",
      " [-] epoch   17/250, train loss 0.314774 in 0.39s\n",
      " [-] epoch   18/250, train loss 0.330185 in 0.37s\n",
      " [-] epoch   19/250, train loss 0.303563 in 0.40s\n",
      " [-] epoch   20/250, train loss 0.308705 in 0.39s\n",
      " [-] epoch   21/250, train loss 0.296244 in 0.39s\n",
      " [-] epoch   22/250, train loss 0.308423 in 0.37s\n",
      " [-] epoch   23/250, train loss 0.328502 in 0.40s\n",
      " [-] epoch   24/250, train loss 0.284212 in 0.38s\n",
      " [-] epoch   25/250, train loss 0.306244 in 0.39s\n",
      " [-] epoch   26/250, train loss 0.303677 in 0.39s\n",
      " [-] epoch   27/250, train loss 0.293795 in 0.37s\n",
      " [-] epoch   28/250, train loss 0.320191 in 0.39s\n",
      " [-] epoch   29/250, train loss 0.280706 in 0.39s\n",
      " [-] epoch   30/250, train loss 0.268136 in 0.38s\n",
      " [-] epoch   31/250, train loss 0.309182 in 0.39s\n",
      " [-] epoch   32/250, train loss 0.294183 in 0.37s\n",
      " [-] epoch   33/250, train loss 0.292143 in 0.40s\n",
      " [-] epoch   34/250, train loss 0.281375 in 0.37s\n",
      " [-] epoch   35/250, train loss 0.278566 in 0.41s\n",
      " [-] epoch   36/250, train loss 0.289281 in 0.40s\n",
      " [-] epoch   37/250, train loss 0.294808 in 0.36s\n",
      " [-] epoch   38/250, train loss 0.276658 in 0.37s\n",
      " [-] epoch   39/250, train loss 0.287739 in 0.41s\n",
      " [-] epoch   40/250, train loss 0.282404 in 0.37s\n",
      " [-] epoch   41/250, train loss 0.282280 in 0.39s\n",
      " [-] epoch   42/250, train loss 0.284147 in 0.36s\n",
      " [-] epoch   43/250, train loss 0.244611 in 0.37s\n",
      " [-] epoch   44/250, train loss 0.265107 in 0.39s\n",
      " [-] epoch   45/250, train loss 0.266392 in 0.39s\n",
      " [-] epoch   46/250, train loss 0.298212 in 0.39s\n",
      " [-] epoch   47/250, train loss 0.279367 in 0.39s\n",
      " [-] epoch   48/250, train loss 0.267366 in 0.41s\n",
      " [-] epoch   49/250, train loss 0.252872 in 0.38s\n",
      " [-] epoch   50/250, train loss 0.272711 in 0.38s\n",
      " [-] epoch   51/250, train loss 0.283361 in 0.36s\n",
      " [-] epoch   52/250, train loss 0.256237 in 0.37s\n",
      " [-] epoch   53/250, train loss 0.243418 in 0.37s\n",
      " [-] epoch   54/250, train loss 0.262471 in 0.36s\n",
      " [-] epoch   55/250, train loss 0.252512 in 0.37s\n",
      " [-] epoch   56/250, train loss 0.276759 in 0.38s\n",
      " [-] epoch   57/250, train loss 0.251604 in 0.39s\n",
      " [-] epoch   58/250, train loss 0.257673 in 0.37s\n",
      " [-] epoch   59/250, train loss 0.248504 in 0.40s\n",
      " [-] epoch   60/250, train loss 0.253277 in 0.39s\n",
      " [-] epoch   61/250, train loss 0.271712 in 0.36s\n",
      " [-] epoch   62/250, train loss 0.265445 in 0.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   63/250, train loss 0.287704 in 0.37s\n",
      " [-] epoch   64/250, train loss 0.272484 in 0.38s\n",
      " [-] epoch   65/250, train loss 0.261752 in 0.39s\n",
      " [-] epoch   66/250, train loss 0.254200 in 0.40s\n",
      " [-] epoch   67/250, train loss 0.251881 in 0.39s\n",
      " [-] epoch   68/250, train loss 0.258759 in 0.42s\n",
      " [-] epoch   69/250, train loss 0.244604 in 0.38s\n",
      " [-] epoch   70/250, train loss 0.244121 in 0.39s\n",
      " [-] epoch   71/250, train loss 0.248398 in 0.37s\n",
      " [-] epoch   72/250, train loss 0.236754 in 0.40s\n",
      " [-] epoch   73/250, train loss 0.267790 in 0.30s\n",
      " [-] epoch   74/250, train loss 0.295551 in 0.34s\n",
      " [-] epoch   75/250, train loss 0.257227 in 0.30s\n",
      " [-] epoch   76/250, train loss 0.243748 in 0.31s\n",
      " [-] epoch   77/250, train loss 0.249078 in 0.23s\n",
      " [-] epoch   78/250, train loss 0.262594 in 0.23s\n",
      " [-] epoch   79/250, train loss 0.238215 in 0.35s\n",
      " [-] epoch   80/250, train loss 0.243806 in 0.24s\n",
      " [-] epoch   81/250, train loss 0.254555 in 0.27s\n",
      " [-] epoch   82/250, train loss 0.237763 in 0.26s\n",
      " [-] epoch   83/250, train loss 0.238554 in 0.26s\n",
      " [-] epoch   84/250, train loss 0.222113 in 0.24s\n",
      " [-] epoch   85/250, train loss 0.247950 in 0.32s\n",
      " [-] epoch   86/250, train loss 0.251874 in 0.25s\n",
      " [-] epoch   87/250, train loss 0.252250 in 0.30s\n",
      " [-] epoch   88/250, train loss 0.240096 in 0.26s\n",
      " [-] epoch   89/250, train loss 0.226573 in 0.33s\n",
      " [-] epoch   90/250, train loss 0.227266 in 0.29s\n",
      " [-] epoch   91/250, train loss 0.245712 in 0.28s\n",
      " [-] epoch   92/250, train loss 0.223948 in 0.25s\n",
      " [-] epoch   93/250, train loss 0.236478 in 0.24s\n",
      " [-] epoch   94/250, train loss 0.243115 in 0.23s\n",
      " [-] epoch   95/250, train loss 0.239300 in 0.24s\n",
      " [-] epoch   96/250, train loss 0.237038 in 0.25s\n",
      " [-] epoch   97/250, train loss 0.239623 in 0.23s\n",
      " [-] epoch   98/250, train loss 0.231938 in 0.28s\n",
      " [-] epoch   99/250, train loss 0.257911 in 0.33s\n",
      " [-] epoch  100/250, train loss 0.243981 in 0.38s\n",
      " [-] epoch  101/250, train loss 0.210273 in 0.31s\n",
      " [-] epoch  102/250, train loss 0.245505 in 0.35s\n",
      " [-] epoch  103/250, train loss 0.225896 in 0.34s\n",
      " [-] epoch  104/250, train loss 0.245356 in 0.40s\n",
      " [-] epoch  105/250, train loss 0.261105 in 0.39s\n",
      " [-] epoch  106/250, train loss 0.244851 in 0.41s\n",
      " [-] epoch  107/250, train loss 0.244051 in 0.39s\n",
      " [-] epoch  108/250, train loss 0.221973 in 0.38s\n",
      " [-] epoch  109/250, train loss 0.226463 in 0.40s\n",
      " [-] epoch  110/250, train loss 0.223871 in 0.41s\n",
      " [-] epoch  111/250, train loss 0.233939 in 0.36s\n",
      " [-] epoch  112/250, train loss 0.215789 in 0.37s\n",
      " [-] epoch  113/250, train loss 0.239605 in 0.41s\n",
      " [-] epoch  114/250, train loss 0.221464 in 0.41s\n",
      " [-] epoch  115/250, train loss 0.240175 in 0.41s\n",
      " [-] epoch  116/250, train loss 0.225665 in 0.39s\n",
      " [-] epoch  117/250, train loss 0.216593 in 0.40s\n",
      " [-] epoch  118/250, train loss 0.216533 in 0.39s\n",
      " [-] epoch  119/250, train loss 0.223430 in 0.43s\n",
      " [-] epoch  120/250, train loss 0.213887 in 0.37s\n",
      " [-] epoch  121/250, train loss 0.223698 in 0.39s\n",
      " [-] epoch  122/250, train loss 0.220897 in 0.41s\n",
      " [-] epoch  123/250, train loss 0.234256 in 0.39s\n",
      " [-] epoch  124/250, train loss 0.225099 in 0.38s\n",
      " [-] epoch  125/250, train loss 0.209938 in 0.40s\n",
      " [-] epoch  126/250, train loss 0.218548 in 0.40s\n",
      " [-] epoch  127/250, train loss 0.220637 in 0.38s\n",
      " [-] epoch  128/250, train loss 0.219366 in 0.38s\n",
      " [-] epoch  129/250, train loss 0.239756 in 0.38s\n",
      " [-] epoch  130/250, train loss 0.225108 in 0.40s\n",
      " [-] epoch  131/250, train loss 0.239774 in 0.39s\n",
      " [-] epoch  132/250, train loss 0.228335 in 0.41s\n",
      " [-] epoch  133/250, train loss 0.223009 in 0.37s\n",
      " [-] epoch  134/250, train loss 0.221236 in 0.39s\n",
      " [-] epoch  135/250, train loss 0.207110 in 0.36s\n",
      " [-] epoch  136/250, train loss 0.222923 in 0.39s\n",
      " [-] epoch  137/250, train loss 0.214696 in 0.40s\n",
      " [-] epoch  138/250, train loss 0.244064 in 0.38s\n",
      " [-] epoch  139/250, train loss 0.247715 in 0.39s\n",
      " [-] epoch  140/250, train loss 0.225073 in 0.38s\n",
      " [-] epoch  141/250, train loss 0.220924 in 0.40s\n",
      " [-] epoch  142/250, train loss 0.219202 in 0.39s\n",
      " [-] epoch  143/250, train loss 0.223267 in 0.38s\n",
      " [-] epoch  144/250, train loss 0.221443 in 0.40s\n",
      " [-] epoch  145/250, train loss 0.212063 in 0.37s\n",
      " [-] epoch  146/250, train loss 0.217625 in 0.40s\n",
      " [-] epoch  147/250, train loss 0.202036 in 0.39s\n",
      " [-] epoch  148/250, train loss 0.208876 in 0.37s\n",
      " [-] epoch  149/250, train loss 0.232950 in 0.40s\n",
      " [-] epoch  150/250, train loss 0.232004 in 0.39s\n",
      " [-] epoch  151/250, train loss 0.234516 in 0.41s\n",
      " [-] epoch  152/250, train loss 0.229787 in 0.43s\n",
      " [-] epoch  153/250, train loss 0.215600 in 0.38s\n",
      " [-] epoch  154/250, train loss 0.199803 in 0.38s\n",
      " [-] epoch  155/250, train loss 0.203421 in 0.40s\n",
      " [-] epoch  156/250, train loss 0.224540 in 0.35s\n",
      " [-] epoch  157/250, train loss 0.204831 in 0.39s\n",
      " [-] epoch  158/250, train loss 0.209687 in 0.41s\n",
      " [-] epoch  159/250, train loss 0.217958 in 0.36s\n",
      " [-] epoch  160/250, train loss 0.212337 in 0.39s\n",
      " [-] epoch  161/250, train loss 0.220307 in 0.41s\n",
      " [-] epoch  162/250, train loss 0.212386 in 0.40s\n",
      " [-] epoch  163/250, train loss 0.213416 in 0.41s\n",
      " [-] epoch  164/250, train loss 0.224332 in 0.37s\n",
      " [-] epoch  165/250, train loss 0.214749 in 0.39s\n",
      " [-] epoch  166/250, train loss 0.209200 in 0.39s\n",
      " [-] epoch  167/250, train loss 0.194707 in 0.38s\n",
      " [-] epoch  168/250, train loss 0.213220 in 0.37s\n",
      " [-] epoch  169/250, train loss 0.217552 in 0.38s\n",
      " [-] epoch  170/250, train loss 0.206858 in 0.38s\n",
      " [-] epoch  171/250, train loss 0.201154 in 0.38s\n",
      " [-] epoch  172/250, train loss 0.227694 in 0.39s\n",
      " [-] epoch  173/250, train loss 0.192504 in 0.40s\n",
      " [-] epoch  174/250, train loss 0.223667 in 0.38s\n",
      " [-] epoch  175/250, train loss 0.209530 in 0.39s\n",
      " [-] epoch  176/250, train loss 0.205617 in 0.38s\n",
      " [-] epoch  177/250, train loss 0.210383 in 0.38s\n",
      " [-] epoch  178/250, train loss 0.243397 in 0.37s\n",
      " [-] epoch  179/250, train loss 0.207215 in 0.38s\n",
      " [-] epoch  180/250, train loss 0.203383 in 0.38s\n",
      " [-] epoch  181/250, train loss 0.209003 in 0.39s\n",
      " [-] epoch  182/250, train loss 0.207332 in 0.40s\n",
      " [-] epoch  183/250, train loss 0.201496 in 0.40s\n",
      " [-] epoch  184/250, train loss 0.211866 in 0.34s\n",
      " [-] epoch  185/250, train loss 0.238795 in 0.39s\n",
      " [-] epoch  186/250, train loss 0.224273 in 0.38s\n",
      " [-] epoch  187/250, train loss 0.200945 in 0.39s\n",
      " [-] epoch  188/250, train loss 0.216701 in 0.40s\n",
      " [-] epoch  189/250, train loss 0.197542 in 0.39s\n",
      " [-] epoch  190/250, train loss 0.214834 in 0.37s\n",
      " [-] epoch  191/250, train loss 0.209744 in 0.40s\n",
      " [-] epoch  192/250, train loss 0.198692 in 0.40s\n",
      " [-] epoch  193/250, train loss 0.198437 in 0.40s\n",
      " [-] epoch  194/250, train loss 0.207696 in 0.36s\n",
      " [-] epoch  195/250, train loss 0.232126 in 0.38s\n",
      " [-] epoch  196/250, train loss 0.205977 in 0.39s\n",
      " [-] epoch  197/250, train loss 0.217282 in 0.39s\n",
      " [-] epoch  198/250, train loss 0.210471 in 0.40s\n",
      " [-] epoch  199/250, train loss 0.192267 in 0.39s\n",
      " [-] epoch  200/250, train loss 0.210919 in 0.40s\n",
      " [-] epoch  201/250, train loss 0.204668 in 0.38s\n",
      " [-] epoch  202/250, train loss 0.236160 in 0.38s\n",
      " [-] epoch  203/250, train loss 0.201353 in 0.38s\n",
      " [-] epoch  204/250, train loss 0.202500 in 0.38s\n",
      " [-] epoch  205/250, train loss 0.209592 in 0.37s\n",
      " [-] epoch  206/250, train loss 0.203077 in 0.39s\n",
      " [-] epoch  207/250, train loss 0.206624 in 0.37s\n",
      " [-] epoch  208/250, train loss 0.204946 in 0.40s\n",
      " [-] epoch  209/250, train loss 0.207550 in 0.40s\n",
      " [-] epoch  210/250, train loss 0.202283 in 0.40s\n",
      " [-] epoch  211/250, train loss 0.187870 in 0.39s\n",
      " [-] epoch  212/250, train loss 0.209004 in 0.38s\n",
      " [-] epoch  213/250, train loss 0.204973 in 0.41s\n",
      " [-] epoch  214/250, train loss 0.194329 in 0.38s\n",
      " [-] epoch  215/250, train loss 0.214188 in 0.36s\n",
      " [-] epoch  216/250, train loss 0.192522 in 0.38s\n",
      " [-] epoch  217/250, train loss 0.193194 in 0.36s\n",
      " [-] epoch  218/250, train loss 0.215134 in 0.37s\n",
      " [-] epoch  219/250, train loss 0.216278 in 0.38s\n",
      " [-] epoch  220/250, train loss 0.188698 in 0.38s\n",
      " [-] epoch  221/250, train loss 0.196695 in 0.38s\n",
      " [-] epoch  222/250, train loss 0.201000 in 0.39s\n",
      " [-] epoch  223/250, train loss 0.220668 in 0.39s\n",
      " [-] epoch  224/250, train loss 0.190311 in 1.56s\n",
      " [-] epoch  225/250, train loss 0.206123 in 0.34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.199803 in 0.40s\n",
      " [-] epoch  227/250, train loss 0.193153 in 0.39s\n",
      " [-] epoch  228/250, train loss 0.199430 in 0.40s\n",
      " [-] epoch  229/250, train loss 0.204775 in 0.41s\n",
      " [-] epoch  230/250, train loss 0.198478 in 0.38s\n",
      " [-] epoch  231/250, train loss 0.203816 in 0.41s\n",
      " [-] epoch  232/250, train loss 0.196293 in 0.41s\n",
      " [-] epoch  233/250, train loss 0.209346 in 0.41s\n",
      " [-] epoch  234/250, train loss 0.194788 in 0.39s\n",
      " [-] epoch  235/250, train loss 0.200292 in 0.40s\n",
      " [-] epoch  236/250, train loss 0.204326 in 0.40s\n",
      " [-] epoch  237/250, train loss 0.213473 in 0.41s\n",
      " [-] epoch  238/250, train loss 0.200485 in 0.39s\n",
      " [-] epoch  239/250, train loss 0.197800 in 0.42s\n",
      " [-] epoch  240/250, train loss 0.201087 in 0.38s\n",
      " [-] epoch  241/250, train loss 0.200860 in 0.41s\n",
      " [-] epoch  242/250, train loss 0.236851 in 0.42s\n",
      " [-] epoch  243/250, train loss 0.206713 in 0.39s\n",
      " [-] epoch  244/250, train loss 0.213568 in 0.40s\n",
      " [-] epoch  245/250, train loss 0.206690 in 0.39s\n",
      " [-] epoch  246/250, train loss 0.206769 in 0.41s\n",
      " [-] epoch  247/250, train loss 0.208763 in 0.41s\n",
      " [-] epoch  248/250, train loss 0.203822 in 0.38s\n",
      " [-] epoch  249/250, train loss 0.216917 in 0.41s\n",
      " [-] epoch  250/250, train loss 0.188072 in 0.39s\n",
      " [-] test acc. 79.444444%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.551067 in 0.64s\n",
      " [-] epoch    2/250, train loss 0.459622 in 0.61s\n",
      " [-] epoch    3/250, train loss 0.475557 in 0.61s\n",
      " [-] epoch    4/250, train loss 0.439251 in 0.60s\n",
      " [-] epoch    5/250, train loss 0.400087 in 0.61s\n",
      " [-] epoch    6/250, train loss 0.383312 in 0.63s\n",
      " [-] epoch    7/250, train loss 0.355997 in 0.58s\n",
      " [-] epoch    8/250, train loss 0.354822 in 0.60s\n",
      " [-] epoch    9/250, train loss 0.341408 in 0.60s\n",
      " [-] epoch   10/250, train loss 0.339788 in 0.60s\n",
      " [-] epoch   11/250, train loss 0.359416 in 0.61s\n",
      " [-] epoch   12/250, train loss 0.348118 in 0.62s\n",
      " [-] epoch   13/250, train loss 0.341055 in 0.62s\n",
      " [-] epoch   14/250, train loss 0.297094 in 0.66s\n",
      " [-] epoch   15/250, train loss 0.345353 in 0.58s\n",
      " [-] epoch   16/250, train loss 0.326495 in 0.61s\n",
      " [-] epoch   17/250, train loss 0.309617 in 0.59s\n",
      " [-] epoch   18/250, train loss 0.323745 in 0.64s\n",
      " [-] epoch   19/250, train loss 0.321866 in 0.60s\n",
      " [-] epoch   20/250, train loss 0.321260 in 0.59s\n",
      " [-] epoch   21/250, train loss 0.281805 in 0.62s\n",
      " [-] epoch   22/250, train loss 0.314472 in 0.61s\n",
      " [-] epoch   23/250, train loss 0.297161 in 0.63s\n",
      " [-] epoch   24/250, train loss 0.275130 in 0.60s\n",
      " [-] epoch   25/250, train loss 0.293495 in 0.59s\n",
      " [-] epoch   26/250, train loss 0.306886 in 0.63s\n",
      " [-] epoch   27/250, train loss 0.328301 in 0.61s\n",
      " [-] epoch   28/250, train loss 0.291314 in 0.60s\n",
      " [-] epoch   29/250, train loss 0.312125 in 0.63s\n",
      " [-] epoch   30/250, train loss 0.292766 in 0.62s\n",
      " [-] epoch   31/250, train loss 0.267625 in 0.61s\n",
      " [-] epoch   32/250, train loss 0.296336 in 0.57s\n",
      " [-] epoch   33/250, train loss 0.307714 in 0.62s\n",
      " [-] epoch   34/250, train loss 0.282123 in 0.63s\n",
      " [-] epoch   35/250, train loss 0.280584 in 0.58s\n",
      " [-] epoch   36/250, train loss 0.274473 in 0.59s\n",
      " [-] epoch   37/250, train loss 0.285308 in 0.61s\n",
      " [-] epoch   38/250, train loss 0.277312 in 0.58s\n",
      " [-] epoch   39/250, train loss 0.261988 in 0.59s\n",
      " [-] epoch   40/250, train loss 0.285107 in 0.62s\n",
      " [-] epoch   41/250, train loss 0.268415 in 0.61s\n",
      " [-] epoch   42/250, train loss 0.285883 in 0.62s\n",
      " [-] epoch   43/250, train loss 0.277600 in 0.62s\n",
      " [-] epoch   44/250, train loss 0.277749 in 0.63s\n",
      " [-] epoch   45/250, train loss 0.288334 in 0.58s\n",
      " [-] epoch   46/250, train loss 0.261078 in 0.59s\n",
      " [-] epoch   47/250, train loss 0.274244 in 0.64s\n",
      " [-] epoch   48/250, train loss 0.267119 in 0.60s\n",
      " [-] epoch   49/250, train loss 0.269236 in 0.62s\n",
      " [-] epoch   50/250, train loss 0.264260 in 0.60s\n",
      " [-] epoch   51/250, train loss 0.275858 in 0.62s\n",
      " [-] epoch   52/250, train loss 0.255560 in 0.58s\n",
      " [-] epoch   53/250, train loss 0.283266 in 0.58s\n",
      " [-] epoch   54/250, train loss 0.261935 in 0.60s\n",
      " [-] epoch   55/250, train loss 0.253228 in 0.63s\n",
      " [-] epoch   56/250, train loss 0.273207 in 0.59s\n",
      " [-] epoch   57/250, train loss 0.271940 in 0.60s\n",
      " [-] epoch   58/250, train loss 0.251122 in 0.62s\n",
      " [-] epoch   59/250, train loss 0.276702 in 0.61s\n",
      " [-] epoch   60/250, train loss 0.256656 in 0.59s\n",
      " [-] epoch   61/250, train loss 0.255740 in 0.59s\n",
      " [-] epoch   62/250, train loss 0.238716 in 0.60s\n",
      " [-] epoch   63/250, train loss 0.247686 in 0.58s\n",
      " [-] epoch   64/250, train loss 0.243475 in 0.61s\n",
      " [-] epoch   65/250, train loss 0.248353 in 0.59s\n",
      " [-] epoch   66/250, train loss 0.244317 in 0.63s\n",
      " [-] epoch   67/250, train loss 0.235847 in 0.61s\n",
      " [-] epoch   68/250, train loss 0.246894 in 0.62s\n",
      " [-] epoch   69/250, train loss 0.249847 in 0.60s\n",
      " [-] epoch   70/250, train loss 0.270946 in 0.59s\n",
      " [-] epoch   71/250, train loss 0.258574 in 0.58s\n",
      " [-] epoch   72/250, train loss 0.234849 in 0.59s\n",
      " [-] epoch   73/250, train loss 0.250935 in 0.58s\n",
      " [-] epoch   74/250, train loss 0.242792 in 0.58s\n",
      " [-] epoch   75/250, train loss 0.244525 in 0.64s\n",
      " [-] epoch   76/250, train loss 0.244281 in 0.61s\n",
      " [-] epoch   77/250, train loss 0.253695 in 0.66s\n",
      " [-] epoch   78/250, train loss 0.243985 in 0.60s\n",
      " [-] epoch   79/250, train loss 0.244503 in 0.63s\n",
      " [-] epoch   80/250, train loss 0.275412 in 0.62s\n",
      " [-] epoch   81/250, train loss 0.240121 in 0.63s\n",
      " [-] epoch   82/250, train loss 0.249939 in 0.63s\n",
      " [-] epoch   83/250, train loss 0.256467 in 0.59s\n",
      " [-] epoch   84/250, train loss 0.253927 in 0.60s\n",
      " [-] epoch   85/250, train loss 0.246729 in 0.63s\n",
      " [-] epoch   86/250, train loss 0.243276 in 0.63s\n",
      " [-] epoch   87/250, train loss 0.259155 in 0.61s\n",
      " [-] epoch   88/250, train loss 0.249255 in 0.61s\n",
      " [-] epoch   89/250, train loss 0.250254 in 0.64s\n",
      " [-] epoch   90/250, train loss 0.237935 in 0.60s\n",
      " [-] epoch   91/250, train loss 0.246585 in 0.63s\n",
      " [-] epoch   92/250, train loss 0.238635 in 0.59s\n",
      " [-] epoch   93/250, train loss 0.242751 in 0.57s\n",
      " [-] epoch   94/250, train loss 0.227114 in 0.64s\n",
      " [-] epoch   95/250, train loss 0.241041 in 0.62s\n",
      " [-] epoch   96/250, train loss 0.241290 in 0.59s\n",
      " [-] epoch   97/250, train loss 0.220670 in 0.59s\n",
      " [-] epoch   98/250, train loss 0.248217 in 0.61s\n",
      " [-] epoch   99/250, train loss 0.245919 in 0.58s\n",
      " [-] epoch  100/250, train loss 0.283497 in 0.64s\n",
      " [-] epoch  101/250, train loss 0.246179 in 0.63s\n",
      " [-] epoch  102/250, train loss 0.234840 in 0.60s\n",
      " [-] epoch  103/250, train loss 0.205763 in 0.60s\n",
      " [-] epoch  104/250, train loss 0.214512 in 0.62s\n",
      " [-] epoch  105/250, train loss 0.225985 in 0.57s\n",
      " [-] epoch  106/250, train loss 0.216615 in 0.58s\n",
      " [-] epoch  107/250, train loss 0.218506 in 0.60s\n",
      " [-] epoch  108/250, train loss 0.226532 in 0.59s\n",
      " [-] epoch  109/250, train loss 0.230099 in 0.60s\n",
      " [-] epoch  110/250, train loss 0.234743 in 0.60s\n",
      " [-] epoch  111/250, train loss 0.232763 in 0.60s\n",
      " [-] epoch  112/250, train loss 0.227675 in 0.57s\n",
      " [-] epoch  113/250, train loss 0.242162 in 0.62s\n",
      " [-] epoch  114/250, train loss 0.251875 in 0.61s\n",
      " [-] epoch  115/250, train loss 0.221576 in 0.60s\n",
      " [-] epoch  116/250, train loss 0.227349 in 0.58s\n",
      " [-] epoch  117/250, train loss 0.215820 in 0.59s\n",
      " [-] epoch  118/250, train loss 0.223126 in 0.58s\n",
      " [-] epoch  119/250, train loss 0.228858 in 0.61s\n",
      " [-] epoch  120/250, train loss 0.224775 in 0.58s\n",
      " [-] epoch  121/250, train loss 0.222088 in 0.62s\n",
      " [-] epoch  122/250, train loss 0.211218 in 0.59s\n",
      " [-] epoch  123/250, train loss 0.226086 in 0.59s\n",
      " [-] epoch  124/250, train loss 0.222139 in 0.46s\n",
      " [-] epoch  125/250, train loss 0.219378 in 0.45s\n",
      " [-] epoch  126/250, train loss 0.226155 in 0.41s\n",
      " [-] epoch  127/250, train loss 0.224957 in 0.39s\n",
      " [-] epoch  128/250, train loss 0.235630 in 0.39s\n",
      " [-] epoch  129/250, train loss 0.230828 in 0.40s\n",
      " [-] epoch  130/250, train loss 0.232409 in 0.34s\n",
      " [-] epoch  131/250, train loss 0.225014 in 0.34s\n",
      " [-] epoch  132/250, train loss 0.206730 in 0.35s\n",
      " [-] epoch  133/250, train loss 0.222521 in 0.37s\n",
      " [-] epoch  134/250, train loss 0.209792 in 0.48s\n",
      " [-] epoch  135/250, train loss 0.215705 in 0.45s\n",
      " [-] epoch  136/250, train loss 0.216013 in 0.58s\n",
      " [-] epoch  137/250, train loss 0.229802 in 0.56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  138/250, train loss 0.219231 in 0.59s\n",
      " [-] epoch  139/250, train loss 0.236029 in 0.57s\n",
      " [-] epoch  140/250, train loss 0.211640 in 0.62s\n",
      " [-] epoch  141/250, train loss 0.221259 in 0.62s\n",
      " [-] epoch  142/250, train loss 0.195145 in 0.61s\n",
      " [-] epoch  143/250, train loss 0.221563 in 0.61s\n",
      " [-] epoch  144/250, train loss 0.219269 in 0.63s\n",
      " [-] epoch  145/250, train loss 0.218868 in 0.64s\n",
      " [-] epoch  146/250, train loss 0.214581 in 0.66s\n",
      " [-] epoch  147/250, train loss 0.228104 in 0.61s\n",
      " [-] epoch  148/250, train loss 0.226730 in 0.62s\n",
      " [-] epoch  149/250, train loss 0.209851 in 0.63s\n",
      " [-] epoch  150/250, train loss 0.198254 in 0.59s\n",
      " [-] epoch  151/250, train loss 0.202496 in 0.64s\n",
      " [-] epoch  152/250, train loss 0.223627 in 0.57s\n",
      " [-] epoch  153/250, train loss 0.225191 in 0.60s\n",
      " [-] epoch  154/250, train loss 0.195736 in 0.65s\n",
      " [-] epoch  155/250, train loss 0.202157 in 0.64s\n",
      " [-] epoch  156/250, train loss 0.196378 in 0.59s\n",
      " [-] epoch  157/250, train loss 0.205617 in 0.57s\n",
      " [-] epoch  158/250, train loss 0.200326 in 0.59s\n",
      " [-] epoch  159/250, train loss 0.205696 in 0.61s\n",
      " [-] epoch  160/250, train loss 0.212131 in 0.56s\n",
      " [-] epoch  161/250, train loss 0.236091 in 0.61s\n",
      " [-] epoch  162/250, train loss 0.217421 in 0.63s\n",
      " [-] epoch  163/250, train loss 0.225170 in 0.61s\n",
      " [-] epoch  164/250, train loss 0.206713 in 0.63s\n",
      " [-] epoch  165/250, train loss 0.209854 in 0.62s\n",
      " [-] epoch  166/250, train loss 0.207011 in 0.63s\n",
      " [-] epoch  167/250, train loss 0.204482 in 0.63s\n",
      " [-] epoch  168/250, train loss 0.191664 in 0.60s\n",
      " [-] epoch  169/250, train loss 0.216352 in 0.61s\n",
      " [-] epoch  170/250, train loss 0.205141 in 0.60s\n",
      " [-] epoch  171/250, train loss 0.202900 in 0.63s\n",
      " [-] epoch  172/250, train loss 0.207785 in 0.62s\n",
      " [-] epoch  173/250, train loss 0.215085 in 0.61s\n",
      " [-] epoch  174/250, train loss 0.208817 in 0.61s\n",
      " [-] epoch  175/250, train loss 0.199935 in 0.63s\n",
      " [-] epoch  176/250, train loss 0.218937 in 0.60s\n",
      " [-] epoch  177/250, train loss 0.212811 in 0.59s\n",
      " [-] epoch  178/250, train loss 0.191380 in 0.59s\n",
      " [-] epoch  179/250, train loss 0.222828 in 0.62s\n",
      " [-] epoch  180/250, train loss 0.209356 in 0.60s\n",
      " [-] epoch  181/250, train loss 0.195512 in 0.59s\n",
      " [-] epoch  182/250, train loss 0.203086 in 0.62s\n",
      " [-] epoch  183/250, train loss 0.203073 in 0.59s\n",
      " [-] epoch  184/250, train loss 0.201974 in 0.61s\n",
      " [-] epoch  185/250, train loss 0.201703 in 0.66s\n",
      " [-] epoch  186/250, train loss 0.205515 in 0.62s\n",
      " [-] epoch  187/250, train loss 0.187056 in 0.61s\n",
      " [-] epoch  188/250, train loss 0.194936 in 0.57s\n",
      " [-] epoch  189/250, train loss 0.204797 in 0.59s\n",
      " [-] epoch  190/250, train loss 0.205216 in 0.62s\n",
      " [-] epoch  191/250, train loss 0.220724 in 0.58s\n",
      " [-] epoch  192/250, train loss 0.217432 in 0.60s\n",
      " [-] epoch  193/250, train loss 0.217816 in 0.59s\n",
      " [-] epoch  194/250, train loss 0.205125 in 0.60s\n",
      " [-] epoch  195/250, train loss 0.199529 in 0.61s\n",
      " [-] epoch  196/250, train loss 0.195357 in 0.61s\n",
      " [-] epoch  197/250, train loss 0.199393 in 0.59s\n",
      " [-] epoch  198/250, train loss 0.197787 in 0.62s\n",
      " [-] epoch  199/250, train loss 0.199533 in 0.62s\n",
      " [-] epoch  200/250, train loss 0.230041 in 0.61s\n",
      " [-] epoch  201/250, train loss 0.184300 in 0.63s\n",
      " [-] epoch  202/250, train loss 0.204448 in 0.61s\n",
      " [-] epoch  203/250, train loss 0.196204 in 0.61s\n",
      " [-] epoch  204/250, train loss 0.192222 in 0.60s\n",
      " [-] epoch  205/250, train loss 0.209561 in 0.58s\n",
      " [-] epoch  206/250, train loss 0.202848 in 0.58s\n",
      " [-] epoch  207/250, train loss 0.188350 in 0.60s\n",
      " [-] epoch  208/250, train loss 0.203358 in 0.59s\n",
      " [-] epoch  209/250, train loss 0.202451 in 0.58s\n",
      " [-] epoch  210/250, train loss 0.195508 in 0.58s\n",
      " [-] epoch  211/250, train loss 0.203761 in 0.56s\n",
      " [-] epoch  212/250, train loss 0.216672 in 0.58s\n",
      " [-] epoch  213/250, train loss 0.201060 in 0.62s\n",
      " [-] epoch  214/250, train loss 0.211621 in 0.59s\n",
      " [-] epoch  215/250, train loss 0.199531 in 0.66s\n",
      " [-] epoch  216/250, train loss 0.211719 in 0.60s\n",
      " [-] epoch  217/250, train loss 0.224557 in 0.61s\n",
      " [-] epoch  218/250, train loss 0.189547 in 0.62s\n",
      " [-] epoch  219/250, train loss 0.203344 in 0.61s\n",
      " [-] epoch  220/250, train loss 0.189920 in 0.58s\n",
      " [-] epoch  221/250, train loss 0.201143 in 0.57s\n",
      " [-] epoch  222/250, train loss 0.200537 in 0.62s\n",
      " [-] epoch  223/250, train loss 0.178074 in 0.61s\n",
      " [-] epoch  224/250, train loss 0.200996 in 0.60s\n",
      " [-] epoch  225/250, train loss 0.196121 in 0.60s\n",
      " [-] epoch  226/250, train loss 0.186766 in 0.59s\n",
      " [-] epoch  227/250, train loss 0.183837 in 0.64s\n",
      " [-] epoch  228/250, train loss 0.199312 in 0.59s\n",
      " [-] epoch  229/250, train loss 0.201659 in 0.61s\n",
      " [-] epoch  230/250, train loss 0.231556 in 0.59s\n",
      " [-] epoch  231/250, train loss 0.207395 in 0.59s\n",
      " [-] epoch  232/250, train loss 0.188666 in 0.59s\n",
      " [-] epoch  233/250, train loss 0.186376 in 0.57s\n",
      " [-] epoch  234/250, train loss 0.198499 in 0.60s\n",
      " [-] epoch  235/250, train loss 0.196733 in 0.60s\n",
      " [-] epoch  236/250, train loss 0.218769 in 0.60s\n",
      " [-] epoch  237/250, train loss 0.202503 in 0.59s\n",
      " [-] epoch  238/250, train loss 0.212865 in 0.62s\n",
      " [-] epoch  239/250, train loss 0.217762 in 0.62s\n",
      " [-] epoch  240/250, train loss 0.177234 in 0.59s\n",
      " [-] epoch  241/250, train loss 0.208844 in 0.61s\n",
      " [-] epoch  242/250, train loss 0.199655 in 0.57s\n",
      " [-] epoch  243/250, train loss 0.187380 in 0.64s\n",
      " [-] epoch  244/250, train loss 0.186312 in 0.59s\n",
      " [-] epoch  245/250, train loss 0.185263 in 0.60s\n",
      " [-] epoch  246/250, train loss 0.194317 in 0.60s\n",
      " [-] epoch  247/250, train loss 0.192113 in 0.59s\n",
      " [-] epoch  248/250, train loss 0.176230 in 0.61s\n",
      " [-] epoch  249/250, train loss 0.207621 in 0.62s\n",
      " [-] epoch  250/250, train loss 0.212202 in 0.59s\n",
      " [-] test acc. 80.277778%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.576436 in 0.76s\n",
      " [-] epoch    2/250, train loss 0.476029 in 0.80s\n",
      " [-] epoch    3/250, train loss 0.412062 in 0.76s\n",
      " [-] epoch    4/250, train loss 0.407282 in 0.76s\n",
      " [-] epoch    5/250, train loss 0.403538 in 0.79s\n",
      " [-] epoch    6/250, train loss 0.370272 in 0.81s\n",
      " [-] epoch    7/250, train loss 0.359951 in 0.76s\n",
      " [-] epoch    8/250, train loss 0.351756 in 0.77s\n",
      " [-] epoch    9/250, train loss 0.345533 in 0.78s\n",
      " [-] epoch   10/250, train loss 0.370367 in 0.80s\n",
      " [-] epoch   11/250, train loss 0.331346 in 0.80s\n",
      " [-] epoch   12/250, train loss 0.335425 in 0.77s\n",
      " [-] epoch   13/250, train loss 0.317300 in 0.79s\n",
      " [-] epoch   14/250, train loss 0.305712 in 0.77s\n",
      " [-] epoch   15/250, train loss 0.296563 in 0.79s\n",
      " [-] epoch   16/250, train loss 0.307394 in 0.76s\n",
      " [-] epoch   17/250, train loss 0.302890 in 0.79s\n",
      " [-] epoch   18/250, train loss 0.314625 in 0.78s\n",
      " [-] epoch   19/250, train loss 0.297509 in 0.77s\n",
      " [-] epoch   20/250, train loss 0.316424 in 0.80s\n",
      " [-] epoch   21/250, train loss 0.320857 in 0.80s\n",
      " [-] epoch   22/250, train loss 0.319149 in 0.81s\n",
      " [-] epoch   23/250, train loss 0.312644 in 0.81s\n",
      " [-] epoch   24/250, train loss 0.314965 in 0.80s\n",
      " [-] epoch   25/250, train loss 0.317868 in 0.78s\n",
      " [-] epoch   26/250, train loss 0.299028 in 0.78s\n",
      " [-] epoch   27/250, train loss 0.316728 in 0.81s\n",
      " [-] epoch   28/250, train loss 0.303547 in 0.78s\n",
      " [-] epoch   29/250, train loss 0.277935 in 0.80s\n",
      " [-] epoch   30/250, train loss 0.276460 in 0.79s\n",
      " [-] epoch   31/250, train loss 0.282165 in 0.79s\n",
      " [-] epoch   32/250, train loss 0.289566 in 0.73s\n",
      " [-] epoch   33/250, train loss 0.287515 in 0.77s\n",
      " [-] epoch   34/250, train loss 0.307689 in 0.78s\n",
      " [-] epoch   35/250, train loss 0.290501 in 0.78s\n",
      " [-] epoch   36/250, train loss 0.281681 in 0.81s\n",
      " [-] epoch   37/250, train loss 0.297506 in 0.80s\n",
      " [-] epoch   38/250, train loss 0.303425 in 0.78s\n",
      " [-] epoch   39/250, train loss 0.272409 in 0.79s\n",
      " [-] epoch   40/250, train loss 0.257588 in 0.76s\n",
      " [-] epoch   41/250, train loss 0.267088 in 0.76s\n",
      " [-] epoch   42/250, train loss 0.282671 in 0.75s\n",
      " [-] epoch   43/250, train loss 0.294126 in 0.57s\n",
      " [-] epoch   44/250, train loss 0.295545 in 0.50s\n",
      " [-] epoch   45/250, train loss 0.253217 in 0.50s\n",
      " [-] epoch   46/250, train loss 0.252629 in 0.63s\n",
      " [-] epoch   47/250, train loss 0.265831 in 0.73s\n",
      " [-] epoch   48/250, train loss 0.250201 in 0.81s\n",
      " [-] epoch   49/250, train loss 0.273017 in 0.77s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.282102 in 0.74s\n",
      " [-] epoch   51/250, train loss 0.255615 in 0.75s\n",
      " [-] epoch   52/250, train loss 0.259391 in 0.83s\n",
      " [-] epoch   53/250, train loss 0.267226 in 0.82s\n",
      " [-] epoch   54/250, train loss 0.261902 in 0.84s\n",
      " [-] epoch   55/250, train loss 0.285584 in 0.79s\n",
      " [-] epoch   56/250, train loss 0.266283 in 0.78s\n",
      " [-] epoch   57/250, train loss 0.239010 in 0.75s\n",
      " [-] epoch   58/250, train loss 0.254969 in 0.78s\n",
      " [-] epoch   59/250, train loss 0.275072 in 0.82s\n",
      " [-] epoch   60/250, train loss 0.259352 in 0.79s\n",
      " [-] epoch   61/250, train loss 0.275377 in 0.79s\n",
      " [-] epoch   62/250, train loss 0.246272 in 0.82s\n",
      " [-] epoch   63/250, train loss 0.266386 in 0.82s\n",
      " [-] epoch   64/250, train loss 0.245731 in 0.83s\n",
      " [-] epoch   65/250, train loss 0.253521 in 0.82s\n",
      " [-] epoch   66/250, train loss 0.237122 in 0.81s\n",
      " [-] epoch   67/250, train loss 0.250016 in 0.78s\n",
      " [-] epoch   68/250, train loss 0.251952 in 0.81s\n",
      " [-] epoch   69/250, train loss 0.257950 in 0.80s\n",
      " [-] epoch   70/250, train loss 0.267047 in 0.82s\n",
      " [-] epoch   71/250, train loss 0.265609 in 0.79s\n",
      " [-] epoch   72/250, train loss 0.256758 in 0.80s\n",
      " [-] epoch   73/250, train loss 0.259572 in 0.83s\n",
      " [-] epoch   74/250, train loss 0.251223 in 0.79s\n",
      " [-] epoch   75/250, train loss 0.242349 in 0.78s\n",
      " [-] epoch   76/250, train loss 0.243320 in 0.79s\n",
      " [-] epoch   77/250, train loss 0.242419 in 0.82s\n",
      " [-] epoch   78/250, train loss 0.249607 in 0.82s\n",
      " [-] epoch   79/250, train loss 0.274140 in 0.78s\n",
      " [-] epoch   80/250, train loss 0.239507 in 0.79s\n",
      " [-] epoch   81/250, train loss 0.238333 in 0.75s\n",
      " [-] epoch   82/250, train loss 0.267136 in 0.78s\n",
      " [-] epoch   83/250, train loss 0.262612 in 0.77s\n",
      " [-] epoch   84/250, train loss 0.239165 in 0.81s\n",
      " [-] epoch   85/250, train loss 0.250715 in 0.76s\n",
      " [-] epoch   86/250, train loss 0.261113 in 0.78s\n",
      " [-] epoch   87/250, train loss 0.246645 in 0.78s\n",
      " [-] epoch   88/250, train loss 0.236209 in 0.80s\n",
      " [-] epoch   89/250, train loss 0.272232 in 0.77s\n",
      " [-] epoch   90/250, train loss 0.247940 in 0.78s\n",
      " [-] epoch   91/250, train loss 0.246972 in 0.82s\n",
      " [-] epoch   92/250, train loss 0.226638 in 0.83s\n",
      " [-] epoch   93/250, train loss 0.218515 in 0.83s\n",
      " [-] epoch   94/250, train loss 0.243307 in 0.79s\n",
      " [-] epoch   95/250, train loss 0.223417 in 0.73s\n",
      " [-] epoch   96/250, train loss 0.218181 in 0.79s\n",
      " [-] epoch   97/250, train loss 0.232307 in 0.77s\n",
      " [-] epoch   98/250, train loss 0.227354 in 0.79s\n",
      " [-] epoch   99/250, train loss 0.242323 in 0.81s\n",
      " [-] epoch  100/250, train loss 0.239308 in 0.77s\n",
      " [-] epoch  101/250, train loss 0.235121 in 0.79s\n",
      " [-] epoch  102/250, train loss 0.273857 in 0.83s\n",
      " [-] epoch  103/250, train loss 0.234668 in 0.79s\n",
      " [-] epoch  104/250, train loss 0.214383 in 0.80s\n",
      " [-] epoch  105/250, train loss 0.234015 in 0.81s\n",
      " [-] epoch  106/250, train loss 0.221286 in 0.85s\n",
      " [-] epoch  107/250, train loss 0.230393 in 0.80s\n",
      " [-] epoch  108/250, train loss 0.241096 in 0.81s\n",
      " [-] epoch  109/250, train loss 0.211942 in 0.81s\n",
      " [-] epoch  110/250, train loss 0.217851 in 0.77s\n",
      " [-] epoch  111/250, train loss 0.234608 in 0.84s\n",
      " [-] epoch  112/250, train loss 0.217235 in 0.81s\n",
      " [-] epoch  113/250, train loss 0.230573 in 0.81s\n",
      " [-] epoch  114/250, train loss 0.219725 in 0.79s\n",
      " [-] epoch  115/250, train loss 0.226334 in 0.78s\n",
      " [-] epoch  116/250, train loss 0.244243 in 0.78s\n",
      " [-] epoch  117/250, train loss 0.229765 in 0.81s\n",
      " [-] epoch  118/250, train loss 0.253134 in 0.76s\n",
      " [-] epoch  119/250, train loss 0.231113 in 0.83s\n",
      " [-] epoch  120/250, train loss 0.232121 in 0.78s\n",
      " [-] epoch  121/250, train loss 0.239537 in 0.82s\n",
      " [-] epoch  122/250, train loss 0.218598 in 0.78s\n",
      " [-] epoch  123/250, train loss 0.215300 in 0.80s\n",
      " [-] epoch  124/250, train loss 0.254921 in 0.81s\n",
      " [-] epoch  125/250, train loss 0.222135 in 0.82s\n",
      " [-] epoch  126/250, train loss 0.241516 in 0.80s\n",
      " [-] epoch  127/250, train loss 0.216865 in 0.80s\n",
      " [-] epoch  128/250, train loss 0.232074 in 0.77s\n",
      " [-] epoch  129/250, train loss 0.230106 in 0.81s\n",
      " [-] epoch  130/250, train loss 0.221102 in 0.79s\n",
      " [-] epoch  131/250, train loss 0.224473 in 0.79s\n",
      " [-] epoch  132/250, train loss 0.209654 in 0.82s\n",
      " [-] epoch  133/250, train loss 0.213969 in 0.80s\n",
      " [-] epoch  134/250, train loss 0.227006 in 0.82s\n",
      " [-] epoch  135/250, train loss 0.226155 in 0.78s\n",
      " [-] epoch  136/250, train loss 0.233651 in 0.79s\n",
      " [-] epoch  137/250, train loss 0.214977 in 0.80s\n",
      " [-] epoch  138/250, train loss 0.230923 in 0.80s\n",
      " [-] epoch  139/250, train loss 0.243223 in 0.79s\n",
      " [-] epoch  140/250, train loss 0.206855 in 0.78s\n",
      " [-] epoch  141/250, train loss 0.202253 in 0.63s\n",
      " [-] epoch  142/250, train loss 0.233477 in 0.70s\n",
      " [-] epoch  143/250, train loss 0.234924 in 0.77s\n",
      " [-] epoch  144/250, train loss 0.221309 in 0.81s\n",
      " [-] epoch  145/250, train loss 0.234848 in 0.83s\n",
      " [-] epoch  146/250, train loss 0.240465 in 0.82s\n",
      " [-] epoch  147/250, train loss 0.227622 in 0.82s\n",
      " [-] epoch  148/250, train loss 0.208713 in 0.78s\n",
      " [-] epoch  149/250, train loss 0.214618 in 0.81s\n",
      " [-] epoch  150/250, train loss 0.217951 in 0.82s\n",
      " [-] epoch  151/250, train loss 0.221677 in 0.83s\n",
      " [-] epoch  152/250, train loss 0.210661 in 0.78s\n",
      " [-] epoch  153/250, train loss 0.230959 in 0.81s\n",
      " [-] epoch  154/250, train loss 0.208789 in 0.78s\n",
      " [-] epoch  155/250, train loss 0.207669 in 0.80s\n",
      " [-] epoch  156/250, train loss 0.218825 in 0.78s\n",
      " [-] epoch  157/250, train loss 0.219197 in 0.80s\n",
      " [-] epoch  158/250, train loss 0.202441 in 0.75s\n",
      " [-] epoch  159/250, train loss 0.216650 in 0.80s\n",
      " [-] epoch  160/250, train loss 0.208583 in 0.82s\n",
      " [-] epoch  161/250, train loss 0.204385 in 0.76s\n",
      " [-] epoch  162/250, train loss 0.208996 in 0.80s\n",
      " [-] epoch  163/250, train loss 0.202823 in 0.81s\n",
      " [-] epoch  164/250, train loss 0.235628 in 0.79s\n",
      " [-] epoch  165/250, train loss 0.207198 in 0.80s\n",
      " [-] epoch  166/250, train loss 0.218785 in 0.81s\n",
      " [-] epoch  167/250, train loss 0.222322 in 0.79s\n",
      " [-] epoch  168/250, train loss 0.228994 in 0.79s\n",
      " [-] epoch  169/250, train loss 0.214466 in 0.77s\n",
      " [-] epoch  170/250, train loss 0.197404 in 0.79s\n",
      " [-] epoch  171/250, train loss 0.206372 in 0.79s\n",
      " [-] epoch  172/250, train loss 0.218743 in 0.77s\n",
      " [-] epoch  173/250, train loss 0.207500 in 0.79s\n",
      " [-] epoch  174/250, train loss 0.211571 in 0.80s\n",
      " [-] epoch  175/250, train loss 0.216697 in 0.81s\n",
      " [-] epoch  176/250, train loss 0.207565 in 0.75s\n",
      " [-] epoch  177/250, train loss 0.224379 in 0.77s\n",
      " [-] epoch  178/250, train loss 0.210069 in 0.78s\n",
      " [-] epoch  179/250, train loss 0.217212 in 0.81s\n",
      " [-] epoch  180/250, train loss 0.228438 in 0.78s\n",
      " [-] epoch  181/250, train loss 0.208924 in 0.78s\n",
      " [-] epoch  182/250, train loss 0.201711 in 0.77s\n",
      " [-] epoch  183/250, train loss 0.214948 in 0.76s\n",
      " [-] epoch  184/250, train loss 0.205603 in 0.77s\n",
      " [-] epoch  185/250, train loss 0.207114 in 0.81s\n",
      " [-] epoch  186/250, train loss 0.219529 in 0.80s\n",
      " [-] epoch  187/250, train loss 0.207710 in 0.73s\n",
      " [-] epoch  188/250, train loss 0.205887 in 0.77s\n",
      " [-] epoch  189/250, train loss 0.213556 in 0.79s\n",
      " [-] epoch  190/250, train loss 0.201539 in 0.80s\n",
      " [-] epoch  191/250, train loss 0.221993 in 0.80s\n",
      " [-] epoch  192/250, train loss 0.198714 in 0.78s\n",
      " [-] epoch  193/250, train loss 0.192328 in 0.76s\n",
      " [-] epoch  194/250, train loss 0.205542 in 0.81s\n",
      " [-] epoch  195/250, train loss 0.201477 in 0.77s\n",
      " [-] epoch  196/250, train loss 0.195079 in 0.78s\n",
      " [-] epoch  197/250, train loss 0.209751 in 0.79s\n",
      " [-] epoch  198/250, train loss 0.202456 in 0.59s\n",
      " [-] epoch  199/250, train loss 0.214086 in 0.60s\n",
      " [-] epoch  200/250, train loss 0.191043 in 0.59s\n",
      " [-] epoch  201/250, train loss 0.200534 in 0.61s\n",
      " [-] epoch  202/250, train loss 0.207075 in 0.53s\n",
      " [-] epoch  203/250, train loss 0.201990 in 0.61s\n",
      " [-] epoch  204/250, train loss 0.215374 in 0.53s\n",
      " [-] epoch  205/250, train loss 0.198597 in 0.55s\n",
      " [-] epoch  206/250, train loss 0.203131 in 0.58s\n",
      " [-] epoch  207/250, train loss 0.191762 in 0.59s\n",
      " [-] epoch  208/250, train loss 0.217252 in 0.63s\n",
      " [-] epoch  209/250, train loss 0.193205 in 0.61s\n",
      " [-] epoch  210/250, train loss 0.202818 in 0.50s\n",
      " [-] epoch  211/250, train loss 0.210028 in 0.50s\n",
      " [-] epoch  212/250, train loss 0.194509 in 0.74s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  213/250, train loss 0.198607 in 0.57s\n",
      " [-] epoch  214/250, train loss 0.205021 in 0.53s\n",
      " [-] epoch  215/250, train loss 0.192962 in 0.67s\n",
      " [-] epoch  216/250, train loss 0.226632 in 0.63s\n",
      " [-] epoch  217/250, train loss 0.228274 in 0.59s\n",
      " [-] epoch  218/250, train loss 0.215031 in 0.74s\n",
      " [-] epoch  219/250, train loss 0.223613 in 0.79s\n",
      " [-] epoch  220/250, train loss 0.218631 in 0.82s\n",
      " [-] epoch  221/250, train loss 0.203306 in 0.82s\n",
      " [-] epoch  222/250, train loss 0.199689 in 0.83s\n",
      " [-] epoch  223/250, train loss 0.196156 in 0.79s\n",
      " [-] epoch  224/250, train loss 0.207219 in 0.80s\n",
      " [-] epoch  225/250, train loss 0.203573 in 0.77s\n",
      " [-] epoch  226/250, train loss 0.203439 in 0.80s\n",
      " [-] epoch  227/250, train loss 0.204414 in 0.83s\n",
      " [-] epoch  228/250, train loss 0.197524 in 0.81s\n",
      " [-] epoch  229/250, train loss 0.216186 in 0.85s\n",
      " [-] epoch  230/250, train loss 0.197014 in 0.84s\n",
      " [-] epoch  231/250, train loss 0.196377 in 0.78s\n",
      " [-] epoch  232/250, train loss 0.204729 in 0.84s\n",
      " [-] epoch  233/250, train loss 0.206309 in 0.80s\n",
      " [-] epoch  234/250, train loss 0.186426 in 0.81s\n",
      " [-] epoch  235/250, train loss 0.191844 in 0.82s\n",
      " [-] epoch  236/250, train loss 0.214723 in 0.82s\n",
      " [-] epoch  237/250, train loss 0.209226 in 0.79s\n",
      " [-] epoch  238/250, train loss 0.198855 in 0.81s\n",
      " [-] epoch  239/250, train loss 0.213274 in 0.66s\n",
      " [-] epoch  240/250, train loss 0.199201 in 0.57s\n",
      " [-] epoch  241/250, train loss 0.208120 in 0.59s\n",
      " [-] epoch  242/250, train loss 0.198520 in 0.64s\n",
      " [-] epoch  243/250, train loss 0.198727 in 0.62s\n",
      " [-] epoch  244/250, train loss 0.191133 in 0.50s\n",
      " [-] epoch  245/250, train loss 0.201131 in 0.53s\n",
      " [-] epoch  246/250, train loss 0.207719 in 0.50s\n",
      " [-] epoch  247/250, train loss 0.216152 in 0.50s\n",
      " [-] epoch  248/250, train loss 0.206325 in 0.69s\n",
      " [-] epoch  249/250, train loss 0.210826 in 0.51s\n",
      " [-] epoch  250/250, train loss 0.203728 in 0.51s\n",
      " [-] test acc. 78.055556%\n",
      "Je vais utiliser 7 layers\n",
      " [-] epoch    1/250, train loss 0.571273 in 0.61s\n",
      " [-] epoch    2/250, train loss 0.455479 in 0.67s\n",
      " [-] epoch    3/250, train loss 0.401761 in 0.97s\n",
      " [-] epoch    4/250, train loss 0.400437 in 1.00s\n",
      " [-] epoch    5/250, train loss 0.405114 in 0.94s\n",
      " [-] epoch    6/250, train loss 0.378810 in 0.96s\n",
      " [-] epoch    7/250, train loss 0.377061 in 0.96s\n",
      " [-] epoch    8/250, train loss 0.347379 in 0.98s\n",
      " [-] epoch    9/250, train loss 0.389569 in 0.99s\n",
      " [-] epoch   10/250, train loss 0.364339 in 0.97s\n",
      " [-] epoch   11/250, train loss 0.336524 in 1.00s\n",
      " [-] epoch   12/250, train loss 0.325029 in 0.99s\n",
      " [-] epoch   13/250, train loss 0.368438 in 0.75s\n",
      " [-] epoch   14/250, train loss 0.358415 in 0.52s\n",
      " [-] epoch   15/250, train loss 0.300758 in 0.47s\n",
      " [-] epoch   16/250, train loss 0.301383 in 0.49s\n",
      " [-] epoch   17/250, train loss 0.319089 in 0.48s\n",
      " [-] epoch   18/250, train loss 0.339973 in 0.46s\n",
      " [-] epoch   19/250, train loss 0.303903 in 0.52s\n",
      " [-] epoch   20/250, train loss 0.306646 in 0.48s\n",
      " [-] epoch   21/250, train loss 0.320899 in 0.51s\n",
      " [-] epoch   22/250, train loss 0.341968 in 0.49s\n",
      " [-] epoch   23/250, train loss 0.330281 in 0.50s\n",
      " [-] epoch   24/250, train loss 0.315717 in 0.48s\n",
      " [-] epoch   25/250, train loss 0.307859 in 0.51s\n",
      " [-] epoch   26/250, train loss 0.339140 in 0.49s\n",
      " [-] epoch   27/250, train loss 0.337187 in 0.50s\n",
      " [-] epoch   28/250, train loss 0.333300 in 0.49s\n",
      " [-] epoch   29/250, train loss 0.330898 in 0.50s\n",
      " [-] epoch   30/250, train loss 0.288753 in 0.48s\n",
      " [-] epoch   31/250, train loss 0.309196 in 0.49s\n",
      " [-] epoch   32/250, train loss 0.304272 in 0.47s\n",
      " [-] epoch   33/250, train loss 0.321600 in 0.49s\n",
      " [-] epoch   34/250, train loss 0.270858 in 0.50s\n",
      " [-] epoch   35/250, train loss 0.303004 in 0.48s\n",
      " [-] epoch   36/250, train loss 0.299652 in 0.49s\n",
      " [-] epoch   37/250, train loss 0.279685 in 0.49s\n",
      " [-] epoch   38/250, train loss 0.288536 in 0.51s\n",
      " [-] epoch   39/250, train loss 0.295392 in 0.93s\n",
      " [-] epoch   40/250, train loss 0.264253 in 1.00s\n",
      " [-] epoch   41/250, train loss 0.283557 in 0.99s\n",
      " [-] epoch   42/250, train loss 0.265396 in 0.99s\n",
      " [-] epoch   43/250, train loss 0.282206 in 0.99s\n",
      " [-] epoch   44/250, train loss 0.265629 in 0.95s\n",
      " [-] epoch   45/250, train loss 0.269765 in 1.00s\n",
      " [-] epoch   46/250, train loss 0.277033 in 1.00s\n",
      " [-] epoch   47/250, train loss 0.265483 in 1.00s\n",
      " [-] epoch   48/250, train loss 0.294741 in 0.96s\n",
      " [-] epoch   49/250, train loss 0.269414 in 0.98s\n",
      " [-] epoch   50/250, train loss 0.296907 in 1.03s\n",
      " [-] epoch   51/250, train loss 0.284186 in 0.97s\n",
      " [-] epoch   52/250, train loss 0.268917 in 0.98s\n",
      " [-] epoch   53/250, train loss 0.252638 in 1.00s\n",
      " [-] epoch   54/250, train loss 0.265107 in 0.96s\n",
      " [-] epoch   55/250, train loss 0.274983 in 0.99s\n",
      " [-] epoch   56/250, train loss 0.260031 in 1.00s\n",
      " [-] epoch   57/250, train loss 0.274201 in 1.00s\n",
      " [-] epoch   58/250, train loss 0.276378 in 0.97s\n",
      " [-] epoch   59/250, train loss 0.254240 in 0.93s\n",
      " [-] epoch   60/250, train loss 0.276925 in 0.98s\n",
      " [-] epoch   61/250, train loss 0.245350 in 0.97s\n",
      " [-] epoch   62/250, train loss 0.252657 in 0.96s\n",
      " [-] epoch   63/250, train loss 0.241956 in 0.95s\n",
      " [-] epoch   64/250, train loss 0.245324 in 0.97s\n",
      " [-] epoch   65/250, train loss 0.253119 in 0.93s\n",
      " [-] epoch   66/250, train loss 0.237182 in 0.97s\n",
      " [-] epoch   67/250, train loss 0.259824 in 1.00s\n",
      " [-] epoch   68/250, train loss 0.280126 in 0.97s\n",
      " [-] epoch   69/250, train loss 0.268317 in 0.98s\n",
      " [-] epoch   70/250, train loss 0.287315 in 0.96s\n",
      " [-] epoch   71/250, train loss 0.252325 in 1.00s\n",
      " [-] epoch   72/250, train loss 0.248794 in 0.98s\n",
      " [-] epoch   73/250, train loss 0.252068 in 0.96s\n",
      " [-] epoch   74/250, train loss 0.282226 in 0.98s\n",
      " [-] epoch   75/250, train loss 0.247485 in 1.01s\n",
      " [-] epoch   76/250, train loss 0.234903 in 0.98s\n",
      " [-] epoch   77/250, train loss 0.266750 in 0.95s\n",
      " [-] epoch   78/250, train loss 0.247244 in 1.00s\n",
      " [-] epoch   79/250, train loss 0.227144 in 0.97s\n",
      " [-] epoch   80/250, train loss 0.250307 in 0.94s\n",
      " [-] epoch   81/250, train loss 0.250206 in 0.98s\n",
      " [-] epoch   82/250, train loss 0.238304 in 0.97s\n",
      " [-] epoch   83/250, train loss 0.266692 in 0.98s\n",
      " [-] epoch   84/250, train loss 0.240250 in 0.98s\n",
      " [-] epoch   85/250, train loss 0.253692 in 0.94s\n",
      " [-] epoch   86/250, train loss 0.271303 in 0.93s\n",
      " [-] epoch   87/250, train loss 0.247965 in 1.00s\n",
      " [-] epoch   88/250, train loss 0.235365 in 0.97s\n",
      " [-] epoch   89/250, train loss 0.222731 in 0.95s\n",
      " [-] epoch   90/250, train loss 0.240984 in 0.96s\n",
      " [-] epoch   91/250, train loss 0.246109 in 0.97s\n",
      " [-] epoch   92/250, train loss 0.242376 in 0.94s\n",
      " [-] epoch   93/250, train loss 0.238921 in 0.98s\n",
      " [-] epoch   94/250, train loss 0.232336 in 0.93s\n",
      " [-] epoch   95/250, train loss 0.219548 in 0.63s\n",
      " [-] epoch   96/250, train loss 0.231552 in 0.81s\n",
      " [-] epoch   97/250, train loss 0.233575 in 0.86s\n",
      " [-] epoch   98/250, train loss 0.239171 in 0.91s\n",
      " [-] epoch   99/250, train loss 0.232465 in 0.99s\n",
      " [-] epoch  100/250, train loss 0.243245 in 0.96s\n",
      " [-] epoch  101/250, train loss 0.224385 in 1.01s\n",
      " [-] epoch  102/250, train loss 0.224923 in 0.96s\n",
      " [-] epoch  103/250, train loss 0.245835 in 0.96s\n",
      " [-] epoch  104/250, train loss 0.219108 in 0.96s\n",
      " [-] epoch  105/250, train loss 0.227280 in 1.01s\n",
      " [-] epoch  106/250, train loss 0.239347 in 0.96s\n",
      " [-] epoch  107/250, train loss 0.225015 in 0.96s\n",
      " [-] epoch  108/250, train loss 0.226072 in 0.98s\n",
      " [-] epoch  109/250, train loss 0.239076 in 0.95s\n",
      " [-] epoch  110/250, train loss 0.249298 in 0.98s\n",
      " [-] epoch  111/250, train loss 0.234781 in 0.97s\n",
      " [-] epoch  112/250, train loss 0.246230 in 0.98s\n",
      " [-] epoch  113/250, train loss 0.229730 in 0.93s\n",
      " [-] epoch  114/250, train loss 0.248118 in 0.95s\n",
      " [-] epoch  115/250, train loss 0.227290 in 0.98s\n",
      " [-] epoch  116/250, train loss 0.225231 in 1.00s\n",
      " [-] epoch  117/250, train loss 0.225054 in 1.00s\n",
      " [-] epoch  118/250, train loss 0.226880 in 0.96s\n",
      " [-] epoch  119/250, train loss 0.218446 in 0.97s\n",
      " [-] epoch  120/250, train loss 0.220012 in 0.97s\n",
      " [-] epoch  121/250, train loss 0.241293 in 0.95s\n",
      " [-] epoch  122/250, train loss 0.235864 in 0.99s\n",
      " [-] epoch  123/250, train loss 0.220601 in 0.95s\n",
      " [-] epoch  124/250, train loss 0.232769 in 0.99s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  125/250, train loss 0.222335 in 0.97s\n",
      " [-] epoch  126/250, train loss 0.238305 in 0.92s\n",
      " [-] epoch  127/250, train loss 0.221391 in 0.99s\n",
      " [-] epoch  128/250, train loss 0.221489 in 0.93s\n",
      " [-] epoch  129/250, train loss 0.224818 in 0.96s\n",
      " [-] epoch  130/250, train loss 0.228067 in 0.99s\n",
      " [-] epoch  131/250, train loss 0.202633 in 0.77s\n",
      " [-] epoch  132/250, train loss 0.219291 in 0.84s\n",
      " [-] epoch  133/250, train loss 0.230023 in 0.86s\n",
      " [-] epoch  134/250, train loss 0.225977 in 0.59s\n",
      " [-] epoch  135/250, train loss 0.231296 in 0.67s\n",
      " [-] epoch  136/250, train loss 0.218549 in 0.71s\n",
      " [-] epoch  137/250, train loss 0.210172 in 0.68s\n",
      " [-] epoch  138/250, train loss 0.231010 in 0.70s\n",
      " [-] epoch  139/250, train loss 0.217217 in 0.69s\n",
      " [-] epoch  140/250, train loss 0.248726 in 0.60s\n",
      " [-] epoch  141/250, train loss 0.223487 in 0.55s\n",
      " [-] epoch  142/250, train loss 0.203341 in 0.70s\n",
      " [-] epoch  143/250, train loss 0.206817 in 0.70s\n",
      " [-] epoch  144/250, train loss 0.224895 in 0.75s\n",
      " [-] epoch  145/250, train loss 0.212660 in 0.89s\n",
      " [-] epoch  146/250, train loss 0.229753 in 0.73s\n",
      " [-] epoch  147/250, train loss 0.209041 in 0.97s\n",
      " [-] epoch  148/250, train loss 0.201582 in 0.78s\n",
      " [-] epoch  149/250, train loss 0.218299 in 0.79s\n",
      " [-] epoch  150/250, train loss 0.225739 in 0.96s\n",
      " [-] epoch  151/250, train loss 0.200754 in 0.83s\n",
      " [-] epoch  152/250, train loss 0.202480 in 0.89s\n",
      " [-] epoch  153/250, train loss 0.226617 in 0.94s\n",
      " [-] epoch  154/250, train loss 0.235446 in 0.90s\n",
      " [-] epoch  155/250, train loss 0.195134 in 0.92s\n",
      " [-] epoch  156/250, train loss 0.199392 in 0.91s\n",
      " [-] epoch  157/250, train loss 0.221250 in 0.78s\n",
      " [-] epoch  158/250, train loss 0.204467 in 0.96s\n",
      " [-] epoch  159/250, train loss 0.224075 in 1.00s\n",
      " [-] epoch  160/250, train loss 0.210081 in 0.81s\n",
      " [-] epoch  161/250, train loss 0.199533 in 0.78s\n",
      " [-] epoch  162/250, train loss 0.212017 in 0.77s\n",
      " [-] epoch  163/250, train loss 0.218490 in 0.55s\n",
      " [-] epoch  164/250, train loss 0.225949 in 0.55s\n",
      " [-] epoch  165/250, train loss 0.221729 in 0.66s\n",
      " [-] epoch  166/250, train loss 0.212828 in 0.75s\n",
      " [-] epoch  167/250, train loss 0.219384 in 0.69s\n",
      " [-] epoch  168/250, train loss 0.231944 in 0.70s\n",
      " [-] epoch  169/250, train loss 0.216587 in 0.99s\n",
      " [-] epoch  170/250, train loss 0.206045 in 0.99s\n",
      " [-] epoch  171/250, train loss 0.224434 in 0.94s\n",
      " [-] epoch  172/250, train loss 0.217042 in 1.02s\n",
      " [-] epoch  173/250, train loss 0.211894 in 0.91s\n",
      " [-] epoch  174/250, train loss 0.216150 in 0.95s\n",
      " [-] epoch  175/250, train loss 0.199940 in 0.65s\n",
      " [-] epoch  176/250, train loss 0.213691 in 0.82s\n",
      " [-] epoch  177/250, train loss 0.209435 in 0.76s\n",
      " [-] epoch  178/250, train loss 0.232028 in 0.73s\n",
      " [-] epoch  179/250, train loss 0.216049 in 0.83s\n",
      " [-] epoch  180/250, train loss 0.214993 in 0.76s\n",
      " [-] epoch  181/250, train loss 0.197683 in 0.70s\n",
      " [-] epoch  182/250, train loss 0.215511 in 0.81s\n",
      " [-] epoch  183/250, train loss 0.230483 in 0.96s\n",
      " [-] epoch  184/250, train loss 0.248206 in 0.71s\n",
      " [-] epoch  185/250, train loss 0.212325 in 0.75s\n",
      " [-] epoch  186/250, train loss 0.219652 in 0.65s\n",
      " [-] epoch  187/250, train loss 0.199091 in 0.77s\n",
      " [-] epoch  188/250, train loss 0.203870 in 0.77s\n",
      " [-] epoch  189/250, train loss 0.200513 in 0.75s\n",
      " [-] epoch  190/250, train loss 0.211065 in 0.76s\n",
      " [-] epoch  191/250, train loss 0.197017 in 0.69s\n",
      " [-] epoch  192/250, train loss 0.228806 in 0.59s\n",
      " [-] epoch  193/250, train loss 0.214732 in 0.59s\n",
      " [-] epoch  194/250, train loss 0.219259 in 0.63s\n",
      " [-] epoch  195/250, train loss 0.202341 in 0.82s\n",
      " [-] epoch  196/250, train loss 0.214896 in 0.71s\n",
      " [-] epoch  197/250, train loss 0.219353 in 0.68s\n",
      " [-] epoch  198/250, train loss 0.197805 in 0.66s\n",
      " [-] epoch  199/250, train loss 0.236426 in 0.82s\n",
      " [-] epoch  200/250, train loss 0.202264 in 0.68s\n",
      " [-] epoch  201/250, train loss 0.219063 in 0.55s\n",
      " [-] epoch  202/250, train loss 0.229531 in 0.81s\n",
      " [-] epoch  203/250, train loss 0.210586 in 0.68s\n",
      " [-] epoch  204/250, train loss 0.242884 in 0.61s\n",
      " [-] epoch  205/250, train loss 0.215528 in 0.72s\n",
      " [-] epoch  206/250, train loss 0.206579 in 0.79s\n",
      " [-] epoch  207/250, train loss 0.213594 in 0.66s\n",
      " [-] epoch  208/250, train loss 0.225207 in 0.80s\n",
      " [-] epoch  209/250, train loss 0.206329 in 0.94s\n",
      " [-] epoch  210/250, train loss 0.208149 in 0.80s\n",
      " [-] epoch  211/250, train loss 0.211243 in 0.72s\n",
      " [-] epoch  212/250, train loss 0.218938 in 0.69s\n",
      " [-] epoch  213/250, train loss 0.215540 in 0.65s\n",
      " [-] epoch  214/250, train loss 0.206302 in 0.67s\n",
      " [-] epoch  215/250, train loss 0.228987 in 0.70s\n",
      " [-] epoch  216/250, train loss 0.214616 in 0.79s\n",
      " [-] epoch  217/250, train loss 0.224684 in 0.77s\n",
      " [-] epoch  218/250, train loss 0.205960 in 0.87s\n",
      " [-] epoch  219/250, train loss 0.189184 in 0.91s\n",
      " [-] epoch  220/250, train loss 0.196101 in 0.80s\n",
      " [-] epoch  221/250, train loss 0.185256 in 0.62s\n",
      " [-] epoch  222/250, train loss 0.200848 in 0.71s\n",
      " [-] epoch  223/250, train loss 0.201849 in 0.87s\n",
      " [-] epoch  224/250, train loss 0.201805 in 0.93s\n",
      " [-] epoch  225/250, train loss 0.200845 in 0.74s\n",
      " [-] epoch  226/250, train loss 0.218440 in 0.77s\n",
      " [-] epoch  227/250, train loss 0.206840 in 0.96s\n",
      " [-] epoch  228/250, train loss 0.189478 in 0.97s\n",
      " [-] epoch  229/250, train loss 0.192692 in 1.01s\n",
      " [-] epoch  230/250, train loss 0.189031 in 0.79s\n",
      " [-] epoch  231/250, train loss 0.211487 in 0.61s\n",
      " [-] epoch  232/250, train loss 0.201234 in 0.54s\n",
      " [-] epoch  233/250, train loss 0.200967 in 0.85s\n",
      " [-] epoch  234/250, train loss 0.195168 in 1.01s\n",
      " [-] epoch  235/250, train loss 0.199133 in 0.96s\n",
      " [-] epoch  236/250, train loss 0.194188 in 0.92s\n",
      " [-] epoch  237/250, train loss 0.207279 in 0.99s\n",
      " [-] epoch  238/250, train loss 0.188555 in 0.97s\n",
      " [-] epoch  239/250, train loss 0.193506 in 0.98s\n",
      " [-] epoch  240/250, train loss 0.206549 in 0.96s\n",
      " [-] epoch  241/250, train loss 0.199133 in 0.89s\n",
      " [-] epoch  242/250, train loss 0.208876 in 0.87s\n",
      " [-] epoch  243/250, train loss 0.205925 in 0.87s\n",
      " [-] epoch  244/250, train loss 0.197197 in 0.95s\n",
      " [-] epoch  245/250, train loss 0.188407 in 0.97s\n",
      " [-] epoch  246/250, train loss 0.200663 in 0.99s\n",
      " [-] epoch  247/250, train loss 0.207529 in 0.99s\n",
      " [-] epoch  248/250, train loss 0.201110 in 0.97s\n",
      " [-] epoch  249/250, train loss 0.210365 in 0.82s\n",
      " [-] epoch  250/250, train loss 0.221857 in 0.88s\n",
      " [-] test acc. 81.666667%\n",
      "Je vais utiliser 8 layers\n",
      " [-] epoch    1/250, train loss 0.617977 in 1.05s\n",
      " [-] epoch    2/250, train loss 0.491472 in 0.93s\n",
      " [-] epoch    3/250, train loss 0.423787 in 0.96s\n",
      " [-] epoch    4/250, train loss 0.413589 in 0.97s\n",
      " [-] epoch    5/250, train loss 0.383639 in 1.04s\n",
      " [-] epoch    6/250, train loss 0.405369 in 1.18s\n",
      " [-] epoch    7/250, train loss 0.390537 in 1.19s\n",
      " [-] epoch    8/250, train loss 0.357361 in 1.20s\n",
      " [-] epoch    9/250, train loss 0.347688 in 0.89s\n",
      " [-] epoch   10/250, train loss 0.364585 in 0.80s\n",
      " [-] epoch   11/250, train loss 0.330094 in 0.78s\n",
      " [-] epoch   12/250, train loss 0.335473 in 1.00s\n",
      " [-] epoch   13/250, train loss 0.329391 in 0.81s\n",
      " [-] epoch   14/250, train loss 0.352434 in 0.80s\n",
      " [-] epoch   15/250, train loss 0.369669 in 0.80s\n",
      " [-] epoch   16/250, train loss 0.304155 in 0.66s\n",
      " [-] epoch   17/250, train loss 0.314146 in 0.84s\n",
      " [-] epoch   18/250, train loss 0.317939 in 0.88s\n",
      " [-] epoch   19/250, train loss 0.297610 in 1.08s\n",
      " [-] epoch   20/250, train loss 0.325990 in 0.86s\n",
      " [-] epoch   21/250, train loss 0.315686 in 1.15s\n",
      " [-] epoch   22/250, train loss 0.328163 in 1.19s\n",
      " [-] epoch   23/250, train loss 0.305611 in 1.12s\n",
      " [-] epoch   24/250, train loss 0.318546 in 1.02s\n",
      " [-] epoch   25/250, train loss 0.290974 in 1.18s\n",
      " [-] epoch   26/250, train loss 0.278903 in 1.19s\n",
      " [-] epoch   27/250, train loss 0.320459 in 1.12s\n",
      " [-] epoch   28/250, train loss 0.309249 in 1.16s\n",
      " [-] epoch   29/250, train loss 0.318131 in 1.17s\n",
      " [-] epoch   30/250, train loss 0.291858 in 1.14s\n",
      " [-] epoch   31/250, train loss 0.291525 in 1.17s\n",
      " [-] epoch   32/250, train loss 0.296334 in 1.15s\n",
      " [-] epoch   33/250, train loss 0.295687 in 1.12s\n",
      " [-] epoch   34/250, train loss 0.291143 in 1.15s\n",
      " [-] epoch   35/250, train loss 0.267633 in 1.17s\n",
      " [-] epoch   36/250, train loss 0.288409 in 1.18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   37/250, train loss 0.272874 in 1.15s\n",
      " [-] epoch   38/250, train loss 0.293904 in 1.15s\n",
      " [-] epoch   39/250, train loss 0.264003 in 1.17s\n",
      " [-] epoch   40/250, train loss 0.293838 in 1.18s\n",
      " [-] epoch   41/250, train loss 0.259109 in 1.15s\n",
      " [-] epoch   42/250, train loss 0.282454 in 0.88s\n",
      " [-] epoch   43/250, train loss 0.293045 in 0.86s\n",
      " [-] epoch   44/250, train loss 0.267713 in 0.97s\n",
      " [-] epoch   45/250, train loss 0.269166 in 0.98s\n",
      " [-] epoch   46/250, train loss 0.309386 in 1.05s\n",
      " [-] epoch   47/250, train loss 0.303614 in 0.98s\n",
      " [-] epoch   48/250, train loss 0.282911 in 0.97s\n",
      " [-] epoch   49/250, train loss 0.276916 in 0.92s\n",
      " [-] epoch   50/250, train loss 0.300682 in 1.02s\n",
      " [-] epoch   51/250, train loss 0.291198 in 1.15s\n",
      " [-] epoch   52/250, train loss 0.288969 in 1.15s\n",
      " [-] epoch   53/250, train loss 0.277308 in 1.18s\n",
      " [-] epoch   54/250, train loss 0.283994 in 1.22s\n",
      " [-] epoch   55/250, train loss 0.246804 in 1.20s\n",
      " [-] epoch   56/250, train loss 0.285749 in 1.24s\n",
      " [-] epoch   57/250, train loss 0.268626 in 1.11s\n",
      " [-] epoch   58/250, train loss 0.278882 in 0.93s\n",
      " [-] epoch   59/250, train loss 0.253201 in 1.00s\n",
      " [-] epoch   60/250, train loss 0.247166 in 1.04s\n",
      " [-] epoch   61/250, train loss 0.268333 in 1.03s\n",
      " [-] epoch   62/250, train loss 0.284331 in 1.07s\n",
      " [-] epoch   63/250, train loss 0.253878 in 1.07s\n",
      " [-] epoch   64/250, train loss 0.242675 in 1.17s\n",
      " [-] epoch   65/250, train loss 0.258289 in 1.10s\n",
      " [-] epoch   66/250, train loss 0.275252 in 1.19s\n",
      " [-] epoch   67/250, train loss 0.265840 in 1.01s\n",
      " [-] epoch   68/250, train loss 0.282788 in 0.93s\n",
      " [-] epoch   69/250, train loss 0.262617 in 1.19s\n",
      " [-] epoch   70/250, train loss 0.263367 in 1.14s\n",
      " [-] epoch   71/250, train loss 0.245867 in 0.94s\n",
      " [-] epoch   72/250, train loss 0.274901 in 1.07s\n",
      " [-] epoch   73/250, train loss 0.273072 in 1.09s\n",
      " [-] epoch   74/250, train loss 0.253810 in 1.12s\n",
      " [-] epoch   75/250, train loss 0.236925 in 1.05s\n",
      " [-] epoch   76/250, train loss 0.246057 in 1.19s\n",
      " [-] epoch   77/250, train loss 0.261025 in 1.10s\n",
      " [-] epoch   78/250, train loss 0.257120 in 1.02s\n",
      " [-] epoch   79/250, train loss 0.253306 in 1.21s\n",
      " [-] epoch   80/250, train loss 0.247402 in 1.07s\n",
      " [-] epoch   81/250, train loss 0.262185 in 0.95s\n",
      " [-] epoch   82/250, train loss 0.263305 in 1.16s\n",
      " [-] epoch   83/250, train loss 0.249447 in 1.17s\n",
      " [-] epoch   84/250, train loss 0.255656 in 0.90s\n",
      " [-] epoch   85/250, train loss 0.258819 in 0.87s\n",
      " [-] epoch   86/250, train loss 0.252277 in 0.67s\n",
      " [-] epoch   87/250, train loss 0.251209 in 0.85s\n",
      " [-] epoch   88/250, train loss 0.248901 in 0.77s\n",
      " [-] epoch   89/250, train loss 0.250774 in 0.78s\n",
      " [-] epoch   90/250, train loss 0.262521 in 0.99s\n",
      " [-] epoch   91/250, train loss 0.260087 in 1.16s\n",
      " [-] epoch   92/250, train loss 0.243735 in 1.18s\n",
      " [-] epoch   93/250, train loss 0.256770 in 1.17s\n",
      " [-] epoch   94/250, train loss 0.267114 in 1.18s\n",
      " [-] epoch   95/250, train loss 0.247648 in 1.08s\n",
      " [-] epoch   96/250, train loss 0.268960 in 0.98s\n",
      " [-] epoch   97/250, train loss 0.238771 in 1.16s\n",
      " [-] epoch   98/250, train loss 0.246547 in 0.77s\n",
      " [-] epoch   99/250, train loss 0.238452 in 0.76s\n",
      " [-] epoch  100/250, train loss 0.277830 in 1.15s\n",
      " [-] epoch  101/250, train loss 0.236843 in 0.95s\n",
      " [-] epoch  102/250, train loss 0.238836 in 0.64s\n",
      " [-] epoch  103/250, train loss 0.233623 in 1.08s\n",
      " [-] epoch  104/250, train loss 0.236325 in 0.96s\n",
      " [-] epoch  105/250, train loss 0.221582 in 0.82s\n",
      " [-] epoch  106/250, train loss 0.242280 in 0.89s\n",
      " [-] epoch  107/250, train loss 0.244988 in 0.96s\n",
      " [-] epoch  108/250, train loss 0.222752 in 0.95s\n",
      " [-] epoch  109/250, train loss 0.239726 in 0.69s\n",
      " [-] epoch  110/250, train loss 0.228420 in 0.91s\n",
      " [-] epoch  111/250, train loss 0.227181 in 0.76s\n",
      " [-] epoch  112/250, train loss 0.225987 in 0.75s\n",
      " [-] epoch  113/250, train loss 0.207462 in 0.68s\n",
      " [-] epoch  114/250, train loss 0.245809 in 0.82s\n",
      " [-] epoch  115/250, train loss 0.225220 in 0.85s\n",
      " [-] epoch  116/250, train loss 0.257483 in 0.83s\n",
      " [-] epoch  117/250, train loss 0.254301 in 1.04s\n",
      " [-] epoch  118/250, train loss 0.237770 in 0.88s\n",
      " [-] epoch  119/250, train loss 0.259239 in 0.96s\n",
      " [-] epoch  120/250, train loss 0.232638 in 1.01s\n",
      " [-] epoch  121/250, train loss 0.239302 in 0.87s\n",
      " [-] epoch  122/250, train loss 0.250007 in 1.00s\n",
      " [-] epoch  123/250, train loss 0.239437 in 1.18s\n",
      " [-] epoch  124/250, train loss 0.234444 in 1.15s\n",
      " [-] epoch  125/250, train loss 0.257034 in 1.20s\n",
      " [-] epoch  126/250, train loss 0.236533 in 1.20s\n",
      " [-] epoch  127/250, train loss 0.256084 in 1.18s\n",
      " [-] epoch  128/250, train loss 0.240430 in 1.05s\n",
      " [-] epoch  129/250, train loss 0.224560 in 1.17s\n",
      " [-] epoch  130/250, train loss 0.222435 in 0.93s\n",
      " [-] epoch  131/250, train loss 0.228806 in 0.68s\n",
      " [-] epoch  132/250, train loss 0.212585 in 0.77s\n",
      " [-] epoch  133/250, train loss 0.214814 in 0.79s\n",
      " [-] epoch  134/250, train loss 0.252747 in 0.88s\n",
      " [-] epoch  135/250, train loss 0.254406 in 1.07s\n",
      " [-] epoch  136/250, train loss 0.226730 in 1.15s\n",
      " [-] epoch  137/250, train loss 0.247212 in 1.16s\n",
      " [-] epoch  138/250, train loss 0.233774 in 1.17s\n",
      " [-] epoch  139/250, train loss 0.217387 in 1.17s\n",
      " [-] epoch  140/250, train loss 0.213550 in 1.00s\n",
      " [-] epoch  141/250, train loss 0.206260 in 0.99s\n",
      " [-] epoch  142/250, train loss 0.211516 in 1.05s\n",
      " [-] epoch  143/250, train loss 0.240589 in 1.13s\n",
      " [-] epoch  144/250, train loss 0.224356 in 0.93s\n",
      " [-] epoch  145/250, train loss 0.222390 in 1.06s\n",
      " [-] epoch  146/250, train loss 0.209779 in 1.12s\n",
      " [-] epoch  147/250, train loss 0.214249 in 0.87s\n",
      " [-] epoch  148/250, train loss 0.243107 in 0.92s\n",
      " [-] epoch  149/250, train loss 0.236741 in 1.00s\n",
      " [-] epoch  150/250, train loss 0.242469 in 0.83s\n",
      " [-] epoch  151/250, train loss 0.221587 in 0.82s\n",
      " [-] epoch  152/250, train loss 0.220526 in 0.97s\n",
      " [-] epoch  153/250, train loss 0.237374 in 0.83s\n",
      " [-] epoch  154/250, train loss 0.217414 in 0.95s\n",
      " [-] epoch  155/250, train loss 0.208308 in 0.78s\n",
      " [-] epoch  156/250, train loss 0.224732 in 0.90s\n",
      " [-] epoch  157/250, train loss 0.205629 in 0.88s\n",
      " [-] epoch  158/250, train loss 0.220556 in 0.82s\n",
      " [-] epoch  159/250, train loss 0.208515 in 0.69s\n",
      " [-] epoch  160/250, train loss 0.235419 in 0.63s\n",
      " [-] epoch  161/250, train loss 0.239972 in 0.85s\n",
      " [-] epoch  162/250, train loss 0.245978 in 0.72s\n",
      " [-] epoch  163/250, train loss 0.201419 in 0.82s\n",
      " [-] epoch  164/250, train loss 0.214834 in 0.80s\n",
      " [-] epoch  165/250, train loss 0.244247 in 0.98s\n",
      " [-] epoch  166/250, train loss 0.224860 in 0.99s\n",
      " [-] epoch  167/250, train loss 0.198848 in 0.94s\n",
      " [-] epoch  168/250, train loss 0.215759 in 0.91s\n",
      " [-] epoch  169/250, train loss 0.220528 in 0.87s\n",
      " [-] epoch  170/250, train loss 0.219212 in 1.09s\n",
      " [-] epoch  171/250, train loss 0.205701 in 0.83s\n",
      " [-] epoch  172/250, train loss 0.222028 in 0.90s\n",
      " [-] epoch  173/250, train loss 0.233388 in 0.93s\n",
      " [-] epoch  174/250, train loss 0.232312 in 0.92s\n",
      " [-] epoch  175/250, train loss 0.230143 in 0.98s\n",
      " [-] epoch  176/250, train loss 0.228338 in 0.96s\n",
      " [-] epoch  177/250, train loss 0.218719 in 0.92s\n",
      " [-] epoch  178/250, train loss 0.208277 in 0.97s\n",
      " [-] epoch  179/250, train loss 0.214344 in 0.96s\n",
      " [-] epoch  180/250, train loss 0.218791 in 1.01s\n",
      " [-] epoch  181/250, train loss 0.209202 in 0.94s\n",
      " [-] epoch  182/250, train loss 0.218263 in 1.11s\n",
      " [-] epoch  183/250, train loss 0.221840 in 0.89s\n",
      " [-] epoch  184/250, train loss 0.209844 in 0.81s\n",
      " [-] epoch  185/250, train loss 0.210524 in 0.85s\n",
      " [-] epoch  186/250, train loss 0.205079 in 0.91s\n",
      " [-] epoch  187/250, train loss 0.207337 in 0.90s\n",
      " [-] epoch  188/250, train loss 0.215096 in 0.81s\n",
      " [-] epoch  189/250, train loss 0.201697 in 0.91s\n",
      " [-] epoch  190/250, train loss 0.217666 in 0.83s\n",
      " [-] epoch  191/250, train loss 0.217227 in 1.00s\n",
      " [-] epoch  192/250, train loss 0.207159 in 0.95s\n",
      " [-] epoch  193/250, train loss 0.201284 in 0.91s\n",
      " [-] epoch  194/250, train loss 0.206505 in 1.08s\n",
      " [-] epoch  195/250, train loss 0.210225 in 0.97s\n",
      " [-] epoch  196/250, train loss 0.218403 in 1.05s\n",
      " [-] epoch  197/250, train loss 0.206803 in 1.17s\n",
      " [-] epoch  198/250, train loss 0.210782 in 1.08s\n",
      " [-] epoch  199/250, train loss 0.220331 in 1.01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  200/250, train loss 0.205886 in 0.98s\n",
      " [-] epoch  201/250, train loss 0.199394 in 0.89s\n",
      " [-] epoch  202/250, train loss 0.211457 in 0.97s\n",
      " [-] epoch  203/250, train loss 0.193522 in 1.07s\n",
      " [-] epoch  204/250, train loss 0.221171 in 1.05s\n",
      " [-] epoch  205/250, train loss 0.237642 in 1.02s\n",
      " [-] epoch  206/250, train loss 0.213160 in 0.94s\n",
      " [-] epoch  207/250, train loss 0.245492 in 0.94s\n",
      " [-] epoch  208/250, train loss 0.209230 in 1.09s\n",
      " [-] epoch  209/250, train loss 0.207812 in 1.03s\n",
      " [-] epoch  210/250, train loss 0.214370 in 1.11s\n",
      " [-] epoch  211/250, train loss 0.217662 in 1.05s\n",
      " [-] epoch  212/250, train loss 0.216180 in 0.98s\n",
      " [-] epoch  213/250, train loss 0.223558 in 1.14s\n",
      " [-] epoch  214/250, train loss 0.209233 in 1.03s\n",
      " [-] epoch  215/250, train loss 0.214595 in 0.90s\n",
      " [-] epoch  216/250, train loss 0.209718 in 1.07s\n",
      " [-] epoch  217/250, train loss 0.213955 in 0.90s\n",
      " [-] epoch  218/250, train loss 0.214032 in 0.79s\n",
      " [-] epoch  219/250, train loss 0.202428 in 0.92s\n",
      " [-] epoch  220/250, train loss 0.186592 in 1.16s\n",
      " [-] epoch  221/250, train loss 0.209723 in 1.15s\n",
      " [-] epoch  222/250, train loss 0.200127 in 1.14s\n",
      " [-] epoch  223/250, train loss 0.190027 in 1.17s\n",
      " [-] epoch  224/250, train loss 0.203372 in 1.17s\n",
      " [-] epoch  225/250, train loss 0.197994 in 1.16s\n",
      " [-] epoch  226/250, train loss 0.205828 in 1.08s\n",
      " [-] epoch  227/250, train loss 0.216455 in 1.08s\n",
      " [-] epoch  228/250, train loss 0.204787 in 1.13s\n",
      " [-] epoch  229/250, train loss 0.204586 in 1.13s\n",
      " [-] epoch  230/250, train loss 0.214322 in 1.08s\n",
      " [-] epoch  231/250, train loss 0.206452 in 0.99s\n",
      " [-] epoch  232/250, train loss 0.205137 in 1.18s\n",
      " [-] epoch  233/250, train loss 0.201656 in 1.08s\n",
      " [-] epoch  234/250, train loss 0.192186 in 1.00s\n",
      " [-] epoch  235/250, train loss 0.197573 in 0.89s\n",
      " [-] epoch  236/250, train loss 0.200709 in 1.00s\n",
      " [-] epoch  237/250, train loss 0.190043 in 1.13s\n",
      " [-] epoch  238/250, train loss 0.193500 in 1.18s\n",
      " [-] epoch  239/250, train loss 0.212235 in 0.83s\n",
      " [-] epoch  240/250, train loss 0.233146 in 1.08s\n",
      " [-] epoch  241/250, train loss 0.223737 in 0.95s\n",
      " [-] epoch  242/250, train loss 0.219889 in 0.94s\n",
      " [-] epoch  243/250, train loss 0.225371 in 1.04s\n",
      " [-] epoch  244/250, train loss 0.214099 in 0.89s\n",
      " [-] epoch  245/250, train loss 0.206753 in 0.86s\n",
      " [-] epoch  246/250, train loss 0.208581 in 0.95s\n",
      " [-] epoch  247/250, train loss 0.199911 in 0.92s\n",
      " [-] epoch  248/250, train loss 0.204574 in 0.99s\n",
      " [-] epoch  249/250, train loss 0.190370 in 0.99s\n",
      " [-] epoch  250/250, train loss 0.197979 in 0.97s\n",
      " [-] test acc. 81.666667%\n",
      "Je vais utiliser 9 layers\n",
      " [-] epoch    1/250, train loss 0.538258 in 1.07s\n",
      " [-] epoch    2/250, train loss 0.477727 in 1.34s\n",
      " [-] epoch    3/250, train loss 0.402020 in 1.29s\n",
      " [-] epoch    4/250, train loss 0.399727 in 1.31s\n",
      " [-] epoch    5/250, train loss 0.396199 in 1.04s\n",
      " [-] epoch    6/250, train loss 0.364095 in 1.04s\n",
      " [-] epoch    7/250, train loss 0.354971 in 0.99s\n",
      " [-] epoch    8/250, train loss 0.355065 in 1.13s\n",
      " [-] epoch    9/250, train loss 0.343078 in 1.17s\n",
      " [-] epoch   10/250, train loss 0.361366 in 1.20s\n",
      " [-] epoch   11/250, train loss 0.331438 in 1.06s\n",
      " [-] epoch   12/250, train loss 0.355784 in 1.23s\n",
      " [-] epoch   13/250, train loss 0.356160 in 1.02s\n",
      " [-] epoch   14/250, train loss 0.342449 in 1.14s\n",
      " [-] epoch   15/250, train loss 0.323505 in 1.06s\n",
      " [-] epoch   16/250, train loss 0.334512 in 1.10s\n",
      " [-] epoch   17/250, train loss 0.328055 in 1.04s\n",
      " [-] epoch   18/250, train loss 0.316550 in 1.16s\n",
      " [-] epoch   19/250, train loss 0.333146 in 1.12s\n",
      " [-] epoch   20/250, train loss 0.342763 in 1.39s\n",
      " [-] epoch   21/250, train loss 0.287912 in 1.34s\n",
      " [-] epoch   22/250, train loss 0.313506 in 1.27s\n",
      " [-] epoch   23/250, train loss 0.315980 in 1.33s\n",
      " [-] epoch   24/250, train loss 0.338393 in 1.26s\n",
      " [-] epoch   25/250, train loss 0.320234 in 1.14s\n",
      " [-] epoch   26/250, train loss 0.305462 in 1.15s\n",
      " [-] epoch   27/250, train loss 0.308162 in 1.10s\n",
      " [-] epoch   28/250, train loss 0.289928 in 1.03s\n",
      " [-] epoch   29/250, train loss 0.312446 in 1.01s\n",
      " [-] epoch   30/250, train loss 0.295129 in 1.13s\n",
      " [-] epoch   31/250, train loss 0.290222 in 0.86s\n",
      " [-] epoch   32/250, train loss 0.320673 in 0.91s\n",
      " [-] epoch   33/250, train loss 0.331880 in 1.16s\n",
      " [-] epoch   34/250, train loss 0.290057 in 1.10s\n",
      " [-] epoch   35/250, train loss 0.297433 in 1.24s\n",
      " [-] epoch   36/250, train loss 0.287336 in 1.19s\n",
      " [-] epoch   37/250, train loss 0.272363 in 1.09s\n",
      " [-] epoch   38/250, train loss 0.280868 in 1.16s\n",
      " [-] epoch   39/250, train loss 0.271483 in 1.18s\n",
      " [-] epoch   40/250, train loss 0.262144 in 1.14s\n",
      " [-] epoch   41/250, train loss 0.297951 in 0.98s\n",
      " [-] epoch   42/250, train loss 0.282324 in 0.90s\n",
      " [-] epoch   43/250, train loss 0.275373 in 1.03s\n",
      " [-] epoch   44/250, train loss 0.279490 in 1.04s\n",
      " [-] epoch   45/250, train loss 0.276747 in 1.33s\n",
      " [-] epoch   46/250, train loss 0.290413 in 1.01s\n",
      " [-] epoch   47/250, train loss 0.270016 in 0.98s\n",
      " [-] epoch   48/250, train loss 0.304034 in 1.10s\n",
      " [-] epoch   49/250, train loss 0.254614 in 1.32s\n",
      " [-] epoch   50/250, train loss 0.293085 in 1.09s\n",
      " [-] epoch   51/250, train loss 0.281704 in 1.08s\n",
      " [-] epoch   52/250, train loss 0.273928 in 1.11s\n",
      " [-] epoch   53/250, train loss 0.273468 in 1.25s\n",
      " [-] epoch   54/250, train loss 0.270929 in 0.94s\n",
      " [-] epoch   55/250, train loss 0.259655 in 1.03s\n",
      " [-] epoch   56/250, train loss 0.264394 in 1.02s\n",
      " [-] epoch   57/250, train loss 0.281135 in 0.99s\n",
      " [-] epoch   58/250, train loss 0.277631 in 1.08s\n",
      " [-] epoch   59/250, train loss 0.267016 in 1.18s\n",
      " [-] epoch   60/250, train loss 0.281396 in 1.29s\n",
      " [-] epoch   61/250, train loss 0.238922 in 1.13s\n",
      " [-] epoch   62/250, train loss 0.289118 in 1.12s\n",
      " [-] epoch   63/250, train loss 0.247834 in 1.21s\n",
      " [-] epoch   64/250, train loss 0.282979 in 1.28s\n",
      " [-] epoch   65/250, train loss 0.280252 in 1.20s\n",
      " [-] epoch   66/250, train loss 0.261922 in 1.05s\n",
      " [-] epoch   67/250, train loss 0.247510 in 1.06s\n",
      " [-] epoch   68/250, train loss 0.244328 in 1.07s\n",
      " [-] epoch   69/250, train loss 0.242947 in 1.03s\n",
      " [-] epoch   70/250, train loss 0.241250 in 0.98s\n",
      " [-] epoch   71/250, train loss 0.248795 in 1.08s\n",
      " [-] epoch   72/250, train loss 0.243764 in 0.90s\n",
      " [-] epoch   73/250, train loss 0.280205 in 1.12s\n",
      " [-] epoch   74/250, train loss 0.260216 in 1.14s\n",
      " [-] epoch   75/250, train loss 0.288413 in 1.28s\n",
      " [-] epoch   76/250, train loss 0.254529 in 1.24s\n",
      " [-] epoch   77/250, train loss 0.238356 in 0.95s\n",
      " [-] epoch   78/250, train loss 0.243881 in 1.24s\n",
      " [-] epoch   79/250, train loss 0.247501 in 1.33s\n",
      " [-] epoch   80/250, train loss 0.252394 in 1.30s\n",
      " [-] epoch   81/250, train loss 0.244491 in 1.32s\n",
      " [-] epoch   82/250, train loss 0.257535 in 0.90s\n",
      " [-] epoch   83/250, train loss 0.261951 in 1.06s\n",
      " [-] epoch   84/250, train loss 0.240693 in 1.00s\n",
      " [-] epoch   85/250, train loss 0.239201 in 1.27s\n",
      " [-] epoch   86/250, train loss 0.265028 in 1.30s\n",
      " [-] epoch   87/250, train loss 0.242771 in 1.26s\n",
      " [-] epoch   88/250, train loss 0.272008 in 1.12s\n",
      " [-] epoch   89/250, train loss 0.240376 in 1.15s\n",
      " [-] epoch   90/250, train loss 0.240617 in 1.13s\n",
      " [-] epoch   91/250, train loss 0.243499 in 1.16s\n",
      " [-] epoch   92/250, train loss 0.226986 in 1.00s\n",
      " [-] epoch   93/250, train loss 0.223843 in 0.97s\n",
      " [-] epoch   94/250, train loss 0.203340 in 1.18s\n",
      " [-] epoch   95/250, train loss 0.230879 in 0.92s\n",
      " [-] epoch   96/250, train loss 0.213682 in 1.09s\n",
      " [-] epoch   97/250, train loss 0.260187 in 1.05s\n",
      " [-] epoch   98/250, train loss 0.239969 in 1.16s\n",
      " [-] epoch   99/250, train loss 0.230584 in 0.95s\n",
      " [-] epoch  100/250, train loss 0.232274 in 1.10s\n",
      " [-] epoch  101/250, train loss 0.258670 in 1.09s\n",
      " [-] epoch  102/250, train loss 0.239613 in 1.02s\n",
      " [-] epoch  103/250, train loss 0.250781 in 0.97s\n",
      " [-] epoch  104/250, train loss 0.252825 in 1.02s\n",
      " [-] epoch  105/250, train loss 0.265582 in 1.16s\n",
      " [-] epoch  106/250, train loss 0.233736 in 0.78s\n",
      " [-] epoch  107/250, train loss 0.216071 in 0.95s\n",
      " [-] epoch  108/250, train loss 0.216860 in 1.01s\n",
      " [-] epoch  109/250, train loss 0.242895 in 0.89s\n",
      " [-] epoch  110/250, train loss 0.229486 in 0.79s\n",
      " [-] epoch  111/250, train loss 0.256433 in 0.91s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  112/250, train loss 0.252408 in 0.80s\n",
      " [-] epoch  113/250, train loss 0.232789 in 0.96s\n",
      " [-] epoch  114/250, train loss 0.219654 in 0.94s\n",
      " [-] epoch  115/250, train loss 0.225035 in 0.95s\n",
      " [-] epoch  116/250, train loss 0.227974 in 0.95s\n",
      " [-] epoch  117/250, train loss 0.254437 in 1.07s\n",
      " [-] epoch  118/250, train loss 0.221545 in 1.10s\n",
      " [-] epoch  119/250, train loss 0.226254 in 1.15s\n",
      " [-] epoch  120/250, train loss 0.237495 in 1.04s\n",
      " [-] epoch  121/250, train loss 0.225981 in 0.95s\n",
      " [-] epoch  122/250, train loss 0.229127 in 1.09s\n",
      " [-] epoch  123/250, train loss 0.254186 in 1.25s\n",
      " [-] epoch  124/250, train loss 0.230120 in 1.14s\n",
      " [-] epoch  125/250, train loss 0.221557 in 0.99s\n",
      " [-] epoch  126/250, train loss 0.215492 in 1.23s\n",
      " [-] epoch  127/250, train loss 0.247656 in 1.02s\n",
      " [-] epoch  128/250, train loss 0.209491 in 1.00s\n",
      " [-] epoch  129/250, train loss 0.208950 in 1.20s\n",
      " [-] epoch  130/250, train loss 0.218683 in 1.29s\n",
      " [-] epoch  131/250, train loss 0.239003 in 1.27s\n",
      " [-] epoch  132/250, train loss 0.235034 in 1.06s\n",
      " [-] epoch  133/250, train loss 0.239861 in 1.19s\n",
      " [-] epoch  134/250, train loss 0.227094 in 1.09s\n",
      " [-] epoch  135/250, train loss 0.228702 in 1.07s\n",
      " [-] epoch  136/250, train loss 0.260594 in 1.18s\n",
      " [-] epoch  137/250, train loss 0.228029 in 1.08s\n",
      " [-] epoch  138/250, train loss 0.240656 in 1.03s\n",
      " [-] epoch  139/250, train loss 0.214536 in 1.03s\n",
      " [-] epoch  140/250, train loss 0.207933 in 0.91s\n",
      " [-] epoch  141/250, train loss 0.207167 in 0.89s\n",
      " [-] epoch  142/250, train loss 0.244417 in 0.83s\n",
      " [-] epoch  143/250, train loss 0.207370 in 0.98s\n",
      " [-] epoch  144/250, train loss 0.210572 in 0.86s\n",
      " [-] epoch  145/250, train loss 0.216438 in 0.89s\n",
      " [-] epoch  146/250, train loss 0.200751 in 0.89s\n",
      " [-] epoch  147/250, train loss 0.203390 in 1.00s\n",
      " [-] epoch  148/250, train loss 0.214924 in 0.82s\n",
      " [-] epoch  149/250, train loss 0.230328 in 0.77s\n",
      " [-] epoch  150/250, train loss 0.222053 in 0.77s\n",
      " [-] epoch  151/250, train loss 0.259767 in 1.01s\n",
      " [-] epoch  152/250, train loss 0.235050 in 1.02s\n",
      " [-] epoch  153/250, train loss 0.223957 in 0.93s\n",
      " [-] epoch  154/250, train loss 0.211615 in 0.92s\n",
      " [-] epoch  155/250, train loss 0.241009 in 0.74s\n",
      " [-] epoch  156/250, train loss 0.215866 in 0.71s\n",
      " [-] epoch  157/250, train loss 0.209965 in 0.81s\n",
      " [-] epoch  158/250, train loss 0.195745 in 1.01s\n",
      " [-] epoch  159/250, train loss 0.214950 in 0.92s\n",
      " [-] epoch  160/250, train loss 0.208497 in 1.16s\n",
      " [-] epoch  161/250, train loss 0.208688 in 1.30s\n",
      " [-] epoch  162/250, train loss 0.202073 in 1.30s\n",
      " [-] epoch  163/250, train loss 0.211416 in 1.07s\n",
      " [-] epoch  164/250, train loss 0.206148 in 1.12s\n",
      " [-] epoch  165/250, train loss 0.213494 in 1.07s\n",
      " [-] epoch  166/250, train loss 0.245519 in 1.02s\n",
      " [-] epoch  167/250, train loss 0.231219 in 1.11s\n",
      " [-] epoch  168/250, train loss 0.228708 in 0.91s\n",
      " [-] epoch  169/250, train loss 0.212305 in 1.12s\n",
      " [-] epoch  170/250, train loss 0.219725 in 1.03s\n",
      " [-] epoch  171/250, train loss 0.201598 in 1.06s\n",
      " [-] epoch  172/250, train loss 0.218014 in 0.99s\n",
      " [-] epoch  173/250, train loss 0.197890 in 1.18s\n",
      " [-] epoch  174/250, train loss 0.204104 in 1.00s\n",
      " [-] epoch  175/250, train loss 0.200385 in 0.92s\n",
      " [-] epoch  176/250, train loss 0.199293 in 1.07s\n",
      " [-] epoch  177/250, train loss 0.206965 in 0.90s\n",
      " [-] epoch  178/250, train loss 0.213692 in 1.08s\n",
      " [-] epoch  179/250, train loss 0.205412 in 0.99s\n",
      " [-] epoch  180/250, train loss 0.196295 in 1.28s\n",
      " [-] epoch  181/250, train loss 0.212414 in 1.29s\n",
      " [-] epoch  182/250, train loss 0.210738 in 1.14s\n",
      " [-] epoch  183/250, train loss 0.204793 in 1.11s\n",
      " [-] epoch  184/250, train loss 0.215568 in 1.25s\n",
      " [-] epoch  185/250, train loss 0.199742 in 1.08s\n",
      " [-] epoch  186/250, train loss 0.202029 in 1.06s\n",
      " [-] epoch  187/250, train loss 0.195840 in 1.33s\n",
      " [-] epoch  188/250, train loss 0.212024 in 1.32s\n",
      " [-] epoch  189/250, train loss 0.221611 in 1.31s\n",
      " [-] epoch  190/250, train loss 0.191928 in 0.95s\n",
      " [-] epoch  191/250, train loss 0.199320 in 0.94s\n",
      " [-] epoch  192/250, train loss 0.197342 in 1.02s\n",
      " [-] epoch  193/250, train loss 0.183051 in 1.04s\n",
      " [-] epoch  194/250, train loss 0.190854 in 1.25s\n",
      " [-] epoch  195/250, train loss 0.199820 in 1.17s\n",
      " [-] epoch  196/250, train loss 0.210043 in 1.16s\n",
      " [-] epoch  197/250, train loss 0.204516 in 1.10s\n",
      " [-] epoch  198/250, train loss 0.195634 in 1.01s\n",
      " [-] epoch  199/250, train loss 0.201650 in 1.18s\n",
      " [-] epoch  200/250, train loss 0.188416 in 1.15s\n",
      " [-] epoch  201/250, train loss 0.214781 in 1.00s\n",
      " [-] epoch  202/250, train loss 0.204848 in 0.87s\n",
      " [-] epoch  203/250, train loss 0.198202 in 1.11s\n",
      " [-] epoch  204/250, train loss 0.194682 in 1.12s\n",
      " [-] epoch  205/250, train loss 0.210229 in 1.00s\n",
      " [-] epoch  206/250, train loss 0.187290 in 0.95s\n",
      " [-] epoch  207/250, train loss 0.198645 in 1.05s\n",
      " [-] epoch  208/250, train loss 0.205769 in 1.09s\n",
      " [-] epoch  209/250, train loss 0.202536 in 0.90s\n",
      " [-] epoch  210/250, train loss 0.211244 in 0.88s\n",
      " [-] epoch  211/250, train loss 0.192721 in 0.87s\n",
      " [-] epoch  212/250, train loss 0.195333 in 0.95s\n",
      " [-] epoch  213/250, train loss 0.211310 in 0.91s\n",
      " [-] epoch  214/250, train loss 0.200644 in 1.08s\n",
      " [-] epoch  215/250, train loss 0.198367 in 1.13s\n",
      " [-] epoch  216/250, train loss 0.204346 in 1.12s\n",
      " [-] epoch  217/250, train loss 0.223629 in 0.94s\n",
      " [-] epoch  218/250, train loss 0.213436 in 1.09s\n",
      " [-] epoch  219/250, train loss 0.179816 in 1.07s\n",
      " [-] epoch  220/250, train loss 0.201229 in 1.12s\n",
      " [-] epoch  221/250, train loss 0.212290 in 1.16s\n",
      " [-] epoch  222/250, train loss 0.221423 in 0.98s\n",
      " [-] epoch  223/250, train loss 0.206146 in 0.90s\n",
      " [-] epoch  224/250, train loss 0.231901 in 1.02s\n",
      " [-] epoch  225/250, train loss 0.187670 in 1.09s\n",
      " [-] epoch  226/250, train loss 0.193108 in 1.03s\n",
      " [-] epoch  227/250, train loss 0.214334 in 0.91s\n",
      " [-] epoch  228/250, train loss 0.243276 in 0.99s\n",
      " [-] epoch  229/250, train loss 0.220880 in 0.85s\n",
      " [-] epoch  230/250, train loss 0.200277 in 0.84s\n",
      " [-] epoch  231/250, train loss 0.192533 in 0.91s\n",
      " [-] epoch  232/250, train loss 0.183193 in 0.80s\n",
      " [-] epoch  233/250, train loss 0.206176 in 0.95s\n",
      " [-] epoch  234/250, train loss 0.182827 in 1.02s\n",
      " [-] epoch  235/250, train loss 0.220486 in 0.95s\n",
      " [-] epoch  236/250, train loss 0.197454 in 0.86s\n",
      " [-] epoch  237/250, train loss 0.200927 in 1.16s\n",
      " [-] epoch  238/250, train loss 0.189080 in 0.92s\n",
      " [-] epoch  239/250, train loss 0.179200 in 1.04s\n",
      " [-] epoch  240/250, train loss 0.216139 in 1.16s\n",
      " [-] epoch  241/250, train loss 0.202196 in 1.10s\n",
      " [-] epoch  242/250, train loss 0.201548 in 1.06s\n",
      " [-] epoch  243/250, train loss 0.204332 in 1.09s\n",
      " [-] epoch  244/250, train loss 0.210812 in 1.06s\n",
      " [-] epoch  245/250, train loss 0.188234 in 1.11s\n",
      " [-] epoch  246/250, train loss 0.202589 in 1.10s\n",
      " [-] epoch  247/250, train loss 0.194688 in 1.07s\n",
      " [-] epoch  248/250, train loss 0.185155 in 1.01s\n",
      " [-] epoch  249/250, train loss 0.202002 in 1.09s\n",
      " [-] epoch  250/250, train loss 0.205441 in 1.10s\n",
      " [-] test acc. 84.722222%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_linear_results(X_train, X_test) # all dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calcul du taux de réussite en classement d'un réseau linéaire multicouches sur les données réduites\n",
    "Cette deuxième étape consiste à avoir les résultats sur les dimensions réduites engendrées par la réduction de dimensionnalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.691972 in 0.03s\n",
      " [-] epoch    2/250, train loss 0.685599 in 0.02s\n",
      " [-] epoch    3/250, train loss 0.683649 in 0.02s\n",
      " [-] epoch    4/250, train loss 0.675693 in 0.02s\n",
      " [-] epoch    5/250, train loss 0.676556 in 0.02s\n",
      " [-] epoch    6/250, train loss 0.672031 in 0.02s\n",
      " [-] epoch    7/250, train loss 0.674704 in 0.03s\n",
      " [-] epoch    8/250, train loss 0.671867 in 0.02s\n",
      " [-] epoch    9/250, train loss 0.678673 in 0.03s\n",
      " [-] epoch   10/250, train loss 0.668243 in 0.03s\n",
      " [-] epoch   11/250, train loss 0.662384 in 0.02s\n",
      " [-] epoch   12/250, train loss 0.669996 in 0.02s\n",
      " [-] epoch   13/250, train loss 0.666203 in 0.02s\n",
      " [-] epoch   14/250, train loss 0.663590 in 0.03s\n",
      " [-] epoch   15/250, train loss 0.661044 in 0.02s\n",
      " [-] epoch   16/250, train loss 0.657437 in 0.02s\n",
      " [-] epoch   17/250, train loss 0.661507 in 0.03s\n",
      " [-] epoch   18/250, train loss 0.655352 in 0.03s\n",
      " [-] epoch   19/250, train loss 0.653724 in 0.02s\n",
      " [-] epoch   20/250, train loss 0.664414 in 0.03s\n",
      " [-] epoch   21/250, train loss 0.661675 in 0.03s\n",
      " [-] epoch   22/250, train loss 0.662525 in 0.02s\n",
      " [-] epoch   23/250, train loss 0.659870 in 0.03s\n",
      " [-] epoch   24/250, train loss 0.657308 in 0.03s\n",
      " [-] epoch   25/250, train loss 0.658433 in 0.02s\n",
      " [-] epoch   26/250, train loss 0.652508 in 0.02s\n",
      " [-] epoch   27/250, train loss 0.656244 in 0.02s\n",
      " [-] epoch   28/250, train loss 0.650857 in 0.03s\n",
      " [-] epoch   29/250, train loss 0.652324 in 0.02s\n",
      " [-] epoch   30/250, train loss 0.654084 in 0.03s\n",
      " [-] epoch   31/250, train loss 0.652421 in 0.02s\n",
      " [-] epoch   32/250, train loss 0.641777 in 0.03s\n",
      " [-] epoch   33/250, train loss 0.651848 in 0.02s\n",
      " [-] epoch   34/250, train loss 0.649425 in 0.03s\n",
      " [-] epoch   35/250, train loss 0.643981 in 0.02s\n",
      " [-] epoch   36/250, train loss 0.654659 in 0.02s\n",
      " [-] epoch   37/250, train loss 0.655843 in 0.02s\n",
      " [-] epoch   38/250, train loss 0.638080 in 0.03s\n",
      " [-] epoch   39/250, train loss 0.651931 in 0.03s\n",
      " [-] epoch   40/250, train loss 0.648335 in 0.02s\n",
      " [-] epoch   41/250, train loss 0.640561 in 0.02s\n",
      " [-] epoch   42/250, train loss 0.641084 in 0.03s\n",
      " [-] epoch   43/250, train loss 0.639855 in 0.02s\n",
      " [-] epoch   44/250, train loss 0.635443 in 0.03s\n",
      " [-] epoch   45/250, train loss 0.635355 in 0.02s\n",
      " [-] epoch   46/250, train loss 0.645068 in 0.03s\n",
      " [-] epoch   47/250, train loss 0.637361 in 0.02s\n",
      " [-] epoch   48/250, train loss 0.639949 in 0.03s\n",
      " [-] epoch   49/250, train loss 0.629767 in 0.03s\n",
      " [-] epoch   50/250, train loss 0.642953 in 0.03s\n",
      " [-] epoch   51/250, train loss 0.637792 in 0.02s\n",
      " [-] epoch   52/250, train loss 0.632359 in 0.03s\n",
      " [-] epoch   53/250, train loss 0.634247 in 0.02s\n",
      " [-] epoch   54/250, train loss 0.627443 in 0.03s\n",
      " [-] epoch   55/250, train loss 0.638288 in 0.03s\n",
      " [-] epoch   56/250, train loss 0.628278 in 0.02s\n",
      " [-] epoch   57/250, train loss 0.645070 in 0.03s\n",
      " [-] epoch   58/250, train loss 0.640570 in 0.02s\n",
      " [-] epoch   59/250, train loss 0.625694 in 0.03s\n",
      " [-] epoch   60/250, train loss 0.640999 in 0.02s\n",
      " [-] epoch   61/250, train loss 0.628046 in 0.02s\n",
      " [-] epoch   62/250, train loss 0.636615 in 0.03s\n",
      " [-] epoch   63/250, train loss 0.622964 in 0.03s\n",
      " [-] epoch   64/250, train loss 0.620805 in 0.02s\n",
      " [-] epoch   65/250, train loss 0.625353 in 0.03s\n",
      " [-] epoch   66/250, train loss 0.630872 in 0.03s\n",
      " [-] epoch   67/250, train loss 0.635324 in 0.03s\n",
      " [-] epoch   68/250, train loss 0.632606 in 0.02s\n",
      " [-] epoch   69/250, train loss 0.629180 in 0.03s\n",
      " [-] epoch   70/250, train loss 0.629648 in 0.03s\n",
      " [-] epoch   71/250, train loss 0.628674 in 0.03s\n",
      " [-] epoch   72/250, train loss 0.628188 in 0.03s\n",
      " [-] epoch   73/250, train loss 0.632811 in 0.03s\n",
      " [-] epoch   74/250, train loss 0.638652 in 0.03s\n",
      " [-] epoch   75/250, train loss 0.629027 in 0.03s\n",
      " [-] epoch   76/250, train loss 0.624513 in 0.03s\n",
      " [-] epoch   77/250, train loss 0.631370 in 0.03s\n",
      " [-] epoch   78/250, train loss 0.621233 in 0.03s\n",
      " [-] epoch   79/250, train loss 0.617678 in 0.03s\n",
      " [-] epoch   80/250, train loss 0.622064 in 0.03s\n",
      " [-] epoch   81/250, train loss 0.621986 in 0.03s\n",
      " [-] epoch   82/250, train loss 0.621747 in 0.02s\n",
      " [-] epoch   83/250, train loss 0.633405 in 0.03s\n",
      " [-] epoch   84/250, train loss 0.621771 in 0.03s\n",
      " [-] epoch   85/250, train loss 0.632159 in 0.02s\n",
      " [-] epoch   86/250, train loss 0.614274 in 0.02s\n",
      " [-] epoch   87/250, train loss 0.619697 in 0.03s\n",
      " [-] epoch   88/250, train loss 0.620697 in 0.02s\n",
      " [-] epoch   89/250, train loss 0.640071 in 0.03s\n",
      " [-] epoch   90/250, train loss 0.614363 in 0.02s\n",
      " [-] epoch   91/250, train loss 0.628078 in 0.02s\n",
      " [-] epoch   92/250, train loss 0.622042 in 0.02s\n",
      " [-] epoch   93/250, train loss 0.618567 in 0.02s\n",
      " [-] epoch   94/250, train loss 0.625910 in 0.02s\n",
      " [-] epoch   95/250, train loss 0.621005 in 0.03s\n",
      " [-] epoch   96/250, train loss 0.627897 in 0.02s\n",
      " [-] epoch   97/250, train loss 0.619377 in 0.02s\n",
      " [-] epoch   98/250, train loss 0.633605 in 0.03s\n",
      " [-] epoch   99/250, train loss 0.619946 in 0.03s\n",
      " [-] epoch  100/250, train loss 0.620185 in 0.03s\n",
      " [-] epoch  101/250, train loss 0.630285 in 0.03s\n",
      " [-] epoch  102/250, train loss 0.634916 in 0.02s\n",
      " [-] epoch  103/250, train loss 0.621306 in 0.02s\n",
      " [-] epoch  104/250, train loss 0.623649 in 0.02s\n",
      " [-] epoch  105/250, train loss 0.622360 in 0.03s\n",
      " [-] epoch  106/250, train loss 0.625462 in 0.03s\n",
      " [-] epoch  107/250, train loss 0.623270 in 0.02s\n",
      " [-] epoch  108/250, train loss 0.613842 in 0.03s\n",
      " [-] epoch  109/250, train loss 0.619502 in 0.03s\n",
      " [-] epoch  110/250, train loss 0.613797 in 0.03s\n",
      " [-] epoch  111/250, train loss 0.610284 in 0.03s\n",
      " [-] epoch  112/250, train loss 0.609470 in 0.03s\n",
      " [-] epoch  113/250, train loss 0.619789 in 0.03s\n",
      " [-] epoch  114/250, train loss 0.616459 in 0.03s\n",
      " [-] epoch  115/250, train loss 0.608982 in 0.03s\n",
      " [-] epoch  116/250, train loss 0.629885 in 0.03s\n",
      " [-] epoch  117/250, train loss 0.617605 in 0.03s\n",
      " [-] epoch  118/250, train loss 0.629191 in 0.03s\n",
      " [-] epoch  119/250, train loss 0.615241 in 0.03s\n",
      " [-] epoch  120/250, train loss 0.610869 in 0.03s\n",
      " [-] epoch  121/250, train loss 0.608470 in 0.03s\n",
      " [-] epoch  122/250, train loss 0.616198 in 0.03s\n",
      " [-] epoch  123/250, train loss 0.611734 in 0.02s\n",
      " [-] epoch  124/250, train loss 0.605723 in 0.03s\n",
      " [-] epoch  125/250, train loss 0.609145 in 0.03s\n",
      " [-] epoch  126/250, train loss 0.618546 in 0.03s\n",
      " [-] epoch  127/250, train loss 0.619574 in 0.03s\n",
      " [-] epoch  128/250, train loss 0.617899 in 0.02s\n",
      " [-] epoch  129/250, train loss 0.616559 in 0.03s\n",
      " [-] epoch  130/250, train loss 0.621076 in 0.02s\n",
      " [-] epoch  131/250, train loss 0.607757 in 0.03s\n",
      " [-] epoch  132/250, train loss 0.598776 in 0.03s\n",
      " [-] epoch  133/250, train loss 0.605755 in 0.03s\n",
      " [-] epoch  134/250, train loss 0.615984 in 0.03s\n",
      " [-] epoch  135/250, train loss 0.608687 in 0.03s\n",
      " [-] epoch  136/250, train loss 0.615371 in 0.03s\n",
      " [-] epoch  137/250, train loss 0.622246 in 0.03s\n",
      " [-] epoch  138/250, train loss 0.618121 in 0.02s\n",
      " [-] epoch  139/250, train loss 0.617892 in 0.02s\n",
      " [-] epoch  140/250, train loss 0.609457 in 0.03s\n",
      " [-] epoch  141/250, train loss 0.616969 in 0.02s\n",
      " [-] epoch  142/250, train loss 0.608980 in 0.02s\n",
      " [-] epoch  143/250, train loss 0.604673 in 0.02s\n",
      " [-] epoch  144/250, train loss 0.619276 in 0.02s\n",
      " [-] epoch  145/250, train loss 0.617063 in 0.03s\n",
      " [-] epoch  146/250, train loss 0.599961 in 0.02s\n",
      " [-] epoch  147/250, train loss 0.603565 in 0.03s\n",
      " [-] epoch  148/250, train loss 0.612725 in 0.02s\n",
      " [-] epoch  149/250, train loss 0.607679 in 0.03s\n",
      " [-] epoch  150/250, train loss 0.609264 in 0.03s\n",
      " [-] epoch  151/250, train loss 0.609021 in 0.03s\n",
      " [-] epoch  152/250, train loss 0.608679 in 0.02s\n",
      " [-] epoch  153/250, train loss 0.604501 in 0.03s\n",
      " [-] epoch  154/250, train loss 0.612668 in 0.02s\n",
      " [-] epoch  155/250, train loss 0.603739 in 0.02s\n",
      " [-] epoch  156/250, train loss 0.601472 in 0.02s\n",
      " [-] epoch  157/250, train loss 0.608428 in 0.03s\n",
      " [-] epoch  158/250, train loss 0.612248 in 0.02s\n",
      " [-] epoch  159/250, train loss 0.593478 in 0.02s\n",
      " [-] epoch  160/250, train loss 0.594906 in 0.02s\n",
      " [-] epoch  161/250, train loss 0.605764 in 0.02s\n",
      " [-] epoch  162/250, train loss 0.615606 in 0.02s\n",
      " [-] epoch  163/250, train loss 0.594126 in 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  164/250, train loss 0.603694 in 0.02s\n",
      " [-] epoch  165/250, train loss 0.609142 in 0.02s\n",
      " [-] epoch  166/250, train loss 0.605283 in 0.03s\n",
      " [-] epoch  167/250, train loss 0.609704 in 0.02s\n",
      " [-] epoch  168/250, train loss 0.608780 in 0.03s\n",
      " [-] epoch  169/250, train loss 0.605533 in 0.03s\n",
      " [-] epoch  170/250, train loss 0.601683 in 0.02s\n",
      " [-] epoch  171/250, train loss 0.597413 in 0.03s\n",
      " [-] epoch  172/250, train loss 0.609986 in 0.02s\n",
      " [-] epoch  173/250, train loss 0.616073 in 0.03s\n",
      " [-] epoch  174/250, train loss 0.613796 in 0.02s\n",
      " [-] epoch  175/250, train loss 0.614703 in 0.03s\n",
      " [-] epoch  176/250, train loss 0.614616 in 0.03s\n",
      " [-] epoch  177/250, train loss 0.614378 in 0.03s\n",
      " [-] epoch  178/250, train loss 0.613399 in 0.03s\n",
      " [-] epoch  179/250, train loss 0.601262 in 0.02s\n",
      " [-] epoch  180/250, train loss 0.615019 in 0.03s\n",
      " [-] epoch  181/250, train loss 0.598610 in 0.03s\n",
      " [-] epoch  182/250, train loss 0.599386 in 0.03s\n",
      " [-] epoch  183/250, train loss 0.609544 in 0.02s\n",
      " [-] epoch  184/250, train loss 0.594583 in 0.02s\n",
      " [-] epoch  185/250, train loss 0.611506 in 0.02s\n",
      " [-] epoch  186/250, train loss 0.615485 in 0.02s\n",
      " [-] epoch  187/250, train loss 0.615778 in 0.02s\n",
      " [-] epoch  188/250, train loss 0.623253 in 0.03s\n",
      " [-] epoch  189/250, train loss 0.599431 in 0.02s\n",
      " [-] epoch  190/250, train loss 0.617705 in 0.03s\n",
      " [-] epoch  191/250, train loss 0.619835 in 0.02s\n",
      " [-] epoch  192/250, train loss 0.600394 in 0.03s\n",
      " [-] epoch  193/250, train loss 0.618996 in 0.03s\n",
      " [-] epoch  194/250, train loss 0.623504 in 0.02s\n",
      " [-] epoch  195/250, train loss 0.601869 in 0.03s\n",
      " [-] epoch  196/250, train loss 0.601821 in 0.02s\n",
      " [-] epoch  197/250, train loss 0.608838 in 0.03s\n",
      " [-] epoch  198/250, train loss 0.602014 in 0.03s\n",
      " [-] epoch  199/250, train loss 0.596924 in 0.03s\n",
      " [-] epoch  200/250, train loss 0.601135 in 0.03s\n",
      " [-] epoch  201/250, train loss 0.593155 in 0.03s\n",
      " [-] epoch  202/250, train loss 0.608790 in 0.02s\n",
      " [-] epoch  203/250, train loss 0.612717 in 0.03s\n",
      " [-] epoch  204/250, train loss 0.593442 in 0.02s\n",
      " [-] epoch  205/250, train loss 0.599233 in 0.03s\n",
      " [-] epoch  206/250, train loss 0.602525 in 0.03s\n",
      " [-] epoch  207/250, train loss 0.605164 in 0.03s\n",
      " [-] epoch  208/250, train loss 0.616940 in 0.02s\n",
      " [-] epoch  209/250, train loss 0.610661 in 0.02s\n",
      " [-] epoch  210/250, train loss 0.593677 in 0.02s\n",
      " [-] epoch  211/250, train loss 0.594257 in 0.02s\n",
      " [-] epoch  212/250, train loss 0.598867 in 0.03s\n",
      " [-] epoch  213/250, train loss 0.594162 in 0.02s\n",
      " [-] epoch  214/250, train loss 0.601868 in 0.03s\n",
      " [-] epoch  215/250, train loss 0.599531 in 0.02s\n",
      " [-] epoch  216/250, train loss 0.606060 in 0.03s\n",
      " [-] epoch  217/250, train loss 0.597374 in 0.02s\n",
      " [-] epoch  218/250, train loss 0.604944 in 0.02s\n",
      " [-] epoch  219/250, train loss 0.601523 in 0.02s\n",
      " [-] epoch  220/250, train loss 0.596585 in 0.02s\n",
      " [-] epoch  221/250, train loss 0.604925 in 0.03s\n",
      " [-] epoch  222/250, train loss 0.594578 in 0.03s\n",
      " [-] epoch  223/250, train loss 0.604087 in 0.02s\n",
      " [-] epoch  224/250, train loss 0.598834 in 0.02s\n",
      " [-] epoch  225/250, train loss 0.590113 in 0.03s\n",
      " [-] epoch  226/250, train loss 0.604320 in 0.03s\n",
      " [-] epoch  227/250, train loss 0.599252 in 0.03s\n",
      " [-] epoch  228/250, train loss 0.598505 in 0.02s\n",
      " [-] epoch  229/250, train loss 0.595482 in 0.03s\n",
      " [-] epoch  230/250, train loss 0.593800 in 0.02s\n",
      " [-] epoch  231/250, train loss 0.606529 in 0.02s\n",
      " [-] epoch  232/250, train loss 0.602061 in 0.03s\n",
      " [-] epoch  233/250, train loss 0.608704 in 0.02s\n",
      " [-] epoch  234/250, train loss 0.614602 in 0.02s\n",
      " [-] epoch  235/250, train loss 0.601115 in 0.02s\n",
      " [-] epoch  236/250, train loss 0.601896 in 0.02s\n",
      " [-] epoch  237/250, train loss 0.600191 in 0.02s\n",
      " [-] epoch  238/250, train loss 0.603375 in 0.02s\n",
      " [-] epoch  239/250, train loss 0.598356 in 0.03s\n",
      " [-] epoch  240/250, train loss 0.576213 in 0.02s\n",
      " [-] epoch  241/250, train loss 0.597420 in 0.02s\n",
      " [-] epoch  242/250, train loss 0.608574 in 0.02s\n",
      " [-] epoch  243/250, train loss 0.594169 in 0.02s\n",
      " [-] epoch  244/250, train loss 0.585378 in 0.02s\n",
      " [-] epoch  245/250, train loss 0.592753 in 0.03s\n",
      " [-] epoch  246/250, train loss 0.587886 in 0.02s\n",
      " [-] epoch  247/250, train loss 0.597281 in 0.02s\n",
      " [-] epoch  248/250, train loss 0.589407 in 0.02s\n",
      " [-] epoch  249/250, train loss 0.595247 in 0.02s\n",
      " [-] epoch  250/250, train loss 0.602865 in 0.03s\n",
      " [-] test acc. 67.222222%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.664277 in 0.07s\n",
      " [-] epoch    2/250, train loss 0.603505 in 0.09s\n",
      " [-] epoch    3/250, train loss 0.616130 in 0.08s\n",
      " [-] epoch    4/250, train loss 0.610565 in 0.09s\n",
      " [-] epoch    5/250, train loss 0.605841 in 0.08s\n",
      " [-] epoch    6/250, train loss 0.600112 in 0.07s\n",
      " [-] epoch    7/250, train loss 0.569463 in 0.07s\n",
      " [-] epoch    8/250, train loss 0.578107 in 0.09s\n",
      " [-] epoch    9/250, train loss 0.581006 in 0.08s\n",
      " [-] epoch   10/250, train loss 0.588548 in 0.07s\n",
      " [-] epoch   11/250, train loss 0.576474 in 0.08s\n",
      " [-] epoch   12/250, train loss 0.554170 in 0.09s\n",
      " [-] epoch   13/250, train loss 0.554137 in 0.07s\n",
      " [-] epoch   14/250, train loss 0.575409 in 0.08s\n",
      " [-] epoch   15/250, train loss 0.560663 in 0.08s\n",
      " [-] epoch   16/250, train loss 0.574055 in 0.07s\n",
      " [-] epoch   17/250, train loss 0.572422 in 0.08s\n",
      " [-] epoch   18/250, train loss 0.579631 in 0.09s\n",
      " [-] epoch   19/250, train loss 0.558385 in 0.08s\n",
      " [-] epoch   20/250, train loss 0.561129 in 0.08s\n",
      " [-] epoch   21/250, train loss 0.550674 in 0.07s\n",
      " [-] epoch   22/250, train loss 0.573172 in 0.08s\n",
      " [-] epoch   23/250, train loss 0.587201 in 0.07s\n",
      " [-] epoch   24/250, train loss 0.521100 in 0.06s\n",
      " [-] epoch   25/250, train loss 0.561826 in 0.06s\n",
      " [-] epoch   26/250, train loss 0.562802 in 0.06s\n",
      " [-] epoch   27/250, train loss 0.528325 in 0.06s\n",
      " [-] epoch   28/250, train loss 0.556164 in 0.06s\n",
      " [-] epoch   29/250, train loss 0.547199 in 0.07s\n",
      " [-] epoch   30/250, train loss 0.552710 in 0.08s\n",
      " [-] epoch   31/250, train loss 0.561766 in 0.07s\n",
      " [-] epoch   32/250, train loss 0.559633 in 0.07s\n",
      " [-] epoch   33/250, train loss 0.544329 in 0.07s\n",
      " [-] epoch   34/250, train loss 0.529187 in 0.07s\n",
      " [-] epoch   35/250, train loss 0.539873 in 0.07s\n",
      " [-] epoch   36/250, train loss 0.524863 in 0.07s\n",
      " [-] epoch   37/250, train loss 0.534856 in 0.06s\n",
      " [-] epoch   38/250, train loss 0.542110 in 0.07s\n",
      " [-] epoch   39/250, train loss 0.574874 in 0.07s\n",
      " [-] epoch   40/250, train loss 0.543540 in 0.06s\n",
      " [-] epoch   41/250, train loss 0.547217 in 0.07s\n",
      " [-] epoch   42/250, train loss 0.497717 in 0.06s\n",
      " [-] epoch   43/250, train loss 0.528564 in 0.07s\n",
      " [-] epoch   44/250, train loss 0.533278 in 0.07s\n",
      " [-] epoch   45/250, train loss 0.514451 in 0.08s\n",
      " [-] epoch   46/250, train loss 0.540506 in 0.06s\n",
      " [-] epoch   47/250, train loss 0.539009 in 0.06s\n",
      " [-] epoch   48/250, train loss 0.560459 in 0.06s\n",
      " [-] epoch   49/250, train loss 0.525908 in 0.07s\n",
      " [-] epoch   50/250, train loss 0.528195 in 0.06s\n",
      " [-] epoch   51/250, train loss 0.547522 in 0.06s\n",
      " [-] epoch   52/250, train loss 0.530562 in 0.07s\n",
      " [-] epoch   53/250, train loss 0.514050 in 0.07s\n",
      " [-] epoch   54/250, train loss 0.536853 in 0.05s\n",
      " [-] epoch   55/250, train loss 0.508445 in 0.06s\n",
      " [-] epoch   56/250, train loss 0.526392 in 0.06s\n",
      " [-] epoch   57/250, train loss 0.516922 in 0.06s\n",
      " [-] epoch   58/250, train loss 0.536044 in 0.07s\n",
      " [-] epoch   59/250, train loss 0.527555 in 0.07s\n",
      " [-] epoch   60/250, train loss 0.510024 in 0.09s\n",
      " [-] epoch   61/250, train loss 0.539434 in 0.06s\n",
      " [-] epoch   62/250, train loss 0.516010 in 0.06s\n",
      " [-] epoch   63/250, train loss 0.526544 in 0.06s\n",
      " [-] epoch   64/250, train loss 0.539703 in 0.06s\n",
      " [-] epoch   65/250, train loss 0.503112 in 0.07s\n",
      " [-] epoch   66/250, train loss 0.522404 in 0.06s\n",
      " [-] epoch   67/250, train loss 0.516169 in 0.06s\n",
      " [-] epoch   68/250, train loss 0.541114 in 0.06s\n",
      " [-] epoch   69/250, train loss 0.534790 in 0.07s\n",
      " [-] epoch   70/250, train loss 0.508726 in 0.06s\n",
      " [-] epoch   71/250, train loss 0.483094 in 0.07s\n",
      " [-] epoch   72/250, train loss 0.500768 in 0.06s\n",
      " [-] epoch   73/250, train loss 0.522477 in 0.07s\n",
      " [-] epoch   74/250, train loss 0.515721 in 0.06s\n",
      " [-] epoch   75/250, train loss 0.537037 in 0.06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.535586 in 0.08s\n",
      " [-] epoch   77/250, train loss 0.517205 in 0.06s\n",
      " [-] epoch   78/250, train loss 0.537457 in 0.06s\n",
      " [-] epoch   79/250, train loss 0.511105 in 0.06s\n",
      " [-] epoch   80/250, train loss 0.534680 in 0.08s\n",
      " [-] epoch   81/250, train loss 0.527200 in 0.07s\n",
      " [-] epoch   82/250, train loss 0.499212 in 0.07s\n",
      " [-] epoch   83/250, train loss 0.525313 in 0.07s\n",
      " [-] epoch   84/250, train loss 0.520197 in 0.09s\n",
      " [-] epoch   85/250, train loss 0.487799 in 0.07s\n",
      " [-] epoch   86/250, train loss 0.512964 in 0.07s\n",
      " [-] epoch   87/250, train loss 0.474990 in 0.07s\n",
      " [-] epoch   88/250, train loss 0.507938 in 0.07s\n",
      " [-] epoch   89/250, train loss 0.534446 in 0.06s\n",
      " [-] epoch   90/250, train loss 0.503080 in 0.09s\n",
      " [-] epoch   91/250, train loss 0.529373 in 0.07s\n",
      " [-] epoch   92/250, train loss 0.490581 in 0.10s\n",
      " [-] epoch   93/250, train loss 0.499377 in 0.08s\n",
      " [-] epoch   94/250, train loss 0.482773 in 0.06s\n",
      " [-] epoch   95/250, train loss 0.521411 in 0.07s\n",
      " [-] epoch   96/250, train loss 0.506080 in 0.08s\n",
      " [-] epoch   97/250, train loss 0.495477 in 0.07s\n",
      " [-] epoch   98/250, train loss 0.497212 in 0.06s\n",
      " [-] epoch   99/250, train loss 0.494765 in 0.06s\n",
      " [-] epoch  100/250, train loss 0.492248 in 0.07s\n",
      " [-] epoch  101/250, train loss 0.501658 in 0.07s\n",
      " [-] epoch  102/250, train loss 0.491932 in 0.06s\n",
      " [-] epoch  103/250, train loss 0.489946 in 0.06s\n",
      " [-] epoch  104/250, train loss 0.500380 in 0.08s\n",
      " [-] epoch  105/250, train loss 0.505611 in 0.08s\n",
      " [-] epoch  106/250, train loss 0.486774 in 0.08s\n",
      " [-] epoch  107/250, train loss 0.483669 in 0.08s\n",
      " [-] epoch  108/250, train loss 0.510222 in 0.08s\n",
      " [-] epoch  109/250, train loss 0.493256 in 0.07s\n",
      " [-] epoch  110/250, train loss 0.476532 in 0.07s\n",
      " [-] epoch  111/250, train loss 0.497959 in 0.07s\n",
      " [-] epoch  112/250, train loss 0.490617 in 0.10s\n",
      " [-] epoch  113/250, train loss 0.486370 in 0.07s\n",
      " [-] epoch  114/250, train loss 0.477678 in 0.07s\n",
      " [-] epoch  115/250, train loss 0.520016 in 0.06s\n",
      " [-] epoch  116/250, train loss 0.466920 in 0.07s\n",
      " [-] epoch  117/250, train loss 0.510908 in 0.07s\n",
      " [-] epoch  118/250, train loss 0.502170 in 0.08s\n",
      " [-] epoch  119/250, train loss 0.496220 in 0.07s\n",
      " [-] epoch  120/250, train loss 0.502978 in 0.07s\n",
      " [-] epoch  121/250, train loss 0.503221 in 0.06s\n",
      " [-] epoch  122/250, train loss 0.472302 in 0.06s\n",
      " [-] epoch  123/250, train loss 0.489986 in 0.06s\n",
      " [-] epoch  124/250, train loss 0.489765 in 0.07s\n",
      " [-] epoch  125/250, train loss 0.490536 in 0.07s\n",
      " [-] epoch  126/250, train loss 0.500784 in 0.07s\n",
      " [-] epoch  127/250, train loss 0.496095 in 0.07s\n",
      " [-] epoch  128/250, train loss 0.508840 in 0.07s\n",
      " [-] epoch  129/250, train loss 0.507083 in 0.07s\n",
      " [-] epoch  130/250, train loss 0.466829 in 0.07s\n",
      " [-] epoch  131/250, train loss 0.479777 in 0.07s\n",
      " [-] epoch  132/250, train loss 0.489987 in 0.06s\n",
      " [-] epoch  133/250, train loss 0.512910 in 0.06s\n",
      " [-] epoch  134/250, train loss 0.487405 in 0.07s\n",
      " [-] epoch  135/250, train loss 0.498298 in 0.08s\n",
      " [-] epoch  136/250, train loss 0.497986 in 0.08s\n",
      " [-] epoch  137/250, train loss 0.487658 in 0.08s\n",
      " [-] epoch  138/250, train loss 0.470637 in 0.07s\n",
      " [-] epoch  139/250, train loss 0.500242 in 0.07s\n",
      " [-] epoch  140/250, train loss 0.508778 in 0.06s\n",
      " [-] epoch  141/250, train loss 0.480962 in 0.07s\n",
      " [-] epoch  142/250, train loss 0.482926 in 0.07s\n",
      " [-] epoch  143/250, train loss 0.496396 in 0.06s\n",
      " [-] epoch  144/250, train loss 0.520168 in 0.07s\n",
      " [-] epoch  145/250, train loss 0.479941 in 0.07s\n",
      " [-] epoch  146/250, train loss 0.493078 in 0.08s\n",
      " [-] epoch  147/250, train loss 0.479030 in 0.07s\n",
      " [-] epoch  148/250, train loss 0.465558 in 0.07s\n",
      " [-] epoch  149/250, train loss 0.467519 in 0.06s\n",
      " [-] epoch  150/250, train loss 0.471894 in 0.07s\n",
      " [-] epoch  151/250, train loss 0.477897 in 0.06s\n",
      " [-] epoch  152/250, train loss 0.483687 in 0.07s\n",
      " [-] epoch  153/250, train loss 0.491348 in 0.07s\n",
      " [-] epoch  154/250, train loss 0.483266 in 0.07s\n",
      " [-] epoch  155/250, train loss 0.474477 in 0.08s\n",
      " [-] epoch  156/250, train loss 0.490631 in 0.06s\n",
      " [-] epoch  157/250, train loss 0.477378 in 0.06s\n",
      " [-] epoch  158/250, train loss 0.486594 in 0.07s\n",
      " [-] epoch  159/250, train loss 0.471026 in 0.08s\n",
      " [-] epoch  160/250, train loss 0.465605 in 0.08s\n",
      " [-] epoch  161/250, train loss 0.459156 in 0.07s\n",
      " [-] epoch  162/250, train loss 0.502631 in 0.07s\n",
      " [-] epoch  163/250, train loss 0.486980 in 0.08s\n",
      " [-] epoch  164/250, train loss 0.486299 in 0.07s\n",
      " [-] epoch  165/250, train loss 0.500419 in 0.07s\n",
      " [-] epoch  166/250, train loss 0.489683 in 0.08s\n",
      " [-] epoch  167/250, train loss 0.508602 in 0.07s\n",
      " [-] epoch  168/250, train loss 0.491254 in 0.07s\n",
      " [-] epoch  169/250, train loss 0.473460 in 0.07s\n",
      " [-] epoch  170/250, train loss 0.507231 in 0.07s\n",
      " [-] epoch  171/250, train loss 0.486808 in 0.07s\n",
      " [-] epoch  172/250, train loss 0.486170 in 0.07s\n",
      " [-] epoch  173/250, train loss 0.478385 in 0.06s\n",
      " [-] epoch  174/250, train loss 0.478930 in 0.06s\n",
      " [-] epoch  175/250, train loss 0.466726 in 0.09s\n",
      " [-] epoch  176/250, train loss 0.509319 in 0.06s\n",
      " [-] epoch  177/250, train loss 0.474067 in 0.06s\n",
      " [-] epoch  178/250, train loss 0.460434 in 0.06s\n",
      " [-] epoch  179/250, train loss 0.499717 in 0.07s\n",
      " [-] epoch  180/250, train loss 0.472709 in 0.07s\n",
      " [-] epoch  181/250, train loss 0.499750 in 0.07s\n",
      " [-] epoch  182/250, train loss 0.478549 in 0.08s\n",
      " [-] epoch  183/250, train loss 0.459382 in 0.06s\n",
      " [-] epoch  184/250, train loss 0.480372 in 0.07s\n",
      " [-] epoch  185/250, train loss 0.475324 in 0.09s\n",
      " [-] epoch  186/250, train loss 0.515177 in 0.07s\n",
      " [-] epoch  187/250, train loss 0.464793 in 0.06s\n",
      " [-] epoch  188/250, train loss 0.474601 in 0.07s\n",
      " [-] epoch  189/250, train loss 0.474313 in 0.07s\n",
      " [-] epoch  190/250, train loss 0.464421 in 0.07s\n",
      " [-] epoch  191/250, train loss 0.463334 in 0.06s\n",
      " [-] epoch  192/250, train loss 0.468377 in 0.06s\n",
      " [-] epoch  193/250, train loss 0.468988 in 0.08s\n",
      " [-] epoch  194/250, train loss 0.461360 in 0.07s\n",
      " [-] epoch  195/250, train loss 0.460255 in 0.07s\n",
      " [-] epoch  196/250, train loss 0.449406 in 0.07s\n",
      " [-] epoch  197/250, train loss 0.482182 in 0.08s\n",
      " [-] epoch  198/250, train loss 0.484811 in 0.08s\n",
      " [-] epoch  199/250, train loss 0.474217 in 0.06s\n",
      " [-] epoch  200/250, train loss 0.471007 in 0.07s\n",
      " [-] epoch  201/250, train loss 0.491915 in 0.07s\n",
      " [-] epoch  202/250, train loss 0.465924 in 0.07s\n",
      " [-] epoch  203/250, train loss 0.470528 in 0.07s\n",
      " [-] epoch  204/250, train loss 0.451983 in 0.06s\n",
      " [-] epoch  205/250, train loss 0.482289 in 0.07s\n",
      " [-] epoch  206/250, train loss 0.493223 in 0.06s\n",
      " [-] epoch  207/250, train loss 0.465629 in 0.07s\n",
      " [-] epoch  208/250, train loss 0.460451 in 0.06s\n",
      " [-] epoch  209/250, train loss 0.481168 in 0.07s\n",
      " [-] epoch  210/250, train loss 0.465318 in 0.07s\n",
      " [-] epoch  211/250, train loss 0.450880 in 0.07s\n",
      " [-] epoch  212/250, train loss 0.450138 in 0.07s\n",
      " [-] epoch  213/250, train loss 0.470620 in 0.07s\n",
      " [-] epoch  214/250, train loss 0.454161 in 0.06s\n",
      " [-] epoch  215/250, train loss 0.474573 in 0.07s\n",
      " [-] epoch  216/250, train loss 0.487778 in 0.08s\n",
      " [-] epoch  217/250, train loss 0.492928 in 0.08s\n",
      " [-] epoch  218/250, train loss 0.453169 in 0.08s\n",
      " [-] epoch  219/250, train loss 0.476645 in 0.06s\n",
      " [-] epoch  220/250, train loss 0.449060 in 0.06s\n",
      " [-] epoch  221/250, train loss 0.456540 in 0.06s\n",
      " [-] epoch  222/250, train loss 0.464045 in 0.06s\n",
      " [-] epoch  223/250, train loss 0.488592 in 0.07s\n",
      " [-] epoch  224/250, train loss 0.476445 in 0.07s\n",
      " [-] epoch  225/250, train loss 0.471570 in 0.06s\n",
      " [-] epoch  226/250, train loss 0.460722 in 0.07s\n",
      " [-] epoch  227/250, train loss 0.482235 in 0.07s\n",
      " [-] epoch  228/250, train loss 0.464139 in 0.07s\n",
      " [-] epoch  229/250, train loss 0.462631 in 0.07s\n",
      " [-] epoch  230/250, train loss 0.461130 in 0.06s\n",
      " [-] epoch  231/250, train loss 0.466050 in 0.07s\n",
      " [-] epoch  232/250, train loss 0.453201 in 0.06s\n",
      " [-] epoch  233/250, train loss 0.460373 in 0.07s\n",
      " [-] epoch  234/250, train loss 0.492546 in 0.06s\n",
      " [-] epoch  235/250, train loss 0.469424 in 0.07s\n",
      " [-] epoch  236/250, train loss 0.472278 in 0.07s\n",
      " [-] epoch  237/250, train loss 0.467212 in 0.07s\n",
      " [-] epoch  238/250, train loss 0.484456 in 0.06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  239/250, train loss 0.451195 in 0.08s\n",
      " [-] epoch  240/250, train loss 0.481513 in 0.06s\n",
      " [-] epoch  241/250, train loss 0.474872 in 0.06s\n",
      " [-] epoch  242/250, train loss 0.458786 in 0.06s\n",
      " [-] epoch  243/250, train loss 0.457489 in 0.06s\n",
      " [-] epoch  244/250, train loss 0.460950 in 0.07s\n",
      " [-] epoch  245/250, train loss 0.487538 in 0.07s\n",
      " [-] epoch  246/250, train loss 0.470728 in 0.06s\n",
      " [-] epoch  247/250, train loss 0.446211 in 0.06s\n",
      " [-] epoch  248/250, train loss 0.431529 in 0.07s\n",
      " [-] epoch  249/250, train loss 0.463077 in 0.07s\n",
      " [-] epoch  250/250, train loss 0.461071 in 0.07s\n",
      " [-] test acc. 60.277778%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.666993 in 0.09s\n",
      " [-] epoch    2/250, train loss 0.634877 in 0.09s\n",
      " [-] epoch    3/250, train loss 0.615360 in 0.09s\n",
      " [-] epoch    4/250, train loss 0.579957 in 0.10s\n",
      " [-] epoch    5/250, train loss 0.581414 in 0.11s\n",
      " [-] epoch    6/250, train loss 0.576385 in 0.13s\n",
      " [-] epoch    7/250, train loss 0.549974 in 0.13s\n",
      " [-] epoch    8/250, train loss 0.571217 in 0.13s\n",
      " [-] epoch    9/250, train loss 0.576624 in 0.15s\n",
      " [-] epoch   10/250, train loss 0.551067 in 0.15s\n",
      " [-] epoch   11/250, train loss 0.548355 in 0.11s\n",
      " [-] epoch   12/250, train loss 0.547982 in 0.10s\n",
      " [-] epoch   13/250, train loss 0.558730 in 0.10s\n",
      " [-] epoch   14/250, train loss 0.561927 in 0.14s\n",
      " [-] epoch   15/250, train loss 0.570373 in 0.11s\n",
      " [-] epoch   16/250, train loss 0.532982 in 0.10s\n",
      " [-] epoch   17/250, train loss 0.545586 in 0.10s\n",
      " [-] epoch   18/250, train loss 0.535936 in 0.13s\n",
      " [-] epoch   19/250, train loss 0.543198 in 0.13s\n",
      " [-] epoch   20/250, train loss 0.530202 in 0.10s\n",
      " [-] epoch   21/250, train loss 0.540452 in 0.10s\n",
      " [-] epoch   22/250, train loss 0.541133 in 0.13s\n",
      " [-] epoch   23/250, train loss 0.538589 in 0.10s\n",
      " [-] epoch   24/250, train loss 0.513943 in 0.10s\n",
      " [-] epoch   25/250, train loss 0.520464 in 0.12s\n",
      " [-] epoch   26/250, train loss 0.518001 in 0.12s\n",
      " [-] epoch   27/250, train loss 0.523471 in 0.11s\n",
      " [-] epoch   28/250, train loss 0.497127 in 0.14s\n",
      " [-] epoch   29/250, train loss 0.533226 in 0.14s\n",
      " [-] epoch   30/250, train loss 0.513041 in 0.13s\n",
      " [-] epoch   31/250, train loss 0.499435 in 0.14s\n",
      " [-] epoch   32/250, train loss 0.502209 in 0.11s\n",
      " [-] epoch   33/250, train loss 0.497427 in 0.11s\n",
      " [-] epoch   34/250, train loss 0.529875 in 0.11s\n",
      " [-] epoch   35/250, train loss 0.512964 in 0.12s\n",
      " [-] epoch   36/250, train loss 0.490799 in 0.12s\n",
      " [-] epoch   37/250, train loss 0.526083 in 0.10s\n",
      " [-] epoch   38/250, train loss 0.493280 in 0.11s\n",
      " [-] epoch   39/250, train loss 0.482426 in 0.13s\n",
      " [-] epoch   40/250, train loss 0.478243 in 0.10s\n",
      " [-] epoch   41/250, train loss 0.484814 in 0.15s\n",
      " [-] epoch   42/250, train loss 0.509817 in 0.14s\n",
      " [-] epoch   43/250, train loss 0.505893 in 0.14s\n",
      " [-] epoch   44/250, train loss 0.495203 in 0.12s\n",
      " [-] epoch   45/250, train loss 0.486191 in 0.12s\n",
      " [-] epoch   46/250, train loss 0.492657 in 0.12s\n",
      " [-] epoch   47/250, train loss 0.486183 in 0.13s\n",
      " [-] epoch   48/250, train loss 0.516257 in 0.14s\n",
      " [-] epoch   49/250, train loss 0.475972 in 0.09s\n",
      " [-] epoch   50/250, train loss 0.483449 in 0.11s\n",
      " [-] epoch   51/250, train loss 0.479630 in 0.13s\n",
      " [-] epoch   52/250, train loss 0.476594 in 0.10s\n",
      " [-] epoch   53/250, train loss 0.490845 in 0.10s\n",
      " [-] epoch   54/250, train loss 0.460017 in 0.09s\n",
      " [-] epoch   55/250, train loss 0.485047 in 0.12s\n",
      " [-] epoch   56/250, train loss 0.511916 in 0.11s\n",
      " [-] epoch   57/250, train loss 0.479612 in 0.10s\n",
      " [-] epoch   58/250, train loss 0.456284 in 0.11s\n",
      " [-] epoch   59/250, train loss 0.482855 in 0.11s\n",
      " [-] epoch   60/250, train loss 0.473812 in 0.12s\n",
      " [-] epoch   61/250, train loss 0.500572 in 0.11s\n",
      " [-] epoch   62/250, train loss 0.486068 in 0.11s\n",
      " [-] epoch   63/250, train loss 0.476927 in 0.12s\n",
      " [-] epoch   64/250, train loss 0.481668 in 0.13s\n",
      " [-] epoch   65/250, train loss 0.467716 in 0.13s\n",
      " [-] epoch   66/250, train loss 0.425974 in 0.13s\n",
      " [-] epoch   67/250, train loss 0.466679 in 0.12s\n",
      " [-] epoch   68/250, train loss 0.446863 in 0.12s\n",
      " [-] epoch   69/250, train loss 0.447638 in 0.11s\n",
      " [-] epoch   70/250, train loss 0.469334 in 0.11s\n",
      " [-] epoch   71/250, train loss 0.448613 in 0.13s\n",
      " [-] epoch   72/250, train loss 0.483720 in 0.13s\n",
      " [-] epoch   73/250, train loss 0.459008 in 0.13s\n",
      " [-] epoch   74/250, train loss 0.439183 in 0.13s\n",
      " [-] epoch   75/250, train loss 0.436815 in 0.12s\n",
      " [-] epoch   76/250, train loss 0.461829 in 0.14s\n",
      " [-] epoch   77/250, train loss 0.446904 in 0.11s\n",
      " [-] epoch   78/250, train loss 0.451717 in 0.11s\n",
      " [-] epoch   79/250, train loss 0.470597 in 0.12s\n",
      " [-] epoch   80/250, train loss 0.454635 in 0.09s\n",
      " [-] epoch   81/250, train loss 0.457752 in 0.10s\n",
      " [-] epoch   82/250, train loss 0.454371 in 0.11s\n",
      " [-] epoch   83/250, train loss 0.454577 in 0.11s\n",
      " [-] epoch   84/250, train loss 0.441649 in 0.10s\n",
      " [-] epoch   85/250, train loss 0.472504 in 0.13s\n",
      " [-] epoch   86/250, train loss 0.461721 in 0.11s\n",
      " [-] epoch   87/250, train loss 0.436671 in 0.12s\n",
      " [-] epoch   88/250, train loss 0.446340 in 0.12s\n",
      " [-] epoch   89/250, train loss 0.407984 in 0.13s\n",
      " [-] epoch   90/250, train loss 0.454637 in 0.13s\n",
      " [-] epoch   91/250, train loss 0.452222 in 0.12s\n",
      " [-] epoch   92/250, train loss 0.455216 in 0.13s\n",
      " [-] epoch   93/250, train loss 0.471940 in 0.14s\n",
      " [-] epoch   94/250, train loss 0.442708 in 0.12s\n",
      " [-] epoch   95/250, train loss 0.440071 in 0.12s\n",
      " [-] epoch   96/250, train loss 0.445679 in 0.13s\n",
      " [-] epoch   97/250, train loss 0.432467 in 0.10s\n",
      " [-] epoch   98/250, train loss 0.438702 in 0.12s\n",
      " [-] epoch   99/250, train loss 0.472515 in 0.15s\n",
      " [-] epoch  100/250, train loss 0.448640 in 0.13s\n",
      " [-] epoch  101/250, train loss 0.456725 in 0.12s\n",
      " [-] epoch  102/250, train loss 0.428383 in 0.10s\n",
      " [-] epoch  103/250, train loss 0.424209 in 0.11s\n",
      " [-] epoch  104/250, train loss 0.402182 in 0.12s\n",
      " [-] epoch  105/250, train loss 0.413097 in 0.10s\n",
      " [-] epoch  106/250, train loss 0.420766 in 0.12s\n",
      " [-] epoch  107/250, train loss 0.437633 in 0.13s\n",
      " [-] epoch  108/250, train loss 0.415023 in 0.13s\n",
      " [-] epoch  109/250, train loss 0.402203 in 0.12s\n",
      " [-] epoch  110/250, train loss 0.461888 in 0.12s\n",
      " [-] epoch  111/250, train loss 0.448385 in 0.13s\n",
      " [-] epoch  112/250, train loss 0.451311 in 0.12s\n",
      " [-] epoch  113/250, train loss 0.437916 in 0.10s\n",
      " [-] epoch  114/250, train loss 0.415153 in 0.12s\n",
      " [-] epoch  115/250, train loss 0.454359 in 0.11s\n",
      " [-] epoch  116/250, train loss 0.437736 in 0.12s\n",
      " [-] epoch  117/250, train loss 0.455570 in 0.12s\n",
      " [-] epoch  118/250, train loss 0.447115 in 0.10s\n",
      " [-] epoch  119/250, train loss 0.412265 in 0.10s\n",
      " [-] epoch  120/250, train loss 0.425165 in 0.10s\n",
      " [-] epoch  121/250, train loss 0.447568 in 0.11s\n",
      " [-] epoch  122/250, train loss 0.421352 in 0.12s\n",
      " [-] epoch  123/250, train loss 0.428145 in 0.13s\n",
      " [-] epoch  124/250, train loss 0.416464 in 0.10s\n",
      " [-] epoch  125/250, train loss 0.419465 in 0.11s\n",
      " [-] epoch  126/250, train loss 0.450446 in 0.12s\n",
      " [-] epoch  127/250, train loss 0.434261 in 0.12s\n",
      " [-] epoch  128/250, train loss 0.417912 in 0.11s\n",
      " [-] epoch  129/250, train loss 0.433271 in 0.09s\n",
      " [-] epoch  130/250, train loss 0.407497 in 0.14s\n",
      " [-] epoch  131/250, train loss 0.394556 in 0.11s\n",
      " [-] epoch  132/250, train loss 0.427415 in 0.10s\n",
      " [-] epoch  133/250, train loss 0.378859 in 0.12s\n",
      " [-] epoch  134/250, train loss 0.454760 in 0.13s\n",
      " [-] epoch  135/250, train loss 0.431624 in 0.12s\n",
      " [-] epoch  136/250, train loss 0.412845 in 0.10s\n",
      " [-] epoch  137/250, train loss 0.421421 in 0.12s\n",
      " [-] epoch  138/250, train loss 0.438614 in 0.11s\n",
      " [-] epoch  139/250, train loss 0.437953 in 0.11s\n",
      " [-] epoch  140/250, train loss 0.444241 in 0.09s\n",
      " [-] epoch  141/250, train loss 0.410982 in 0.10s\n",
      " [-] epoch  142/250, train loss 0.438659 in 0.11s\n",
      " [-] epoch  143/250, train loss 0.406924 in 0.11s\n",
      " [-] epoch  144/250, train loss 0.442907 in 0.11s\n",
      " [-] epoch  145/250, train loss 0.440413 in 0.09s\n",
      " [-] epoch  146/250, train loss 0.412897 in 0.12s\n",
      " [-] epoch  147/250, train loss 0.412142 in 0.10s\n",
      " [-] epoch  148/250, train loss 0.421708 in 0.10s\n",
      " [-] epoch  149/250, train loss 0.421001 in 0.10s\n",
      " [-] epoch  150/250, train loss 0.420453 in 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.407647 in 0.11s\n",
      " [-] epoch  152/250, train loss 0.420191 in 0.10s\n",
      " [-] epoch  153/250, train loss 0.429501 in 0.10s\n",
      " [-] epoch  154/250, train loss 0.400269 in 0.11s\n",
      " [-] epoch  155/250, train loss 0.431194 in 0.15s\n",
      " [-] epoch  156/250, train loss 0.424530 in 0.15s\n",
      " [-] epoch  157/250, train loss 0.414018 in 0.13s\n",
      " [-] epoch  158/250, train loss 0.398419 in 0.15s\n",
      " [-] epoch  159/250, train loss 0.429969 in 0.13s\n",
      " [-] epoch  160/250, train loss 0.418781 in 0.15s\n",
      " [-] epoch  161/250, train loss 0.422375 in 0.14s\n",
      " [-] epoch  162/250, train loss 0.418943 in 0.13s\n",
      " [-] epoch  163/250, train loss 0.418474 in 0.15s\n",
      " [-] epoch  164/250, train loss 0.409737 in 0.13s\n",
      " [-] epoch  165/250, train loss 0.416724 in 0.14s\n",
      " [-] epoch  166/250, train loss 0.426375 in 0.14s\n",
      " [-] epoch  167/250, train loss 0.413167 in 0.13s\n",
      " [-] epoch  168/250, train loss 0.413011 in 0.10s\n",
      " [-] epoch  169/250, train loss 0.416265 in 0.11s\n",
      " [-] epoch  170/250, train loss 0.402829 in 0.10s\n",
      " [-] epoch  171/250, train loss 0.407948 in 0.10s\n",
      " [-] epoch  172/250, train loss 0.410210 in 0.10s\n",
      " [-] epoch  173/250, train loss 0.409756 in 0.12s\n",
      " [-] epoch  174/250, train loss 0.394634 in 0.12s\n",
      " [-] epoch  175/250, train loss 0.422160 in 0.15s\n",
      " [-] epoch  176/250, train loss 0.429194 in 0.15s\n",
      " [-] epoch  177/250, train loss 0.401145 in 0.14s\n",
      " [-] epoch  178/250, train loss 0.419951 in 0.14s\n",
      " [-] epoch  179/250, train loss 0.422957 in 0.15s\n",
      " [-] epoch  180/250, train loss 0.403158 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.443104 in 0.15s\n",
      " [-] epoch  182/250, train loss 0.396601 in 0.14s\n",
      " [-] epoch  183/250, train loss 0.423423 in 0.11s\n",
      " [-] epoch  184/250, train loss 0.397833 in 0.11s\n",
      " [-] epoch  185/250, train loss 0.434207 in 0.12s\n",
      " [-] epoch  186/250, train loss 0.440032 in 0.10s\n",
      " [-] epoch  187/250, train loss 0.413133 in 0.10s\n",
      " [-] epoch  188/250, train loss 0.400054 in 0.12s\n",
      " [-] epoch  189/250, train loss 0.382895 in 0.14s\n",
      " [-] epoch  190/250, train loss 0.423273 in 0.16s\n",
      " [-] epoch  191/250, train loss 0.422267 in 0.10s\n",
      " [-] epoch  192/250, train loss 0.425332 in 0.10s\n",
      " [-] epoch  193/250, train loss 0.390983 in 0.09s\n",
      " [-] epoch  194/250, train loss 0.397657 in 0.11s\n",
      " [-] epoch  195/250, train loss 0.402535 in 0.15s\n",
      " [-] epoch  196/250, train loss 0.407511 in 0.10s\n",
      " [-] epoch  197/250, train loss 0.426044 in 0.11s\n",
      " [-] epoch  198/250, train loss 0.392839 in 0.11s\n",
      " [-] epoch  199/250, train loss 0.392947 in 0.12s\n",
      " [-] epoch  200/250, train loss 0.406110 in 0.13s\n",
      " [-] epoch  201/250, train loss 0.416439 in 0.11s\n",
      " [-] epoch  202/250, train loss 0.416766 in 0.11s\n",
      " [-] epoch  203/250, train loss 0.428664 in 0.11s\n",
      " [-] epoch  204/250, train loss 0.397125 in 0.11s\n",
      " [-] epoch  205/250, train loss 0.407078 in 0.12s\n",
      " [-] epoch  206/250, train loss 0.383825 in 0.10s\n",
      " [-] epoch  207/250, train loss 0.392563 in 0.10s\n",
      " [-] epoch  208/250, train loss 0.425564 in 0.11s\n",
      " [-] epoch  209/250, train loss 0.423403 in 0.10s\n",
      " [-] epoch  210/250, train loss 0.406659 in 0.10s\n",
      " [-] epoch  211/250, train loss 0.458171 in 0.11s\n",
      " [-] epoch  212/250, train loss 0.410776 in 0.11s\n",
      " [-] epoch  213/250, train loss 0.415118 in 0.10s\n",
      " [-] epoch  214/250, train loss 0.364773 in 0.11s\n",
      " [-] epoch  215/250, train loss 0.406314 in 0.14s\n",
      " [-] epoch  216/250, train loss 0.401258 in 0.11s\n",
      " [-] epoch  217/250, train loss 0.387028 in 0.10s\n",
      " [-] epoch  218/250, train loss 0.400142 in 0.11s\n",
      " [-] epoch  219/250, train loss 0.392061 in 0.11s\n",
      " [-] epoch  220/250, train loss 0.415324 in 0.10s\n",
      " [-] epoch  221/250, train loss 0.379676 in 0.09s\n",
      " [-] epoch  222/250, train loss 0.398113 in 0.11s\n",
      " [-] epoch  223/250, train loss 0.427578 in 0.10s\n",
      " [-] epoch  224/250, train loss 0.415149 in 0.11s\n",
      " [-] epoch  225/250, train loss 0.401210 in 0.09s\n",
      " [-] epoch  226/250, train loss 0.410981 in 0.12s\n",
      " [-] epoch  227/250, train loss 0.422375 in 0.15s\n",
      " [-] epoch  228/250, train loss 0.416086 in 0.12s\n",
      " [-] epoch  229/250, train loss 0.402155 in 0.13s\n",
      " [-] epoch  230/250, train loss 0.416899 in 0.12s\n",
      " [-] epoch  231/250, train loss 0.409224 in 0.15s\n",
      " [-] epoch  232/250, train loss 0.398221 in 0.11s\n",
      " [-] epoch  233/250, train loss 0.381361 in 0.11s\n",
      " [-] epoch  234/250, train loss 0.402022 in 0.10s\n",
      " [-] epoch  235/250, train loss 0.409523 in 0.10s\n",
      " [-] epoch  236/250, train loss 0.391160 in 0.11s\n",
      " [-] epoch  237/250, train loss 0.385579 in 0.11s\n",
      " [-] epoch  238/250, train loss 0.406395 in 0.10s\n",
      " [-] epoch  239/250, train loss 0.370432 in 0.10s\n",
      " [-] epoch  240/250, train loss 0.376344 in 0.12s\n",
      " [-] epoch  241/250, train loss 0.401571 in 0.10s\n",
      " [-] epoch  242/250, train loss 0.390107 in 0.11s\n",
      " [-] epoch  243/250, train loss 0.385320 in 0.12s\n",
      " [-] epoch  244/250, train loss 0.376490 in 0.12s\n",
      " [-] epoch  245/250, train loss 0.398087 in 0.11s\n",
      " [-] epoch  246/250, train loss 0.409449 in 0.13s\n",
      " [-] epoch  247/250, train loss 0.400070 in 0.13s\n",
      " [-] epoch  248/250, train loss 0.387933 in 0.13s\n",
      " [-] epoch  249/250, train loss 0.365937 in 0.12s\n",
      " [-] epoch  250/250, train loss 0.366937 in 0.12s\n",
      " [-] test acc. 58.055556%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.683913 in 0.14s\n",
      " [-] epoch    2/250, train loss 0.623344 in 0.17s\n",
      " [-] epoch    3/250, train loss 0.595268 in 0.15s\n",
      " [-] epoch    4/250, train loss 0.588527 in 0.14s\n",
      " [-] epoch    5/250, train loss 0.575431 in 0.15s\n",
      " [-] epoch    6/250, train loss 0.565498 in 0.13s\n",
      " [-] epoch    7/250, train loss 0.567926 in 0.18s\n",
      " [-] epoch    8/250, train loss 0.547103 in 0.14s\n",
      " [-] epoch    9/250, train loss 0.533699 in 0.15s\n",
      " [-] epoch   10/250, train loss 0.550048 in 0.15s\n",
      " [-] epoch   11/250, train loss 0.545705 in 0.14s\n",
      " [-] epoch   12/250, train loss 0.521864 in 0.14s\n",
      " [-] epoch   13/250, train loss 0.538021 in 0.17s\n",
      " [-] epoch   14/250, train loss 0.528039 in 0.15s\n",
      " [-] epoch   15/250, train loss 0.528373 in 0.16s\n",
      " [-] epoch   16/250, train loss 0.509313 in 0.16s\n",
      " [-] epoch   17/250, train loss 0.526583 in 0.16s\n",
      " [-] epoch   18/250, train loss 0.512167 in 0.13s\n",
      " [-] epoch   19/250, train loss 0.520292 in 0.13s\n",
      " [-] epoch   20/250, train loss 0.495178 in 0.16s\n",
      " [-] epoch   21/250, train loss 0.517833 in 0.14s\n",
      " [-] epoch   22/250, train loss 0.508095 in 0.14s\n",
      " [-] epoch   23/250, train loss 0.489623 in 0.15s\n",
      " [-] epoch   24/250, train loss 0.514114 in 0.16s\n",
      " [-] epoch   25/250, train loss 0.532732 in 0.15s\n",
      " [-] epoch   26/250, train loss 0.514556 in 0.16s\n",
      " [-] epoch   27/250, train loss 0.472098 in 0.17s\n",
      " [-] epoch   28/250, train loss 0.493333 in 0.13s\n",
      " [-] epoch   29/250, train loss 0.496180 in 0.17s\n",
      " [-] epoch   30/250, train loss 0.508449 in 0.14s\n",
      " [-] epoch   31/250, train loss 0.483439 in 0.13s\n",
      " [-] epoch   32/250, train loss 0.477892 in 0.16s\n",
      " [-] epoch   33/250, train loss 0.510700 in 0.17s\n",
      " [-] epoch   34/250, train loss 0.495326 in 0.16s\n",
      " [-] epoch   35/250, train loss 0.495578 in 0.17s\n",
      " [-] epoch   36/250, train loss 0.483701 in 0.17s\n",
      " [-] epoch   37/250, train loss 0.509955 in 0.17s\n",
      " [-] epoch   38/250, train loss 0.469336 in 0.19s\n",
      " [-] epoch   39/250, train loss 0.484140 in 0.17s\n",
      " [-] epoch   40/250, train loss 0.449606 in 0.15s\n",
      " [-] epoch   41/250, train loss 0.441846 in 0.13s\n",
      " [-] epoch   42/250, train loss 0.481678 in 0.16s\n",
      " [-] epoch   43/250, train loss 0.478667 in 0.15s\n",
      " [-] epoch   44/250, train loss 0.491804 in 0.12s\n",
      " [-] epoch   45/250, train loss 0.463501 in 0.14s\n",
      " [-] epoch   46/250, train loss 0.461239 in 0.16s\n",
      " [-] epoch   47/250, train loss 0.463840 in 0.16s\n",
      " [-] epoch   48/250, train loss 0.466584 in 0.16s\n",
      " [-] epoch   49/250, train loss 0.470398 in 0.13s\n",
      " [-] epoch   50/250, train loss 0.434874 in 0.12s\n",
      " [-] epoch   51/250, train loss 0.497756 in 0.14s\n",
      " [-] epoch   52/250, train loss 0.472694 in 0.17s\n",
      " [-] epoch   53/250, train loss 0.438067 in 0.16s\n",
      " [-] epoch   54/250, train loss 0.422390 in 0.14s\n",
      " [-] epoch   55/250, train loss 0.438430 in 0.12s\n",
      " [-] epoch   56/250, train loss 0.425526 in 0.12s\n",
      " [-] epoch   57/250, train loss 0.447689 in 0.14s\n",
      " [-] epoch   58/250, train loss 0.434253 in 0.18s\n",
      " [-] epoch   59/250, train loss 0.471930 in 0.17s\n",
      " [-] epoch   60/250, train loss 0.468658 in 0.14s\n",
      " [-] epoch   61/250, train loss 0.467970 in 0.12s\n",
      " [-] epoch   62/250, train loss 0.459771 in 0.13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   63/250, train loss 0.467840 in 0.17s\n",
      " [-] epoch   64/250, train loss 0.459045 in 0.17s\n",
      " [-] epoch   65/250, train loss 0.429501 in 0.20s\n",
      " [-] epoch   66/250, train loss 0.450277 in 0.13s\n",
      " [-] epoch   67/250, train loss 0.432724 in 0.15s\n",
      " [-] epoch   68/250, train loss 0.440041 in 0.19s\n",
      " [-] epoch   69/250, train loss 0.444753 in 0.21s\n",
      " [-] epoch   70/250, train loss 0.471999 in 0.18s\n",
      " [-] epoch   71/250, train loss 0.415073 in 0.17s\n",
      " [-] epoch   72/250, train loss 0.437519 in 0.18s\n",
      " [-] epoch   73/250, train loss 0.449583 in 0.20s\n",
      " [-] epoch   74/250, train loss 0.453583 in 0.14s\n",
      " [-] epoch   75/250, train loss 0.443709 in 0.12s\n",
      " [-] epoch   76/250, train loss 0.457237 in 0.15s\n",
      " [-] epoch   77/250, train loss 0.443818 in 0.21s\n",
      " [-] epoch   78/250, train loss 0.434927 in 0.19s\n",
      " [-] epoch   79/250, train loss 0.420145 in 0.19s\n",
      " [-] epoch   80/250, train loss 0.438665 in 0.18s\n",
      " [-] epoch   81/250, train loss 0.413711 in 0.21s\n",
      " [-] epoch   82/250, train loss 0.436282 in 0.20s\n",
      " [-] epoch   83/250, train loss 0.418168 in 0.17s\n",
      " [-] epoch   84/250, train loss 0.430107 in 0.19s\n",
      " [-] epoch   85/250, train loss 0.420159 in 0.20s\n",
      " [-] epoch   86/250, train loss 0.417924 in 0.17s\n",
      " [-] epoch   87/250, train loss 0.437296 in 0.19s\n",
      " [-] epoch   88/250, train loss 0.427034 in 0.21s\n",
      " [-] epoch   89/250, train loss 0.429707 in 0.18s\n",
      " [-] epoch   90/250, train loss 0.413624 in 0.20s\n",
      " [-] epoch   91/250, train loss 0.408739 in 0.20s\n",
      " [-] epoch   92/250, train loss 0.405920 in 0.14s\n",
      " [-] epoch   93/250, train loss 0.422920 in 0.15s\n",
      " [-] epoch   94/250, train loss 0.410014 in 0.14s\n",
      " [-] epoch   95/250, train loss 0.411648 in 0.17s\n",
      " [-] epoch   96/250, train loss 0.409576 in 0.15s\n",
      " [-] epoch   97/250, train loss 0.392048 in 0.16s\n",
      " [-] epoch   98/250, train loss 0.387697 in 0.18s\n",
      " [-] epoch   99/250, train loss 0.413568 in 0.16s\n",
      " [-] epoch  100/250, train loss 0.408411 in 0.15s\n",
      " [-] epoch  101/250, train loss 0.414096 in 0.15s\n",
      " [-] epoch  102/250, train loss 0.411009 in 0.14s\n",
      " [-] epoch  103/250, train loss 0.449214 in 0.14s\n",
      " [-] epoch  104/250, train loss 0.408867 in 0.15s\n",
      " [-] epoch  105/250, train loss 0.429901 in 0.17s\n",
      " [-] epoch  106/250, train loss 0.412998 in 0.18s\n",
      " [-] epoch  107/250, train loss 0.427124 in 0.19s\n",
      " [-] epoch  108/250, train loss 0.417509 in 0.16s\n",
      " [-] epoch  109/250, train loss 0.409855 in 0.16s\n",
      " [-] epoch  110/250, train loss 0.413014 in 0.17s\n",
      " [-] epoch  111/250, train loss 0.397864 in 0.17s\n",
      " [-] epoch  112/250, train loss 0.427689 in 0.17s\n",
      " [-] epoch  113/250, train loss 0.440237 in 0.15s\n",
      " [-] epoch  114/250, train loss 0.425991 in 0.16s\n",
      " [-] epoch  115/250, train loss 0.401631 in 0.16s\n",
      " [-] epoch  116/250, train loss 0.406669 in 0.16s\n",
      " [-] epoch  117/250, train loss 0.390530 in 0.17s\n",
      " [-] epoch  118/250, train loss 0.390881 in 0.15s\n",
      " [-] epoch  119/250, train loss 0.406811 in 0.17s\n",
      " [-] epoch  120/250, train loss 0.423643 in 0.17s\n",
      " [-] epoch  121/250, train loss 0.436585 in 0.14s\n",
      " [-] epoch  122/250, train loss 0.413225 in 0.17s\n",
      " [-] epoch  123/250, train loss 0.404681 in 0.17s\n",
      " [-] epoch  124/250, train loss 0.408635 in 0.17s\n",
      " [-] epoch  125/250, train loss 0.427405 in 0.15s\n",
      " [-] epoch  126/250, train loss 0.354828 in 0.17s\n",
      " [-] epoch  127/250, train loss 0.427983 in 0.17s\n",
      " [-] epoch  128/250, train loss 0.445729 in 0.18s\n",
      " [-] epoch  129/250, train loss 0.419702 in 0.17s\n",
      " [-] epoch  130/250, train loss 0.403561 in 0.15s\n",
      " [-] epoch  131/250, train loss 0.401645 in 0.16s\n",
      " [-] epoch  132/250, train loss 0.406124 in 0.15s\n",
      " [-] epoch  133/250, train loss 0.392815 in 0.17s\n",
      " [-] epoch  134/250, train loss 0.398969 in 0.16s\n",
      " [-] epoch  135/250, train loss 0.409162 in 0.16s\n",
      " [-] epoch  136/250, train loss 0.416426 in 0.17s\n",
      " [-] epoch  137/250, train loss 0.377292 in 0.15s\n",
      " [-] epoch  138/250, train loss 0.400586 in 0.15s\n",
      " [-] epoch  139/250, train loss 0.418170 in 0.14s\n",
      " [-] epoch  140/250, train loss 0.397397 in 0.15s\n",
      " [-] epoch  141/250, train loss 0.399336 in 0.16s\n",
      " [-] epoch  142/250, train loss 0.385047 in 0.14s\n",
      " [-] epoch  143/250, train loss 0.403522 in 0.16s\n",
      " [-] epoch  144/250, train loss 0.384594 in 0.16s\n",
      " [-] epoch  145/250, train loss 0.413496 in 0.16s\n",
      " [-] epoch  146/250, train loss 0.396762 in 0.18s\n",
      " [-] epoch  147/250, train loss 0.422013 in 0.15s\n",
      " [-] epoch  148/250, train loss 0.427299 in 0.16s\n",
      " [-] epoch  149/250, train loss 0.414235 in 0.15s\n",
      " [-] epoch  150/250, train loss 0.411457 in 0.15s\n",
      " [-] epoch  151/250, train loss 0.410567 in 0.15s\n",
      " [-] epoch  152/250, train loss 0.361873 in 0.17s\n",
      " [-] epoch  153/250, train loss 0.389827 in 0.13s\n",
      " [-] epoch  154/250, train loss 0.400698 in 0.12s\n",
      " [-] epoch  155/250, train loss 0.411601 in 0.15s\n",
      " [-] epoch  156/250, train loss 0.382574 in 0.13s\n",
      " [-] epoch  157/250, train loss 0.394786 in 0.14s\n",
      " [-] epoch  158/250, train loss 0.401489 in 0.16s\n",
      " [-] epoch  159/250, train loss 0.383820 in 0.16s\n",
      " [-] epoch  160/250, train loss 0.383875 in 0.17s\n",
      " [-] epoch  161/250, train loss 0.408632 in 0.16s\n",
      " [-] epoch  162/250, train loss 0.363586 in 0.21s\n",
      " [-] epoch  163/250, train loss 0.383417 in 0.19s\n",
      " [-] epoch  164/250, train loss 0.407919 in 0.20s\n",
      " [-] epoch  165/250, train loss 0.391280 in 0.16s\n",
      " [-] epoch  166/250, train loss 0.402167 in 0.20s\n",
      " [-] epoch  167/250, train loss 0.371733 in 0.20s\n",
      " [-] epoch  168/250, train loss 0.391466 in 0.16s\n",
      " [-] epoch  169/250, train loss 0.420991 in 0.17s\n",
      " [-] epoch  170/250, train loss 0.408453 in 0.13s\n",
      " [-] epoch  171/250, train loss 0.381532 in 0.16s\n",
      " [-] epoch  172/250, train loss 0.425740 in 0.16s\n",
      " [-] epoch  173/250, train loss 0.406503 in 0.17s\n",
      " [-] epoch  174/250, train loss 0.415126 in 0.15s\n",
      " [-] epoch  175/250, train loss 0.382789 in 0.17s\n",
      " [-] epoch  176/250, train loss 0.387891 in 0.16s\n",
      " [-] epoch  177/250, train loss 0.362069 in 0.18s\n",
      " [-] epoch  178/250, train loss 0.374065 in 0.18s\n",
      " [-] epoch  179/250, train loss 0.375572 in 0.18s\n",
      " [-] epoch  180/250, train loss 0.379442 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.415302 in 0.13s\n",
      " [-] epoch  182/250, train loss 0.350519 in 0.16s\n",
      " [-] epoch  183/250, train loss 0.399850 in 0.16s\n",
      " [-] epoch  184/250, train loss 0.373239 in 0.16s\n",
      " [-] epoch  185/250, train loss 0.359819 in 0.14s\n",
      " [-] epoch  186/250, train loss 0.359754 in 0.16s\n",
      " [-] epoch  187/250, train loss 0.387435 in 0.15s\n",
      " [-] epoch  188/250, train loss 0.389723 in 0.12s\n",
      " [-] epoch  189/250, train loss 0.380851 in 0.13s\n",
      " [-] epoch  190/250, train loss 0.368180 in 0.13s\n",
      " [-] epoch  191/250, train loss 0.398197 in 0.17s\n",
      " [-] epoch  192/250, train loss 0.380694 in 0.19s\n",
      " [-] epoch  193/250, train loss 0.354138 in 0.16s\n",
      " [-] epoch  194/250, train loss 0.396870 in 0.16s\n",
      " [-] epoch  195/250, train loss 0.356264 in 0.15s\n",
      " [-] epoch  196/250, train loss 0.361978 in 0.19s\n",
      " [-] epoch  197/250, train loss 0.377259 in 0.16s\n",
      " [-] epoch  198/250, train loss 0.349853 in 0.16s\n",
      " [-] epoch  199/250, train loss 0.384281 in 0.17s\n",
      " [-] epoch  200/250, train loss 0.357102 in 0.17s\n",
      " [-] epoch  201/250, train loss 0.392118 in 0.16s\n",
      " [-] epoch  202/250, train loss 0.388687 in 0.15s\n",
      " [-] epoch  203/250, train loss 0.395612 in 0.19s\n",
      " [-] epoch  204/250, train loss 0.371143 in 0.17s\n",
      " [-] epoch  205/250, train loss 0.376985 in 0.17s\n",
      " [-] epoch  206/250, train loss 0.374638 in 0.17s\n",
      " [-] epoch  207/250, train loss 0.361065 in 0.15s\n",
      " [-] epoch  208/250, train loss 0.357852 in 0.20s\n",
      " [-] epoch  209/250, train loss 0.390767 in 0.17s\n",
      " [-] epoch  210/250, train loss 0.412880 in 0.16s\n",
      " [-] epoch  211/250, train loss 0.369427 in 0.16s\n",
      " [-] epoch  212/250, train loss 0.366297 in 0.16s\n",
      " [-] epoch  213/250, train loss 0.420930 in 0.15s\n",
      " [-] epoch  214/250, train loss 0.382663 in 0.15s\n",
      " [-] epoch  215/250, train loss 0.376967 in 0.18s\n",
      " [-] epoch  216/250, train loss 0.366564 in 0.19s\n",
      " [-] epoch  217/250, train loss 0.357943 in 0.14s\n",
      " [-] epoch  218/250, train loss 0.385292 in 0.12s\n",
      " [-] epoch  219/250, train loss 0.390158 in 0.17s\n",
      " [-] epoch  220/250, train loss 0.369784 in 0.18s\n",
      " [-] epoch  221/250, train loss 0.349451 in 0.13s\n",
      " [-] epoch  222/250, train loss 0.383963 in 0.14s\n",
      " [-] epoch  223/250, train loss 0.392111 in 0.14s\n",
      " [-] epoch  224/250, train loss 0.381455 in 0.14s\n",
      " [-] epoch  225/250, train loss 0.367359 in 0.17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.365634 in 0.15s\n",
      " [-] epoch  227/250, train loss 0.355450 in 0.14s\n",
      " [-] epoch  228/250, train loss 0.352913 in 0.18s\n",
      " [-] epoch  229/250, train loss 0.425764 in 0.16s\n",
      " [-] epoch  230/250, train loss 0.396907 in 0.16s\n",
      " [-] epoch  231/250, train loss 0.388374 in 0.17s\n",
      " [-] epoch  232/250, train loss 0.368411 in 0.16s\n",
      " [-] epoch  233/250, train loss 0.363625 in 0.19s\n",
      " [-] epoch  234/250, train loss 0.361337 in 0.17s\n",
      " [-] epoch  235/250, train loss 0.360227 in 0.18s\n",
      " [-] epoch  236/250, train loss 0.397721 in 0.20s\n",
      " [-] epoch  237/250, train loss 0.360581 in 0.18s\n",
      " [-] epoch  238/250, train loss 0.376360 in 0.17s\n",
      " [-] epoch  239/250, train loss 0.418550 in 0.19s\n",
      " [-] epoch  240/250, train loss 0.432566 in 0.17s\n",
      " [-] epoch  241/250, train loss 0.391511 in 0.17s\n",
      " [-] epoch  242/250, train loss 0.352999 in 0.16s\n",
      " [-] epoch  243/250, train loss 0.357486 in 0.14s\n",
      " [-] epoch  244/250, train loss 0.357659 in 0.15s\n",
      " [-] epoch  245/250, train loss 0.384603 in 0.15s\n",
      " [-] epoch  246/250, train loss 0.376049 in 0.19s\n",
      " [-] epoch  247/250, train loss 0.371938 in 0.20s\n",
      " [-] epoch  248/250, train loss 0.369380 in 0.19s\n",
      " [-] epoch  249/250, train loss 0.359138 in 0.15s\n",
      " [-] epoch  250/250, train loss 0.371865 in 0.14s\n",
      " [-] test acc. 68.055556%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.640505 in 0.15s\n",
      " [-] epoch    2/250, train loss 0.602178 in 0.18s\n",
      " [-] epoch    3/250, train loss 0.569622 in 0.17s\n",
      " [-] epoch    4/250, train loss 0.584025 in 0.15s\n",
      " [-] epoch    5/250, train loss 0.578740 in 0.16s\n",
      " [-] epoch    6/250, train loss 0.565957 in 0.16s\n",
      " [-] epoch    7/250, train loss 0.563434 in 0.16s\n",
      " [-] epoch    8/250, train loss 0.542206 in 0.16s\n",
      " [-] epoch    9/250, train loss 0.518728 in 0.19s\n",
      " [-] epoch   10/250, train loss 0.553899 in 0.17s\n",
      " [-] epoch   11/250, train loss 0.527883 in 0.17s\n",
      " [-] epoch   12/250, train loss 0.539528 in 0.21s\n",
      " [-] epoch   13/250, train loss 0.518020 in 0.18s\n",
      " [-] epoch   14/250, train loss 0.546300 in 0.16s\n",
      " [-] epoch   15/250, train loss 0.537723 in 0.17s\n",
      " [-] epoch   16/250, train loss 0.508429 in 0.23s\n",
      " [-] epoch   17/250, train loss 0.526030 in 0.20s\n",
      " [-] epoch   18/250, train loss 0.493851 in 0.16s\n",
      " [-] epoch   19/250, train loss 0.517970 in 0.14s\n",
      " [-] epoch   20/250, train loss 0.508516 in 0.15s\n",
      " [-] epoch   21/250, train loss 0.500833 in 0.18s\n",
      " [-] epoch   22/250, train loss 0.494118 in 0.15s\n",
      " [-] epoch   23/250, train loss 0.489048 in 0.15s\n",
      " [-] epoch   24/250, train loss 0.497677 in 0.15s\n",
      " [-] epoch   25/250, train loss 0.504248 in 0.15s\n",
      " [-] epoch   26/250, train loss 0.481648 in 0.19s\n",
      " [-] epoch   27/250, train loss 0.481790 in 0.15s\n",
      " [-] epoch   28/250, train loss 0.502364 in 0.22s\n",
      " [-] epoch   29/250, train loss 0.492507 in 0.19s\n",
      " [-] epoch   30/250, train loss 0.484159 in 0.16s\n",
      " [-] epoch   31/250, train loss 0.523547 in 0.18s\n",
      " [-] epoch   32/250, train loss 0.475213 in 0.17s\n",
      " [-] epoch   33/250, train loss 0.484701 in 0.19s\n",
      " [-] epoch   34/250, train loss 0.499591 in 0.21s\n",
      " [-] epoch   35/250, train loss 0.491700 in 0.17s\n",
      " [-] epoch   36/250, train loss 0.451031 in 0.15s\n",
      " [-] epoch   37/250, train loss 0.468304 in 0.16s\n",
      " [-] epoch   38/250, train loss 0.498299 in 0.16s\n",
      " [-] epoch   39/250, train loss 0.466298 in 0.15s\n",
      " [-] epoch   40/250, train loss 0.506221 in 0.18s\n",
      " [-] epoch   41/250, train loss 0.499045 in 0.15s\n",
      " [-] epoch   42/250, train loss 0.473563 in 0.16s\n",
      " [-] epoch   43/250, train loss 0.433313 in 0.20s\n",
      " [-] epoch   44/250, train loss 0.483644 in 0.18s\n",
      " [-] epoch   45/250, train loss 0.454777 in 0.18s\n",
      " [-] epoch   46/250, train loss 0.484257 in 0.16s\n",
      " [-] epoch   47/250, train loss 0.473456 in 0.14s\n",
      " [-] epoch   48/250, train loss 0.478497 in 0.16s\n",
      " [-] epoch   49/250, train loss 0.450561 in 0.21s\n",
      " [-] epoch   50/250, train loss 0.440398 in 0.15s\n",
      " [-] epoch   51/250, train loss 0.450570 in 0.15s\n",
      " [-] epoch   52/250, train loss 0.466634 in 0.16s\n",
      " [-] epoch   53/250, train loss 0.462997 in 0.19s\n",
      " [-] epoch   54/250, train loss 0.417282 in 0.16s\n",
      " [-] epoch   55/250, train loss 0.435173 in 0.16s\n",
      " [-] epoch   56/250, train loss 0.463563 in 0.18s\n",
      " [-] epoch   57/250, train loss 0.453905 in 0.19s\n",
      " [-] epoch   58/250, train loss 0.427257 in 0.19s\n",
      " [-] epoch   59/250, train loss 0.445383 in 0.19s\n",
      " [-] epoch   60/250, train loss 0.426146 in 0.16s\n",
      " [-] epoch   61/250, train loss 0.464692 in 0.16s\n",
      " [-] epoch   62/250, train loss 0.453687 in 0.19s\n",
      " [-] epoch   63/250, train loss 0.444852 in 0.20s\n",
      " [-] epoch   64/250, train loss 0.470268 in 0.15s\n",
      " [-] epoch   65/250, train loss 0.433298 in 0.12s\n",
      " [-] epoch   66/250, train loss 0.428115 in 0.16s\n",
      " [-] epoch   67/250, train loss 0.468563 in 0.17s\n",
      " [-] epoch   68/250, train loss 0.424133 in 0.18s\n",
      " [-] epoch   69/250, train loss 0.435094 in 0.15s\n",
      " [-] epoch   70/250, train loss 0.433188 in 0.15s\n",
      " [-] epoch   71/250, train loss 0.429069 in 0.13s\n",
      " [-] epoch   72/250, train loss 0.441868 in 0.13s\n",
      " [-] epoch   73/250, train loss 0.454966 in 0.18s\n",
      " [-] epoch   74/250, train loss 0.457457 in 0.15s\n",
      " [-] epoch   75/250, train loss 0.455435 in 0.14s\n",
      " [-] epoch   76/250, train loss 0.428716 in 0.15s\n",
      " [-] epoch   77/250, train loss 0.427944 in 0.15s\n",
      " [-] epoch   78/250, train loss 0.462079 in 0.18s\n",
      " [-] epoch   79/250, train loss 0.436538 in 0.17s\n",
      " [-] epoch   80/250, train loss 0.419719 in 0.18s\n",
      " [-] epoch   81/250, train loss 0.436794 in 0.13s\n",
      " [-] epoch   82/250, train loss 0.438485 in 0.12s\n",
      " [-] epoch   83/250, train loss 0.418240 in 0.17s\n",
      " [-] epoch   84/250, train loss 0.406228 in 0.14s\n",
      " [-] epoch   85/250, train loss 0.396302 in 0.18s\n",
      " [-] epoch   86/250, train loss 0.383727 in 0.15s\n",
      " [-] epoch   87/250, train loss 0.424507 in 0.15s\n",
      " [-] epoch   88/250, train loss 0.420773 in 0.16s\n",
      " [-] epoch   89/250, train loss 0.407142 in 0.13s\n",
      " [-] epoch   90/250, train loss 0.425210 in 0.19s\n",
      " [-] epoch   91/250, train loss 0.401411 in 0.21s\n",
      " [-] epoch   92/250, train loss 0.426372 in 0.19s\n",
      " [-] epoch   93/250, train loss 0.413428 in 0.12s\n",
      " [-] epoch   94/250, train loss 0.419122 in 0.12s\n",
      " [-] epoch   95/250, train loss 0.410698 in 0.16s\n",
      " [-] epoch   96/250, train loss 0.379642 in 0.20s\n",
      " [-] epoch   97/250, train loss 0.402360 in 0.14s\n",
      " [-] epoch   98/250, train loss 0.423345 in 0.20s\n",
      " [-] epoch   99/250, train loss 0.426895 in 0.20s\n",
      " [-] epoch  100/250, train loss 0.432843 in 0.20s\n",
      " [-] epoch  101/250, train loss 0.385763 in 0.19s\n",
      " [-] epoch  102/250, train loss 0.403746 in 0.20s\n",
      " [-] epoch  103/250, train loss 0.426778 in 0.19s\n",
      " [-] epoch  104/250, train loss 0.426076 in 0.21s\n",
      " [-] epoch  105/250, train loss 0.402250 in 0.17s\n",
      " [-] epoch  106/250, train loss 0.394036 in 0.18s\n",
      " [-] epoch  107/250, train loss 0.392062 in 0.22s\n",
      " [-] epoch  108/250, train loss 0.392835 in 0.20s\n",
      " [-] epoch  109/250, train loss 0.403995 in 0.20s\n",
      " [-] epoch  110/250, train loss 0.417792 in 0.17s\n",
      " [-] epoch  111/250, train loss 0.393817 in 0.21s\n",
      " [-] epoch  112/250, train loss 0.426480 in 0.20s\n",
      " [-] epoch  113/250, train loss 0.413470 in 0.20s\n",
      " [-] epoch  114/250, train loss 0.405806 in 0.18s\n",
      " [-] epoch  115/250, train loss 0.392475 in 0.19s\n",
      " [-] epoch  116/250, train loss 0.417038 in 0.20s\n",
      " [-] epoch  117/250, train loss 0.396093 in 0.20s\n",
      " [-] epoch  118/250, train loss 0.374393 in 0.19s\n",
      " [-] epoch  119/250, train loss 0.397556 in 0.19s\n",
      " [-] epoch  120/250, train loss 0.390849 in 0.19s\n",
      " [-] epoch  121/250, train loss 0.411673 in 0.17s\n",
      " [-] epoch  122/250, train loss 0.429628 in 0.18s\n",
      " [-] epoch  123/250, train loss 0.421872 in 0.17s\n",
      " [-] epoch  124/250, train loss 0.388342 in 0.17s\n",
      " [-] epoch  125/250, train loss 0.391672 in 0.15s\n",
      " [-] epoch  126/250, train loss 0.401093 in 0.20s\n",
      " [-] epoch  127/250, train loss 0.403753 in 0.19s\n",
      " [-] epoch  128/250, train loss 0.454589 in 0.20s\n",
      " [-] epoch  129/250, train loss 0.392389 in 0.20s\n",
      " [-] epoch  130/250, train loss 0.423346 in 0.20s\n",
      " [-] epoch  131/250, train loss 0.363699 in 0.19s\n",
      " [-] epoch  132/250, train loss 0.394326 in 0.18s\n",
      " [-] epoch  133/250, train loss 0.394920 in 0.21s\n",
      " [-] epoch  134/250, train loss 0.365178 in 0.16s\n",
      " [-] epoch  135/250, train loss 0.406426 in 0.17s\n",
      " [-] epoch  136/250, train loss 0.390415 in 0.18s\n",
      " [-] epoch  137/250, train loss 0.398745 in 0.17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  138/250, train loss 0.394818 in 0.18s\n",
      " [-] epoch  139/250, train loss 0.390120 in 0.21s\n",
      " [-] epoch  140/250, train loss 0.391070 in 0.22s\n",
      " [-] epoch  141/250, train loss 0.392149 in 0.20s\n",
      " [-] epoch  142/250, train loss 0.385804 in 0.20s\n",
      " [-] epoch  143/250, train loss 0.393566 in 0.21s\n",
      " [-] epoch  144/250, train loss 0.373374 in 0.21s\n",
      " [-] epoch  145/250, train loss 0.387160 in 0.22s\n",
      " [-] epoch  146/250, train loss 0.360725 in 0.19s\n",
      " [-] epoch  147/250, train loss 0.404219 in 0.18s\n",
      " [-] epoch  148/250, train loss 0.374826 in 0.18s\n",
      " [-] epoch  149/250, train loss 0.403130 in 0.20s\n",
      " [-] epoch  150/250, train loss 0.389652 in 0.21s\n",
      " [-] epoch  151/250, train loss 0.390774 in 0.22s\n",
      " [-] epoch  152/250, train loss 0.416269 in 0.20s\n",
      " [-] epoch  153/250, train loss 0.346315 in 0.23s\n",
      " [-] epoch  154/250, train loss 0.390886 in 0.20s\n",
      " [-] epoch  155/250, train loss 0.367882 in 0.20s\n",
      " [-] epoch  156/250, train loss 0.380248 in 0.21s\n",
      " [-] epoch  157/250, train loss 0.349017 in 0.21s\n",
      " [-] epoch  158/250, train loss 0.417388 in 0.19s\n",
      " [-] epoch  159/250, train loss 0.404261 in 0.19s\n",
      " [-] epoch  160/250, train loss 0.398869 in 0.21s\n",
      " [-] epoch  161/250, train loss 0.380874 in 0.22s\n",
      " [-] epoch  162/250, train loss 0.354470 in 0.22s\n",
      " [-] epoch  163/250, train loss 0.364319 in 0.20s\n",
      " [-] epoch  164/250, train loss 0.419472 in 0.19s\n",
      " [-] epoch  165/250, train loss 0.375121 in 0.14s\n",
      " [-] epoch  166/250, train loss 0.369134 in 0.14s\n",
      " [-] epoch  167/250, train loss 0.368598 in 0.17s\n",
      " [-] epoch  168/250, train loss 0.398831 in 0.20s\n",
      " [-] epoch  169/250, train loss 0.366499 in 0.18s\n",
      " [-] epoch  170/250, train loss 0.376075 in 0.13s\n",
      " [-] epoch  171/250, train loss 0.370199 in 0.15s\n",
      " [-] epoch  172/250, train loss 0.367339 in 0.19s\n",
      " [-] epoch  173/250, train loss 0.365667 in 0.18s\n",
      " [-] epoch  174/250, train loss 0.396929 in 0.13s\n",
      " [-] epoch  175/250, train loss 0.389107 in 0.15s\n",
      " [-] epoch  176/250, train loss 0.362444 in 0.16s\n",
      " [-] epoch  177/250, train loss 0.399474 in 0.20s\n",
      " [-] epoch  178/250, train loss 0.411582 in 0.18s\n",
      " [-] epoch  179/250, train loss 0.386347 in 0.20s\n",
      " [-] epoch  180/250, train loss 0.369895 in 0.19s\n",
      " [-] epoch  181/250, train loss 0.383171 in 0.22s\n",
      " [-] epoch  182/250, train loss 0.344034 in 0.21s\n",
      " [-] epoch  183/250, train loss 0.340710 in 0.16s\n",
      " [-] epoch  184/250, train loss 0.387301 in 0.13s\n",
      " [-] epoch  185/250, train loss 0.379986 in 0.13s\n",
      " [-] epoch  186/250, train loss 0.374674 in 0.15s\n",
      " [-] epoch  187/250, train loss 0.379963 in 0.15s\n",
      " [-] epoch  188/250, train loss 0.383370 in 0.19s\n",
      " [-] epoch  189/250, train loss 0.366259 in 0.19s\n",
      " [-] epoch  190/250, train loss 0.345900 in 0.14s\n",
      " [-] epoch  191/250, train loss 0.383905 in 0.21s\n",
      " [-] epoch  192/250, train loss 0.355088 in 0.15s\n",
      " [-] epoch  193/250, train loss 0.358985 in 0.14s\n",
      " [-] epoch  194/250, train loss 0.386628 in 0.15s\n",
      " [-] epoch  195/250, train loss 0.377366 in 0.14s\n",
      " [-] epoch  196/250, train loss 0.382469 in 0.16s\n",
      " [-] epoch  197/250, train loss 0.355544 in 0.16s\n",
      " [-] epoch  198/250, train loss 0.356062 in 0.16s\n",
      " [-] epoch  199/250, train loss 0.348039 in 0.15s\n",
      " [-] epoch  200/250, train loss 0.351240 in 0.15s\n",
      " [-] epoch  201/250, train loss 0.374579 in 0.16s\n",
      " [-] epoch  202/250, train loss 0.381550 in 0.16s\n",
      " [-] epoch  203/250, train loss 0.365887 in 0.15s\n",
      " [-] epoch  204/250, train loss 0.349637 in 0.93s\n",
      " [-] epoch  205/250, train loss 0.350682 in 0.42s\n",
      " [-] epoch  206/250, train loss 0.367205 in 0.11s\n",
      " [-] epoch  207/250, train loss 0.366617 in 0.12s\n",
      " [-] epoch  208/250, train loss 0.374613 in 0.12s\n",
      " [-] epoch  209/250, train loss 0.380013 in 0.12s\n",
      " [-] epoch  210/250, train loss 0.371642 in 0.15s\n",
      " [-] epoch  211/250, train loss 0.356543 in 0.15s\n",
      " [-] epoch  212/250, train loss 0.386886 in 0.17s\n",
      " [-] epoch  213/250, train loss 0.364564 in 0.14s\n",
      " [-] epoch  214/250, train loss 0.332963 in 0.17s\n",
      " [-] epoch  215/250, train loss 0.357734 in 0.18s\n",
      " [-] epoch  216/250, train loss 0.349664 in 0.14s\n",
      " [-] epoch  217/250, train loss 0.403582 in 0.14s\n",
      " [-] epoch  218/250, train loss 0.378194 in 0.14s\n",
      " [-] epoch  219/250, train loss 0.377137 in 0.14s\n",
      " [-] epoch  220/250, train loss 0.372071 in 0.18s\n",
      " [-] epoch  221/250, train loss 0.365109 in 0.20s\n",
      " [-] epoch  222/250, train loss 0.348228 in 0.22s\n",
      " [-] epoch  223/250, train loss 0.375194 in 0.16s\n",
      " [-] epoch  224/250, train loss 0.344887 in 0.14s\n",
      " [-] epoch  225/250, train loss 0.350135 in 0.14s\n",
      " [-] epoch  226/250, train loss 0.352822 in 0.15s\n",
      " [-] epoch  227/250, train loss 0.383645 in 0.19s\n",
      " [-] epoch  228/250, train loss 0.305662 in 0.15s\n",
      " [-] epoch  229/250, train loss 0.340799 in 0.19s\n",
      " [-] epoch  230/250, train loss 0.341818 in 0.13s\n",
      " [-] epoch  231/250, train loss 0.353862 in 0.13s\n",
      " [-] epoch  232/250, train loss 0.357280 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.354715 in 0.14s\n",
      " [-] epoch  234/250, train loss 0.358321 in 0.19s\n",
      " [-] epoch  235/250, train loss 0.352992 in 0.19s\n",
      " [-] epoch  236/250, train loss 0.365996 in 0.12s\n",
      " [-] epoch  237/250, train loss 0.340275 in 0.13s\n",
      " [-] epoch  238/250, train loss 0.344480 in 0.18s\n",
      " [-] epoch  239/250, train loss 0.354410 in 0.19s\n",
      " [-] epoch  240/250, train loss 0.321085 in 0.13s\n",
      " [-] epoch  241/250, train loss 0.350194 in 0.20s\n",
      " [-] epoch  242/250, train loss 0.370380 in 0.21s\n",
      " [-] epoch  243/250, train loss 0.377240 in 0.20s\n",
      " [-] epoch  244/250, train loss 0.325000 in 0.18s\n",
      " [-] epoch  245/250, train loss 0.349138 in 0.20s\n",
      " [-] epoch  246/250, train loss 0.347431 in 0.23s\n",
      " [-] epoch  247/250, train loss 0.345078 in 0.18s\n",
      " [-] epoch  248/250, train loss 0.342210 in 0.20s\n",
      " [-] epoch  249/250, train loss 0.346194 in 0.20s\n",
      " [-] epoch  250/250, train loss 0.357209 in 0.19s\n",
      " [-] test acc. 73.333333%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.639327 in 0.25s\n",
      " [-] epoch    2/250, train loss 0.597636 in 0.26s\n",
      " [-] epoch    3/250, train loss 0.591185 in 0.28s\n",
      " [-] epoch    4/250, train loss 0.595330 in 0.28s\n",
      " [-] epoch    5/250, train loss 0.565598 in 0.19s\n",
      " [-] epoch    6/250, train loss 0.564141 in 0.16s\n",
      " [-] epoch    7/250, train loss 0.541672 in 0.15s\n",
      " [-] epoch    8/250, train loss 0.536922 in 0.21s\n",
      " [-] epoch    9/250, train loss 0.539700 in 0.21s\n",
      " [-] epoch   10/250, train loss 0.520366 in 0.23s\n",
      " [-] epoch   11/250, train loss 0.519207 in 0.20s\n",
      " [-] epoch   12/250, train loss 0.528851 in 0.26s\n",
      " [-] epoch   13/250, train loss 0.538153 in 0.17s\n",
      " [-] epoch   14/250, train loss 0.540036 in 0.18s\n",
      " [-] epoch   15/250, train loss 0.532798 in 0.21s\n",
      " [-] epoch   16/250, train loss 0.480333 in 0.21s\n",
      " [-] epoch   17/250, train loss 0.510204 in 0.24s\n",
      " [-] epoch   18/250, train loss 0.511082 in 0.20s\n",
      " [-] epoch   19/250, train loss 0.499385 in 0.26s\n",
      " [-] epoch   20/250, train loss 0.505996 in 0.18s\n",
      " [-] epoch   21/250, train loss 0.500983 in 0.19s\n",
      " [-] epoch   22/250, train loss 0.496082 in 0.19s\n",
      " [-] epoch   23/250, train loss 0.498243 in 0.22s\n",
      " [-] epoch   24/250, train loss 0.481254 in 0.20s\n",
      " [-] epoch   25/250, train loss 0.494471 in 0.22s\n",
      " [-] epoch   26/250, train loss 0.499334 in 0.21s\n",
      " [-] epoch   27/250, train loss 0.479829 in 0.22s\n",
      " [-] epoch   28/250, train loss 0.479549 in 0.22s\n",
      " [-] epoch   29/250, train loss 0.488952 in 0.19s\n",
      " [-] epoch   30/250, train loss 0.470743 in 0.18s\n",
      " [-] epoch   31/250, train loss 0.463143 in 0.20s\n",
      " [-] epoch   32/250, train loss 0.468587 in 0.19s\n",
      " [-] epoch   33/250, train loss 0.445536 in 0.22s\n",
      " [-] epoch   34/250, train loss 0.465503 in 0.26s\n",
      " [-] epoch   35/250, train loss 0.458965 in 0.26s\n",
      " [-] epoch   36/250, train loss 0.458326 in 0.21s\n",
      " [-] epoch   37/250, train loss 0.448612 in 0.18s\n",
      " [-] epoch   38/250, train loss 0.466561 in 0.18s\n",
      " [-] epoch   39/250, train loss 0.463028 in 0.24s\n",
      " [-] epoch   40/250, train loss 0.468803 in 0.22s\n",
      " [-] epoch   41/250, train loss 0.429393 in 0.26s\n",
      " [-] epoch   42/250, train loss 0.476001 in 0.26s\n",
      " [-] epoch   43/250, train loss 0.453877 in 0.23s\n",
      " [-] epoch   44/250, train loss 0.416209 in 0.24s\n",
      " [-] epoch   45/250, train loss 0.462174 in 0.25s\n",
      " [-] epoch   46/250, train loss 0.451880 in 0.26s\n",
      " [-] epoch   47/250, train loss 0.439213 in 0.23s\n",
      " [-] epoch   48/250, train loss 0.452830 in 0.25s\n",
      " [-] epoch   49/250, train loss 0.440519 in 0.27s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.468548 in 0.23s\n",
      " [-] epoch   51/250, train loss 0.440009 in 0.23s\n",
      " [-] epoch   52/250, train loss 0.443316 in 0.28s\n",
      " [-] epoch   53/250, train loss 0.440384 in 0.25s\n",
      " [-] epoch   54/250, train loss 0.463673 in 0.28s\n",
      " [-] epoch   55/250, train loss 0.421267 in 0.23s\n",
      " [-] epoch   56/250, train loss 0.426798 in 0.22s\n",
      " [-] epoch   57/250, train loss 0.425959 in 0.17s\n",
      " [-] epoch   58/250, train loss 0.432984 in 0.27s\n",
      " [-] epoch   59/250, train loss 0.439727 in 0.20s\n",
      " [-] epoch   60/250, train loss 0.431466 in 0.16s\n",
      " [-] epoch   61/250, train loss 0.423297 in 0.24s\n",
      " [-] epoch   62/250, train loss 0.423885 in 0.25s\n",
      " [-] epoch   63/250, train loss 0.424517 in 0.25s\n",
      " [-] epoch   64/250, train loss 0.430963 in 0.25s\n",
      " [-] epoch   65/250, train loss 0.425117 in 0.27s\n",
      " [-] epoch   66/250, train loss 0.428619 in 0.25s\n",
      " [-] epoch   67/250, train loss 0.430554 in 0.19s\n",
      " [-] epoch   68/250, train loss 0.392684 in 0.17s\n",
      " [-] epoch   69/250, train loss 0.408760 in 0.24s\n",
      " [-] epoch   70/250, train loss 0.418838 in 0.16s\n",
      " [-] epoch   71/250, train loss 0.418974 in 0.23s\n",
      " [-] epoch   72/250, train loss 0.426657 in 0.26s\n",
      " [-] epoch   73/250, train loss 0.417879 in 0.22s\n",
      " [-] epoch   74/250, train loss 0.392057 in 0.24s\n",
      " [-] epoch   75/250, train loss 0.399603 in 0.27s\n",
      " [-] epoch   76/250, train loss 0.417023 in 0.20s\n",
      " [-] epoch   77/250, train loss 0.394160 in 0.16s\n",
      " [-] epoch   78/250, train loss 0.444843 in 0.16s\n",
      " [-] epoch   79/250, train loss 0.389334 in 0.23s\n",
      " [-] epoch   80/250, train loss 0.407489 in 0.25s\n",
      " [-] epoch   81/250, train loss 0.405009 in 0.24s\n",
      " [-] epoch   82/250, train loss 0.409099 in 0.18s\n",
      " [-] epoch   83/250, train loss 0.392541 in 0.16s\n",
      " [-] epoch   84/250, train loss 0.365427 in 0.18s\n",
      " [-] epoch   85/250, train loss 0.395812 in 0.23s\n",
      " [-] epoch   86/250, train loss 0.391773 in 0.16s\n",
      " [-] epoch   87/250, train loss 0.415970 in 0.22s\n",
      " [-] epoch   88/250, train loss 0.380234 in 0.16s\n",
      " [-] epoch   89/250, train loss 0.407035 in 0.20s\n",
      " [-] epoch   90/250, train loss 0.367078 in 0.26s\n",
      " [-] epoch   91/250, train loss 0.399176 in 0.23s\n",
      " [-] epoch   92/250, train loss 0.388639 in 0.17s\n",
      " [-] epoch   93/250, train loss 0.378042 in 0.21s\n",
      " [-] epoch   94/250, train loss 0.400241 in 0.27s\n",
      " [-] epoch   95/250, train loss 0.402578 in 0.24s\n",
      " [-] epoch   96/250, train loss 0.402097 in 0.23s\n",
      " [-] epoch   97/250, train loss 0.378140 in 0.18s\n",
      " [-] epoch   98/250, train loss 0.421343 in 0.22s\n",
      " [-] epoch   99/250, train loss 0.421776 in 0.27s\n",
      " [-] epoch  100/250, train loss 0.405021 in 0.26s\n",
      " [-] epoch  101/250, train loss 0.385456 in 0.16s\n",
      " [-] epoch  102/250, train loss 0.402434 in 0.21s\n",
      " [-] epoch  103/250, train loss 0.410937 in 0.18s\n",
      " [-] epoch  104/250, train loss 0.361546 in 0.20s\n",
      " [-] epoch  105/250, train loss 0.373162 in 0.20s\n",
      " [-] epoch  106/250, train loss 0.371165 in 0.16s\n",
      " [-] epoch  107/250, train loss 0.360786 in 0.18s\n",
      " [-] epoch  108/250, train loss 0.369907 in 0.21s\n",
      " [-] epoch  109/250, train loss 0.385236 in 0.17s\n",
      " [-] epoch  110/250, train loss 0.380900 in 0.18s\n",
      " [-] epoch  111/250, train loss 0.347532 in 0.24s\n",
      " [-] epoch  112/250, train loss 0.413212 in 0.27s\n",
      " [-] epoch  113/250, train loss 0.360268 in 0.22s\n",
      " [-] epoch  114/250, train loss 0.352238 in 0.17s\n",
      " [-] epoch  115/250, train loss 0.405379 in 0.18s\n",
      " [-] epoch  116/250, train loss 0.369809 in 0.24s\n",
      " [-] epoch  117/250, train loss 0.392203 in 0.20s\n",
      " [-] epoch  118/250, train loss 0.376028 in 0.22s\n",
      " [-] epoch  119/250, train loss 0.378865 in 0.26s\n",
      " [-] epoch  120/250, train loss 0.385249 in 0.22s\n",
      " [-] epoch  121/250, train loss 0.369150 in 0.24s\n",
      " [-] epoch  122/250, train loss 0.353774 in 0.22s\n",
      " [-] epoch  123/250, train loss 0.359313 in 0.16s\n",
      " [-] epoch  124/250, train loss 0.398553 in 0.23s\n",
      " [-] epoch  125/250, train loss 0.364874 in 0.25s\n",
      " [-] epoch  126/250, train loss 0.381698 in 0.22s\n",
      " [-] epoch  127/250, train loss 0.351340 in 0.20s\n",
      " [-] epoch  128/250, train loss 0.361361 in 0.26s\n",
      " [-] epoch  129/250, train loss 0.394933 in 0.26s\n",
      " [-] epoch  130/250, train loss 0.320008 in 0.25s\n",
      " [-] epoch  131/250, train loss 0.356923 in 0.27s\n",
      " [-] epoch  132/250, train loss 0.375917 in 0.26s\n",
      " [-] epoch  133/250, train loss 0.361181 in 0.22s\n",
      " [-] epoch  134/250, train loss 0.382327 in 0.26s\n",
      " [-] epoch  135/250, train loss 0.364597 in 0.16s\n",
      " [-] epoch  136/250, train loss 0.370979 in 0.16s\n",
      " [-] epoch  137/250, train loss 0.369023 in 0.24s\n",
      " [-] epoch  138/250, train loss 0.334295 in 0.25s\n",
      " [-] epoch  139/250, train loss 0.342468 in 0.16s\n",
      " [-] epoch  140/250, train loss 0.353770 in 0.22s\n",
      " [-] epoch  141/250, train loss 0.367314 in 0.20s\n",
      " [-] epoch  142/250, train loss 0.363832 in 0.26s\n",
      " [-] epoch  143/250, train loss 0.336968 in 0.22s\n",
      " [-] epoch  144/250, train loss 0.383091 in 0.17s\n",
      " [-] epoch  145/250, train loss 0.355287 in 0.26s\n",
      " [-] epoch  146/250, train loss 0.352106 in 0.28s\n",
      " [-] epoch  147/250, train loss 0.375125 in 0.21s\n",
      " [-] epoch  148/250, train loss 0.360125 in 0.17s\n",
      " [-] epoch  149/250, train loss 0.356257 in 0.23s\n",
      " [-] epoch  150/250, train loss 0.340285 in 0.27s\n",
      " [-] epoch  151/250, train loss 0.347446 in 0.25s\n",
      " [-] epoch  152/250, train loss 0.353625 in 0.19s\n",
      " [-] epoch  153/250, train loss 0.375062 in 0.25s\n",
      " [-] epoch  154/250, train loss 0.354073 in 0.19s\n",
      " [-] epoch  155/250, train loss 0.376251 in 0.22s\n",
      " [-] epoch  156/250, train loss 0.350964 in 0.20s\n",
      " [-] epoch  157/250, train loss 0.369091 in 0.17s\n",
      " [-] epoch  158/250, train loss 0.341138 in 0.22s\n",
      " [-] epoch  159/250, train loss 0.340761 in 0.20s\n",
      " [-] epoch  160/250, train loss 0.348877 in 0.19s\n",
      " [-] epoch  161/250, train loss 0.382290 in 0.20s\n",
      " [-] epoch  162/250, train loss 0.360774 in 0.25s\n",
      " [-] epoch  163/250, train loss 0.353588 in 0.25s\n",
      " [-] epoch  164/250, train loss 0.343836 in 0.27s\n",
      " [-] epoch  165/250, train loss 0.344780 in 0.25s\n",
      " [-] epoch  166/250, train loss 0.350177 in 0.24s\n",
      " [-] epoch  167/250, train loss 0.333135 in 0.24s\n",
      " [-] epoch  168/250, train loss 0.331840 in 0.25s\n",
      " [-] epoch  169/250, train loss 0.334950 in 0.24s\n",
      " [-] epoch  170/250, train loss 0.354931 in 0.25s\n",
      " [-] epoch  171/250, train loss 0.332403 in 0.25s\n",
      " [-] epoch  172/250, train loss 0.370291 in 0.27s\n",
      " [-] epoch  173/250, train loss 0.312241 in 0.25s\n",
      " [-] epoch  174/250, train loss 0.348709 in 0.15s\n",
      " [-] epoch  175/250, train loss 0.334537 in 0.16s\n",
      " [-] epoch  176/250, train loss 0.315018 in 0.23s\n",
      " [-] epoch  177/250, train loss 0.354281 in 0.25s\n",
      " [-] epoch  178/250, train loss 0.354294 in 0.27s\n",
      " [-] epoch  179/250, train loss 0.327719 in 0.27s\n",
      " [-] epoch  180/250, train loss 0.316540 in 0.23s\n",
      " [-] epoch  181/250, train loss 0.363400 in 0.23s\n",
      " [-] epoch  182/250, train loss 0.319202 in 0.19s\n",
      " [-] epoch  183/250, train loss 0.343164 in 0.16s\n",
      " [-] epoch  184/250, train loss 0.342314 in 0.21s\n",
      " [-] epoch  185/250, train loss 0.364511 in 0.16s\n",
      " [-] epoch  186/250, train loss 0.342262 in 0.22s\n",
      " [-] epoch  187/250, train loss 0.340715 in 0.24s\n",
      " [-] epoch  188/250, train loss 0.370664 in 0.16s\n",
      " [-] epoch  189/250, train loss 0.359041 in 0.17s\n",
      " [-] epoch  190/250, train loss 0.328115 in 0.18s\n",
      " [-] epoch  191/250, train loss 0.350772 in 0.18s\n",
      " [-] epoch  192/250, train loss 0.311398 in 0.23s\n",
      " [-] epoch  193/250, train loss 0.351860 in 0.23s\n",
      " [-] epoch  194/250, train loss 0.353953 in 0.24s\n",
      " [-] epoch  195/250, train loss 0.341208 in 0.23s\n",
      " [-] epoch  196/250, train loss 0.335159 in 0.24s\n",
      " [-] epoch  197/250, train loss 0.344097 in 0.16s\n",
      " [-] epoch  198/250, train loss 0.327222 in 0.18s\n",
      " [-] epoch  199/250, train loss 0.323734 in 0.18s\n",
      " [-] epoch  200/250, train loss 0.301419 in 0.17s\n",
      " [-] epoch  201/250, train loss 0.336863 in 0.16s\n",
      " [-] epoch  202/250, train loss 0.348237 in 0.18s\n",
      " [-] epoch  203/250, train loss 0.319190 in 0.19s\n",
      " [-] epoch  204/250, train loss 0.336865 in 0.22s\n",
      " [-] epoch  205/250, train loss 0.313266 in 0.18s\n",
      " [-] epoch  206/250, train loss 0.324388 in 0.20s\n",
      " [-] epoch  207/250, train loss 0.331569 in 0.17s\n",
      " [-] epoch  208/250, train loss 0.325298 in 0.18s\n",
      " [-] epoch  209/250, train loss 0.379560 in 0.21s\n",
      " [-] epoch  210/250, train loss 0.344160 in 0.18s\n",
      " [-] epoch  211/250, train loss 0.341993 in 0.16s\n",
      " [-] epoch  212/250, train loss 0.320672 in 0.16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  213/250, train loss 0.333944 in 0.19s\n",
      " [-] epoch  214/250, train loss 0.311539 in 0.17s\n",
      " [-] epoch  215/250, train loss 0.295808 in 0.16s\n",
      " [-] epoch  216/250, train loss 0.297011 in 0.19s\n",
      " [-] epoch  217/250, train loss 0.328455 in 0.17s\n",
      " [-] epoch  218/250, train loss 0.311645 in 0.17s\n",
      " [-] epoch  219/250, train loss 0.327740 in 0.16s\n",
      " [-] epoch  220/250, train loss 0.336486 in 0.16s\n",
      " [-] epoch  221/250, train loss 0.332637 in 0.16s\n",
      " [-] epoch  222/250, train loss 0.347580 in 0.18s\n",
      " [-] epoch  223/250, train loss 0.339018 in 0.17s\n",
      " [-] epoch  224/250, train loss 0.314536 in 0.18s\n",
      " [-] epoch  225/250, train loss 0.305386 in 0.18s\n",
      " [-] epoch  226/250, train loss 0.344919 in 0.16s\n",
      " [-] epoch  227/250, train loss 0.342442 in 0.16s\n",
      " [-] epoch  228/250, train loss 0.323554 in 0.16s\n",
      " [-] epoch  229/250, train loss 0.332426 in 0.17s\n",
      " [-] epoch  230/250, train loss 0.313232 in 0.17s\n",
      " [-] epoch  231/250, train loss 0.324277 in 0.19s\n",
      " [-] epoch  232/250, train loss 0.327631 in 0.26s\n",
      " [-] epoch  233/250, train loss 0.331648 in 0.16s\n",
      " [-] epoch  234/250, train loss 0.332726 in 0.16s\n",
      " [-] epoch  235/250, train loss 0.300428 in 0.17s\n",
      " [-] epoch  236/250, train loss 0.293953 in 0.23s\n",
      " [-] epoch  237/250, train loss 0.311959 in 0.26s\n",
      " [-] epoch  238/250, train loss 0.336426 in 0.21s\n",
      " [-] epoch  239/250, train loss 0.341341 in 0.17s\n",
      " [-] epoch  240/250, train loss 0.318776 in 0.17s\n",
      " [-] epoch  241/250, train loss 0.327348 in 0.17s\n",
      " [-] epoch  242/250, train loss 0.329439 in 0.20s\n",
      " [-] epoch  243/250, train loss 0.324371 in 0.22s\n",
      " [-] epoch  244/250, train loss 0.328522 in 0.24s\n",
      " [-] epoch  245/250, train loss 0.318070 in 0.27s\n",
      " [-] epoch  246/250, train loss 0.308230 in 0.28s\n",
      " [-] epoch  247/250, train loss 0.318548 in 0.21s\n",
      " [-] epoch  248/250, train loss 0.314757 in 0.16s\n",
      " [-] epoch  249/250, train loss 0.306570 in 0.16s\n",
      " [-] epoch  250/250, train loss 0.314810 in 0.16s\n",
      " [-] test acc. 70.833333%\n",
      "Je vais utiliser 7 layers\n",
      " [-] epoch    1/250, train loss 0.667394 in 0.26s\n",
      " [-] epoch    2/250, train loss 0.621397 in 0.26s\n",
      " [-] epoch    3/250, train loss 0.602892 in 0.28s\n",
      " [-] epoch    4/250, train loss 0.591030 in 0.25s\n",
      " [-] epoch    5/250, train loss 0.560355 in 0.22s\n",
      " [-] epoch    6/250, train loss 0.594657 in 0.26s\n",
      " [-] epoch    7/250, train loss 0.551232 in 0.22s\n",
      " [-] epoch    8/250, train loss 0.553639 in 0.21s\n",
      " [-] epoch    9/250, train loss 0.548446 in 0.22s\n",
      " [-] epoch   10/250, train loss 0.565323 in 0.24s\n",
      " [-] epoch   11/250, train loss 0.532297 in 0.25s\n",
      " [-] epoch   12/250, train loss 0.541098 in 0.26s\n",
      " [-] epoch   13/250, train loss 0.525727 in 0.28s\n",
      " [-] epoch   14/250, train loss 0.512554 in 0.24s\n",
      " [-] epoch   15/250, train loss 0.515257 in 0.27s\n",
      " [-] epoch   16/250, train loss 0.516245 in 0.33s\n",
      " [-] epoch   17/250, train loss 0.511991 in 0.30s\n",
      " [-] epoch   18/250, train loss 0.508307 in 0.24s\n",
      " [-] epoch   19/250, train loss 0.511729 in 0.29s\n",
      " [-] epoch   20/250, train loss 0.515875 in 0.30s\n",
      " [-] epoch   21/250, train loss 0.483741 in 0.22s\n",
      " [-] epoch   22/250, train loss 0.532157 in 0.23s\n",
      " [-] epoch   23/250, train loss 0.466189 in 0.29s\n",
      " [-] epoch   24/250, train loss 0.514561 in 0.23s\n",
      " [-] epoch   25/250, train loss 0.487115 in 0.23s\n",
      " [-] epoch   26/250, train loss 0.483320 in 0.22s\n",
      " [-] epoch   27/250, train loss 0.480776 in 0.21s\n",
      " [-] epoch   28/250, train loss 0.496106 in 0.22s\n",
      " [-] epoch   29/250, train loss 0.485925 in 0.28s\n",
      " [-] epoch   30/250, train loss 0.497066 in 0.22s\n",
      " [-] epoch   31/250, train loss 0.501579 in 0.21s\n",
      " [-] epoch   32/250, train loss 0.478026 in 0.23s\n",
      " [-] epoch   33/250, train loss 0.467190 in 0.18s\n",
      " [-] epoch   34/250, train loss 0.487787 in 0.23s\n",
      " [-] epoch   35/250, train loss 0.459559 in 0.24s\n",
      " [-] epoch   36/250, train loss 0.454125 in 0.28s\n",
      " [-] epoch   37/250, train loss 0.441953 in 0.20s\n",
      " [-] epoch   38/250, train loss 0.457391 in 0.23s\n",
      " [-] epoch   39/250, train loss 0.477018 in 0.24s\n",
      " [-] epoch   40/250, train loss 0.444926 in 0.23s\n",
      " [-] epoch   41/250, train loss 0.451361 in 0.24s\n",
      " [-] epoch   42/250, train loss 0.436578 in 0.22s\n",
      " [-] epoch   43/250, train loss 0.444570 in 0.21s\n",
      " [-] epoch   44/250, train loss 0.450761 in 0.25s\n",
      " [-] epoch   45/250, train loss 0.473909 in 0.24s\n",
      " [-] epoch   46/250, train loss 0.440276 in 0.23s\n",
      " [-] epoch   47/250, train loss 0.425345 in 0.26s\n",
      " [-] epoch   48/250, train loss 0.427279 in 0.28s\n",
      " [-] epoch   49/250, train loss 0.466736 in 0.28s\n",
      " [-] epoch   50/250, train loss 0.445602 in 0.28s\n",
      " [-] epoch   51/250, train loss 0.449790 in 0.30s\n",
      " [-] epoch   52/250, train loss 0.469663 in 0.26s\n",
      " [-] epoch   53/250, train loss 0.441902 in 0.20s\n",
      " [-] epoch   54/250, train loss 0.441224 in 0.31s\n",
      " [-] epoch   55/250, train loss 0.467105 in 0.24s\n",
      " [-] epoch   56/250, train loss 0.428042 in 0.27s\n",
      " [-] epoch   57/250, train loss 0.445290 in 0.31s\n",
      " [-] epoch   58/250, train loss 0.439949 in 0.19s\n",
      " [-] epoch   59/250, train loss 0.414668 in 0.28s\n",
      " [-] epoch   60/250, train loss 0.417264 in 0.20s\n",
      " [-] epoch   61/250, train loss 0.421746 in 0.30s\n",
      " [-] epoch   62/250, train loss 0.419444 in 0.25s\n",
      " [-] epoch   63/250, train loss 0.448871 in 0.24s\n",
      " [-] epoch   64/250, train loss 0.413243 in 0.22s\n",
      " [-] epoch   65/250, train loss 0.417077 in 0.21s\n",
      " [-] epoch   66/250, train loss 0.434250 in 0.25s\n",
      " [-] epoch   67/250, train loss 0.442996 in 0.31s\n",
      " [-] epoch   68/250, train loss 0.418391 in 0.21s\n",
      " [-] epoch   69/250, train loss 0.430008 in 0.28s\n",
      " [-] epoch   70/250, train loss 0.407900 in 0.23s\n",
      " [-] epoch   71/250, train loss 0.461497 in 0.32s\n",
      " [-] epoch   72/250, train loss 0.406896 in 0.18s\n",
      " [-] epoch   73/250, train loss 0.430199 in 0.24s\n",
      " [-] epoch   74/250, train loss 0.440631 in 0.23s\n",
      " [-] epoch   75/250, train loss 0.419274 in 0.20s\n",
      " [-] epoch   76/250, train loss 0.401607 in 0.26s\n",
      " [-] epoch   77/250, train loss 0.405480 in 0.25s\n",
      " [-] epoch   78/250, train loss 0.415486 in 0.29s\n",
      " [-] epoch   79/250, train loss 0.434646 in 0.20s\n",
      " [-] epoch   80/250, train loss 0.420845 in 0.20s\n",
      " [-] epoch   81/250, train loss 0.421456 in 0.22s\n",
      " [-] epoch   82/250, train loss 0.426015 in 0.19s\n",
      " [-] epoch   83/250, train loss 0.412911 in 0.19s\n",
      " [-] epoch   84/250, train loss 0.414001 in 0.18s\n",
      " [-] epoch   85/250, train loss 0.403614 in 0.22s\n",
      " [-] epoch   86/250, train loss 0.441287 in 0.22s\n",
      " [-] epoch   87/250, train loss 0.406411 in 0.26s\n",
      " [-] epoch   88/250, train loss 0.409643 in 0.22s\n",
      " [-] epoch   89/250, train loss 0.399824 in 0.21s\n",
      " [-] epoch   90/250, train loss 0.430397 in 0.23s\n",
      " [-] epoch   91/250, train loss 0.401078 in 0.18s\n",
      " [-] epoch   92/250, train loss 0.433080 in 0.26s\n",
      " [-] epoch   93/250, train loss 0.441466 in 0.23s\n",
      " [-] epoch   94/250, train loss 0.403511 in 0.26s\n",
      " [-] epoch   95/250, train loss 0.426649 in 0.23s\n",
      " [-] epoch   96/250, train loss 0.381243 in 0.30s\n",
      " [-] epoch   97/250, train loss 0.388034 in 0.26s\n",
      " [-] epoch   98/250, train loss 0.415247 in 0.27s\n",
      " [-] epoch   99/250, train loss 0.404034 in 0.29s\n",
      " [-] epoch  100/250, train loss 0.399353 in 0.28s\n",
      " [-] epoch  101/250, train loss 0.388562 in 0.30s\n",
      " [-] epoch  102/250, train loss 0.395732 in 0.30s\n",
      " [-] epoch  103/250, train loss 0.387370 in 0.32s\n",
      " [-] epoch  104/250, train loss 0.360658 in 0.17s\n",
      " [-] epoch  105/250, train loss 0.424823 in 0.23s\n",
      " [-] epoch  106/250, train loss 0.373369 in 0.27s\n",
      " [-] epoch  107/250, train loss 0.376062 in 0.26s\n",
      " [-] epoch  108/250, train loss 0.406206 in 0.24s\n",
      " [-] epoch  109/250, train loss 0.390722 in 0.21s\n",
      " [-] epoch  110/250, train loss 0.388556 in 0.24s\n",
      " [-] epoch  111/250, train loss 0.393652 in 0.23s\n",
      " [-] epoch  112/250, train loss 0.402927 in 0.29s\n",
      " [-] epoch  113/250, train loss 0.402548 in 0.27s\n",
      " [-] epoch  114/250, train loss 0.403404 in 0.22s\n",
      " [-] epoch  115/250, train loss 0.377284 in 0.26s\n",
      " [-] epoch  116/250, train loss 0.391627 in 0.22s\n",
      " [-] epoch  117/250, train loss 0.385294 in 0.27s\n",
      " [-] epoch  118/250, train loss 0.398291 in 0.26s\n",
      " [-] epoch  119/250, train loss 0.396424 in 0.20s\n",
      " [-] epoch  120/250, train loss 0.374072 in 0.28s\n",
      " [-] epoch  121/250, train loss 0.371578 in 0.19s\n",
      " [-] epoch  122/250, train loss 0.381113 in 0.30s\n",
      " [-] epoch  123/250, train loss 0.357502 in 0.22s\n",
      " [-] epoch  124/250, train loss 0.391791 in 0.27s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  125/250, train loss 0.377211 in 0.19s\n",
      " [-] epoch  126/250, train loss 0.385229 in 0.24s\n",
      " [-] epoch  127/250, train loss 0.398271 in 0.25s\n",
      " [-] epoch  128/250, train loss 0.367977 in 0.20s\n",
      " [-] epoch  129/250, train loss 0.366809 in 0.21s\n",
      " [-] epoch  130/250, train loss 0.407094 in 0.21s\n",
      " [-] epoch  131/250, train loss 0.407874 in 0.19s\n",
      " [-] epoch  132/250, train loss 0.369942 in 0.28s\n",
      " [-] epoch  133/250, train loss 0.364296 in 0.33s\n",
      " [-] epoch  134/250, train loss 0.370750 in 0.25s\n",
      " [-] epoch  135/250, train loss 0.355591 in 0.20s\n",
      " [-] epoch  136/250, train loss 0.366769 in 0.32s\n",
      " [-] epoch  137/250, train loss 0.362791 in 0.20s\n",
      " [-] epoch  138/250, train loss 0.403163 in 0.27s\n",
      " [-] epoch  139/250, train loss 0.385920 in 0.26s\n",
      " [-] epoch  140/250, train loss 0.367179 in 0.20s\n",
      " [-] epoch  141/250, train loss 0.369334 in 0.31s\n",
      " [-] epoch  142/250, train loss 0.351830 in 0.34s\n",
      " [-] epoch  143/250, train loss 0.366474 in 0.22s\n",
      " [-] epoch  144/250, train loss 0.348171 in 0.25s\n",
      " [-] epoch  145/250, train loss 0.352187 in 0.22s\n",
      " [-] epoch  146/250, train loss 0.368847 in 0.24s\n",
      " [-] epoch  147/250, train loss 0.408003 in 0.31s\n",
      " [-] epoch  148/250, train loss 0.352778 in 0.23s\n",
      " [-] epoch  149/250, train loss 0.394598 in 0.25s\n",
      " [-] epoch  150/250, train loss 0.385789 in 0.31s\n",
      " [-] epoch  151/250, train loss 0.380222 in 0.34s\n",
      " [-] epoch  152/250, train loss 0.364437 in 0.25s\n",
      " [-] epoch  153/250, train loss 0.346754 in 0.23s\n",
      " [-] epoch  154/250, train loss 0.340139 in 0.30s\n",
      " [-] epoch  155/250, train loss 0.376917 in 0.21s\n",
      " [-] epoch  156/250, train loss 0.369637 in 0.24s\n",
      " [-] epoch  157/250, train loss 0.348533 in 0.33s\n",
      " [-] epoch  158/250, train loss 0.361364 in 0.26s\n",
      " [-] epoch  159/250, train loss 0.363687 in 0.25s\n",
      " [-] epoch  160/250, train loss 0.348130 in 0.29s\n",
      " [-] epoch  161/250, train loss 0.371096 in 0.19s\n",
      " [-] epoch  162/250, train loss 0.327751 in 0.26s\n",
      " [-] epoch  163/250, train loss 0.342604 in 0.26s\n",
      " [-] epoch  164/250, train loss 0.328049 in 0.25s\n",
      " [-] epoch  165/250, train loss 0.369822 in 0.23s\n",
      " [-] epoch  166/250, train loss 0.367789 in 0.22s\n",
      " [-] epoch  167/250, train loss 0.363993 in 0.20s\n",
      " [-] epoch  168/250, train loss 0.375624 in 0.26s\n",
      " [-] epoch  169/250, train loss 0.353449 in 0.27s\n",
      " [-] epoch  170/250, train loss 0.363271 in 0.22s\n",
      " [-] epoch  171/250, train loss 0.357491 in 0.24s\n",
      " [-] epoch  172/250, train loss 0.369474 in 0.24s\n",
      " [-] epoch  173/250, train loss 0.352235 in 0.21s\n",
      " [-] epoch  174/250, train loss 0.347379 in 0.19s\n",
      " [-] epoch  175/250, train loss 0.379165 in 0.20s\n",
      " [-] epoch  176/250, train loss 0.334749 in 0.19s\n",
      " [-] epoch  177/250, train loss 0.372133 in 0.26s\n",
      " [-] epoch  178/250, train loss 0.347583 in 0.32s\n",
      " [-] epoch  179/250, train loss 0.371807 in 0.21s\n",
      " [-] epoch  180/250, train loss 0.363152 in 0.30s\n",
      " [-] epoch  181/250, train loss 0.376268 in 0.29s\n",
      " [-] epoch  182/250, train loss 0.357554 in 0.29s\n",
      " [-] epoch  183/250, train loss 0.348024 in 0.20s\n",
      " [-] epoch  184/250, train loss 0.342803 in 0.24s\n",
      " [-] epoch  185/250, train loss 0.339597 in 0.31s\n",
      " [-] epoch  186/250, train loss 0.340431 in 0.33s\n",
      " [-] epoch  187/250, train loss 0.351839 in 0.31s\n",
      " [-] epoch  188/250, train loss 0.372387 in 0.30s\n",
      " [-] epoch  189/250, train loss 0.354237 in 0.27s\n",
      " [-] epoch  190/250, train loss 0.368484 in 0.31s\n",
      " [-] epoch  191/250, train loss 0.360053 in 0.32s\n",
      " [-] epoch  192/250, train loss 0.339624 in 0.33s\n",
      " [-] epoch  193/250, train loss 0.362252 in 0.32s\n",
      " [-] epoch  194/250, train loss 0.351081 in 0.32s\n",
      " [-] epoch  195/250, train loss 0.337983 in 0.29s\n",
      " [-] epoch  196/250, train loss 0.317802 in 0.24s\n",
      " [-] epoch  197/250, train loss 0.343455 in 0.19s\n",
      " [-] epoch  198/250, train loss 0.322126 in 0.21s\n",
      " [-] epoch  199/250, train loss 0.335032 in 0.23s\n",
      " [-] epoch  200/250, train loss 0.338058 in 0.32s\n",
      " [-] epoch  201/250, train loss 0.326896 in 0.32s\n",
      " [-] epoch  202/250, train loss 0.354544 in 0.25s\n",
      " [-] epoch  203/250, train loss 0.356870 in 0.26s\n",
      " [-] epoch  204/250, train loss 0.352338 in 0.21s\n",
      " [-] epoch  205/250, train loss 0.357956 in 0.20s\n",
      " [-] epoch  206/250, train loss 0.328295 in 0.19s\n",
      " [-] epoch  207/250, train loss 0.347254 in 0.20s\n",
      " [-] epoch  208/250, train loss 0.334666 in 0.23s\n",
      " [-] epoch  209/250, train loss 0.323903 in 0.21s\n",
      " [-] epoch  210/250, train loss 0.344408 in 0.19s\n",
      " [-] epoch  211/250, train loss 0.325220 in 0.26s\n",
      " [-] epoch  212/250, train loss 0.335155 in 0.22s\n",
      " [-] epoch  213/250, train loss 0.344435 in 0.20s\n",
      " [-] epoch  214/250, train loss 0.345069 in 0.24s\n",
      " [-] epoch  215/250, train loss 0.346414 in 0.20s\n",
      " [-] epoch  216/250, train loss 0.334287 in 0.20s\n",
      " [-] epoch  217/250, train loss 0.341647 in 0.20s\n",
      " [-] epoch  218/250, train loss 0.324830 in 0.24s\n",
      " [-] epoch  219/250, train loss 0.336590 in 0.23s\n",
      " [-] epoch  220/250, train loss 0.386911 in 0.23s\n",
      " [-] epoch  221/250, train loss 0.365603 in 0.23s\n",
      " [-] epoch  222/250, train loss 0.341434 in 0.26s\n",
      " [-] epoch  223/250, train loss 0.334472 in 0.19s\n",
      " [-] epoch  224/250, train loss 0.346178 in 0.22s\n",
      " [-] epoch  225/250, train loss 0.320682 in 0.22s\n",
      " [-] epoch  226/250, train loss 0.311161 in 0.23s\n",
      " [-] epoch  227/250, train loss 0.316086 in 0.24s\n",
      " [-] epoch  228/250, train loss 0.346363 in 0.23s\n",
      " [-] epoch  229/250, train loss 0.342198 in 0.33s\n",
      " [-] epoch  230/250, train loss 0.326615 in 0.27s\n",
      " [-] epoch  231/250, train loss 0.313683 in 0.29s\n",
      " [-] epoch  232/250, train loss 0.326736 in 0.29s\n",
      " [-] epoch  233/250, train loss 0.328859 in 0.33s\n",
      " [-] epoch  234/250, train loss 0.330108 in 0.28s\n",
      " [-] epoch  235/250, train loss 0.348633 in 0.21s\n",
      " [-] epoch  236/250, train loss 0.317665 in 0.29s\n",
      " [-] epoch  237/250, train loss 0.367143 in 0.25s\n",
      " [-] epoch  238/250, train loss 0.332694 in 0.27s\n",
      " [-] epoch  239/250, train loss 0.330893 in 0.28s\n",
      " [-] epoch  240/250, train loss 0.332284 in 0.20s\n",
      " [-] epoch  241/250, train loss 0.346504 in 0.21s\n",
      " [-] epoch  242/250, train loss 0.317062 in 0.23s\n",
      " [-] epoch  243/250, train loss 0.336534 in 0.22s\n",
      " [-] epoch  244/250, train loss 0.330501 in 0.28s\n",
      " [-] epoch  245/250, train loss 0.335058 in 0.20s\n",
      " [-] epoch  246/250, train loss 0.319078 in 0.20s\n",
      " [-] epoch  247/250, train loss 0.301880 in 0.19s\n",
      " [-] epoch  248/250, train loss 0.338182 in 0.23s\n",
      " [-] epoch  249/250, train loss 0.357964 in 0.27s\n",
      " [-] epoch  250/250, train loss 0.327863 in 0.30s\n",
      " [-] test acc. 65.277778%\n",
      "Je vais utiliser 8 layers\n",
      " [-] epoch    1/250, train loss 0.644346 in 0.30s\n",
      " [-] epoch    2/250, train loss 0.600523 in 0.34s\n",
      " [-] epoch    3/250, train loss 0.577475 in 0.23s\n",
      " [-] epoch    4/250, train loss 0.580219 in 0.23s\n",
      " [-] epoch    5/250, train loss 0.550569 in 0.22s\n",
      " [-] epoch    6/250, train loss 0.549057 in 0.22s\n",
      " [-] epoch    7/250, train loss 0.526037 in 0.22s\n",
      " [-] epoch    8/250, train loss 0.550275 in 0.23s\n",
      " [-] epoch    9/250, train loss 0.538768 in 0.23s\n",
      " [-] epoch   10/250, train loss 0.539157 in 0.24s\n",
      " [-] epoch   11/250, train loss 0.534780 in 0.23s\n",
      " [-] epoch   12/250, train loss 0.548537 in 0.26s\n",
      " [-] epoch   13/250, train loss 0.519088 in 0.23s\n",
      " [-] epoch   14/250, train loss 0.511267 in 0.23s\n",
      " [-] epoch   15/250, train loss 0.475437 in 0.25s\n",
      " [-] epoch   16/250, train loss 0.496386 in 0.23s\n",
      " [-] epoch   17/250, train loss 0.510776 in 0.26s\n",
      " [-] epoch   18/250, train loss 0.497387 in 0.28s\n",
      " [-] epoch   19/250, train loss 0.505905 in 0.31s\n",
      " [-] epoch   20/250, train loss 0.508415 in 0.21s\n",
      " [-] epoch   21/250, train loss 0.502063 in 0.27s\n",
      " [-] epoch   22/250, train loss 0.483003 in 0.32s\n",
      " [-] epoch   23/250, train loss 0.501037 in 0.27s\n",
      " [-] epoch   24/250, train loss 0.515288 in 0.29s\n",
      " [-] epoch   25/250, train loss 0.489326 in 0.29s\n",
      " [-] epoch   26/250, train loss 0.481944 in 0.28s\n",
      " [-] epoch   27/250, train loss 0.465758 in 0.26s\n",
      " [-] epoch   28/250, train loss 0.485308 in 0.32s\n",
      " [-] epoch   29/250, train loss 0.492179 in 0.30s\n",
      " [-] epoch   30/250, train loss 0.470281 in 0.21s\n",
      " [-] epoch   31/250, train loss 0.431766 in 0.24s\n",
      " [-] epoch   32/250, train loss 0.475813 in 0.28s\n",
      " [-] epoch   33/250, train loss 0.492342 in 0.27s\n",
      " [-] epoch   34/250, train loss 0.445573 in 0.30s\n",
      " [-] epoch   35/250, train loss 0.464844 in 0.28s\n",
      " [-] epoch   36/250, train loss 0.464500 in 0.28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   37/250, train loss 0.477165 in 0.27s\n",
      " [-] epoch   38/250, train loss 0.453403 in 0.26s\n",
      " [-] epoch   39/250, train loss 0.465691 in 0.27s\n",
      " [-] epoch   40/250, train loss 0.429977 in 0.29s\n",
      " [-] epoch   41/250, train loss 0.458675 in 0.28s\n",
      " [-] epoch   42/250, train loss 0.449794 in 0.28s\n",
      " [-] epoch   43/250, train loss 0.449861 in 0.29s\n",
      " [-] epoch   44/250, train loss 0.445850 in 0.28s\n",
      " [-] epoch   45/250, train loss 0.420164 in 0.29s\n",
      " [-] epoch   46/250, train loss 0.400244 in 0.27s\n",
      " [-] epoch   47/250, train loss 0.451193 in 0.28s\n",
      " [-] epoch   48/250, train loss 0.451886 in 0.29s\n",
      " [-] epoch   49/250, train loss 0.422620 in 0.27s\n",
      " [-] epoch   50/250, train loss 0.446718 in 0.25s\n",
      " [-] epoch   51/250, train loss 0.439221 in 0.32s\n",
      " [-] epoch   52/250, train loss 0.447714 in 0.26s\n",
      " [-] epoch   53/250, train loss 0.412198 in 0.26s\n",
      " [-] epoch   54/250, train loss 0.394942 in 0.29s\n",
      " [-] epoch   55/250, train loss 0.424477 in 0.29s\n",
      " [-] epoch   56/250, train loss 0.435745 in 0.22s\n",
      " [-] epoch   57/250, train loss 0.443184 in 0.21s\n",
      " [-] epoch   58/250, train loss 0.443321 in 0.21s\n",
      " [-] epoch   59/250, train loss 0.439618 in 0.20s\n",
      " [-] epoch   60/250, train loss 0.437937 in 0.20s\n",
      " [-] epoch   61/250, train loss 0.422234 in 0.20s\n",
      " [-] epoch   62/250, train loss 0.450784 in 0.21s\n",
      " [-] epoch   63/250, train loss 0.414331 in 0.20s\n",
      " [-] epoch   64/250, train loss 0.421009 in 0.20s\n",
      " [-] epoch   65/250, train loss 0.431232 in 0.20s\n",
      " [-] epoch   66/250, train loss 0.441762 in 0.20s\n",
      " [-] epoch   67/250, train loss 0.438710 in 0.21s\n",
      " [-] epoch   68/250, train loss 0.431488 in 0.20s\n",
      " [-] epoch   69/250, train loss 0.426186 in 0.20s\n",
      " [-] epoch   70/250, train loss 0.430228 in 0.21s\n",
      " [-] epoch   71/250, train loss 0.425936 in 0.19s\n",
      " [-] epoch   72/250, train loss 0.414297 in 0.20s\n",
      " [-] epoch   73/250, train loss 0.432648 in 0.23s\n",
      " [-] epoch   74/250, train loss 0.429476 in 0.36s\n",
      " [-] epoch   75/250, train loss 0.399760 in 0.28s\n",
      " [-] epoch   76/250, train loss 0.436336 in 0.29s\n",
      " [-] epoch   77/250, train loss 0.381043 in 0.33s\n",
      " [-] epoch   78/250, train loss 0.415742 in 0.28s\n",
      " [-] epoch   79/250, train loss 0.390937 in 0.25s\n",
      " [-] epoch   80/250, train loss 0.407220 in 0.26s\n",
      " [-] epoch   81/250, train loss 0.408678 in 0.30s\n",
      " [-] epoch   82/250, train loss 0.407100 in 0.27s\n",
      " [-] epoch   83/250, train loss 0.395400 in 0.28s\n",
      " [-] epoch   84/250, train loss 0.388366 in 0.32s\n",
      " [-] epoch   85/250, train loss 0.403075 in 0.30s\n",
      " [-] epoch   86/250, train loss 0.385775 in 0.35s\n",
      " [-] epoch   87/250, train loss 0.379017 in 0.29s\n",
      " [-] epoch   88/250, train loss 0.376720 in 0.27s\n",
      " [-] epoch   89/250, train loss 0.403659 in 0.30s\n",
      " [-] epoch   90/250, train loss 0.387038 in 0.29s\n",
      " [-] epoch   91/250, train loss 0.413937 in 0.25s\n",
      " [-] epoch   92/250, train loss 0.378956 in 0.26s\n",
      " [-] epoch   93/250, train loss 0.402417 in 0.29s\n",
      " [-] epoch   94/250, train loss 0.395259 in 0.31s\n",
      " [-] epoch   95/250, train loss 0.400496 in 0.37s\n",
      " [-] epoch   96/250, train loss 0.412846 in 0.32s\n",
      " [-] epoch   97/250, train loss 0.412499 in 0.36s\n",
      " [-] epoch   98/250, train loss 0.361381 in 0.25s\n",
      " [-] epoch   99/250, train loss 0.398163 in 0.36s\n",
      " [-] epoch  100/250, train loss 0.398469 in 0.29s\n",
      " [-] epoch  101/250, train loss 0.402908 in 0.28s\n",
      " [-] epoch  102/250, train loss 0.396135 in 0.24s\n",
      " [-] epoch  103/250, train loss 0.400524 in 0.34s\n",
      " [-] epoch  104/250, train loss 0.401298 in 0.27s\n",
      " [-] epoch  105/250, train loss 0.371499 in 0.30s\n",
      " [-] epoch  106/250, train loss 0.384209 in 0.31s\n",
      " [-] epoch  107/250, train loss 0.390014 in 0.23s\n",
      " [-] epoch  108/250, train loss 0.383453 in 0.39s\n",
      " [-] epoch  109/250, train loss 0.382335 in 0.31s\n",
      " [-] epoch  110/250, train loss 0.403638 in 0.24s\n",
      " [-] epoch  111/250, train loss 0.382803 in 0.27s\n",
      " [-] epoch  112/250, train loss 0.360300 in 0.22s\n",
      " [-] epoch  113/250, train loss 0.351068 in 0.21s\n",
      " [-] epoch  114/250, train loss 0.354846 in 0.23s\n",
      " [-] epoch  115/250, train loss 0.384177 in 0.27s\n",
      " [-] epoch  116/250, train loss 0.362954 in 0.35s\n",
      " [-] epoch  117/250, train loss 0.353685 in 0.36s\n",
      " [-] epoch  118/250, train loss 0.359102 in 0.34s\n",
      " [-] epoch  119/250, train loss 0.376741 in 0.38s\n",
      " [-] epoch  120/250, train loss 0.410559 in 0.37s\n",
      " [-] epoch  121/250, train loss 0.374811 in 0.31s\n",
      " [-] epoch  122/250, train loss 0.371131 in 0.38s\n",
      " [-] epoch  123/250, train loss 0.391853 in 0.30s\n",
      " [-] epoch  124/250, train loss 0.362355 in 0.32s\n",
      " [-] epoch  125/250, train loss 0.363978 in 0.25s\n",
      " [-] epoch  126/250, train loss 0.381301 in 0.30s\n",
      " [-] epoch  127/250, train loss 0.368575 in 0.31s\n",
      " [-] epoch  128/250, train loss 0.353092 in 0.22s\n",
      " [-] epoch  129/250, train loss 0.391142 in 0.22s\n",
      " [-] epoch  130/250, train loss 0.382701 in 0.22s\n",
      " [-] epoch  131/250, train loss 0.373033 in 0.25s\n",
      " [-] epoch  132/250, train loss 0.383967 in 0.32s\n",
      " [-] epoch  133/250, train loss 0.364072 in 0.37s\n",
      " [-] epoch  134/250, train loss 0.358855 in 0.26s\n",
      " [-] epoch  135/250, train loss 0.402936 in 0.23s\n",
      " [-] epoch  136/250, train loss 0.375951 in 0.25s\n",
      " [-] epoch  137/250, train loss 0.384907 in 0.28s\n",
      " [-] epoch  138/250, train loss 0.361194 in 0.25s\n",
      " [-] epoch  139/250, train loss 0.376524 in 0.28s\n",
      " [-] epoch  140/250, train loss 0.372215 in 0.29s\n",
      " [-] epoch  141/250, train loss 0.380302 in 0.24s\n",
      " [-] epoch  142/250, train loss 0.362453 in 0.31s\n",
      " [-] epoch  143/250, train loss 0.404194 in 0.28s\n",
      " [-] epoch  144/250, train loss 0.367166 in 0.34s\n",
      " [-] epoch  145/250, train loss 0.343739 in 0.25s\n",
      " [-] epoch  146/250, train loss 0.397643 in 0.30s\n",
      " [-] epoch  147/250, train loss 0.405108 in 0.37s\n",
      " [-] epoch  148/250, train loss 0.340560 in 0.36s\n",
      " [-] epoch  149/250, train loss 0.363671 in 0.33s\n",
      " [-] epoch  150/250, train loss 0.383756 in 0.36s\n",
      " [-] epoch  151/250, train loss 0.360851 in 0.27s\n",
      " [-] epoch  152/250, train loss 0.353138 in 0.27s\n",
      " [-] epoch  153/250, train loss 0.358492 in 0.36s\n",
      " [-] epoch  154/250, train loss 0.347577 in 0.29s\n",
      " [-] epoch  155/250, train loss 0.361287 in 0.38s\n",
      " [-] epoch  156/250, train loss 0.397063 in 0.31s\n",
      " [-] epoch  157/250, train loss 0.361824 in 0.22s\n",
      " [-] epoch  158/250, train loss 0.348331 in 0.25s\n",
      " [-] epoch  159/250, train loss 0.370767 in 0.33s\n",
      " [-] epoch  160/250, train loss 0.351778 in 0.35s\n",
      " [-] epoch  161/250, train loss 0.332254 in 0.32s\n",
      " [-] epoch  162/250, train loss 0.390547 in 0.37s\n",
      " [-] epoch  163/250, train loss 0.370511 in 0.37s\n",
      " [-] epoch  164/250, train loss 0.364879 in 0.28s\n",
      " [-] epoch  165/250, train loss 0.333694 in 0.25s\n",
      " [-] epoch  166/250, train loss 0.357217 in 0.25s\n",
      " [-] epoch  167/250, train loss 0.358656 in 0.23s\n",
      " [-] epoch  168/250, train loss 0.332544 in 0.26s\n",
      " [-] epoch  169/250, train loss 0.358926 in 0.29s\n",
      " [-] epoch  170/250, train loss 0.320096 in 0.24s\n",
      " [-] epoch  171/250, train loss 0.341373 in 0.31s\n",
      " [-] epoch  172/250, train loss 0.339790 in 0.22s\n",
      " [-] epoch  173/250, train loss 0.368375 in 0.27s\n",
      " [-] epoch  174/250, train loss 0.343723 in 0.23s\n",
      " [-] epoch  175/250, train loss 0.329465 in 0.24s\n",
      " [-] epoch  176/250, train loss 0.373934 in 0.22s\n",
      " [-] epoch  177/250, train loss 0.336760 in 0.22s\n",
      " [-] epoch  178/250, train loss 0.352797 in 0.23s\n",
      " [-] epoch  179/250, train loss 0.339026 in 0.23s\n",
      " [-] epoch  180/250, train loss 0.372299 in 0.22s\n",
      " [-] epoch  181/250, train loss 0.370046 in 0.22s\n",
      " [-] epoch  182/250, train loss 0.372926 in 0.27s\n",
      " [-] epoch  183/250, train loss 0.362571 in 0.27s\n",
      " [-] epoch  184/250, train loss 0.314798 in 0.25s\n",
      " [-] epoch  185/250, train loss 0.366358 in 0.23s\n",
      " [-] epoch  186/250, train loss 0.385710 in 0.24s\n",
      " [-] epoch  187/250, train loss 0.351613 in 0.36s\n",
      " [-] epoch  188/250, train loss 0.336435 in 0.33s\n",
      " [-] epoch  189/250, train loss 0.349228 in 0.34s\n",
      " [-] epoch  190/250, train loss 0.363921 in 0.34s\n",
      " [-] epoch  191/250, train loss 0.326231 in 0.22s\n",
      " [-] epoch  192/250, train loss 0.340488 in 0.37s\n",
      " [-] epoch  193/250, train loss 0.346876 in 0.35s\n",
      " [-] epoch  194/250, train loss 0.334113 in 0.36s\n",
      " [-] epoch  195/250, train loss 0.364483 in 0.39s\n",
      " [-] epoch  196/250, train loss 0.310067 in 0.34s\n",
      " [-] epoch  197/250, train loss 0.354510 in 0.28s\n",
      " [-] epoch  198/250, train loss 0.343314 in 0.30s\n",
      " [-] epoch  199/250, train loss 0.315949 in 0.30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  200/250, train loss 0.337424 in 0.28s\n",
      " [-] epoch  201/250, train loss 0.373366 in 0.35s\n",
      " [-] epoch  202/250, train loss 0.346265 in 0.27s\n",
      " [-] epoch  203/250, train loss 0.313903 in 0.31s\n",
      " [-] epoch  204/250, train loss 0.344012 in 0.35s\n",
      " [-] epoch  205/250, train loss 0.367528 in 0.23s\n",
      " [-] epoch  206/250, train loss 0.334196 in 0.23s\n",
      " [-] epoch  207/250, train loss 0.315525 in 0.22s\n",
      " [-] epoch  208/250, train loss 0.319748 in 0.22s\n",
      " [-] epoch  209/250, train loss 0.356747 in 0.23s\n",
      " [-] epoch  210/250, train loss 0.365810 in 0.22s\n",
      " [-] epoch  211/250, train loss 0.353806 in 0.23s\n",
      " [-] epoch  212/250, train loss 0.359456 in 0.23s\n",
      " [-] epoch  213/250, train loss 0.350350 in 0.23s\n",
      " [-] epoch  214/250, train loss 0.344139 in 0.23s\n",
      " [-] epoch  215/250, train loss 0.352363 in 0.25s\n",
      " [-] epoch  216/250, train loss 0.331172 in 0.32s\n",
      " [-] epoch  217/250, train loss 0.330197 in 0.22s\n",
      " [-] epoch  218/250, train loss 0.313697 in 0.25s\n",
      " [-] epoch  219/250, train loss 0.348597 in 0.37s\n",
      " [-] epoch  220/250, train loss 0.358375 in 0.31s\n",
      " [-] epoch  221/250, train loss 0.343123 in 0.22s\n",
      " [-] epoch  222/250, train loss 0.335810 in 0.24s\n",
      " [-] epoch  223/250, train loss 0.308543 in 0.24s\n",
      " [-] epoch  224/250, train loss 0.316792 in 0.25s\n",
      " [-] epoch  225/250, train loss 0.312249 in 0.22s\n",
      " [-] epoch  226/250, train loss 0.348994 in 0.23s\n",
      " [-] epoch  227/250, train loss 0.347555 in 0.22s\n",
      " [-] epoch  228/250, train loss 0.323538 in 0.22s\n",
      " [-] epoch  229/250, train loss 0.327751 in 0.22s\n",
      " [-] epoch  230/250, train loss 0.329573 in 0.30s\n",
      " [-] epoch  231/250, train loss 0.332899 in 0.29s\n",
      " [-] epoch  232/250, train loss 0.348483 in 0.26s\n",
      " [-] epoch  233/250, train loss 0.339395 in 0.24s\n",
      " [-] epoch  234/250, train loss 0.327715 in 0.26s\n",
      " [-] epoch  235/250, train loss 0.331427 in 0.24s\n",
      " [-] epoch  236/250, train loss 0.327378 in 0.23s\n",
      " [-] epoch  237/250, train loss 0.349481 in 0.25s\n",
      " [-] epoch  238/250, train loss 0.324827 in 0.24s\n",
      " [-] epoch  239/250, train loss 0.308547 in 0.24s\n",
      " [-] epoch  240/250, train loss 0.307506 in 0.30s\n",
      " [-] epoch  241/250, train loss 0.308354 in 0.33s\n",
      " [-] epoch  242/250, train loss 0.319849 in 0.22s\n",
      " [-] epoch  243/250, train loss 0.295878 in 0.23s\n",
      " [-] epoch  244/250, train loss 0.299960 in 0.31s\n",
      " [-] epoch  245/250, train loss 0.333269 in 0.35s\n",
      " [-] epoch  246/250, train loss 0.311565 in 0.34s\n",
      " [-] epoch  247/250, train loss 0.313532 in 0.39s\n",
      " [-] epoch  248/250, train loss 0.317841 in 0.33s\n",
      " [-] epoch  249/250, train loss 0.313832 in 0.28s\n",
      " [-] epoch  250/250, train loss 0.337437 in 0.36s\n",
      " [-] test acc. 70.833333%\n",
      "Je vais utiliser 9 layers\n",
      " [-] epoch    1/250, train loss 0.651073 in 0.34s\n",
      " [-] epoch    2/250, train loss 0.594954 in 0.25s\n",
      " [-] epoch    3/250, train loss 0.592412 in 0.24s\n",
      " [-] epoch    4/250, train loss 0.577731 in 0.28s\n",
      " [-] epoch    5/250, train loss 0.569396 in 0.24s\n",
      " [-] epoch    6/250, train loss 0.562306 in 0.37s\n",
      " [-] epoch    7/250, train loss 0.543767 in 0.25s\n",
      " [-] epoch    8/250, train loss 0.534880 in 0.34s\n",
      " [-] epoch    9/250, train loss 0.518740 in 0.35s\n",
      " [-] epoch   10/250, train loss 0.525969 in 0.28s\n",
      " [-] epoch   11/250, train loss 0.487774 in 0.28s\n",
      " [-] epoch   12/250, train loss 0.523421 in 0.31s\n",
      " [-] epoch   13/250, train loss 0.499221 in 0.29s\n",
      " [-] epoch   14/250, train loss 0.493418 in 0.34s\n",
      " [-] epoch   15/250, train loss 0.496041 in 0.28s\n",
      " [-] epoch   16/250, train loss 0.511775 in 0.30s\n",
      " [-] epoch   17/250, train loss 0.489115 in 0.30s\n",
      " [-] epoch   18/250, train loss 0.500672 in 0.30s\n",
      " [-] epoch   19/250, train loss 0.503121 in 0.36s\n",
      " [-] epoch   20/250, train loss 0.483569 in 0.38s\n",
      " [-] epoch   21/250, train loss 0.473934 in 0.24s\n",
      " [-] epoch   22/250, train loss 0.460459 in 0.25s\n",
      " [-] epoch   23/250, train loss 0.456513 in 0.26s\n",
      " [-] epoch   24/250, train loss 0.462904 in 0.28s\n",
      " [-] epoch   25/250, train loss 0.491098 in 0.29s\n",
      " [-] epoch   26/250, train loss 0.457854 in 0.36s\n",
      " [-] epoch   27/250, train loss 0.464162 in 0.39s\n",
      " [-] epoch   28/250, train loss 0.511015 in 0.34s\n",
      " [-] epoch   29/250, train loss 0.465595 in 0.26s\n",
      " [-] epoch   30/250, train loss 0.465364 in 0.28s\n",
      " [-] epoch   31/250, train loss 0.482804 in 0.24s\n",
      " [-] epoch   32/250, train loss 0.443962 in 0.24s\n",
      " [-] epoch   33/250, train loss 0.441633 in 0.25s\n",
      " [-] epoch   34/250, train loss 0.487312 in 0.25s\n",
      " [-] epoch   35/250, train loss 0.489082 in 0.24s\n",
      " [-] epoch   36/250, train loss 0.459140 in 0.27s\n",
      " [-] epoch   37/250, train loss 0.491971 in 0.27s\n",
      " [-] epoch   38/250, train loss 0.433068 in 0.29s\n",
      " [-] epoch   39/250, train loss 0.466771 in 0.40s\n",
      " [-] epoch   40/250, train loss 0.435431 in 0.23s\n",
      " [-] epoch   41/250, train loss 0.440991 in 0.31s\n",
      " [-] epoch   42/250, train loss 0.452472 in 0.40s\n",
      " [-] epoch   43/250, train loss 0.464647 in 0.28s\n",
      " [-] epoch   44/250, train loss 0.461104 in 0.26s\n",
      " [-] epoch   45/250, train loss 0.441626 in 0.22s\n",
      " [-] epoch   46/250, train loss 0.438761 in 0.33s\n",
      " [-] epoch   47/250, train loss 0.453340 in 0.36s\n",
      " [-] epoch   48/250, train loss 0.419852 in 0.35s\n",
      " [-] epoch   49/250, train loss 0.462502 in 0.32s\n",
      " [-] epoch   50/250, train loss 0.442381 in 0.22s\n",
      " [-] epoch   51/250, train loss 0.453953 in 0.23s\n",
      " [-] epoch   52/250, train loss 0.442848 in 0.39s\n",
      " [-] epoch   53/250, train loss 0.466799 in 0.27s\n",
      " [-] epoch   54/250, train loss 0.397742 in 0.23s\n",
      " [-] epoch   55/250, train loss 0.456596 in 0.31s\n",
      " [-] epoch   56/250, train loss 0.432460 in 0.26s\n",
      " [-] epoch   57/250, train loss 0.421555 in 0.34s\n",
      " [-] epoch   58/250, train loss 0.431306 in 0.35s\n",
      " [-] epoch   59/250, train loss 0.440691 in 0.33s\n",
      " [-] epoch   60/250, train loss 0.423173 in 0.22s\n",
      " [-] epoch   61/250, train loss 0.411536 in 0.22s\n",
      " [-] epoch   62/250, train loss 0.421141 in 0.23s\n",
      " [-] epoch   63/250, train loss 0.423601 in 0.31s\n",
      " [-] epoch   64/250, train loss 0.435696 in 0.35s\n",
      " [-] epoch   65/250, train loss 0.399058 in 0.29s\n",
      " [-] epoch   66/250, train loss 0.425024 in 0.26s\n",
      " [-] epoch   67/250, train loss 0.419648 in 0.22s\n",
      " [-] epoch   68/250, train loss 0.414300 in 0.24s\n",
      " [-] epoch   69/250, train loss 0.397748 in 0.32s\n",
      " [-] epoch   70/250, train loss 0.417586 in 0.35s\n",
      " [-] epoch   71/250, train loss 0.413152 in 0.35s\n",
      " [-] epoch   72/250, train loss 0.443514 in 0.36s\n",
      " [-] epoch   73/250, train loss 0.407936 in 0.35s\n",
      " [-] epoch   74/250, train loss 0.422101 in 0.35s\n",
      " [-] epoch   75/250, train loss 0.417006 in 0.38s\n",
      " [-] epoch   76/250, train loss 0.436044 in 0.37s\n",
      " [-] epoch   77/250, train loss 0.422089 in 0.39s\n",
      " [-] epoch   78/250, train loss 0.413451 in 0.37s\n",
      " [-] epoch   79/250, train loss 0.412707 in 0.35s\n",
      " [-] epoch   80/250, train loss 0.368389 in 0.34s\n",
      " [-] epoch   81/250, train loss 0.406550 in 0.37s\n",
      " [-] epoch   82/250, train loss 0.397295 in 0.36s\n",
      " [-] epoch   83/250, train loss 0.403454 in 0.38s\n",
      " [-] epoch   84/250, train loss 0.395192 in 0.35s\n",
      " [-] epoch   85/250, train loss 0.382263 in 0.35s\n",
      " [-] epoch   86/250, train loss 0.394422 in 0.38s\n",
      " [-] epoch   87/250, train loss 0.390149 in 0.39s\n",
      " [-] epoch   88/250, train loss 0.406999 in 0.36s\n",
      " [-] epoch   89/250, train loss 0.434480 in 0.37s\n",
      " [-] epoch   90/250, train loss 0.428823 in 0.33s\n",
      " [-] epoch   91/250, train loss 0.386338 in 0.37s\n",
      " [-] epoch   92/250, train loss 0.402770 in 0.38s\n",
      " [-] epoch   93/250, train loss 0.394442 in 0.36s\n",
      " [-] epoch   94/250, train loss 0.391408 in 0.34s\n",
      " [-] epoch   95/250, train loss 0.386659 in 0.39s\n",
      " [-] epoch   96/250, train loss 0.392569 in 0.36s\n",
      " [-] epoch   97/250, train loss 0.397724 in 0.32s\n",
      " [-] epoch   98/250, train loss 0.378812 in 0.38s\n",
      " [-] epoch   99/250, train loss 0.378666 in 0.37s\n",
      " [-] epoch  100/250, train loss 0.380101 in 0.37s\n",
      " [-] epoch  101/250, train loss 0.378190 in 0.35s\n",
      " [-] epoch  102/250, train loss 0.391637 in 0.39s\n",
      " [-] epoch  103/250, train loss 0.406204 in 0.34s\n",
      " [-] epoch  104/250, train loss 0.402845 in 0.36s\n",
      " [-] epoch  105/250, train loss 0.367652 in 0.38s\n",
      " [-] epoch  106/250, train loss 0.368656 in 0.36s\n",
      " [-] epoch  107/250, train loss 0.385381 in 0.38s\n",
      " [-] epoch  108/250, train loss 0.373641 in 0.34s\n",
      " [-] epoch  109/250, train loss 0.384603 in 0.34s\n",
      " [-] epoch  110/250, train loss 0.389544 in 0.33s\n",
      " [-] epoch  111/250, train loss 0.394100 in 0.34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  112/250, train loss 0.389339 in 0.37s\n",
      " [-] epoch  113/250, train loss 0.391114 in 0.40s\n",
      " [-] epoch  114/250, train loss 0.376023 in 0.40s\n",
      " [-] epoch  115/250, train loss 0.366628 in 0.36s\n",
      " [-] epoch  116/250, train loss 0.400709 in 0.39s\n",
      " [-] epoch  117/250, train loss 0.383913 in 0.40s\n",
      " [-] epoch  118/250, train loss 0.374116 in 0.41s\n",
      " [-] epoch  119/250, train loss 0.360409 in 0.37s\n",
      " [-] epoch  120/250, train loss 0.394139 in 0.39s\n",
      " [-] epoch  121/250, train loss 0.372041 in 0.43s\n",
      " [-] epoch  122/250, train loss 0.366583 in 0.39s\n",
      " [-] epoch  123/250, train loss 0.371858 in 0.39s\n",
      " [-] epoch  124/250, train loss 0.372022 in 0.39s\n",
      " [-] epoch  125/250, train loss 0.381439 in 0.36s\n",
      " [-] epoch  126/250, train loss 0.356833 in 0.41s\n",
      " [-] epoch  127/250, train loss 0.376921 in 0.42s\n",
      " [-] epoch  128/250, train loss 0.365302 in 0.43s\n",
      " [-] epoch  129/250, train loss 0.371884 in 0.35s\n",
      " [-] epoch  130/250, train loss 0.356346 in 0.37s\n",
      " [-] epoch  131/250, train loss 0.371279 in 0.39s\n",
      " [-] epoch  132/250, train loss 0.336241 in 0.39s\n",
      " [-] epoch  133/250, train loss 0.364935 in 0.38s\n",
      " [-] epoch  134/250, train loss 0.376406 in 0.33s\n",
      " [-] epoch  135/250, train loss 0.348915 in 0.39s\n",
      " [-] epoch  136/250, train loss 0.359113 in 0.38s\n",
      " [-] epoch  137/250, train loss 0.400710 in 0.37s\n",
      " [-] epoch  138/250, train loss 0.404452 in 0.41s\n",
      " [-] epoch  139/250, train loss 0.359113 in 0.37s\n",
      " [-] epoch  140/250, train loss 0.366447 in 0.37s\n",
      " [-] epoch  141/250, train loss 0.345131 in 0.35s\n",
      " [-] epoch  142/250, train loss 0.336980 in 0.35s\n",
      " [-] epoch  143/250, train loss 0.333258 in 0.38s\n",
      " [-] epoch  144/250, train loss 0.376499 in 0.39s\n",
      " [-] epoch  145/250, train loss 0.386319 in 0.38s\n",
      " [-] epoch  146/250, train loss 0.348172 in 0.38s\n",
      " [-] epoch  147/250, train loss 0.332441 in 0.33s\n",
      " [-] epoch  148/250, train loss 0.366496 in 0.35s\n",
      " [-] epoch  149/250, train loss 0.374967 in 0.38s\n",
      " [-] epoch  150/250, train loss 0.356160 in 0.40s\n",
      " [-] epoch  151/250, train loss 0.346576 in 0.38s\n",
      " [-] epoch  152/250, train loss 0.352564 in 0.37s\n",
      " [-] epoch  153/250, train loss 0.338333 in 0.37s\n",
      " [-] epoch  154/250, train loss 0.347028 in 0.39s\n",
      " [-] epoch  155/250, train loss 0.342099 in 0.38s\n",
      " [-] epoch  156/250, train loss 0.353475 in 0.36s\n",
      " [-] epoch  157/250, train loss 0.379524 in 0.40s\n",
      " [-] epoch  158/250, train loss 0.362653 in 0.38s\n",
      " [-] epoch  159/250, train loss 0.345517 in 0.38s\n",
      " [-] epoch  160/250, train loss 0.364981 in 0.38s\n",
      " [-] epoch  161/250, train loss 0.367946 in 0.36s\n",
      " [-] epoch  162/250, train loss 0.359037 in 0.37s\n",
      " [-] epoch  163/250, train loss 0.333513 in 0.37s\n",
      " [-] epoch  164/250, train loss 0.364143 in 0.40s\n",
      " [-] epoch  165/250, train loss 0.315679 in 0.39s\n",
      " [-] epoch  166/250, train loss 0.372874 in 0.39s\n",
      " [-] epoch  167/250, train loss 0.345931 in 0.37s\n",
      " [-] epoch  168/250, train loss 0.333415 in 0.35s\n",
      " [-] epoch  169/250, train loss 0.372873 in 0.40s\n",
      " [-] epoch  170/250, train loss 0.344362 in 0.40s\n",
      " [-] epoch  171/250, train loss 0.315296 in 0.37s\n",
      " [-] epoch  172/250, train loss 0.335854 in 0.40s\n",
      " [-] epoch  173/250, train loss 0.367693 in 0.39s\n",
      " [-] epoch  174/250, train loss 0.348909 in 0.39s\n",
      " [-] epoch  175/250, train loss 0.370875 in 0.35s\n",
      " [-] epoch  176/250, train loss 0.355556 in 0.37s\n",
      " [-] epoch  177/250, train loss 0.355928 in 0.36s\n",
      " [-] epoch  178/250, train loss 0.341423 in 0.36s\n",
      " [-] epoch  179/250, train loss 0.377483 in 0.36s\n",
      " [-] epoch  180/250, train loss 0.359756 in 0.37s\n",
      " [-] epoch  181/250, train loss 0.352129 in 0.39s\n",
      " [-] epoch  182/250, train loss 0.345707 in 0.40s\n",
      " [-] epoch  183/250, train loss 0.364280 in 0.34s\n",
      " [-] epoch  184/250, train loss 0.331816 in 0.35s\n",
      " [-] epoch  185/250, train loss 0.356562 in 0.36s\n",
      " [-] epoch  186/250, train loss 0.370310 in 0.37s\n",
      " [-] epoch  187/250, train loss 0.336657 in 0.40s\n",
      " [-] epoch  188/250, train loss 0.338146 in 0.37s\n",
      " [-] epoch  189/250, train loss 0.344582 in 0.35s\n",
      " [-] epoch  190/250, train loss 0.330015 in 0.40s\n",
      " [-] epoch  191/250, train loss 0.343200 in 0.35s\n",
      " [-] epoch  192/250, train loss 0.342300 in 0.38s\n",
      " [-] epoch  193/250, train loss 0.358924 in 0.39s\n",
      " [-] epoch  194/250, train loss 0.316022 in 0.36s\n",
      " [-] epoch  195/250, train loss 0.337845 in 0.38s\n",
      " [-] epoch  196/250, train loss 0.352509 in 0.35s\n",
      " [-] epoch  197/250, train loss 0.316482 in 0.39s\n",
      " [-] epoch  198/250, train loss 0.346459 in 0.37s\n",
      " [-] epoch  199/250, train loss 0.330031 in 0.37s\n",
      " [-] epoch  200/250, train loss 0.336397 in 0.39s\n",
      " [-] epoch  201/250, train loss 0.349158 in 0.39s\n",
      " [-] epoch  202/250, train loss 0.335883 in 0.42s\n",
      " [-] epoch  203/250, train loss 0.327349 in 0.35s\n",
      " [-] epoch  204/250, train loss 0.361528 in 0.38s\n",
      " [-] epoch  205/250, train loss 0.368121 in 0.36s\n",
      " [-] epoch  206/250, train loss 0.335068 in 0.40s\n",
      " [-] epoch  207/250, train loss 0.365371 in 0.33s\n",
      " [-] epoch  208/250, train loss 0.315936 in 0.38s\n",
      " [-] epoch  209/250, train loss 0.350493 in 0.38s\n",
      " [-] epoch  210/250, train loss 0.333052 in 0.38s\n",
      " [-] epoch  211/250, train loss 0.321166 in 0.34s\n",
      " [-] epoch  212/250, train loss 0.341669 in 0.40s\n",
      " [-] epoch  213/250, train loss 0.326335 in 0.36s\n",
      " [-] epoch  214/250, train loss 0.336000 in 0.38s\n",
      " [-] epoch  215/250, train loss 0.334135 in 0.40s\n",
      " [-] epoch  216/250, train loss 0.299107 in 0.37s\n",
      " [-] epoch  217/250, train loss 0.331017 in 0.38s\n",
      " [-] epoch  218/250, train loss 0.353071 in 0.37s\n",
      " [-] epoch  219/250, train loss 0.319688 in 0.40s\n",
      " [-] epoch  220/250, train loss 0.332889 in 0.37s\n",
      " [-] epoch  221/250, train loss 0.319799 in 0.35s\n",
      " [-] epoch  222/250, train loss 0.342724 in 0.37s\n",
      " [-] epoch  223/250, train loss 0.335065 in 0.37s\n",
      " [-] epoch  224/250, train loss 0.331165 in 0.38s\n",
      " [-] epoch  225/250, train loss 0.337372 in 0.39s\n",
      " [-] epoch  226/250, train loss 0.339999 in 0.39s\n",
      " [-] epoch  227/250, train loss 0.307122 in 0.39s\n",
      " [-] epoch  228/250, train loss 0.310301 in 0.34s\n",
      " [-] epoch  229/250, train loss 0.346350 in 0.37s\n",
      " [-] epoch  230/250, train loss 0.346630 in 0.38s\n",
      " [-] epoch  231/250, train loss 0.331739 in 0.35s\n",
      " [-] epoch  232/250, train loss 0.346211 in 0.37s\n",
      " [-] epoch  233/250, train loss 0.326490 in 0.41s\n",
      " [-] epoch  234/250, train loss 0.302523 in 0.37s\n",
      " [-] epoch  235/250, train loss 0.325345 in 0.36s\n",
      " [-] epoch  236/250, train loss 0.327803 in 0.39s\n",
      " [-] epoch  237/250, train loss 0.338968 in 0.37s\n",
      " [-] epoch  238/250, train loss 0.326454 in 0.39s\n",
      " [-] epoch  239/250, train loss 0.302502 in 0.36s\n",
      " [-] epoch  240/250, train loss 0.309926 in 0.33s\n",
      " [-] epoch  241/250, train loss 0.294158 in 0.30s\n",
      " [-] epoch  242/250, train loss 0.309748 in 0.24s\n",
      " [-] epoch  243/250, train loss 0.308918 in 0.24s\n",
      " [-] epoch  244/250, train loss 0.325157 in 0.23s\n",
      " [-] epoch  245/250, train loss 0.327535 in 0.23s\n",
      " [-] epoch  246/250, train loss 0.336336 in 0.23s\n",
      " [-] epoch  247/250, train loss 0.291100 in 0.22s\n",
      " [-] epoch  248/250, train loss 0.319421 in 0.24s\n",
      " [-] epoch  249/250, train loss 0.329752 in 0.23s\n",
      " [-] epoch  250/250, train loss 0.352561 in 0.23s\n",
      " [-] test acc. 66.666667%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_linear_results(X_train_reduced, X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Expérimentations - Déclaration d'un réseau de neurones de type convolution\n",
    "Cette partie agit à titre d'expérimentation.\n",
    "\n",
    "Le réseau de neurones à convolution est facile à paramétrer: il est possible d'y indiquer, en paramètre d'entrée, le nombre de filtres de convolution à rajouter à l'implémentation de base (pour augmenter la complexité du réseau à notre guise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau par convolution permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numberOfFilters):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.numberOfFilters = numberOfFilters\n",
    "        \n",
    "        self.C1 = nn.Conv1d(1, 32, kernel_size=251, stride=2, padding=125, bias=False)\n",
    "        self.B1 = nn.BatchNorm1d(32)\n",
    "        self.C2 = nn.Conv1d(32, 64, kernel_size=175, stride=2, padding=87, bias=False)\n",
    "        \n",
    "        self.B = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.C = nn.Conv1d(64, 64, kernel_size=175, stride=2, padding=87, bias=False)\n",
    "        \n",
    "        self.activation = nn.ELU()\n",
    "        self.F1 = nn.Linear(64, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Sélectionne la taille batch à l'entrée\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.activation(self.B1(self.C1(x)))\n",
    "        x = self.activation(self.B(self.C2(x)))\n",
    "        \n",
    "        for i in range(self.numberOfFilters):\n",
    "            x = self.activation(self.B(self.C(x)))\n",
    "        \n",
    "        # Fait un average pooling sur les caractéristiques\n",
    "        # de chaque filtre\n",
    "        x = x.view(batch_size, 64, -1).mean(dim=2)\n",
    "        \n",
    "        # Couche lineaire et sigmoide\n",
    "        x = self.F1(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Déclaration de la fonction permettant la classification par réseau de neurones profond de type convolution\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_for_convolution(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        values, targets = batch\n",
    "        values = values.unsqueeze(1)\n",
    "        values = values.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(values)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()\n",
    "\n",
    "def compute_convolution_results(X_train, X_test):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 10\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "    \n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "#     test_accu.append(0)\n",
    "#     test_accu.append(0)\n",
    "#     test_accu.append(0)\n",
    "#     test_accu.append(0)\n",
    "#     test_accu.append(0)\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        print(\"Je vais ajouter \" + str(i) + \" filters\")\n",
    "        \n",
    "        # Instancier un réseau RAMQNetConvolution\n",
    "        # dans une variable nommée \"model\"\n",
    "        model = RAMQNetConvolution(i)\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "\n",
    "                values = values.unsqueeze(1)\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy_for_convolution(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Calcul du taux de réussite en classement d'un réseau de convolution sur les données non réduites - référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais ajouter 1 filters\n",
      " [-] epoch    1/10, train loss 0.649880 in 2.48s\n",
      " [-] epoch    2/10, train loss 0.541793 in 2.43s\n",
      " [-] epoch    3/10, train loss 0.460036 in 2.44s\n",
      " [-] epoch    4/10, train loss 0.356663 in 2.50s\n",
      " [-] epoch    5/10, train loss 0.315211 in 2.56s\n",
      " [-] epoch    6/10, train loss 0.319489 in 2.54s\n",
      " [-] epoch    7/10, train loss 0.334724 in 2.54s\n",
      " [-] epoch    8/10, train loss 0.289350 in 2.54s\n",
      " [-] epoch    9/10, train loss 0.303419 in 2.57s\n",
      " [-] epoch   10/10, train loss 0.249432 in 2.57s\n",
      " [-] test acc. 55.000000%\n",
      "Je vais ajouter 2 filters\n",
      " [-] epoch    1/10, train loss 0.603194 in 3.13s\n",
      " [-] epoch    2/10, train loss 0.455592 in 3.13s\n",
      " [-] epoch    3/10, train loss 0.393886 in 3.11s\n",
      " [-] epoch    4/10, train loss 0.328554 in 3.11s\n",
      " [-] epoch    5/10, train loss 0.343695 in 3.10s\n",
      " [-] epoch    6/10, train loss 0.291574 in 3.11s\n",
      " [-] epoch    7/10, train loss 0.278741 in 3.11s\n",
      " [-] epoch    8/10, train loss 0.288595 in 3.11s\n",
      " [-] epoch    9/10, train loss 0.272868 in 3.12s\n",
      " [-] epoch   10/10, train loss 0.268388 in 3.10s\n",
      " [-] test acc. 58.055556%\n",
      "Je vais ajouter 3 filters\n",
      " [-] epoch    1/10, train loss 0.517566 in 3.47s\n",
      " [-] epoch    2/10, train loss 0.491429 in 3.47s\n",
      " [-] epoch    3/10, train loss 0.397448 in 3.47s\n",
      " [-] epoch    4/10, train loss 0.340303 in 3.49s\n",
      " [-] epoch    5/10, train loss 0.343636 in 3.49s\n",
      " [-] epoch    6/10, train loss 0.318452 in 3.50s\n",
      " [-] epoch    7/10, train loss 0.298089 in 3.49s\n",
      " [-] epoch    8/10, train loss 0.291876 in 3.50s\n",
      " [-] epoch    9/10, train loss 0.264229 in 3.49s\n",
      " [-] epoch   10/10, train loss 0.270122 in 3.50s\n",
      " [-] test acc. 77.777778%\n",
      "Je vais ajouter 4 filters\n",
      " [-] epoch    1/10, train loss 0.581412 in 3.77s\n",
      " [-] epoch    2/10, train loss 0.407785 in 3.77s\n",
      " [-] epoch    3/10, train loss 0.361436 in 3.77s\n",
      " [-] epoch    4/10, train loss 0.376506 in 3.78s\n",
      " [-] epoch    5/10, train loss 0.290593 in 3.76s\n",
      " [-] epoch    6/10, train loss 0.364835 in 3.77s\n",
      " [-] epoch    7/10, train loss 0.328539 in 3.76s\n",
      " [-] epoch    8/10, train loss 0.305524 in 3.76s\n",
      " [-] epoch    9/10, train loss 0.262570 in 3.78s\n",
      " [-] epoch   10/10, train loss 0.294046 in 3.77s\n",
      " [-] test acc. 84.444444%\n",
      "Je vais ajouter 5 filters\n",
      " [-] epoch    1/10, train loss 0.532621 in 4.02s\n",
      " [-] epoch    2/10, train loss 0.415733 in 4.04s\n",
      " [-] epoch    3/10, train loss 0.339706 in 4.04s\n",
      " [-] epoch    4/10, train loss 0.344659 in 4.05s\n",
      " [-] epoch    5/10, train loss 0.325331 in 4.05s\n",
      " [-] epoch    6/10, train loss 0.319243 in 4.04s\n",
      " [-] epoch    7/10, train loss 0.292311 in 4.03s\n",
      " [-] epoch    8/10, train loss 0.289920 in 4.03s\n",
      " [-] epoch    9/10, train loss 0.252309 in 4.03s\n",
      " [-] epoch   10/10, train loss 0.264598 in 4.04s\n",
      " [-] test acc. 85.833333%\n",
      "Je vais ajouter 6 filters\n",
      " [-] epoch    1/10, train loss 0.499379 in 4.30s\n",
      " [-] epoch    2/10, train loss 0.393617 in 4.30s\n",
      " [-] epoch    3/10, train loss 0.331544 in 4.29s\n",
      " [-] epoch    4/10, train loss 0.323533 in 4.30s\n",
      " [-] epoch    5/10, train loss 0.304360 in 4.30s\n",
      " [-] epoch    6/10, train loss 0.321253 in 4.31s\n",
      " [-] epoch    7/10, train loss 0.277048 in 4.29s\n",
      " [-] epoch    8/10, train loss 0.211903 in 4.30s\n",
      " [-] epoch    9/10, train loss 0.282800 in 4.30s\n",
      " [-] epoch   10/10, train loss 0.270342 in 4.31s\n",
      " [-] test acc. 85.277778%\n",
      "Je vais ajouter 7 filters\n",
      " [-] epoch    1/10, train loss 0.543922 in 4.57s\n",
      " [-] epoch    2/10, train loss 0.366147 in 4.56s\n",
      " [-] epoch    3/10, train loss 0.337414 in 4.57s\n",
      " [-] epoch    4/10, train loss 0.327078 in 4.57s\n",
      " [-] epoch    5/10, train loss 0.352235 in 4.58s\n",
      " [-] epoch    6/10, train loss 0.294339 in 4.57s\n",
      " [-] epoch    7/10, train loss 0.295298 in 4.56s\n",
      " [-] epoch    8/10, train loss 0.265616 in 4.55s\n",
      " [-] epoch    9/10, train loss 0.283510 in 4.56s\n",
      " [-] epoch   10/10, train loss 0.263580 in 4.57s\n",
      " [-] test acc. 86.666667%\n",
      "Je vais ajouter 8 filters\n",
      " [-] epoch    1/10, train loss 0.514821 in 4.84s\n",
      " [-] epoch    2/10, train loss 0.360880 in 4.84s\n",
      " [-] epoch    3/10, train loss 0.338356 in 4.86s\n",
      " [-] epoch    4/10, train loss 0.311854 in 4.84s\n",
      " [-] epoch    5/10, train loss 0.305808 in 4.78s\n",
      " [-] epoch    6/10, train loss 0.311377 in 4.75s\n",
      " [-] epoch    7/10, train loss 0.259008 in 4.73s\n",
      " [-] epoch    8/10, train loss 0.267765 in 4.75s\n",
      " [-] epoch    9/10, train loss 0.255818 in 4.79s\n",
      " [-] epoch   10/10, train loss 0.241468 in 4.82s\n",
      " [-] test acc. 82.500000%\n",
      "Je vais ajouter 9 filters\n",
      " [-] epoch    1/10, train loss 0.552294 in 5.09s\n",
      " [-] epoch    2/10, train loss 0.433084 in 5.09s\n",
      " [-] epoch    3/10, train loss 0.344251 in 5.08s\n",
      " [-] epoch    4/10, train loss 0.326106 in 5.09s\n",
      " [-] epoch    5/10, train loss 0.336131 in 5.08s\n",
      " [-] epoch    6/10, train loss 0.302841 in 5.08s\n",
      " [-] epoch    7/10, train loss 0.257334 in 5.09s\n",
      " [-] epoch    8/10, train loss 0.298213 in 5.10s\n",
      " [-] epoch    9/10, train loss 0.220388 in 5.08s\n",
      " [-] epoch   10/10, train loss 0.257935 in 5.08s\n",
      " [-] test acc. 82.500000%\n",
      "Je vais ajouter 10 filters\n",
      " [-] epoch    1/10, train loss 0.552617 in 5.36s\n",
      " [-] epoch    2/10, train loss 0.435564 in 5.37s\n",
      " [-] epoch    3/10, train loss 0.388047 in 5.37s\n",
      " [-] epoch    4/10, train loss 0.306031 in 5.37s\n",
      " [-] epoch    5/10, train loss 0.352601 in 5.38s\n",
      " [-] epoch    6/10, train loss 0.258962 in 5.38s\n",
      " [-] epoch    7/10, train loss 0.254442 in 5.37s\n",
      " [-] epoch    8/10, train loss 0.274880 in 5.36s\n",
      " [-] epoch    9/10, train loss 0.252755 in 5.35s\n",
      " [-] epoch   10/10, train loss 0.275208 in 5.37s\n",
      " [-] test acc. 80.833333%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_convolution_results(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Calcul du taux de réussite en classement d'un réseau de convolution sur les données réduites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais ajouter 1 filters\n",
      " [-] epoch    1/10, train loss 0.661160 in 0.70s\n",
      " [-] epoch    2/10, train loss 0.625633 in 0.70s\n",
      " [-] epoch    3/10, train loss 0.567836 in 0.59s\n",
      " [-] epoch    4/10, train loss 0.570159 in 0.59s\n",
      " [-] epoch    5/10, train loss 0.548566 in 0.60s\n",
      " [-] epoch    6/10, train loss 0.544793 in 0.59s\n",
      " [-] epoch    7/10, train loss 0.537454 in 0.77s\n",
      " [-] epoch    8/10, train loss 0.537653 in 0.75s\n",
      " [-] epoch    9/10, train loss 0.526692 in 0.67s\n",
      " [-] epoch   10/10, train loss 0.511782 in 0.66s\n",
      " [-] test acc. 67.500000%\n",
      "Je vais ajouter 2 filters\n",
      " [-] epoch    1/10, train loss 0.662841 in 1.00s\n",
      " [-] epoch    2/10, train loss 0.658505 in 0.99s\n",
      " [-] epoch    3/10, train loss 0.602539 in 0.99s\n",
      " [-] epoch    4/10, train loss 0.614907 in 1.00s\n",
      " [-] epoch    5/10, train loss 0.555207 in 0.98s\n",
      " [-] epoch    6/10, train loss 0.590134 in 0.99s\n",
      " [-] epoch    7/10, train loss 0.555016 in 0.99s\n",
      " [-] epoch    8/10, train loss 0.545348 in 0.99s\n",
      " [-] epoch    9/10, train loss 0.533741 in 0.99s\n",
      " [-] epoch   10/10, train loss 0.543162 in 0.97s\n",
      " [-] test acc. 74.722222%\n",
      "Je vais ajouter 3 filters\n",
      " [-] epoch    1/10, train loss 0.679484 in 1.27s\n",
      " [-] epoch    2/10, train loss 0.637086 in 1.27s\n",
      " [-] epoch    3/10, train loss 0.584668 in 1.23s\n",
      " [-] epoch    4/10, train loss 0.586652 in 1.22s\n",
      " [-] epoch    5/10, train loss 0.559330 in 1.18s\n",
      " [-] epoch    6/10, train loss 0.592010 in 1.18s\n",
      " [-] epoch    7/10, train loss 0.528097 in 1.19s\n",
      " [-] epoch    8/10, train loss 0.514954 in 1.22s\n",
      " [-] epoch    9/10, train loss 0.537171 in 1.23s\n",
      " [-] epoch   10/10, train loss 0.538888 in 1.21s\n",
      " [-] test acc. 72.777778%\n",
      "Je vais ajouter 4 filters\n",
      " [-] epoch    1/10, train loss 0.673842 in 1.45s\n",
      " [-] epoch    2/10, train loss 0.615999 in 1.47s\n",
      " [-] epoch    3/10, train loss 0.587589 in 1.46s\n",
      " [-] epoch    4/10, train loss 0.570029 in 1.47s\n",
      " [-] epoch    5/10, train loss 0.585758 in 1.46s\n",
      " [-] epoch    6/10, train loss 0.563709 in 1.46s\n",
      " [-] epoch    7/10, train loss 0.537904 in 1.43s\n",
      " [-] epoch    8/10, train loss 0.535711 in 1.44s\n",
      " [-] epoch    9/10, train loss 0.528165 in 1.45s\n",
      " [-] epoch   10/10, train loss 0.524048 in 1.43s\n",
      " [-] test acc. 60.555556%\n",
      "Je vais ajouter 5 filters\n",
      " [-] epoch    1/10, train loss 0.680918 in 1.70s\n",
      " [-] epoch    2/10, train loss 0.608963 in 1.70s\n",
      " [-] epoch    3/10, train loss 0.568188 in 1.70s\n",
      " [-] epoch    4/10, train loss 0.570835 in 1.69s\n",
      " [-] epoch    5/10, train loss 0.546046 in 1.68s\n",
      " [-] epoch    6/10, train loss 0.538222 in 1.69s\n",
      " [-] epoch    7/10, train loss 0.559036 in 1.71s\n",
      " [-] epoch    8/10, train loss 0.512045 in 1.70s\n",
      " [-] epoch    9/10, train loss 0.516377 in 1.70s\n",
      " [-] epoch   10/10, train loss 0.522138 in 1.68s\n",
      " [-] test acc. 63.611111%\n",
      "Je vais ajouter 6 filters\n",
      " [-] epoch    1/10, train loss 0.654690 in 1.92s\n",
      " [-] epoch    2/10, train loss 0.629343 in 1.92s\n",
      " [-] epoch    3/10, train loss 0.579943 in 1.93s\n",
      " [-] epoch    4/10, train loss 0.563419 in 1.94s\n",
      " [-] epoch    5/10, train loss 0.531927 in 1.92s\n",
      " [-] epoch    6/10, train loss 0.510868 in 1.93s\n",
      " [-] epoch    7/10, train loss 0.548094 in 1.92s\n",
      " [-] epoch    8/10, train loss 0.563637 in 1.92s\n",
      " [-] epoch    9/10, train loss 0.514224 in 1.92s\n",
      " [-] epoch   10/10, train loss 0.487423 in 1.92s\n",
      " [-] test acc. 65.555556%\n",
      "Je vais ajouter 7 filters\n",
      " [-] epoch    1/10, train loss 0.686254 in 2.20s\n",
      " [-] epoch    2/10, train loss 0.631619 in 2.22s\n",
      " [-] epoch    3/10, train loss 0.587930 in 2.25s\n",
      " [-] epoch    4/10, train loss 0.563340 in 2.26s\n",
      " [-] epoch    5/10, train loss 0.534514 in 2.20s\n",
      " [-] epoch    6/10, train loss 0.568419 in 2.20s\n",
      " [-] epoch    7/10, train loss 0.525509 in 2.20s\n",
      " [-] epoch    8/10, train loss 0.543243 in 2.21s\n",
      " [-] epoch    9/10, train loss 0.509072 in 2.20s\n",
      " [-] epoch   10/10, train loss 0.528269 in 2.21s\n",
      " [-] test acc. 52.500000%\n",
      "Je vais ajouter 8 filters\n",
      " [-] epoch    1/10, train loss 0.673726 in 2.44s\n",
      " [-] epoch    2/10, train loss 0.582459 in 2.44s\n",
      " [-] epoch    3/10, train loss 0.553514 in 2.43s\n",
      " [-] epoch    4/10, train loss 0.539030 in 2.43s\n",
      " [-] epoch    5/10, train loss 0.549438 in 2.44s\n",
      " [-] epoch    6/10, train loss 0.525724 in 2.43s\n",
      " [-] epoch    7/10, train loss 0.549764 in 2.45s\n",
      " [-] epoch    8/10, train loss 0.530120 in 2.47s\n",
      " [-] epoch    9/10, train loss 0.472397 in 2.46s\n",
      " [-] epoch   10/10, train loss 0.497650 in 2.46s\n",
      " [-] test acc. 65.555556%\n",
      "Je vais ajouter 9 filters\n",
      " [-] epoch    1/10, train loss 0.684872 in 2.74s\n",
      " [-] epoch    2/10, train loss 0.635169 in 2.74s\n",
      " [-] epoch    3/10, train loss 0.588207 in 2.75s\n",
      " [-] epoch    4/10, train loss 0.537349 in 2.74s\n",
      " [-] epoch    5/10, train loss 0.582155 in 2.75s\n",
      " [-] epoch    6/10, train loss 0.556075 in 2.73s\n",
      " [-] epoch    7/10, train loss 0.474268 in 2.73s\n",
      " [-] epoch    8/10, train loss 0.536348 in 2.73s\n",
      " [-] epoch    9/10, train loss 0.535489 in 2.76s\n",
      " [-] epoch   10/10, train loss 0.496138 in 2.76s\n",
      " [-] test acc. 51.944444%\n",
      "Je vais ajouter 10 filters\n",
      " [-] epoch    1/10, train loss 0.652412 in 2.94s\n",
      " [-] epoch    2/10, train loss 0.602708 in 2.95s\n",
      " [-] epoch    3/10, train loss 0.591728 in 2.96s\n",
      " [-] epoch    4/10, train loss 0.528929 in 2.96s\n",
      " [-] epoch    5/10, train loss 0.557661 in 2.97s\n",
      " [-] epoch    6/10, train loss 0.502085 in 2.96s\n",
      " [-] epoch    7/10, train loss 0.499977 in 2.96s\n",
      " [-] epoch    8/10, train loss 0.492027 in 2.99s\n",
      " [-] epoch    9/10, train loss 0.504996 in 2.98s\n",
      " [-] epoch   10/10, train loss 0.512136 in 2.94s\n",
      " [-] test acc. 53.611111%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_convolution_results(X_train_reduced, X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

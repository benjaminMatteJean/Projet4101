{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning pour Classification\n",
    "\n",
    "Ce notebook regroupe quelques configurations d'algorithmes de Deep Learning pour la classification de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Variable                        Possibilités             Nombre de colonne (+1)\n",
      "Spécialité de médecins\t        33 codes de spécialité\t        1-33\n",
      "Sexe de médecin \t            2\t                            34-35\n",
      "Langue de correspondance \t    2\t                            36-37\n",
      "Université de graduation \t    14 universités\t                38-51\n",
      "Plage horaire de facturation\t3 (AM, PM, toute la journée)\t52-54\n",
      "Agence de représentation\t    845 codes d’agences             55-899\n",
      "Établissements                  241 établissements\t            900-1140\n",
      "ActeMedical                     2 (Oui et Non)\t                1141-1142\n",
      "Activités médico-admin          2 (Oui et Non)\t                1143-1144\n",
      "Activités d’enseignement\t    2 (Oui et Non)\t                1145-1146\n",
      "Entente LE\t                    2 (Oui et Non)\t                1147-1148\n",
      "Activité en santé communautaire\t2 (Oui et Non)\t                1149-1150\n",
      "Activité en santé comm. CSST\t2 (Oui et Non)\t                1151-1152\n",
      "Activité en santé comm. INSP\t2 (Oui et Non)\t                1153-1154\n",
      "Année de graduation\t                                            1155\n",
      "semaine de l’année associée à la facturation\t                1156\n",
      "Nombre de jours dès la dernière facture avant le Perdiem\t    1157\n",
      "Année de naissance\t                                            1158\n",
      "Année de début de pratique\t                                    1159\n",
      "Expérience\t                                                    1160\n",
      "Salaire régulier\t                                            1161\n",
      "Salaire payé\t                                                1162\n",
      "Nombre de Perdiem pendant l’année\t                            1163\n",
      "Jour de la semaine associé à la facturation\t                    1164\n",
      "Nombre de Perdiem\t                                            1165\n",
      "Nombre d’heures facturées\t                                    1166\n",
      "Nombre de services avec bénéficiaire\t                        1167\n",
      "Nombre de services sans bénéficiaire\t                        1168\n",
      "Nombre de patients\t                                            1169\n",
      "Montant réclamé avec bénéficiaire\t                            1170\n",
      "Montant réclamé sans bénéficiaire\t                            1171\n",
      "Nombre de service total\t                                        1172\n",
      "Nombre de Perdiem facturé le jour avant\t                        1173\n",
      "Nombre d’heures facturées le jour avant\t                        1174\n",
      "Nombre de services avec bénéficiaire le jour avant\t            1175\n",
      "Nombre de services sans bénéficiaire le jour avant\t            1176\n",
      "Nombre de patients le jour avant\t                            1177\n",
      "Montant réclamé avec bénéficiaire le jour avant\t                1178\n",
      "Montant réclamé sans bénéficiaire le jour avant\t                1179\n",
      "Nombre de services total le jour avant\t                        1180\n",
      "âge de médecin le jour avant \t                                1181\n",
      "Nombre de Perdiem facturés la semaine d’avant\t                1182\n",
      "\n",
      "Target:\n",
      "Conformité\n",
      "(-1 : non contrôlé, 1 : cas conforme, 0 : cas non-conforme)\t    1183\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData = numpy.load('labeled.npy')\n",
    "numberOfData = labeledData.shape[0] # 1441\n",
    "dimensions = labeledData.shape[1] # 1183\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train = minmax_scale(labeledData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test = minmax_scale(labeledData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données\n",
    "\n",
    "def create_balanced_sampler(dataset):\n",
    "    def make_weights_for_balanced_classes(images, n_classes):                        \n",
    "        count = [0] * n_classes                                                      \n",
    "        for item in images:                                                         \n",
    "            count[int(item[1])] += 1                                                     \n",
    "        weight_per_class = [0.] * n_classes                                      \n",
    "        N = float(sum(count))                                                   \n",
    "        for i in range(n_classes):                                                   \n",
    "            weight_per_class[i] = N/float(count[i])                                 \n",
    "        weight = [0] * len(images)                                              \n",
    "        for idx, val in enumerate(images):                                          \n",
    "            weight[idx] = weight_per_class[int(val[1])]                                  \n",
    "        return weight\n",
    "\n",
    "    n_classes = numpy.unique(dataset.targets)\n",
    "    weights = make_weights_for_balanced_classes(dataset.data, len(n_classes))                                                         \n",
    "    weights = torch.DoubleTensor(weights)                 \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    return sampler\n",
    "\n",
    "def compute_accuracy(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Cette classe sert à définir le dataset RAMQ pour PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for elem in data:\n",
    "            if isinstance(elem, numpy.ndarray):\n",
    "                elem = elem.tolist()\n",
    "                elem_data = torch.Tensor(elem[:-1])\n",
    "                elem_target = torch.FloatTensor([elem[-1]])\n",
    "            # garde les paramètres en mémoire\n",
    "            self.data += [(elem_data, elem_target[0])]\n",
    "            \n",
    "        self.targets = numpy.array(data[:, -1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_layers, activation = nn.ELU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        reduction_ratio = (1/n_inputs)**(1/n_layers)\n",
    "        \n",
    "        layers = []\n",
    "        layer_lenght = n_inputs\n",
    "        \n",
    "        # Calculate nb of layers and nb of neurons\n",
    "        # Builds a list (ex:[1000, 500, 250, ... until 1])\n",
    "        while layer_lenght > 1:\n",
    "            layers.append(layer_lenght)\n",
    "            layer_lenght = int(layer_lenght*reduction_ratio)\n",
    "        layers.append(1)\n",
    "        \n",
    "        # Build encoder layers\n",
    "        network_layers = []\n",
    "        for i, n_neurons in enumerate(layers[:-1]):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            network_layers.append(layer)\n",
    "            network_layers.append(activation)\n",
    "            network_layers.append(nn.BatchNorm1d(layers[i+1]))\n",
    "            # encoder_layers.append(nn.Dropout(0.5))\n",
    "        network_layers[-1] = nn.Sigmoid()\n",
    "            \n",
    "        # Build pytorch sequential network\n",
    "        self.network = nn.Sequential(\n",
    "            *network_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Exécute l'inférence du réseau. L'ordre \n",
    "        # d'exécution ici est important.\n",
    "        x = self.network(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNet2(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO Q1B\n",
    "        # Initialiser ici les modules contenant des \n",
    "        # paramètres à optimiser. Ces modules seront\n",
    "        # utilisés dans la méthode 'forward'\n",
    "        \n",
    "        self.F1 = nn.Linear(1182, 1)\n",
    "#         self.F2 = nn.Linear(590, 295)\n",
    "#         self.F3 = nn.Linear(295, 147)\n",
    "#         self.F4 = nn.Linear(147, 74)\n",
    "#         self.F5 = nn.Linear(74, 37)\n",
    "#         self.F6 = nn.Linear(37, 18)\n",
    "#         self.F7 = nn.Linear(18, 9)\n",
    "#         self.F8 = nn.Linear(9, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Exécute l'inférence du réseau. L'ordre \n",
    "        # d'exécution ici est important.\n",
    "        x = self.F1(x)\n",
    "#         x = F.elu(self.F2(x))\n",
    "#         x = F.elu(self.F3(x))\n",
    "#         x = F.elu(self.F4(x))\n",
    "#         x = F.elu(self.F5(x))\n",
    "#         x = F.elu(self.F6(x))\n",
    "#         x = F.elu(self.F7(x))\n",
    "#         x = F.elu(self.F8(x))\n",
    "        \n",
    "        # Couche sigmoide\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification sans fonction d'activation de 1182 dimensions à 1: +- 84% de taux de réussite\n",
    "\n",
    "Classification avec fonction d'activation elu à 9 couches: +- 82% de taux de réussite\n",
    "\n",
    "Classification avec fonction d'activation relu à 9 couches: +- 50% de taux de réussite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(X_train, X_test):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cpu\" #if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 250\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "\n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "\n",
    "    # Instancier un réseau RAMQNet\n",
    "    # dans une variable nommée \"model\"\n",
    "    for i in range(1,10):\n",
    "        print(\"Je vais utiliser \" + str(i) + \" layers\")\n",
    "        model = RAMQNet(1182, i)\n",
    "\n",
    "        # Tranfert le réseau au bon endroit\n",
    "        model.to(device)\n",
    "\n",
    "        # Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "        # et la mettre dans une variable nommée criterion\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        # Ne pas oublier de lui donner les hyperparamètres\n",
    "        # d'entraînement : learning rate et momentum!\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer\n",
    "                # une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(test_accu):\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(test_accu, label='Test accuracy')\n",
    "    ax.set_xlim(1, 9)\n",
    "    pyplot.grid()\n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(X_train, X_test) # all dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.684016 in 0.04s\n",
      " [-] epoch    2/250, train loss 0.661862 in 0.04s\n",
      " [-] epoch    3/250, train loss 0.640795 in 0.03s\n",
      " [-] epoch    4/250, train loss 0.614888 in 0.03s\n",
      " [-] epoch    5/250, train loss 0.608405 in 0.03s\n",
      " [-] epoch    6/250, train loss 0.593249 in 0.03s\n",
      " [-] epoch    7/250, train loss 0.576537 in 0.03s\n",
      " [-] epoch    8/250, train loss 0.567168 in 0.03s\n",
      " [-] epoch    9/250, train loss 0.564787 in 0.03s\n",
      " [-] epoch   10/250, train loss 0.559755 in 0.03s\n",
      " [-] epoch   11/250, train loss 0.555342 in 0.03s\n",
      " [-] epoch   12/250, train loss 0.547373 in 0.03s\n",
      " [-] epoch   13/250, train loss 0.549609 in 0.02s\n",
      " [-] epoch   14/250, train loss 0.540102 in 0.05s\n",
      " [-] epoch   15/250, train loss 0.522033 in 0.03s\n",
      " [-] epoch   16/250, train loss 0.518664 in 0.03s\n",
      " [-] epoch   17/250, train loss 0.524882 in 0.03s\n",
      " [-] epoch   18/250, train loss 0.503531 in 0.03s\n",
      " [-] epoch   19/250, train loss 0.521698 in 0.03s\n",
      " [-] epoch   20/250, train loss 0.521270 in 0.03s\n",
      " [-] epoch   21/250, train loss 0.502487 in 0.03s\n",
      " [-] epoch   22/250, train loss 0.534969 in 0.03s\n",
      " [-] epoch   23/250, train loss 0.496650 in 0.03s\n",
      " [-] epoch   24/250, train loss 0.494238 in 0.03s\n",
      " [-] epoch   25/250, train loss 0.501529 in 0.03s\n",
      " [-] epoch   26/250, train loss 0.510826 in 0.03s\n",
      " [-] epoch   27/250, train loss 0.501441 in 0.03s\n",
      " [-] epoch   28/250, train loss 0.492484 in 0.03s\n",
      " [-] epoch   29/250, train loss 0.482536 in 0.04s\n",
      " [-] epoch   30/250, train loss 0.471378 in 0.03s\n",
      " [-] epoch   31/250, train loss 0.509099 in 0.03s\n",
      " [-] epoch   32/250, train loss 0.490442 in 0.03s\n",
      " [-] epoch   33/250, train loss 0.488804 in 0.03s\n",
      " [-] epoch   34/250, train loss 0.490623 in 0.04s\n",
      " [-] epoch   35/250, train loss 0.484788 in 0.03s\n",
      " [-] epoch   36/250, train loss 0.482819 in 0.03s\n",
      " [-] epoch   37/250, train loss 0.481675 in 0.03s\n",
      " [-] epoch   38/250, train loss 0.471772 in 0.03s\n",
      " [-] epoch   39/250, train loss 0.485897 in 0.03s\n",
      " [-] epoch   40/250, train loss 0.471332 in 0.03s\n",
      " [-] epoch   41/250, train loss 0.476712 in 0.03s\n",
      " [-] epoch   42/250, train loss 0.473556 in 0.02s\n",
      " [-] epoch   43/250, train loss 0.486631 in 0.04s\n",
      " [-] epoch   44/250, train loss 0.477861 in 0.03s\n",
      " [-] epoch   45/250, train loss 0.486819 in 0.03s\n",
      " [-] epoch   46/250, train loss 0.481841 in 0.03s\n",
      " [-] epoch   47/250, train loss 0.463890 in 0.03s\n",
      " [-] epoch   48/250, train loss 0.462846 in 0.03s\n",
      " [-] epoch   49/250, train loss 0.472180 in 0.03s\n",
      " [-] epoch   50/250, train loss 0.461909 in 0.03s\n",
      " [-] epoch   51/250, train loss 0.457904 in 0.03s\n",
      " [-] epoch   52/250, train loss 0.466871 in 0.03s\n",
      " [-] epoch   53/250, train loss 0.464220 in 0.03s\n",
      " [-] epoch   54/250, train loss 0.465482 in 0.03s\n",
      " [-] epoch   55/250, train loss 0.459574 in 0.03s\n",
      " [-] epoch   56/250, train loss 0.456375 in 0.03s\n",
      " [-] epoch   57/250, train loss 0.461338 in 0.03s\n",
      " [-] epoch   58/250, train loss 0.462743 in 0.03s\n",
      " [-] epoch   59/250, train loss 0.454476 in 0.03s\n",
      " [-] epoch   60/250, train loss 0.451188 in 0.03s\n",
      " [-] epoch   61/250, train loss 0.475960 in 0.03s\n",
      " [-] epoch   62/250, train loss 0.453796 in 0.03s\n",
      " [-] epoch   63/250, train loss 0.463274 in 0.03s\n",
      " [-] epoch   64/250, train loss 0.467748 in 0.03s\n",
      " [-] epoch   65/250, train loss 0.446704 in 0.03s\n",
      " [-] epoch   66/250, train loss 0.451959 in 0.03s\n",
      " [-] epoch   67/250, train loss 0.444930 in 0.03s\n",
      " [-] epoch   68/250, train loss 0.454156 in 0.03s\n",
      " [-] epoch   69/250, train loss 0.450595 in 0.03s\n",
      " [-] epoch   70/250, train loss 0.456900 in 0.04s\n",
      " [-] epoch   71/250, train loss 0.462263 in 0.03s\n",
      " [-] epoch   72/250, train loss 0.438255 in 0.03s\n",
      " [-] epoch   73/250, train loss 0.469479 in 0.03s\n",
      " [-] epoch   74/250, train loss 0.452198 in 0.03s\n",
      " [-] epoch   75/250, train loss 0.455786 in 0.03s\n",
      " [-] epoch   76/250, train loss 0.446805 in 0.03s\n",
      " [-] epoch   77/250, train loss 0.457375 in 0.03s\n",
      " [-] epoch   78/250, train loss 0.459439 in 0.03s\n",
      " [-] epoch   79/250, train loss 0.446057 in 0.03s\n",
      " [-] epoch   80/250, train loss 0.468165 in 0.03s\n",
      " [-] epoch   81/250, train loss 0.439359 in 0.03s\n",
      " [-] epoch   82/250, train loss 0.451720 in 0.03s\n",
      " [-] epoch   83/250, train loss 0.454192 in 0.03s\n",
      " [-] epoch   84/250, train loss 0.420678 in 0.03s\n",
      " [-] epoch   85/250, train loss 0.448942 in 0.03s\n",
      " [-] epoch   86/250, train loss 0.460424 in 0.03s\n",
      " [-] epoch   87/250, train loss 0.428707 in 0.03s\n",
      " [-] epoch   88/250, train loss 0.446319 in 0.03s\n",
      " [-] epoch   89/250, train loss 0.461545 in 0.03s\n",
      " [-] epoch   90/250, train loss 0.436013 in 0.03s\n",
      " [-] epoch   91/250, train loss 0.426638 in 0.03s\n",
      " [-] epoch   92/250, train loss 0.432789 in 0.03s\n",
      " [-] epoch   93/250, train loss 0.446090 in 0.03s\n",
      " [-] epoch   94/250, train loss 0.412534 in 0.03s\n",
      " [-] epoch   95/250, train loss 0.437610 in 0.03s\n",
      " [-] epoch   96/250, train loss 0.447002 in 0.04s\n",
      " [-] epoch   97/250, train loss 0.440853 in 0.03s\n",
      " [-] epoch   98/250, train loss 0.427338 in 0.03s\n",
      " [-] epoch   99/250, train loss 0.437879 in 0.03s\n",
      " [-] epoch  100/250, train loss 0.448309 in 0.03s\n",
      " [-] epoch  101/250, train loss 0.412747 in 0.03s\n",
      " [-] epoch  102/250, train loss 0.427816 in 0.03s\n",
      " [-] epoch  103/250, train loss 0.435501 in 0.03s\n",
      " [-] epoch  104/250, train loss 0.429760 in 0.03s\n",
      " [-] epoch  105/250, train loss 0.428341 in 0.03s\n",
      " [-] epoch  106/250, train loss 0.412664 in 0.03s\n",
      " [-] epoch  107/250, train loss 0.418106 in 0.03s\n",
      " [-] epoch  108/250, train loss 0.435728 in 0.03s\n",
      " [-] epoch  109/250, train loss 0.417942 in 0.03s\n",
      " [-] epoch  110/250, train loss 0.429682 in 0.03s\n",
      " [-] epoch  111/250, train loss 0.436542 in 0.03s\n",
      " [-] epoch  112/250, train loss 0.434402 in 0.03s\n",
      " [-] epoch  113/250, train loss 0.421988 in 0.03s\n",
      " [-] epoch  114/250, train loss 0.441730 in 0.03s\n",
      " [-] epoch  115/250, train loss 0.419347 in 0.03s\n",
      " [-] epoch  116/250, train loss 0.432303 in 0.03s\n",
      " [-] epoch  117/250, train loss 0.427615 in 0.03s\n",
      " [-] epoch  118/250, train loss 0.427436 in 0.03s\n",
      " [-] epoch  119/250, train loss 0.439676 in 0.03s\n",
      " [-] epoch  120/250, train loss 0.433731 in 0.03s\n",
      " [-] epoch  121/250, train loss 0.438123 in 0.03s\n",
      " [-] epoch  122/250, train loss 0.434099 in 0.03s\n",
      " [-] epoch  123/250, train loss 0.423622 in 0.03s\n",
      " [-] epoch  124/250, train loss 0.431120 in 0.03s\n",
      " [-] epoch  125/250, train loss 0.430752 in 0.03s\n",
      " [-] epoch  126/250, train loss 0.414440 in 0.03s\n",
      " [-] epoch  127/250, train loss 0.423003 in 0.03s\n",
      " [-] epoch  128/250, train loss 0.408604 in 0.03s\n",
      " [-] epoch  129/250, train loss 0.424962 in 0.04s\n",
      " [-] epoch  130/250, train loss 0.434246 in 0.03s\n",
      " [-] epoch  131/250, train loss 0.418739 in 0.03s\n",
      " [-] epoch  132/250, train loss 0.455272 in 0.03s\n",
      " [-] epoch  133/250, train loss 0.429688 in 0.03s\n",
      " [-] epoch  134/250, train loss 0.416152 in 0.03s\n",
      " [-] epoch  135/250, train loss 0.404331 in 0.03s\n",
      " [-] epoch  136/250, train loss 0.424754 in 0.03s\n",
      " [-] epoch  137/250, train loss 0.422749 in 0.03s\n",
      " [-] epoch  138/250, train loss 0.422321 in 0.03s\n",
      " [-] epoch  139/250, train loss 0.402128 in 0.03s\n",
      " [-] epoch  140/250, train loss 0.414987 in 0.03s\n",
      " [-] epoch  141/250, train loss 0.402829 in 0.03s\n",
      " [-] epoch  142/250, train loss 0.422748 in 0.03s\n",
      " [-] epoch  143/250, train loss 0.427084 in 0.03s\n",
      " [-] epoch  144/250, train loss 0.409052 in 0.03s\n",
      " [-] epoch  145/250, train loss 0.420926 in 0.03s\n",
      " [-] epoch  146/250, train loss 0.403639 in 0.03s\n",
      " [-] epoch  147/250, train loss 0.420135 in 0.03s\n",
      " [-] epoch  148/250, train loss 0.409321 in 0.03s\n",
      " [-] epoch  149/250, train loss 0.417782 in 0.03s\n",
      " [-] epoch  150/250, train loss 0.444764 in 0.03s\n",
      " [-] epoch  151/250, train loss 0.414347 in 0.03s\n",
      " [-] epoch  152/250, train loss 0.424043 in 0.03s\n",
      " [-] epoch  153/250, train loss 0.407035 in 0.03s\n",
      " [-] epoch  154/250, train loss 0.400960 in 0.03s\n",
      " [-] epoch  155/250, train loss 0.412265 in 0.03s\n",
      " [-] epoch  156/250, train loss 0.403379 in 0.03s\n",
      " [-] epoch  157/250, train loss 0.421838 in 0.03s\n",
      " [-] epoch  158/250, train loss 0.419798 in 0.03s\n",
      " [-] epoch  159/250, train loss 0.419802 in 0.03s\n",
      " [-] epoch  160/250, train loss 0.428532 in 0.03s\n",
      " [-] epoch  161/250, train loss 0.396697 in 0.03s\n",
      " [-] epoch  162/250, train loss 0.409762 in 0.03s\n",
      " [-] epoch  163/250, train loss 0.416305 in 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  164/250, train loss 0.414458 in 0.03s\n",
      " [-] epoch  165/250, train loss 0.405862 in 0.03s\n",
      " [-] epoch  166/250, train loss 0.405939 in 0.04s\n",
      " [-] epoch  167/250, train loss 0.393107 in 0.03s\n",
      " [-] epoch  168/250, train loss 0.405447 in 0.03s\n",
      " [-] epoch  169/250, train loss 0.411389 in 0.03s\n",
      " [-] epoch  170/250, train loss 0.403053 in 0.03s\n",
      " [-] epoch  171/250, train loss 0.389008 in 0.03s\n",
      " [-] epoch  172/250, train loss 0.406091 in 0.03s\n",
      " [-] epoch  173/250, train loss 0.423183 in 0.03s\n",
      " [-] epoch  174/250, train loss 0.420626 in 0.03s\n",
      " [-] epoch  175/250, train loss 0.415777 in 0.03s\n",
      " [-] epoch  176/250, train loss 0.417823 in 0.03s\n",
      " [-] epoch  177/250, train loss 0.419064 in 0.03s\n",
      " [-] epoch  178/250, train loss 0.402400 in 0.03s\n",
      " [-] epoch  179/250, train loss 0.390408 in 0.03s\n",
      " [-] epoch  180/250, train loss 0.400570 in 0.03s\n",
      " [-] epoch  181/250, train loss 0.384642 in 0.03s\n",
      " [-] epoch  182/250, train loss 0.405511 in 0.03s\n",
      " [-] epoch  183/250, train loss 0.407632 in 0.03s\n",
      " [-] epoch  184/250, train loss 0.405843 in 0.03s\n",
      " [-] epoch  185/250, train loss 0.415859 in 0.03s\n",
      " [-] epoch  186/250, train loss 0.402932 in 0.03s\n",
      " [-] epoch  187/250, train loss 0.386986 in 0.03s\n",
      " [-] epoch  188/250, train loss 0.428461 in 0.03s\n",
      " [-] epoch  189/250, train loss 0.413777 in 0.03s\n",
      " [-] epoch  190/250, train loss 0.396415 in 0.03s\n",
      " [-] epoch  191/250, train loss 0.409334 in 0.03s\n",
      " [-] epoch  192/250, train loss 0.396398 in 0.03s\n",
      " [-] epoch  193/250, train loss 0.416840 in 0.03s\n",
      " [-] epoch  194/250, train loss 0.395477 in 0.03s\n",
      " [-] epoch  195/250, train loss 0.404209 in 0.03s\n",
      " [-] epoch  196/250, train loss 0.398830 in 0.03s\n",
      " [-] epoch  197/250, train loss 0.404343 in 0.03s\n",
      " [-] epoch  198/250, train loss 0.426488 in 0.03s\n",
      " [-] epoch  199/250, train loss 0.399025 in 0.03s\n",
      " [-] epoch  200/250, train loss 0.402747 in 0.02s\n",
      " [-] epoch  201/250, train loss 0.396914 in 0.03s\n",
      " [-] epoch  202/250, train loss 0.399595 in 0.03s\n",
      " [-] epoch  203/250, train loss 0.415590 in 0.03s\n",
      " [-] epoch  204/250, train loss 0.407110 in 0.03s\n",
      " [-] epoch  205/250, train loss 0.391866 in 0.03s\n",
      " [-] epoch  206/250, train loss 0.398684 in 0.03s\n",
      " [-] epoch  207/250, train loss 0.404937 in 0.03s\n",
      " [-] epoch  208/250, train loss 0.398340 in 0.03s\n",
      " [-] epoch  209/250, train loss 0.401831 in 0.03s\n",
      " [-] epoch  210/250, train loss 0.418592 in 0.03s\n",
      " [-] epoch  211/250, train loss 0.397868 in 0.03s\n",
      " [-] epoch  212/250, train loss 0.401346 in 0.03s\n",
      " [-] epoch  213/250, train loss 0.389976 in 0.03s\n",
      " [-] epoch  214/250, train loss 0.376561 in 0.03s\n",
      " [-] epoch  215/250, train loss 0.396787 in 0.03s\n",
      " [-] epoch  216/250, train loss 0.402994 in 0.03s\n",
      " [-] epoch  217/250, train loss 0.399261 in 0.03s\n",
      " [-] epoch  218/250, train loss 0.387848 in 0.03s\n",
      " [-] epoch  219/250, train loss 0.420875 in 0.03s\n",
      " [-] epoch  220/250, train loss 0.384040 in 0.03s\n",
      " [-] epoch  221/250, train loss 0.397639 in 0.03s\n",
      " [-] epoch  222/250, train loss 0.387733 in 0.03s\n",
      " [-] epoch  223/250, train loss 0.400679 in 0.03s\n",
      " [-] epoch  224/250, train loss 0.408714 in 0.03s\n",
      " [-] epoch  225/250, train loss 0.382507 in 0.03s\n",
      " [-] epoch  226/250, train loss 0.413494 in 0.03s\n",
      " [-] epoch  227/250, train loss 0.417434 in 0.03s\n",
      " [-] epoch  228/250, train loss 0.385234 in 0.03s\n",
      " [-] epoch  229/250, train loss 0.390323 in 0.03s\n",
      " [-] epoch  230/250, train loss 0.417188 in 0.03s\n",
      " [-] epoch  231/250, train loss 0.409067 in 0.03s\n",
      " [-] epoch  232/250, train loss 0.403671 in 0.03s\n",
      " [-] epoch  233/250, train loss 0.386528 in 0.03s\n",
      " [-] epoch  234/250, train loss 0.397956 in 0.03s\n",
      " [-] epoch  235/250, train loss 0.382584 in 0.03s\n",
      " [-] epoch  236/250, train loss 0.403538 in 0.03s\n",
      " [-] epoch  237/250, train loss 0.391996 in 0.03s\n",
      " [-] epoch  238/250, train loss 0.371346 in 0.03s\n",
      " [-] epoch  239/250, train loss 0.401189 in 0.03s\n",
      " [-] epoch  240/250, train loss 0.390028 in 0.03s\n",
      " [-] epoch  241/250, train loss 0.412775 in 0.03s\n",
      " [-] epoch  242/250, train loss 0.384274 in 0.03s\n",
      " [-] epoch  243/250, train loss 0.403596 in 0.03s\n",
      " [-] epoch  244/250, train loss 0.392834 in 0.03s\n",
      " [-] epoch  245/250, train loss 0.388590 in 0.03s\n",
      " [-] epoch  246/250, train loss 0.386438 in 0.03s\n",
      " [-] epoch  247/250, train loss 0.405067 in 0.03s\n",
      " [-] epoch  248/250, train loss 0.406069 in 0.03s\n",
      " [-] epoch  249/250, train loss 0.401871 in 0.03s\n",
      " [-] epoch  250/250, train loss 0.382550 in 0.03s\n",
      " [-] test acc. 83.611111%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.556698 in 0.06s\n",
      " [-] epoch    2/250, train loss 0.465218 in 0.07s\n",
      " [-] epoch    3/250, train loss 0.449804 in 0.08s\n",
      " [-] epoch    4/250, train loss 0.435600 in 0.07s\n",
      " [-] epoch    5/250, train loss 0.427103 in 0.08s\n",
      " [-] epoch    6/250, train loss 0.411961 in 0.07s\n",
      " [-] epoch    7/250, train loss 0.389245 in 0.07s\n",
      " [-] epoch    8/250, train loss 0.409969 in 0.07s\n",
      " [-] epoch    9/250, train loss 0.377279 in 0.07s\n",
      " [-] epoch   10/250, train loss 0.422418 in 0.08s\n",
      " [-] epoch   11/250, train loss 0.369093 in 0.07s\n",
      " [-] epoch   12/250, train loss 0.429092 in 0.06s\n",
      " [-] epoch   13/250, train loss 0.396532 in 0.07s\n",
      " [-] epoch   14/250, train loss 0.362309 in 0.07s\n",
      " [-] epoch   15/250, train loss 0.370435 in 0.06s\n",
      " [-] epoch   16/250, train loss 0.356958 in 0.08s\n",
      " [-] epoch   17/250, train loss 0.383833 in 0.07s\n",
      " [-] epoch   18/250, train loss 0.375950 in 0.07s\n",
      " [-] epoch   19/250, train loss 0.361884 in 0.08s\n",
      " [-] epoch   20/250, train loss 0.366224 in 0.07s\n",
      " [-] epoch   21/250, train loss 0.387109 in 0.07s\n",
      " [-] epoch   22/250, train loss 0.358685 in 0.07s\n",
      " [-] epoch   23/250, train loss 0.355894 in 0.08s\n",
      " [-] epoch   24/250, train loss 0.385940 in 0.07s\n",
      " [-] epoch   25/250, train loss 0.396241 in 0.07s\n",
      " [-] epoch   26/250, train loss 0.361015 in 0.08s\n",
      " [-] epoch   27/250, train loss 0.374303 in 0.07s\n",
      " [-] epoch   28/250, train loss 0.380855 in 0.07s\n",
      " [-] epoch   29/250, train loss 0.384477 in 0.06s\n",
      " [-] epoch   30/250, train loss 0.336944 in 0.07s\n",
      " [-] epoch   31/250, train loss 0.353683 in 0.07s\n",
      " [-] epoch   32/250, train loss 0.366617 in 0.06s\n",
      " [-] epoch   33/250, train loss 0.358042 in 0.06s\n",
      " [-] epoch   34/250, train loss 0.346369 in 0.09s\n",
      " [-] epoch   35/250, train loss 0.357338 in 0.07s\n",
      " [-] epoch   36/250, train loss 0.355613 in 0.07s\n",
      " [-] epoch   37/250, train loss 0.352427 in 0.07s\n",
      " [-] epoch   38/250, train loss 0.356684 in 0.07s\n",
      " [-] epoch   39/250, train loss 0.384513 in 0.07s\n",
      " [-] epoch   40/250, train loss 0.344840 in 0.08s\n",
      " [-] epoch   41/250, train loss 0.332331 in 0.07s\n",
      " [-] epoch   42/250, train loss 0.339569 in 0.07s\n",
      " [-] epoch   43/250, train loss 0.362919 in 0.07s\n",
      " [-] epoch   44/250, train loss 0.343291 in 0.08s\n",
      " [-] epoch   45/250, train loss 0.340316 in 0.07s\n",
      " [-] epoch   46/250, train loss 0.339816 in 0.07s\n",
      " [-] epoch   47/250, train loss 0.345668 in 0.07s\n",
      " [-] epoch   48/250, train loss 0.354179 in 0.07s\n",
      " [-] epoch   49/250, train loss 0.350751 in 0.06s\n",
      " [-] epoch   50/250, train loss 0.332599 in 0.07s\n",
      " [-] epoch   51/250, train loss 0.313617 in 0.07s\n",
      " [-] epoch   52/250, train loss 0.321086 in 0.07s\n",
      " [-] epoch   53/250, train loss 0.340565 in 0.07s\n",
      " [-] epoch   54/250, train loss 0.335607 in 0.07s\n",
      " [-] epoch   55/250, train loss 0.333915 in 0.07s\n",
      " [-] epoch   56/250, train loss 0.325182 in 0.07s\n",
      " [-] epoch   57/250, train loss 0.337701 in 0.07s\n",
      " [-] epoch   58/250, train loss 0.331084 in 0.07s\n",
      " [-] epoch   59/250, train loss 0.341958 in 0.07s\n",
      " [-] epoch   60/250, train loss 0.307662 in 0.07s\n",
      " [-] epoch   61/250, train loss 0.353116 in 0.07s\n",
      " [-] epoch   62/250, train loss 0.345543 in 0.07s\n",
      " [-] epoch   63/250, train loss 0.330507 in 0.08s\n",
      " [-] epoch   64/250, train loss 0.322384 in 0.07s\n",
      " [-] epoch   65/250, train loss 0.329304 in 0.07s\n",
      " [-] epoch   66/250, train loss 0.333761 in 0.07s\n",
      " [-] epoch   67/250, train loss 0.320818 in 0.06s\n",
      " [-] epoch   68/250, train loss 0.319694 in 0.07s\n",
      " [-] epoch   69/250, train loss 0.357884 in 0.07s\n",
      " [-] epoch   70/250, train loss 0.353241 in 0.06s\n",
      " [-] epoch   71/250, train loss 0.341476 in 0.07s\n",
      " [-] epoch   72/250, train loss 0.333289 in 0.07s\n",
      " [-] epoch   73/250, train loss 0.331280 in 0.08s\n",
      " [-] epoch   74/250, train loss 0.313432 in 0.06s\n",
      " [-] epoch   75/250, train loss 0.346374 in 0.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.325216 in 0.07s\n",
      " [-] epoch   77/250, train loss 0.341046 in 0.08s\n",
      " [-] epoch   78/250, train loss 0.321242 in 0.07s\n",
      " [-] epoch   79/250, train loss 0.320729 in 0.08s\n",
      " [-] epoch   80/250, train loss 0.329151 in 0.06s\n",
      " [-] epoch   81/250, train loss 0.339699 in 0.07s\n",
      " [-] epoch   82/250, train loss 0.332607 in 0.07s\n",
      " [-] epoch   83/250, train loss 0.324163 in 0.06s\n",
      " [-] epoch   84/250, train loss 0.334242 in 0.08s\n",
      " [-] epoch   85/250, train loss 0.312684 in 0.06s\n",
      " [-] epoch   86/250, train loss 0.309928 in 0.06s\n",
      " [-] epoch   87/250, train loss 0.334483 in 0.08s\n",
      " [-] epoch   88/250, train loss 0.327341 in 0.07s\n",
      " [-] epoch   89/250, train loss 0.331884 in 0.06s\n",
      " [-] epoch   90/250, train loss 0.318575 in 0.08s\n",
      " [-] epoch   91/250, train loss 0.317480 in 0.07s\n",
      " [-] epoch   92/250, train loss 0.314461 in 0.07s\n",
      " [-] epoch   93/250, train loss 0.306076 in 0.07s\n",
      " [-] epoch   94/250, train loss 0.310591 in 0.07s\n",
      " [-] epoch   95/250, train loss 0.303035 in 0.07s\n",
      " [-] epoch   96/250, train loss 0.332772 in 0.08s\n",
      " [-] epoch   97/250, train loss 0.334126 in 0.07s\n",
      " [-] epoch   98/250, train loss 0.310138 in 0.07s\n",
      " [-] epoch   99/250, train loss 0.303368 in 0.08s\n",
      " [-] epoch  100/250, train loss 0.299658 in 0.07s\n",
      " [-] epoch  101/250, train loss 0.313260 in 0.07s\n",
      " [-] epoch  102/250, train loss 0.322857 in 0.06s\n",
      " [-] epoch  103/250, train loss 0.311584 in 0.08s\n",
      " [-] epoch  104/250, train loss 0.306290 in 0.07s\n",
      " [-] epoch  105/250, train loss 0.317628 in 0.06s\n",
      " [-] epoch  106/250, train loss 0.325752 in 0.07s\n",
      " [-] epoch  107/250, train loss 0.302719 in 0.07s\n",
      " [-] epoch  108/250, train loss 0.305700 in 0.07s\n",
      " [-] epoch  109/250, train loss 0.327411 in 0.07s\n",
      " [-] epoch  110/250, train loss 0.308761 in 0.08s\n",
      " [-] epoch  111/250, train loss 0.295912 in 0.07s\n",
      " [-] epoch  112/250, train loss 0.309937 in 0.07s\n",
      " [-] epoch  113/250, train loss 0.325254 in 0.07s\n",
      " [-] epoch  114/250, train loss 0.311359 in 0.07s\n",
      " [-] epoch  115/250, train loss 0.306719 in 0.07s\n",
      " [-] epoch  116/250, train loss 0.289830 in 0.07s\n",
      " [-] epoch  117/250, train loss 0.319741 in 0.06s\n",
      " [-] epoch  118/250, train loss 0.306056 in 0.06s\n",
      " [-] epoch  119/250, train loss 0.323689 in 0.07s\n",
      " [-] epoch  120/250, train loss 0.304896 in 0.07s\n",
      " [-] epoch  121/250, train loss 0.304859 in 0.08s\n",
      " [-] epoch  122/250, train loss 0.313368 in 0.06s\n",
      " [-] epoch  123/250, train loss 0.301100 in 0.08s\n",
      " [-] epoch  124/250, train loss 0.319527 in 0.07s\n",
      " [-] epoch  125/250, train loss 0.308407 in 0.06s\n",
      " [-] epoch  126/250, train loss 0.294999 in 0.07s\n",
      " [-] epoch  127/250, train loss 0.323987 in 0.07s\n",
      " [-] epoch  128/250, train loss 0.301821 in 0.08s\n",
      " [-] epoch  129/250, train loss 0.301501 in 0.07s\n",
      " [-] epoch  130/250, train loss 0.302577 in 0.07s\n",
      " [-] epoch  131/250, train loss 0.302972 in 0.07s\n",
      " [-] epoch  132/250, train loss 0.316383 in 0.06s\n",
      " [-] epoch  133/250, train loss 0.289222 in 0.08s\n",
      " [-] epoch  134/250, train loss 0.302849 in 0.07s\n",
      " [-] epoch  135/250, train loss 0.297182 in 0.07s\n",
      " [-] epoch  136/250, train loss 0.287466 in 0.07s\n",
      " [-] epoch  137/250, train loss 0.302072 in 0.08s\n",
      " [-] epoch  138/250, train loss 0.308056 in 0.07s\n",
      " [-] epoch  139/250, train loss 0.303327 in 0.07s\n",
      " [-] epoch  140/250, train loss 0.277913 in 0.06s\n",
      " [-] epoch  141/250, train loss 0.294558 in 0.08s\n",
      " [-] epoch  142/250, train loss 0.287209 in 0.08s\n",
      " [-] epoch  143/250, train loss 0.290716 in 0.07s\n",
      " [-] epoch  144/250, train loss 0.312308 in 0.08s\n",
      " [-] epoch  145/250, train loss 0.283826 in 0.07s\n",
      " [-] epoch  146/250, train loss 0.289151 in 0.08s\n",
      " [-] epoch  147/250, train loss 0.337402 in 0.07s\n",
      " [-] epoch  148/250, train loss 0.302757 in 0.07s\n",
      " [-] epoch  149/250, train loss 0.303716 in 0.06s\n",
      " [-] epoch  150/250, train loss 0.322445 in 0.08s\n",
      " [-] epoch  151/250, train loss 0.293128 in 0.07s\n",
      " [-] epoch  152/250, train loss 0.300829 in 0.06s\n",
      " [-] epoch  153/250, train loss 0.295438 in 0.07s\n",
      " [-] epoch  154/250, train loss 0.308869 in 0.06s\n",
      " [-] epoch  155/250, train loss 0.279792 in 0.08s\n",
      " [-] epoch  156/250, train loss 0.293031 in 0.07s\n",
      " [-] epoch  157/250, train loss 0.323071 in 0.08s\n",
      " [-] epoch  158/250, train loss 0.266319 in 0.07s\n",
      " [-] epoch  159/250, train loss 0.311781 in 0.06s\n",
      " [-] epoch  160/250, train loss 0.312343 in 0.07s\n",
      " [-] epoch  161/250, train loss 0.284299 in 0.06s\n",
      " [-] epoch  162/250, train loss 0.284502 in 0.07s\n",
      " [-] epoch  163/250, train loss 0.305483 in 0.08s\n",
      " [-] epoch  164/250, train loss 0.311750 in 0.07s\n",
      " [-] epoch  165/250, train loss 0.293436 in 0.07s\n",
      " [-] epoch  166/250, train loss 0.268793 in 0.06s\n",
      " [-] epoch  167/250, train loss 0.293468 in 0.08s\n",
      " [-] epoch  168/250, train loss 0.275522 in 0.06s\n",
      " [-] epoch  169/250, train loss 0.296890 in 0.10s\n",
      " [-] epoch  170/250, train loss 0.292029 in 0.12s\n",
      " [-] epoch  171/250, train loss 0.301512 in 0.15s\n",
      " [-] epoch  172/250, train loss 0.282774 in 0.24s\n",
      " [-] epoch  173/250, train loss 0.311221 in 0.30s\n",
      " [-] epoch  174/250, train loss 0.299995 in 0.28s\n",
      " [-] epoch  175/250, train loss 0.312867 in 0.29s\n",
      " [-] epoch  176/250, train loss 0.290401 in 0.28s\n",
      " [-] epoch  177/250, train loss 0.290580 in 0.28s\n",
      " [-] epoch  178/250, train loss 0.304522 in 0.29s\n",
      " [-] epoch  179/250, train loss 0.291274 in 0.27s\n",
      " [-] epoch  180/250, train loss 0.293310 in 0.31s\n",
      " [-] epoch  181/250, train loss 0.291954 in 0.29s\n",
      " [-] epoch  182/250, train loss 0.267534 in 0.28s\n",
      " [-] epoch  183/250, train loss 0.289771 in 0.30s\n",
      " [-] epoch  184/250, train loss 0.279279 in 0.29s\n",
      " [-] epoch  185/250, train loss 0.335775 in 0.33s\n",
      " [-] epoch  186/250, train loss 0.270881 in 0.30s\n",
      " [-] epoch  187/250, train loss 0.279461 in 0.27s\n",
      " [-] epoch  188/250, train loss 0.283517 in 0.39s\n",
      " [-] epoch  189/250, train loss 0.272620 in 0.40s\n",
      " [-] epoch  190/250, train loss 0.294314 in 0.33s\n",
      " [-] epoch  191/250, train loss 0.293859 in 0.43s\n",
      " [-] epoch  192/250, train loss 0.299962 in 0.61s\n",
      " [-] epoch  193/250, train loss 0.279401 in 0.31s\n",
      " [-] epoch  194/250, train loss 0.277485 in 0.41s\n",
      " [-] epoch  195/250, train loss 0.280375 in 0.35s\n",
      " [-] epoch  196/250, train loss 0.281935 in 0.41s\n",
      " [-] epoch  197/250, train loss 0.283347 in 0.42s\n",
      " [-] epoch  198/250, train loss 0.274363 in 0.31s\n",
      " [-] epoch  199/250, train loss 0.307031 in 0.33s\n",
      " [-] epoch  200/250, train loss 0.277325 in 0.30s\n",
      " [-] epoch  201/250, train loss 0.284880 in 0.32s\n",
      " [-] epoch  202/250, train loss 0.287854 in 0.31s\n",
      " [-] epoch  203/250, train loss 0.273056 in 0.31s\n",
      " [-] epoch  204/250, train loss 0.294610 in 0.34s\n",
      " [-] epoch  205/250, train loss 0.296675 in 0.32s\n",
      " [-] epoch  206/250, train loss 0.289954 in 0.34s\n",
      " [-] epoch  207/250, train loss 0.297269 in 0.29s\n",
      " [-] epoch  208/250, train loss 0.281947 in 0.31s\n",
      " [-] epoch  209/250, train loss 0.284561 in 0.31s\n",
      " [-] epoch  210/250, train loss 0.251510 in 0.33s\n",
      " [-] epoch  211/250, train loss 0.294590 in 0.29s\n",
      " [-] epoch  212/250, train loss 0.312166 in 0.30s\n",
      " [-] epoch  213/250, train loss 0.291521 in 0.29s\n",
      " [-] epoch  214/250, train loss 0.301900 in 0.32s\n",
      " [-] epoch  215/250, train loss 0.279048 in 0.30s\n",
      " [-] epoch  216/250, train loss 0.282821 in 0.29s\n",
      " [-] epoch  217/250, train loss 0.277437 in 0.30s\n",
      " [-] epoch  218/250, train loss 0.277173 in 0.30s\n",
      " [-] epoch  219/250, train loss 0.283688 in 0.29s\n",
      " [-] epoch  220/250, train loss 0.269963 in 0.31s\n",
      " [-] epoch  221/250, train loss 0.275006 in 0.31s\n",
      " [-] epoch  222/250, train loss 0.266031 in 0.31s\n",
      " [-] epoch  223/250, train loss 0.298584 in 0.29s\n",
      " [-] epoch  224/250, train loss 0.281779 in 0.27s\n",
      " [-] epoch  225/250, train loss 0.273568 in 0.27s\n",
      " [-] epoch  226/250, train loss 0.288373 in 0.30s\n",
      " [-] epoch  227/250, train loss 0.283472 in 0.28s\n",
      " [-] epoch  228/250, train loss 0.323365 in 0.30s\n",
      " [-] epoch  229/250, train loss 0.274833 in 0.28s\n",
      " [-] epoch  230/250, train loss 0.267703 in 0.29s\n",
      " [-] epoch  231/250, train loss 0.298524 in 0.30s\n",
      " [-] epoch  232/250, train loss 0.280740 in 0.30s\n",
      " [-] epoch  233/250, train loss 0.291069 in 0.29s\n",
      " [-] epoch  234/250, train loss 0.262488 in 0.29s\n",
      " [-] epoch  235/250, train loss 0.280440 in 0.31s\n",
      " [-] epoch  236/250, train loss 0.275966 in 0.30s\n",
      " [-] epoch  237/250, train loss 0.287082 in 0.29s\n",
      " [-] epoch  238/250, train loss 0.268150 in 0.28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  239/250, train loss 0.304131 in 0.23s\n",
      " [-] epoch  240/250, train loss 0.260637 in 0.20s\n",
      " [-] epoch  241/250, train loss 0.276081 in 0.17s\n",
      " [-] epoch  242/250, train loss 0.293006 in 0.15s\n",
      " [-] epoch  243/250, train loss 0.265084 in 0.15s\n",
      " [-] epoch  244/250, train loss 0.288482 in 0.12s\n",
      " [-] epoch  245/250, train loss 0.304602 in 0.12s\n",
      " [-] epoch  246/250, train loss 0.274558 in 0.10s\n",
      " [-] epoch  247/250, train loss 0.282636 in 0.12s\n",
      " [-] epoch  248/250, train loss 0.306250 in 0.12s\n",
      " [-] epoch  249/250, train loss 0.284318 in 0.12s\n",
      " [-] epoch  250/250, train loss 0.280544 in 0.12s\n",
      " [-] test acc. 79.166667%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.573590 in 0.20s\n",
      " [-] epoch    2/250, train loss 0.453906 in 0.20s\n",
      " [-] epoch    3/250, train loss 0.441202 in 0.20s\n",
      " [-] epoch    4/250, train loss 0.377244 in 0.22s\n",
      " [-] epoch    5/250, train loss 0.419342 in 0.22s\n",
      " [-] epoch    6/250, train loss 0.409461 in 0.22s\n",
      " [-] epoch    7/250, train loss 0.376592 in 0.22s\n",
      " [-] epoch    8/250, train loss 0.367417 in 0.22s\n",
      " [-] epoch    9/250, train loss 0.364418 in 0.19s\n",
      " [-] epoch   10/250, train loss 0.371504 in 0.20s\n",
      " [-] epoch   11/250, train loss 0.350781 in 0.22s\n",
      " [-] epoch   12/250, train loss 0.334718 in 0.19s\n",
      " [-] epoch   13/250, train loss 0.324922 in 0.23s\n",
      " [-] epoch   14/250, train loss 0.360711 in 0.19s\n",
      " [-] epoch   15/250, train loss 0.355803 in 0.20s\n",
      " [-] epoch   16/250, train loss 0.332895 in 0.20s\n",
      " [-] epoch   17/250, train loss 0.330470 in 0.22s\n",
      " [-] epoch   18/250, train loss 0.321642 in 0.23s\n",
      " [-] epoch   19/250, train loss 0.310638 in 0.19s\n",
      " [-] epoch   20/250, train loss 0.313310 in 0.18s\n",
      " [-] epoch   21/250, train loss 0.319765 in 0.21s\n",
      " [-] epoch   22/250, train loss 0.314590 in 0.19s\n",
      " [-] epoch   23/250, train loss 0.330997 in 0.20s\n",
      " [-] epoch   24/250, train loss 0.318188 in 0.17s\n",
      " [-] epoch   25/250, train loss 0.314788 in 0.18s\n",
      " [-] epoch   26/250, train loss 0.325952 in 0.20s\n",
      " [-] epoch   27/250, train loss 0.308881 in 0.20s\n",
      " [-] epoch   28/250, train loss 0.306437 in 0.19s\n",
      " [-] epoch   29/250, train loss 0.310080 in 0.20s\n",
      " [-] epoch   30/250, train loss 0.316530 in 0.17s\n",
      " [-] epoch   31/250, train loss 0.299957 in 0.20s\n",
      " [-] epoch   32/250, train loss 0.304313 in 0.20s\n",
      " [-] epoch   33/250, train loss 0.279669 in 0.20s\n",
      " [-] epoch   34/250, train loss 0.286623 in 0.18s\n",
      " [-] epoch   35/250, train loss 0.282417 in 0.17s\n",
      " [-] epoch   36/250, train loss 0.286006 in 0.18s\n",
      " [-] epoch   37/250, train loss 0.303066 in 0.20s\n",
      " [-] epoch   38/250, train loss 0.293013 in 0.17s\n",
      " [-] epoch   39/250, train loss 0.289570 in 0.18s\n",
      " [-] epoch   40/250, train loss 0.282970 in 0.18s\n",
      " [-] epoch   41/250, train loss 0.249845 in 0.19s\n",
      " [-] epoch   42/250, train loss 0.285492 in 0.18s\n",
      " [-] epoch   43/250, train loss 0.287846 in 0.17s\n",
      " [-] epoch   44/250, train loss 0.290176 in 0.18s\n",
      " [-] epoch   45/250, train loss 0.271233 in 0.16s\n",
      " [-] epoch   46/250, train loss 0.279235 in 0.17s\n",
      " [-] epoch   47/250, train loss 0.285383 in 0.19s\n",
      " [-] epoch   48/250, train loss 0.290477 in 0.19s\n",
      " [-] epoch   49/250, train loss 0.278858 in 0.17s\n",
      " [-] epoch   50/250, train loss 0.284392 in 0.17s\n",
      " [-] epoch   51/250, train loss 0.264136 in 0.19s\n",
      " [-] epoch   52/250, train loss 0.271089 in 0.18s\n",
      " [-] epoch   53/250, train loss 0.282348 in 0.17s\n",
      " [-] epoch   54/250, train loss 0.254507 in 0.17s\n",
      " [-] epoch   55/250, train loss 0.262034 in 0.16s\n",
      " [-] epoch   56/250, train loss 0.288335 in 0.15s\n",
      " [-] epoch   57/250, train loss 0.268466 in 0.18s\n",
      " [-] epoch   58/250, train loss 0.258081 in 0.17s\n",
      " [-] epoch   59/250, train loss 0.270786 in 0.17s\n",
      " [-] epoch   60/250, train loss 0.269181 in 0.17s\n",
      " [-] epoch   61/250, train loss 0.273713 in 0.17s\n",
      " [-] epoch   62/250, train loss 0.265095 in 0.16s\n",
      " [-] epoch   63/250, train loss 0.254972 in 0.18s\n",
      " [-] epoch   64/250, train loss 0.255849 in 0.17s\n",
      " [-] epoch   65/250, train loss 0.272160 in 0.16s\n",
      " [-] epoch   66/250, train loss 0.260960 in 0.18s\n",
      " [-] epoch   67/250, train loss 0.274401 in 0.16s\n",
      " [-] epoch   68/250, train loss 0.260686 in 0.17s\n",
      " [-] epoch   69/250, train loss 0.254119 in 0.17s\n",
      " [-] epoch   70/250, train loss 0.265913 in 0.16s\n",
      " [-] epoch   71/250, train loss 0.273880 in 0.18s\n",
      " [-] epoch   72/250, train loss 0.247602 in 0.15s\n",
      " [-] epoch   73/250, train loss 0.249984 in 0.16s\n",
      " [-] epoch   74/250, train loss 0.277678 in 0.17s\n",
      " [-] epoch   75/250, train loss 0.263052 in 0.17s\n",
      " [-] epoch   76/250, train loss 0.253548 in 0.16s\n",
      " [-] epoch   77/250, train loss 0.253234 in 0.15s\n",
      " [-] epoch   78/250, train loss 0.249686 in 0.18s\n",
      " [-] epoch   79/250, train loss 0.244459 in 0.16s\n",
      " [-] epoch   80/250, train loss 0.260121 in 0.17s\n",
      " [-] epoch   81/250, train loss 0.279275 in 0.17s\n",
      " [-] epoch   82/250, train loss 0.267714 in 0.15s\n",
      " [-] epoch   83/250, train loss 0.260483 in 0.17s\n",
      " [-] epoch   84/250, train loss 0.259507 in 0.15s\n",
      " [-] epoch   85/250, train loss 0.254508 in 0.16s\n",
      " [-] epoch   86/250, train loss 0.229621 in 0.17s\n",
      " [-] epoch   87/250, train loss 0.254940 in 0.16s\n",
      " [-] epoch   88/250, train loss 0.267405 in 0.15s\n",
      " [-] epoch   89/250, train loss 0.252818 in 0.16s\n",
      " [-] epoch   90/250, train loss 0.239386 in 0.15s\n",
      " [-] epoch   91/250, train loss 0.257277 in 0.17s\n",
      " [-] epoch   92/250, train loss 0.246533 in 0.16s\n",
      " [-] epoch   93/250, train loss 0.245795 in 0.16s\n",
      " [-] epoch   94/250, train loss 0.259823 in 0.17s\n",
      " [-] epoch   95/250, train loss 0.250198 in 0.16s\n",
      " [-] epoch   96/250, train loss 0.247727 in 0.15s\n",
      " [-] epoch   97/250, train loss 0.258821 in 0.13s\n",
      " [-] epoch   98/250, train loss 0.244115 in 0.15s\n",
      " [-] epoch   99/250, train loss 0.247003 in 0.15s\n",
      " [-] epoch  100/250, train loss 0.243707 in 0.16s\n",
      " [-] epoch  101/250, train loss 0.248124 in 0.17s\n",
      " [-] epoch  102/250, train loss 0.229960 in 0.17s\n",
      " [-] epoch  103/250, train loss 0.262967 in 0.17s\n",
      " [-] epoch  104/250, train loss 0.250151 in 0.15s\n",
      " [-] epoch  105/250, train loss 0.243421 in 0.15s\n",
      " [-] epoch  106/250, train loss 0.231493 in 0.15s\n",
      " [-] epoch  107/250, train loss 0.270344 in 0.15s\n",
      " [-] epoch  108/250, train loss 0.240260 in 0.15s\n",
      " [-] epoch  109/250, train loss 0.263639 in 0.15s\n",
      " [-] epoch  110/250, train loss 0.260443 in 0.16s\n",
      " [-] epoch  111/250, train loss 0.251585 in 0.17s\n",
      " [-] epoch  112/250, train loss 0.257116 in 0.16s\n",
      " [-] epoch  113/250, train loss 0.229559 in 0.15s\n",
      " [-] epoch  114/250, train loss 0.244571 in 0.15s\n",
      " [-] epoch  115/250, train loss 0.243895 in 0.15s\n",
      " [-] epoch  116/250, train loss 0.261364 in 0.15s\n",
      " [-] epoch  117/250, train loss 0.258879 in 0.15s\n",
      " [-] epoch  118/250, train loss 0.252337 in 0.15s\n",
      " [-] epoch  119/250, train loss 0.216186 in 0.13s\n",
      " [-] epoch  120/250, train loss 0.233537 in 0.17s\n",
      " [-] epoch  121/250, train loss 0.245873 in 0.15s\n",
      " [-] epoch  122/250, train loss 0.242342 in 0.15s\n",
      " [-] epoch  123/250, train loss 0.238530 in 0.17s\n",
      " [-] epoch  124/250, train loss 0.241345 in 0.15s\n",
      " [-] epoch  125/250, train loss 0.256055 in 0.16s\n",
      " [-] epoch  126/250, train loss 0.256143 in 0.15s\n",
      " [-] epoch  127/250, train loss 0.255901 in 0.15s\n",
      " [-] epoch  128/250, train loss 0.228632 in 0.15s\n",
      " [-] epoch  129/250, train loss 0.270991 in 0.17s\n",
      " [-] epoch  130/250, train loss 0.248413 in 0.15s\n",
      " [-] epoch  131/250, train loss 0.218721 in 0.14s\n",
      " [-] epoch  132/250, train loss 0.223331 in 0.15s\n",
      " [-] epoch  133/250, train loss 0.238451 in 0.15s\n",
      " [-] epoch  134/250, train loss 0.249616 in 0.15s\n",
      " [-] epoch  135/250, train loss 0.248928 in 0.15s\n",
      " [-] epoch  136/250, train loss 0.261582 in 0.13s\n",
      " [-] epoch  137/250, train loss 0.236056 in 0.15s\n",
      " [-] epoch  138/250, train loss 0.224617 in 0.13s\n",
      " [-] epoch  139/250, train loss 0.244981 in 0.15s\n",
      " [-] epoch  140/250, train loss 0.231122 in 0.15s\n",
      " [-] epoch  141/250, train loss 0.227545 in 0.17s\n",
      " [-] epoch  142/250, train loss 0.234975 in 0.13s\n",
      " [-] epoch  143/250, train loss 0.236794 in 0.16s\n",
      " [-] epoch  144/250, train loss 0.240623 in 0.15s\n",
      " [-] epoch  145/250, train loss 0.226298 in 0.14s\n",
      " [-] epoch  146/250, train loss 0.213899 in 0.15s\n",
      " [-] epoch  147/250, train loss 0.244028 in 0.15s\n",
      " [-] epoch  148/250, train loss 0.241106 in 0.17s\n",
      " [-] epoch  149/250, train loss 0.232083 in 0.13s\n",
      " [-] epoch  150/250, train loss 0.222483 in 0.15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.227968 in 0.15s\n",
      " [-] epoch  152/250, train loss 0.249769 in 0.15s\n",
      " [-] epoch  153/250, train loss 0.228946 in 0.15s\n",
      " [-] epoch  154/250, train loss 0.218816 in 0.15s\n",
      " [-] epoch  155/250, train loss 0.253945 in 0.14s\n",
      " [-] epoch  156/250, train loss 0.231303 in 0.14s\n",
      " [-] epoch  157/250, train loss 0.239086 in 0.15s\n",
      " [-] epoch  158/250, train loss 0.224775 in 0.13s\n",
      " [-] epoch  159/250, train loss 0.221559 in 0.15s\n",
      " [-] epoch  160/250, train loss 0.235301 in 0.13s\n",
      " [-] epoch  161/250, train loss 0.230864 in 0.15s\n",
      " [-] epoch  162/250, train loss 0.228827 in 0.13s\n",
      " [-] epoch  163/250, train loss 0.254608 in 0.13s\n",
      " [-] epoch  164/250, train loss 0.224647 in 0.15s\n",
      " [-] epoch  165/250, train loss 0.237250 in 0.15s\n",
      " [-] epoch  166/250, train loss 0.229280 in 0.13s\n",
      " [-] epoch  167/250, train loss 0.219061 in 0.15s\n",
      " [-] epoch  168/250, train loss 0.232664 in 0.13s\n",
      " [-] epoch  169/250, train loss 0.227717 in 0.15s\n",
      " [-] epoch  170/250, train loss 0.233768 in 0.13s\n",
      " [-] epoch  171/250, train loss 0.219353 in 0.13s\n",
      " [-] epoch  172/250, train loss 0.230909 in 0.15s\n",
      " [-] epoch  173/250, train loss 0.228537 in 0.14s\n",
      " [-] epoch  174/250, train loss 0.220646 in 0.15s\n",
      " [-] epoch  175/250, train loss 0.215486 in 0.15s\n",
      " [-] epoch  176/250, train loss 0.238397 in 0.13s\n",
      " [-] epoch  177/250, train loss 0.232625 in 0.12s\n",
      " [-] epoch  178/250, train loss 0.244943 in 0.15s\n",
      " [-] epoch  179/250, train loss 0.234607 in 0.14s\n",
      " [-] epoch  180/250, train loss 0.244927 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.231693 in 0.13s\n",
      " [-] epoch  182/250, train loss 0.229128 in 0.15s\n",
      " [-] epoch  183/250, train loss 0.234215 in 0.13s\n",
      " [-] epoch  184/250, train loss 0.215858 in 0.13s\n",
      " [-] epoch  185/250, train loss 0.226661 in 0.14s\n",
      " [-] epoch  186/250, train loss 0.220779 in 0.13s\n",
      " [-] epoch  187/250, train loss 0.228824 in 0.13s\n",
      " [-] epoch  188/250, train loss 0.229353 in 0.13s\n",
      " [-] epoch  189/250, train loss 0.239242 in 0.15s\n",
      " [-] epoch  190/250, train loss 0.230145 in 0.15s\n",
      " [-] epoch  191/250, train loss 0.254128 in 0.13s\n",
      " [-] epoch  192/250, train loss 0.245532 in 0.15s\n",
      " [-] epoch  193/250, train loss 0.219057 in 0.15s\n",
      " [-] epoch  194/250, train loss 0.208796 in 0.15s\n",
      " [-] epoch  195/250, train loss 0.231802 in 0.12s\n",
      " [-] epoch  196/250, train loss 0.218319 in 0.14s\n",
      " [-] epoch  197/250, train loss 0.217239 in 0.13s\n",
      " [-] epoch  198/250, train loss 0.238982 in 0.15s\n",
      " [-] epoch  199/250, train loss 0.224620 in 0.12s\n",
      " [-] epoch  200/250, train loss 0.238742 in 0.13s\n",
      " [-] epoch  201/250, train loss 0.229756 in 0.15s\n",
      " [-] epoch  202/250, train loss 0.227299 in 0.13s\n",
      " [-] epoch  203/250, train loss 0.215735 in 0.15s\n",
      " [-] epoch  204/250, train loss 0.233437 in 0.13s\n",
      " [-] epoch  205/250, train loss 0.240410 in 0.15s\n",
      " [-] epoch  206/250, train loss 0.223551 in 0.13s\n",
      " [-] epoch  207/250, train loss 0.199061 in 0.13s\n",
      " [-] epoch  208/250, train loss 0.226001 in 0.13s\n",
      " [-] epoch  209/250, train loss 0.225843 in 0.13s\n",
      " [-] epoch  210/250, train loss 0.228586 in 0.15s\n",
      " [-] epoch  211/250, train loss 0.205816 in 0.15s\n",
      " [-] epoch  212/250, train loss 0.233466 in 0.15s\n",
      " [-] epoch  213/250, train loss 0.234170 in 0.14s\n",
      " [-] epoch  214/250, train loss 0.233809 in 0.13s\n",
      " [-] epoch  215/250, train loss 0.216033 in 0.13s\n",
      " [-] epoch  216/250, train loss 0.243383 in 0.15s\n",
      " [-] epoch  217/250, train loss 0.255943 in 0.13s\n",
      " [-] epoch  218/250, train loss 0.224333 in 0.13s\n",
      " [-] epoch  219/250, train loss 0.215023 in 0.13s\n",
      " [-] epoch  220/250, train loss 0.252005 in 0.15s\n",
      " [-] epoch  221/250, train loss 0.229892 in 0.13s\n",
      " [-] epoch  222/250, train loss 0.226712 in 0.13s\n",
      " [-] epoch  223/250, train loss 0.224972 in 0.15s\n",
      " [-] epoch  224/250, train loss 0.235308 in 0.13s\n",
      " [-] epoch  225/250, train loss 0.216514 in 0.13s\n",
      " [-] epoch  226/250, train loss 0.232932 in 0.13s\n",
      " [-] epoch  227/250, train loss 0.229905 in 0.14s\n",
      " [-] epoch  228/250, train loss 0.252696 in 0.12s\n",
      " [-] epoch  229/250, train loss 0.239686 in 0.13s\n",
      " [-] epoch  230/250, train loss 0.227394 in 0.13s\n",
      " [-] epoch  231/250, train loss 0.221519 in 0.12s\n",
      " [-] epoch  232/250, train loss 0.219534 in 0.13s\n",
      " [-] epoch  233/250, train loss 0.220133 in 0.13s\n",
      " [-] epoch  234/250, train loss 0.229382 in 0.15s\n",
      " [-] epoch  235/250, train loss 0.228951 in 0.13s\n",
      " [-] epoch  236/250, train loss 0.208394 in 0.13s\n",
      " [-] epoch  237/250, train loss 0.214012 in 0.13s\n",
      " [-] epoch  238/250, train loss 0.211254 in 0.13s\n",
      " [-] epoch  239/250, train loss 0.216059 in 0.15s\n",
      " [-] epoch  240/250, train loss 0.212193 in 0.13s\n",
      " [-] epoch  241/250, train loss 0.218884 in 0.13s\n",
      " [-] epoch  242/250, train loss 0.227044 in 0.13s\n",
      " [-] epoch  243/250, train loss 0.225732 in 0.13s\n",
      " [-] epoch  244/250, train loss 0.217953 in 0.13s\n",
      " [-] epoch  245/250, train loss 0.219231 in 0.13s\n",
      " [-] epoch  246/250, train loss 0.217011 in 0.15s\n",
      " [-] epoch  247/250, train loss 0.219657 in 0.13s\n",
      " [-] epoch  248/250, train loss 0.237137 in 0.13s\n",
      " [-] epoch  249/250, train loss 0.214867 in 0.13s\n",
      " [-] epoch  250/250, train loss 0.200024 in 0.15s\n",
      " [-] test acc. 83.055556%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.616692 in 0.21s\n",
      " [-] epoch    2/250, train loss 0.468595 in 0.19s\n",
      " [-] epoch    3/250, train loss 0.463234 in 0.20s\n",
      " [-] epoch    4/250, train loss 0.411784 in 0.22s\n",
      " [-] epoch    5/250, train loss 0.415708 in 0.21s\n",
      " [-] epoch    6/250, train loss 0.367618 in 0.21s\n",
      " [-] epoch    7/250, train loss 0.391644 in 0.19s\n",
      " [-] epoch    8/250, train loss 0.370124 in 0.20s\n",
      " [-] epoch    9/250, train loss 0.400628 in 0.21s\n",
      " [-] epoch   10/250, train loss 0.362264 in 0.21s\n",
      " [-] epoch   11/250, train loss 0.361024 in 0.19s\n",
      " [-] epoch   12/250, train loss 0.318215 in 0.20s\n",
      " [-] epoch   13/250, train loss 0.348313 in 0.21s\n",
      " [-] epoch   14/250, train loss 0.335553 in 0.20s\n",
      " [-] epoch   15/250, train loss 0.315102 in 0.21s\n",
      " [-] epoch   16/250, train loss 0.310714 in 0.21s\n",
      " [-] epoch   17/250, train loss 0.338056 in 0.22s\n",
      " [-] epoch   18/250, train loss 0.310857 in 0.19s\n",
      " [-] epoch   19/250, train loss 0.310247 in 0.21s\n",
      " [-] epoch   20/250, train loss 0.314520 in 0.20s\n",
      " [-] epoch   21/250, train loss 0.306964 in 0.21s\n",
      " [-] epoch   22/250, train loss 0.298271 in 0.20s\n",
      " [-] epoch   23/250, train loss 0.292044 in 0.20s\n",
      " [-] epoch   24/250, train loss 0.326117 in 0.19s\n",
      " [-] epoch   25/250, train loss 0.282896 in 0.20s\n",
      " [-] epoch   26/250, train loss 0.296001 in 0.19s\n",
      " [-] epoch   27/250, train loss 0.280809 in 0.21s\n",
      " [-] epoch   28/250, train loss 0.284762 in 0.22s\n",
      " [-] epoch   29/250, train loss 0.286913 in 0.21s\n",
      " [-] epoch   30/250, train loss 0.282020 in 0.23s\n",
      " [-] epoch   31/250, train loss 0.303028 in 0.19s\n",
      " [-] epoch   32/250, train loss 0.275738 in 0.23s\n",
      " [-] epoch   33/250, train loss 0.272796 in 0.21s\n",
      " [-] epoch   34/250, train loss 0.291233 in 0.22s\n",
      " [-] epoch   35/250, train loss 0.290525 in 0.21s\n",
      " [-] epoch   36/250, train loss 0.271115 in 0.21s\n",
      " [-] epoch   37/250, train loss 0.267930 in 0.22s\n",
      " [-] epoch   38/250, train loss 0.270149 in 0.22s\n",
      " [-] epoch   39/250, train loss 0.290833 in 0.20s\n",
      " [-] epoch   40/250, train loss 0.271139 in 0.20s\n",
      " [-] epoch   41/250, train loss 0.271547 in 0.22s\n",
      " [-] epoch   42/250, train loss 0.296888 in 0.22s\n",
      " [-] epoch   43/250, train loss 0.289505 in 0.19s\n",
      " [-] epoch   44/250, train loss 0.271775 in 0.20s\n",
      " [-] epoch   45/250, train loss 0.264227 in 0.20s\n",
      " [-] epoch   46/250, train loss 0.280039 in 0.20s\n",
      " [-] epoch   47/250, train loss 0.263979 in 0.22s\n",
      " [-] epoch   48/250, train loss 0.267761 in 0.20s\n",
      " [-] epoch   49/250, train loss 0.267579 in 0.21s\n",
      " [-] epoch   50/250, train loss 0.250482 in 0.21s\n",
      " [-] epoch   51/250, train loss 0.266653 in 0.20s\n",
      " [-] epoch   52/250, train loss 0.297202 in 0.21s\n",
      " [-] epoch   53/250, train loss 0.242463 in 0.21s\n",
      " [-] epoch   54/250, train loss 0.237143 in 0.21s\n",
      " [-] epoch   55/250, train loss 0.269472 in 0.20s\n",
      " [-] epoch   56/250, train loss 0.255200 in 0.19s\n",
      " [-] epoch   57/250, train loss 0.265850 in 0.22s\n",
      " [-] epoch   58/250, train loss 0.265782 in 0.23s\n",
      " [-] epoch   59/250, train loss 0.259794 in 0.21s\n",
      " [-] epoch   60/250, train loss 0.242126 in 0.22s\n",
      " [-] epoch   61/250, train loss 0.264635 in 0.21s\n",
      " [-] epoch   62/250, train loss 0.245098 in 0.21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   63/250, train loss 0.250287 in 0.20s\n",
      " [-] epoch   64/250, train loss 0.269422 in 0.22s\n",
      " [-] epoch   65/250, train loss 0.278733 in 0.20s\n",
      " [-] epoch   66/250, train loss 0.254127 in 0.22s\n",
      " [-] epoch   67/250, train loss 0.252218 in 0.21s\n",
      " [-] epoch   68/250, train loss 0.246277 in 0.19s\n",
      " [-] epoch   69/250, train loss 0.270233 in 0.20s\n",
      " [-] epoch   70/250, train loss 0.253645 in 0.18s\n",
      " [-] epoch   71/250, train loss 0.256966 in 0.20s\n",
      " [-] epoch   72/250, train loss 0.240483 in 0.21s\n",
      " [-] epoch   73/250, train loss 0.267124 in 0.22s\n",
      " [-] epoch   74/250, train loss 0.245809 in 0.21s\n",
      " [-] epoch   75/250, train loss 0.257684 in 0.20s\n",
      " [-] epoch   76/250, train loss 0.239357 in 0.20s\n",
      " [-] epoch   77/250, train loss 0.260114 in 0.22s\n",
      " [-] epoch   78/250, train loss 0.252699 in 0.23s\n",
      " [-] epoch   79/250, train loss 0.250252 in 0.22s\n",
      " [-] epoch   80/250, train loss 0.245354 in 0.22s\n",
      " [-] epoch   81/250, train loss 0.251261 in 0.21s\n",
      " [-] epoch   82/250, train loss 0.260362 in 0.22s\n",
      " [-] epoch   83/250, train loss 0.254548 in 0.21s\n",
      " [-] epoch   84/250, train loss 0.252588 in 0.22s\n",
      " [-] epoch   85/250, train loss 0.251862 in 0.22s\n",
      " [-] epoch   86/250, train loss 0.262012 in 0.21s\n",
      " [-] epoch   87/250, train loss 0.236206 in 0.22s\n",
      " [-] epoch   88/250, train loss 0.224289 in 0.19s\n",
      " [-] epoch   89/250, train loss 0.225966 in 0.20s\n",
      " [-] epoch   90/250, train loss 0.238371 in 0.19s\n",
      " [-] epoch   91/250, train loss 0.211178 in 0.21s\n",
      " [-] epoch   92/250, train loss 0.253827 in 0.21s\n",
      " [-] epoch   93/250, train loss 0.236852 in 0.23s\n",
      " [-] epoch   94/250, train loss 0.232976 in 0.23s\n",
      " [-] epoch   95/250, train loss 0.242257 in 0.21s\n",
      " [-] epoch   96/250, train loss 0.228185 in 0.21s\n",
      " [-] epoch   97/250, train loss 0.242074 in 0.22s\n",
      " [-] epoch   98/250, train loss 0.230797 in 0.22s\n",
      " [-] epoch   99/250, train loss 0.252877 in 0.21s\n",
      " [-] epoch  100/250, train loss 0.236474 in 0.23s\n",
      " [-] epoch  101/250, train loss 0.235645 in 0.22s\n",
      " [-] epoch  102/250, train loss 0.219946 in 0.22s\n",
      " [-] epoch  103/250, train loss 0.230235 in 0.22s\n",
      " [-] epoch  104/250, train loss 0.218034 in 0.21s\n",
      " [-] epoch  105/250, train loss 0.243929 in 0.23s\n",
      " [-] epoch  106/250, train loss 0.223226 in 0.23s\n",
      " [-] epoch  107/250, train loss 0.247713 in 0.23s\n",
      " [-] epoch  108/250, train loss 0.212801 in 0.23s\n",
      " [-] epoch  109/250, train loss 0.221180 in 0.23s\n",
      " [-] epoch  110/250, train loss 0.231480 in 0.23s\n",
      " [-] epoch  111/250, train loss 0.223957 in 0.23s\n",
      " [-] epoch  112/250, train loss 0.234957 in 0.23s\n",
      " [-] epoch  113/250, train loss 0.230181 in 0.23s\n",
      " [-] epoch  114/250, train loss 0.236758 in 0.23s\n",
      " [-] epoch  115/250, train loss 0.228566 in 0.24s\n",
      " [-] epoch  116/250, train loss 0.230863 in 0.23s\n",
      " [-] epoch  117/250, train loss 0.235266 in 0.22s\n",
      " [-] epoch  118/250, train loss 0.230066 in 0.22s\n",
      " [-] epoch  119/250, train loss 0.222829 in 0.22s\n",
      " [-] epoch  120/250, train loss 0.229661 in 0.22s\n",
      " [-] epoch  121/250, train loss 0.237788 in 0.21s\n",
      " [-] epoch  122/250, train loss 0.224720 in 0.20s\n",
      " [-] epoch  123/250, train loss 0.232088 in 0.24s\n",
      " [-] epoch  124/250, train loss 0.236490 in 0.22s\n",
      " [-] epoch  125/250, train loss 0.216430 in 0.24s\n",
      " [-] epoch  126/250, train loss 0.217676 in 0.20s\n",
      " [-] epoch  127/250, train loss 0.208366 in 0.18s\n",
      " [-] epoch  128/250, train loss 0.237680 in 0.20s\n",
      " [-] epoch  129/250, train loss 0.244944 in 0.20s\n",
      " [-] epoch  130/250, train loss 0.236808 in 0.20s\n",
      " [-] epoch  131/250, train loss 0.214466 in 0.20s\n",
      " [-] epoch  132/250, train loss 0.205761 in 0.21s\n",
      " [-] epoch  133/250, train loss 0.231486 in 0.21s\n",
      " [-] epoch  134/250, train loss 0.218700 in 0.21s\n",
      " [-] epoch  135/250, train loss 0.218771 in 0.21s\n",
      " [-] epoch  136/250, train loss 0.214232 in 0.21s\n",
      " [-] epoch  137/250, train loss 0.242394 in 0.20s\n",
      " [-] epoch  138/250, train loss 0.221545 in 0.22s\n",
      " [-] epoch  139/250, train loss 0.234614 in 0.22s\n",
      " [-] epoch  140/250, train loss 0.214243 in 0.21s\n",
      " [-] epoch  141/250, train loss 0.206911 in 0.23s\n",
      " [-] epoch  142/250, train loss 0.229345 in 0.21s\n",
      " [-] epoch  143/250, train loss 0.219081 in 0.21s\n",
      " [-] epoch  144/250, train loss 0.225788 in 0.22s\n",
      " [-] epoch  145/250, train loss 0.219055 in 0.22s\n",
      " [-] epoch  146/250, train loss 0.217618 in 0.21s\n",
      " [-] epoch  147/250, train loss 0.222014 in 0.20s\n",
      " [-] epoch  148/250, train loss 0.218056 in 0.21s\n",
      " [-] epoch  149/250, train loss 0.230878 in 0.22s\n",
      " [-] epoch  150/250, train loss 0.206262 in 0.22s\n",
      " [-] epoch  151/250, train loss 0.214979 in 0.22s\n",
      " [-] epoch  152/250, train loss 0.200482 in 0.23s\n",
      " [-] epoch  153/250, train loss 0.238167 in 0.23s\n",
      " [-] epoch  154/250, train loss 0.232513 in 0.20s\n",
      " [-] epoch  155/250, train loss 0.214554 in 0.22s\n",
      " [-] epoch  156/250, train loss 0.217270 in 0.22s\n",
      " [-] epoch  157/250, train loss 0.208945 in 0.22s\n",
      " [-] epoch  158/250, train loss 0.226716 in 0.22s\n",
      " [-] epoch  159/250, train loss 0.191026 in 0.22s\n",
      " [-] epoch  160/250, train loss 0.214330 in 0.23s\n",
      " [-] epoch  161/250, train loss 0.222633 in 0.22s\n",
      " [-] epoch  162/250, train loss 0.229368 in 0.23s\n",
      " [-] epoch  163/250, train loss 0.217533 in 0.22s\n",
      " [-] epoch  164/250, train loss 0.219344 in 0.23s\n",
      " [-] epoch  165/250, train loss 0.223501 in 0.24s\n",
      " [-] epoch  166/250, train loss 0.217888 in 0.23s\n",
      " [-] epoch  167/250, train loss 0.220968 in 0.23s\n",
      " [-] epoch  168/250, train loss 0.216946 in 0.24s\n",
      " [-] epoch  169/250, train loss 0.223518 in 0.24s\n",
      " [-] epoch  170/250, train loss 0.209944 in 0.23s\n",
      " [-] epoch  171/250, train loss 0.217369 in 0.24s\n",
      " [-] epoch  172/250, train loss 0.204136 in 0.24s\n",
      " [-] epoch  173/250, train loss 0.215637 in 0.22s\n",
      " [-] epoch  174/250, train loss 0.239520 in 0.24s\n",
      " [-] epoch  175/250, train loss 0.213470 in 0.20s\n",
      " [-] epoch  176/250, train loss 0.212334 in 0.21s\n",
      " [-] epoch  177/250, train loss 0.203824 in 0.22s\n",
      " [-] epoch  178/250, train loss 0.220773 in 0.23s\n",
      " [-] epoch  179/250, train loss 0.200601 in 0.23s\n",
      " [-] epoch  180/250, train loss 0.213822 in 0.22s\n",
      " [-] epoch  181/250, train loss 0.225705 in 0.23s\n",
      " [-] epoch  182/250, train loss 0.226775 in 0.22s\n",
      " [-] epoch  183/250, train loss 0.209075 in 0.22s\n",
      " [-] epoch  184/250, train loss 0.229321 in 0.22s\n",
      " [-] epoch  185/250, train loss 0.215719 in 0.19s\n",
      " [-] epoch  186/250, train loss 0.217629 in 0.22s\n",
      " [-] epoch  187/250, train loss 0.216709 in 0.22s\n",
      " [-] epoch  188/250, train loss 0.205236 in 0.21s\n",
      " [-] epoch  189/250, train loss 0.223109 in 0.21s\n",
      " [-] epoch  190/250, train loss 0.229375 in 0.23s\n",
      " [-] epoch  191/250, train loss 0.230593 in 0.21s\n",
      " [-] epoch  192/250, train loss 0.217219 in 0.24s\n",
      " [-] epoch  193/250, train loss 0.213584 in 0.22s\n",
      " [-] epoch  194/250, train loss 0.222980 in 0.21s\n",
      " [-] epoch  195/250, train loss 0.215071 in 0.22s\n",
      " [-] epoch  196/250, train loss 0.209900 in 0.23s\n",
      " [-] epoch  197/250, train loss 0.211851 in 0.22s\n",
      " [-] epoch  198/250, train loss 0.211498 in 0.22s\n",
      " [-] epoch  199/250, train loss 0.201296 in 0.22s\n",
      " [-] epoch  200/250, train loss 0.195139 in 0.21s\n",
      " [-] epoch  201/250, train loss 0.212472 in 0.21s\n",
      " [-] epoch  202/250, train loss 0.215603 in 0.23s\n",
      " [-] epoch  203/250, train loss 0.206845 in 0.22s\n",
      " [-] epoch  204/250, train loss 0.213483 in 0.22s\n",
      " [-] epoch  205/250, train loss 0.191335 in 0.21s\n",
      " [-] epoch  206/250, train loss 0.232441 in 0.22s\n",
      " [-] epoch  207/250, train loss 0.231662 in 0.21s\n",
      " [-] epoch  208/250, train loss 0.228282 in 0.23s\n",
      " [-] epoch  209/250, train loss 0.217894 in 0.20s\n",
      " [-] epoch  210/250, train loss 0.218387 in 0.22s\n",
      " [-] epoch  211/250, train loss 0.197526 in 0.23s\n",
      " [-] epoch  212/250, train loss 0.219604 in 0.21s\n",
      " [-] epoch  213/250, train loss 0.220643 in 0.23s\n",
      " [-] epoch  214/250, train loss 0.214560 in 0.21s\n",
      " [-] epoch  215/250, train loss 0.208336 in 0.22s\n",
      " [-] epoch  216/250, train loss 0.225017 in 0.21s\n",
      " [-] epoch  217/250, train loss 0.217302 in 0.20s\n",
      " [-] epoch  218/250, train loss 0.218791 in 0.23s\n",
      " [-] epoch  219/250, train loss 0.192051 in 0.21s\n",
      " [-] epoch  220/250, train loss 0.219585 in 0.22s\n",
      " [-] epoch  221/250, train loss 0.206317 in 0.21s\n",
      " [-] epoch  222/250, train loss 0.207255 in 0.21s\n",
      " [-] epoch  223/250, train loss 0.194588 in 0.20s\n",
      " [-] epoch  224/250, train loss 0.202832 in 0.23s\n",
      " [-] epoch  225/250, train loss 0.204517 in 0.20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.204192 in 0.20s\n",
      " [-] epoch  227/250, train loss 0.220727 in 0.23s\n",
      " [-] epoch  228/250, train loss 0.218987 in 0.21s\n",
      " [-] epoch  229/250, train loss 0.218472 in 0.21s\n",
      " [-] epoch  230/250, train loss 0.222913 in 0.19s\n",
      " [-] epoch  231/250, train loss 0.201420 in 0.21s\n",
      " [-] epoch  232/250, train loss 0.215861 in 0.21s\n",
      " [-] epoch  233/250, train loss 0.208750 in 0.21s\n",
      " [-] epoch  234/250, train loss 0.214478 in 0.22s\n",
      " [-] epoch  235/250, train loss 0.206910 in 0.21s\n",
      " [-] epoch  236/250, train loss 0.191383 in 0.21s\n",
      " [-] epoch  237/250, train loss 0.197014 in 0.20s\n",
      " [-] epoch  238/250, train loss 0.188810 in 0.20s\n",
      " [-] epoch  239/250, train loss 0.195770 in 0.21s\n",
      " [-] epoch  240/250, train loss 0.203091 in 0.20s\n",
      " [-] epoch  241/250, train loss 0.217243 in 0.22s\n",
      " [-] epoch  242/250, train loss 0.213428 in 0.21s\n",
      " [-] epoch  243/250, train loss 0.203277 in 0.21s\n",
      " [-] epoch  244/250, train loss 0.204291 in 0.20s\n",
      " [-] epoch  245/250, train loss 0.192533 in 0.21s\n",
      " [-] epoch  246/250, train loss 0.197498 in 0.19s\n",
      " [-] epoch  247/250, train loss 0.184791 in 0.22s\n",
      " [-] epoch  248/250, train loss 0.188453 in 0.22s\n",
      " [-] epoch  249/250, train loss 0.196883 in 0.19s\n",
      " [-] epoch  250/250, train loss 0.194905 in 0.21s\n",
      " [-] test acc. 78.611111%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.561833 in 0.30s\n",
      " [-] epoch    2/250, train loss 0.466412 in 0.32s\n",
      " [-] epoch    3/250, train loss 0.442894 in 0.32s\n",
      " [-] epoch    4/250, train loss 0.386039 in 0.30s\n",
      " [-] epoch    5/250, train loss 0.389071 in 0.31s\n",
      " [-] epoch    6/250, train loss 0.383814 in 0.33s\n",
      " [-] epoch    7/250, train loss 0.364575 in 0.29s\n",
      " [-] epoch    8/250, train loss 0.366174 in 0.33s\n",
      " [-] epoch    9/250, train loss 0.383479 in 0.32s\n",
      " [-] epoch   10/250, train loss 0.345971 in 0.31s\n",
      " [-] epoch   11/250, train loss 0.345349 in 0.33s\n",
      " [-] epoch   12/250, train loss 0.321495 in 0.33s\n",
      " [-] epoch   13/250, train loss 0.341626 in 0.29s\n",
      " [-] epoch   14/250, train loss 0.345909 in 0.30s\n",
      " [-] epoch   15/250, train loss 0.310325 in 0.31s\n",
      " [-] epoch   16/250, train loss 0.329318 in 0.34s\n",
      " [-] epoch   17/250, train loss 0.305348 in 0.30s\n",
      " [-] epoch   18/250, train loss 0.304972 in 0.31s\n",
      " [-] epoch   19/250, train loss 0.297549 in 0.31s\n",
      " [-] epoch   20/250, train loss 0.288946 in 0.30s\n",
      " [-] epoch   21/250, train loss 0.297878 in 0.31s\n",
      " [-] epoch   22/250, train loss 0.310102 in 0.31s\n",
      " [-] epoch   23/250, train loss 0.332187 in 0.30s\n",
      " [-] epoch   24/250, train loss 0.285322 in 0.30s\n",
      " [-] epoch   25/250, train loss 0.289043 in 0.33s\n",
      " [-] epoch   26/250, train loss 0.301690 in 0.32s\n",
      " [-] epoch   27/250, train loss 0.299330 in 0.31s\n",
      " [-] epoch   28/250, train loss 0.286704 in 0.29s\n",
      " [-] epoch   29/250, train loss 0.285244 in 0.30s\n",
      " [-] epoch   30/250, train loss 0.278754 in 0.31s\n",
      " [-] epoch   31/250, train loss 0.272825 in 0.33s\n",
      " [-] epoch   32/250, train loss 0.276028 in 0.30s\n",
      " [-] epoch   33/250, train loss 0.275867 in 0.32s\n",
      " [-] epoch   34/250, train loss 0.278189 in 0.30s\n",
      " [-] epoch   35/250, train loss 0.292597 in 0.30s\n",
      " [-] epoch   36/250, train loss 0.290224 in 0.30s\n",
      " [-] epoch   37/250, train loss 0.299746 in 0.33s\n",
      " [-] epoch   38/250, train loss 0.313202 in 0.31s\n",
      " [-] epoch   39/250, train loss 0.295469 in 0.30s\n",
      " [-] epoch   40/250, train loss 0.271661 in 0.32s\n",
      " [-] epoch   41/250, train loss 0.269170 in 0.30s\n",
      " [-] epoch   42/250, train loss 0.285918 in 0.31s\n",
      " [-] epoch   43/250, train loss 0.266133 in 0.32s\n",
      " [-] epoch   44/250, train loss 0.305604 in 0.32s\n",
      " [-] epoch   45/250, train loss 0.267066 in 0.33s\n",
      " [-] epoch   46/250, train loss 0.280299 in 0.31s\n",
      " [-] epoch   47/250, train loss 0.271030 in 0.29s\n",
      " [-] epoch   48/250, train loss 0.285517 in 0.30s\n",
      " [-] epoch   49/250, train loss 0.281465 in 0.30s\n",
      " [-] epoch   50/250, train loss 0.276293 in 0.31s\n",
      " [-] epoch   51/250, train loss 0.272289 in 0.30s\n",
      " [-] epoch   52/250, train loss 0.276695 in 0.32s\n",
      " [-] epoch   53/250, train loss 0.243907 in 0.31s\n",
      " [-] epoch   54/250, train loss 0.290806 in 0.32s\n",
      " [-] epoch   55/250, train loss 0.258097 in 0.32s\n",
      " [-] epoch   56/250, train loss 0.240706 in 0.35s\n",
      " [-] epoch   57/250, train loss 0.267800 in 0.32s\n",
      " [-] epoch   58/250, train loss 0.260874 in 0.33s\n",
      " [-] epoch   59/250, train loss 0.254947 in 0.31s\n",
      " [-] epoch   60/250, train loss 0.287373 in 0.32s\n",
      " [-] epoch   61/250, train loss 0.263476 in 0.30s\n",
      " [-] epoch   62/250, train loss 0.271682 in 0.33s\n",
      " [-] epoch   63/250, train loss 0.269610 in 0.34s\n",
      " [-] epoch   64/250, train loss 0.271882 in 0.34s\n",
      " [-] epoch   65/250, train loss 0.276572 in 0.34s\n",
      " [-] epoch   66/250, train loss 0.268419 in 0.32s\n",
      " [-] epoch   67/250, train loss 0.261122 in 0.32s\n",
      " [-] epoch   68/250, train loss 0.257886 in 0.33s\n",
      " [-] epoch   69/250, train loss 0.259078 in 0.31s\n",
      " [-] epoch   70/250, train loss 0.246752 in 0.30s\n",
      " [-] epoch   71/250, train loss 0.235073 in 0.31s\n",
      " [-] epoch   72/250, train loss 0.239623 in 0.35s\n",
      " [-] epoch   73/250, train loss 0.265445 in 0.30s\n",
      " [-] epoch   74/250, train loss 0.266684 in 0.34s\n",
      " [-] epoch   75/250, train loss 0.260524 in 0.30s\n",
      " [-] epoch   76/250, train loss 0.253950 in 0.31s\n",
      " [-] epoch   77/250, train loss 0.251456 in 0.30s\n",
      " [-] epoch   78/250, train loss 0.253720 in 0.32s\n",
      " [-] epoch   79/250, train loss 0.253138 in 0.31s\n",
      " [-] epoch   80/250, train loss 0.240480 in 0.30s\n",
      " [-] epoch   81/250, train loss 0.254471 in 0.34s\n",
      " [-] epoch   82/250, train loss 0.240821 in 0.33s\n",
      " [-] epoch   83/250, train loss 0.245335 in 0.34s\n",
      " [-] epoch   84/250, train loss 0.250875 in 0.30s\n",
      " [-] epoch   85/250, train loss 0.262976 in 0.31s\n",
      " [-] epoch   86/250, train loss 0.245977 in 0.31s\n",
      " [-] epoch   87/250, train loss 0.249145 in 0.31s\n",
      " [-] epoch   88/250, train loss 0.251692 in 0.32s\n",
      " [-] epoch   89/250, train loss 0.257941 in 0.32s\n",
      " [-] epoch   90/250, train loss 0.261544 in 0.31s\n",
      " [-] epoch   91/250, train loss 0.251798 in 0.32s\n",
      " [-] epoch   92/250, train loss 0.234954 in 0.32s\n",
      " [-] epoch   93/250, train loss 0.222920 in 0.33s\n",
      " [-] epoch   94/250, train loss 0.232932 in 0.30s\n",
      " [-] epoch   95/250, train loss 0.239658 in 0.34s\n",
      " [-] epoch   96/250, train loss 0.238736 in 0.32s\n",
      " [-] epoch   97/250, train loss 0.235801 in 0.31s\n",
      " [-] epoch   98/250, train loss 0.239512 in 0.31s\n",
      " [-] epoch   99/250, train loss 0.238281 in 0.34s\n",
      " [-] epoch  100/250, train loss 0.239799 in 0.31s\n",
      " [-] epoch  101/250, train loss 0.234369 in 0.32s\n",
      " [-] epoch  102/250, train loss 0.241575 in 0.30s\n",
      " [-] epoch  103/250, train loss 0.232645 in 0.32s\n",
      " [-] epoch  104/250, train loss 0.232028 in 0.30s\n",
      " [-] epoch  105/250, train loss 0.237806 in 0.33s\n",
      " [-] epoch  106/250, train loss 0.228392 in 0.30s\n",
      " [-] epoch  107/250, train loss 0.225589 in 0.31s\n",
      " [-] epoch  108/250, train loss 0.234040 in 0.34s\n",
      " [-] epoch  109/250, train loss 0.228814 in 0.34s\n",
      " [-] epoch  110/250, train loss 0.245872 in 0.32s\n",
      " [-] epoch  111/250, train loss 0.243346 in 0.33s\n",
      " [-] epoch  112/250, train loss 0.234954 in 0.32s\n",
      " [-] epoch  113/250, train loss 0.210646 in 0.32s\n",
      " [-] epoch  114/250, train loss 0.228369 in 0.33s\n",
      " [-] epoch  115/250, train loss 0.238730 in 0.32s\n",
      " [-] epoch  116/250, train loss 0.250391 in 0.31s\n",
      " [-] epoch  117/250, train loss 0.245446 in 0.32s\n",
      " [-] epoch  118/250, train loss 0.242097 in 0.32s\n",
      " [-] epoch  119/250, train loss 0.236828 in 0.32s\n",
      " [-] epoch  120/250, train loss 0.245310 in 0.31s\n",
      " [-] epoch  121/250, train loss 0.242227 in 0.33s\n",
      " [-] epoch  122/250, train loss 0.223990 in 0.33s\n",
      " [-] epoch  123/250, train loss 0.239878 in 0.29s\n",
      " [-] epoch  124/250, train loss 0.216046 in 0.32s\n",
      " [-] epoch  125/250, train loss 0.233101 in 0.32s\n",
      " [-] epoch  126/250, train loss 0.219805 in 0.32s\n",
      " [-] epoch  127/250, train loss 0.228762 in 0.30s\n",
      " [-] epoch  128/250, train loss 0.236714 in 0.32s\n",
      " [-] epoch  129/250, train loss 0.247351 in 0.31s\n",
      " [-] epoch  130/250, train loss 0.231302 in 0.32s\n",
      " [-] epoch  131/250, train loss 0.224675 in 0.31s\n",
      " [-] epoch  132/250, train loss 0.220157 in 0.30s\n",
      " [-] epoch  133/250, train loss 0.229432 in 0.30s\n",
      " [-] epoch  134/250, train loss 0.223564 in 0.31s\n",
      " [-] epoch  135/250, train loss 0.224092 in 0.31s\n",
      " [-] epoch  136/250, train loss 0.220439 in 0.32s\n",
      " [-] epoch  137/250, train loss 0.224883 in 0.35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  138/250, train loss 0.231395 in 0.34s\n",
      " [-] epoch  139/250, train loss 0.230087 in 0.31s\n",
      " [-] epoch  140/250, train loss 0.224450 in 0.36s\n",
      " [-] epoch  141/250, train loss 0.220628 in 0.32s\n",
      " [-] epoch  142/250, train loss 0.231560 in 0.32s\n",
      " [-] epoch  143/250, train loss 0.226314 in 0.31s\n",
      " [-] epoch  144/250, train loss 0.225306 in 0.33s\n",
      " [-] epoch  145/250, train loss 0.222398 in 0.33s\n",
      " [-] epoch  146/250, train loss 0.238202 in 0.30s\n",
      " [-] epoch  147/250, train loss 0.227511 in 0.32s\n",
      " [-] epoch  148/250, train loss 0.237000 in 0.32s\n",
      " [-] epoch  149/250, train loss 0.226649 in 0.30s\n",
      " [-] epoch  150/250, train loss 0.221411 in 0.31s\n",
      " [-] epoch  151/250, train loss 0.217749 in 0.32s\n",
      " [-] epoch  152/250, train loss 0.229349 in 0.32s\n",
      " [-] epoch  153/250, train loss 0.242800 in 0.30s\n",
      " [-] epoch  154/250, train loss 0.225047 in 0.31s\n",
      " [-] epoch  155/250, train loss 0.201508 in 0.32s\n",
      " [-] epoch  156/250, train loss 0.220211 in 0.30s\n",
      " [-] epoch  157/250, train loss 0.220883 in 0.33s\n",
      " [-] epoch  158/250, train loss 0.227679 in 0.31s\n",
      " [-] epoch  159/250, train loss 0.225344 in 0.31s\n",
      " [-] epoch  160/250, train loss 0.203496 in 0.35s\n",
      " [-] epoch  161/250, train loss 0.230379 in 0.32s\n",
      " [-] epoch  162/250, train loss 0.225441 in 0.31s\n",
      " [-] epoch  163/250, train loss 0.232000 in 0.32s\n",
      " [-] epoch  164/250, train loss 0.233870 in 0.30s\n",
      " [-] epoch  165/250, train loss 0.218605 in 0.34s\n",
      " [-] epoch  166/250, train loss 0.209821 in 0.34s\n",
      " [-] epoch  167/250, train loss 0.230921 in 0.31s\n",
      " [-] epoch  168/250, train loss 0.214567 in 0.30s\n",
      " [-] epoch  169/250, train loss 0.225565 in 0.34s\n",
      " [-] epoch  170/250, train loss 0.228941 in 0.30s\n",
      " [-] epoch  171/250, train loss 0.220656 in 0.30s\n",
      " [-] epoch  172/250, train loss 0.222743 in 0.31s\n",
      " [-] epoch  173/250, train loss 0.220437 in 0.31s\n",
      " [-] epoch  174/250, train loss 0.222324 in 0.30s\n",
      " [-] epoch  175/250, train loss 0.206805 in 0.34s\n",
      " [-] epoch  176/250, train loss 0.221300 in 0.32s\n",
      " [-] epoch  177/250, train loss 0.231209 in 0.32s\n",
      " [-] epoch  178/250, train loss 0.230004 in 0.32s\n",
      " [-] epoch  179/250, train loss 0.219275 in 0.32s\n",
      " [-] epoch  180/250, train loss 0.208151 in 0.30s\n",
      " [-] epoch  181/250, train loss 0.206727 in 0.32s\n",
      " [-] epoch  182/250, train loss 0.230626 in 0.30s\n",
      " [-] epoch  183/250, train loss 0.219614 in 0.30s\n",
      " [-] epoch  184/250, train loss 0.210344 in 0.32s\n",
      " [-] epoch  185/250, train loss 0.201289 in 0.32s\n",
      " [-] epoch  186/250, train loss 0.215048 in 0.32s\n",
      " [-] epoch  187/250, train loss 0.218130 in 0.32s\n",
      " [-] epoch  188/250, train loss 0.197720 in 0.31s\n",
      " [-] epoch  189/250, train loss 0.210319 in 0.32s\n",
      " [-] epoch  190/250, train loss 0.206697 in 0.31s\n",
      " [-] epoch  191/250, train loss 0.218847 in 0.31s\n",
      " [-] epoch  192/250, train loss 0.211205 in 0.31s\n",
      " [-] epoch  193/250, train loss 0.219960 in 0.30s\n",
      " [-] epoch  194/250, train loss 0.204280 in 0.30s\n",
      " [-] epoch  195/250, train loss 0.207991 in 0.32s\n",
      " [-] epoch  196/250, train loss 0.199312 in 0.33s\n",
      " [-] epoch  197/250, train loss 0.193496 in 0.33s\n",
      " [-] epoch  198/250, train loss 0.216027 in 0.32s\n",
      " [-] epoch  199/250, train loss 0.211133 in 0.32s\n",
      " [-] epoch  200/250, train loss 0.222316 in 0.32s\n",
      " [-] epoch  201/250, train loss 0.228101 in 0.34s\n",
      " [-] epoch  202/250, train loss 0.211915 in 0.31s\n",
      " [-] epoch  203/250, train loss 0.219980 in 0.33s\n",
      " [-] epoch  204/250, train loss 0.208535 in 0.30s\n",
      " [-] epoch  205/250, train loss 0.205626 in 0.32s\n",
      " [-] epoch  206/250, train loss 0.221810 in 0.32s\n",
      " [-] epoch  207/250, train loss 0.202112 in 0.32s\n",
      " [-] epoch  208/250, train loss 0.237529 in 0.34s\n",
      " [-] epoch  209/250, train loss 0.221070 in 0.31s\n",
      " [-] epoch  210/250, train loss 0.218346 in 0.30s\n",
      " [-] epoch  211/250, train loss 0.213300 in 0.33s\n",
      " [-] epoch  212/250, train loss 0.226219 in 0.31s\n",
      " [-] epoch  213/250, train loss 0.217827 in 0.32s\n",
      " [-] epoch  214/250, train loss 0.221628 in 0.33s\n",
      " [-] epoch  215/250, train loss 0.222188 in 0.30s\n",
      " [-] epoch  216/250, train loss 0.201043 in 0.32s\n",
      " [-] epoch  217/250, train loss 0.205590 in 0.29s\n",
      " [-] epoch  218/250, train loss 0.223438 in 0.29s\n",
      " [-] epoch  219/250, train loss 0.206840 in 0.33s\n",
      " [-] epoch  220/250, train loss 0.205322 in 0.30s\n",
      " [-] epoch  221/250, train loss 0.206807 in 0.32s\n",
      " [-] epoch  222/250, train loss 0.217271 in 0.33s\n",
      " [-] epoch  223/250, train loss 0.218368 in 0.32s\n",
      " [-] epoch  224/250, train loss 0.200591 in 0.32s\n",
      " [-] epoch  225/250, train loss 0.205908 in 0.31s\n",
      " [-] epoch  226/250, train loss 0.219446 in 0.30s\n",
      " [-] epoch  227/250, train loss 0.203101 in 0.33s\n",
      " [-] epoch  228/250, train loss 0.191961 in 0.33s\n",
      " [-] epoch  229/250, train loss 0.198430 in 0.32s\n",
      " [-] epoch  230/250, train loss 0.205652 in 0.32s\n",
      " [-] epoch  231/250, train loss 0.226536 in 0.30s\n",
      " [-] epoch  232/250, train loss 0.230853 in 0.32s\n",
      " [-] epoch  233/250, train loss 0.219257 in 0.31s\n",
      " [-] epoch  234/250, train loss 0.206491 in 0.31s\n",
      " [-] epoch  235/250, train loss 0.206719 in 0.32s\n",
      " [-] epoch  236/250, train loss 0.222315 in 0.34s\n",
      " [-] epoch  237/250, train loss 0.207926 in 0.30s\n",
      " [-] epoch  238/250, train loss 0.198823 in 0.34s\n",
      " [-] epoch  239/250, train loss 0.217272 in 0.34s\n",
      " [-] epoch  240/250, train loss 0.209491 in 0.32s\n",
      " [-] epoch  241/250, train loss 0.200766 in 0.34s\n",
      " [-] epoch  242/250, train loss 0.213172 in 0.32s\n",
      " [-] epoch  243/250, train loss 0.209964 in 0.32s\n",
      " [-] epoch  244/250, train loss 0.215812 in 0.34s\n",
      " [-] epoch  245/250, train loss 0.183096 in 0.31s\n",
      " [-] epoch  246/250, train loss 0.201124 in 0.30s\n",
      " [-] epoch  247/250, train loss 0.213291 in 0.33s\n",
      " [-] epoch  248/250, train loss 0.215437 in 0.31s\n",
      " [-] epoch  249/250, train loss 0.205216 in 0.33s\n",
      " [-] epoch  250/250, train loss 0.208615 in 0.32s\n",
      " [-] test acc. 80.000000%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.603556 in 0.39s\n",
      " [-] epoch    2/250, train loss 0.461621 in 0.38s\n",
      " [-] epoch    3/250, train loss 0.440689 in 0.42s\n",
      " [-] epoch    4/250, train loss 0.421591 in 0.40s\n",
      " [-] epoch    5/250, train loss 0.382540 in 0.44s\n",
      " [-] epoch    6/250, train loss 0.380232 in 0.41s\n",
      " [-] epoch    7/250, train loss 0.363862 in 0.42s\n",
      " [-] epoch    8/250, train loss 0.334937 in 0.42s\n",
      " [-] epoch    9/250, train loss 0.363788 in 0.42s\n",
      " [-] epoch   10/250, train loss 0.360122 in 0.46s\n",
      " [-] epoch   11/250, train loss 0.350330 in 0.41s\n",
      " [-] epoch   12/250, train loss 0.322088 in 0.46s\n",
      " [-] epoch   13/250, train loss 0.327590 in 0.44s\n",
      " [-] epoch   14/250, train loss 0.361725 in 0.44s\n",
      " [-] epoch   15/250, train loss 0.304684 in 0.42s\n",
      " [-] epoch   16/250, train loss 0.333587 in 0.45s\n",
      " [-] epoch   17/250, train loss 0.339064 in 0.41s\n",
      " [-] epoch   18/250, train loss 0.328770 in 0.43s\n",
      " [-] epoch   19/250, train loss 0.318837 in 0.42s\n",
      " [-] epoch   20/250, train loss 0.310524 in 0.39s\n",
      " [-] epoch   21/250, train loss 0.300097 in 0.41s\n",
      " [-] epoch   22/250, train loss 0.325086 in 0.44s\n",
      " [-] epoch   23/250, train loss 0.332415 in 0.44s\n",
      " [-] epoch   24/250, train loss 0.310980 in 0.42s\n",
      " [-] epoch   25/250, train loss 0.306596 in 0.42s\n",
      " [-] epoch   26/250, train loss 0.285898 in 0.41s\n",
      " [-] epoch   27/250, train loss 0.303984 in 0.42s\n",
      " [-] epoch   28/250, train loss 0.298450 in 0.44s\n",
      " [-] epoch   29/250, train loss 0.297432 in 0.41s\n",
      " [-] epoch   30/250, train loss 0.298956 in 0.41s\n",
      " [-] epoch   31/250, train loss 0.278299 in 0.42s\n",
      " [-] epoch   32/250, train loss 0.298474 in 0.39s\n",
      " [-] epoch   33/250, train loss 0.313343 in 0.42s\n",
      " [-] epoch   34/250, train loss 0.284220 in 0.42s\n",
      " [-] epoch   35/250, train loss 0.273975 in 0.42s\n",
      " [-] epoch   36/250, train loss 0.279100 in 0.41s\n",
      " [-] epoch   37/250, train loss 0.288429 in 0.40s\n",
      " [-] epoch   38/250, train loss 0.267468 in 0.39s\n",
      " [-] epoch   39/250, train loss 0.280815 in 0.41s\n",
      " [-] epoch   40/250, train loss 0.290889 in 0.40s\n",
      " [-] epoch   41/250, train loss 0.285907 in 0.40s\n",
      " [-] epoch   42/250, train loss 0.282804 in 0.42s\n",
      " [-] epoch   43/250, train loss 0.280093 in 0.41s\n",
      " [-] epoch   44/250, train loss 0.276079 in 0.41s\n",
      " [-] epoch   45/250, train loss 0.271740 in 0.41s\n",
      " [-] epoch   46/250, train loss 0.260918 in 0.42s\n",
      " [-] epoch   47/250, train loss 0.266549 in 0.41s\n",
      " [-] epoch   48/250, train loss 0.282133 in 0.38s\n",
      " [-] epoch   49/250, train loss 0.301489 in 0.42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.273279 in 0.40s\n",
      " [-] epoch   51/250, train loss 0.276461 in 0.43s\n",
      " [-] epoch   52/250, train loss 0.289009 in 0.41s\n",
      " [-] epoch   53/250, train loss 0.266421 in 0.43s\n",
      " [-] epoch   54/250, train loss 0.250611 in 0.40s\n",
      " [-] epoch   55/250, train loss 0.286093 in 0.40s\n",
      " [-] epoch   56/250, train loss 0.281653 in 0.42s\n",
      " [-] epoch   57/250, train loss 0.264741 in 0.40s\n",
      " [-] epoch   58/250, train loss 0.252563 in 0.38s\n",
      " [-] epoch   59/250, train loss 0.280141 in 0.42s\n",
      " [-] epoch   60/250, train loss 0.269041 in 0.40s\n",
      " [-] epoch   61/250, train loss 0.239710 in 0.44s\n",
      " [-] epoch   62/250, train loss 0.252984 in 0.42s\n",
      " [-] epoch   63/250, train loss 0.258506 in 0.42s\n",
      " [-] epoch   64/250, train loss 0.276205 in 0.43s\n",
      " [-] epoch   65/250, train loss 0.252156 in 0.43s\n",
      " [-] epoch   66/250, train loss 0.249483 in 0.42s\n",
      " [-] epoch   67/250, train loss 0.248084 in 0.41s\n",
      " [-] epoch   68/250, train loss 0.261335 in 0.39s\n",
      " [-] epoch   69/250, train loss 0.255085 in 0.41s\n",
      " [-] epoch   70/250, train loss 0.249045 in 0.43s\n",
      " [-] epoch   71/250, train loss 0.267089 in 0.43s\n",
      " [-] epoch   72/250, train loss 0.272855 in 0.38s\n",
      " [-] epoch   73/250, train loss 0.247445 in 0.40s\n",
      " [-] epoch   74/250, train loss 0.240747 in 0.41s\n",
      " [-] epoch   75/250, train loss 0.244768 in 0.40s\n",
      " [-] epoch   76/250, train loss 0.229055 in 0.41s\n",
      " [-] epoch   77/250, train loss 0.247699 in 0.41s\n",
      " [-] epoch   78/250, train loss 0.230766 in 0.42s\n",
      " [-] epoch   79/250, train loss 0.260796 in 0.41s\n",
      " [-] epoch   80/250, train loss 0.239827 in 0.41s\n",
      " [-] epoch   81/250, train loss 0.237722 in 0.42s\n",
      " [-] epoch   82/250, train loss 0.245827 in 0.42s\n",
      " [-] epoch   83/250, train loss 0.228139 in 0.42s\n",
      " [-] epoch   84/250, train loss 0.242108 in 0.42s\n",
      " [-] epoch   85/250, train loss 0.245682 in 0.41s\n",
      " [-] epoch   86/250, train loss 0.224259 in 0.40s\n",
      " [-] epoch   87/250, train loss 0.253067 in 0.44s\n",
      " [-] epoch   88/250, train loss 0.235545 in 0.41s\n",
      " [-] epoch   89/250, train loss 0.239070 in 0.42s\n",
      " [-] epoch   90/250, train loss 0.251784 in 0.42s\n",
      " [-] epoch   91/250, train loss 0.248177 in 0.42s\n",
      " [-] epoch   92/250, train loss 0.268212 in 0.42s\n",
      " [-] epoch   93/250, train loss 0.234824 in 0.40s\n",
      " [-] epoch   94/250, train loss 0.202131 in 0.42s\n",
      " [-] epoch   95/250, train loss 0.255128 in 0.41s\n",
      " [-] epoch   96/250, train loss 0.254328 in 0.44s\n",
      " [-] epoch   97/250, train loss 0.240247 in 0.40s\n",
      " [-] epoch   98/250, train loss 0.243784 in 0.42s\n",
      " [-] epoch   99/250, train loss 0.242680 in 0.41s\n",
      " [-] epoch  100/250, train loss 0.236652 in 0.42s\n",
      " [-] epoch  101/250, train loss 0.245920 in 0.41s\n",
      " [-] epoch  102/250, train loss 0.229740 in 0.40s\n",
      " [-] epoch  103/250, train loss 0.238100 in 0.43s\n",
      " [-] epoch  104/250, train loss 0.229017 in 0.42s\n",
      " [-] epoch  105/250, train loss 0.246108 in 0.39s\n",
      " [-] epoch  106/250, train loss 0.246927 in 0.42s\n",
      " [-] epoch  107/250, train loss 0.223221 in 0.38s\n",
      " [-] epoch  108/250, train loss 0.248926 in 0.41s\n",
      " [-] epoch  109/250, train loss 0.222956 in 0.41s\n",
      " [-] epoch  110/250, train loss 0.241743 in 0.41s\n",
      " [-] epoch  111/250, train loss 0.232381 in 0.41s\n",
      " [-] epoch  112/250, train loss 0.244338 in 0.41s\n",
      " [-] epoch  113/250, train loss 0.223295 in 0.45s\n",
      " [-] epoch  114/250, train loss 0.244411 in 0.39s\n",
      " [-] epoch  115/250, train loss 0.219731 in 0.42s\n",
      " [-] epoch  116/250, train loss 0.210105 in 0.42s\n",
      " [-] epoch  117/250, train loss 0.225461 in 0.39s\n",
      " [-] epoch  118/250, train loss 0.227669 in 0.44s\n",
      " [-] epoch  119/250, train loss 0.244192 in 0.41s\n",
      " [-] epoch  120/250, train loss 0.243359 in 0.41s\n",
      " [-] epoch  121/250, train loss 0.229641 in 0.41s\n",
      " [-] epoch  122/250, train loss 0.231556 in 0.42s\n",
      " [-] epoch  123/250, train loss 0.221451 in 0.39s\n",
      " [-] epoch  124/250, train loss 0.216089 in 0.40s\n",
      " [-] epoch  125/250, train loss 0.248118 in 0.42s\n",
      " [-] epoch  126/250, train loss 0.212219 in 0.41s\n",
      " [-] epoch  127/250, train loss 0.213107 in 0.40s\n",
      " [-] epoch  128/250, train loss 0.226422 in 0.42s\n",
      " [-] epoch  129/250, train loss 0.219331 in 0.44s\n",
      " [-] epoch  130/250, train loss 0.226750 in 0.43s\n",
      " [-] epoch  131/250, train loss 0.253805 in 0.41s\n",
      " [-] epoch  132/250, train loss 0.221259 in 0.40s\n",
      " [-] epoch  133/250, train loss 0.218571 in 0.41s\n",
      " [-] epoch  134/250, train loss 0.212422 in 0.43s\n",
      " [-] epoch  135/250, train loss 0.238459 in 0.44s\n",
      " [-] epoch  136/250, train loss 0.227346 in 0.43s\n",
      " [-] epoch  137/250, train loss 0.255999 in 0.41s\n",
      " [-] epoch  138/250, train loss 0.223702 in 0.43s\n",
      " [-] epoch  139/250, train loss 0.203212 in 0.44s\n",
      " [-] epoch  140/250, train loss 0.239317 in 0.41s\n",
      " [-] epoch  141/250, train loss 0.233474 in 0.40s\n",
      " [-] epoch  142/250, train loss 0.211290 in 0.42s\n",
      " [-] epoch  143/250, train loss 0.217090 in 0.40s\n",
      " [-] epoch  144/250, train loss 0.214029 in 0.40s\n",
      " [-] epoch  145/250, train loss 0.234437 in 0.42s\n",
      " [-] epoch  146/250, train loss 0.229937 in 0.42s\n",
      " [-] epoch  147/250, train loss 0.250232 in 0.42s\n",
      " [-] epoch  148/250, train loss 0.218991 in 0.40s\n",
      " [-] epoch  149/250, train loss 0.204953 in 0.39s\n",
      " [-] epoch  150/250, train loss 0.237492 in 0.40s\n",
      " [-] epoch  151/250, train loss 0.213723 in 0.40s\n",
      " [-] epoch  152/250, train loss 0.220735 in 0.42s\n",
      " [-] epoch  153/250, train loss 0.235067 in 0.39s\n",
      " [-] epoch  154/250, train loss 0.214200 in 0.39s\n",
      " [-] epoch  155/250, train loss 0.221043 in 0.41s\n",
      " [-] epoch  156/250, train loss 0.202362 in 0.41s\n",
      " [-] epoch  157/250, train loss 0.211346 in 0.38s\n",
      " [-] epoch  158/250, train loss 0.234426 in 0.40s\n",
      " [-] epoch  159/250, train loss 0.231750 in 0.42s\n",
      " [-] epoch  160/250, train loss 0.206515 in 0.40s\n",
      " [-] epoch  161/250, train loss 0.209187 in 0.40s\n",
      " [-] epoch  162/250, train loss 0.212552 in 0.42s\n",
      " [-] epoch  163/250, train loss 0.187394 in 0.45s\n",
      " [-] epoch  164/250, train loss 0.232097 in 0.42s\n",
      " [-] epoch  165/250, train loss 0.208526 in 0.44s\n",
      " [-] epoch  166/250, train loss 0.209416 in 0.44s\n",
      " [-] epoch  167/250, train loss 0.204312 in 0.41s\n",
      " [-] epoch  168/250, train loss 0.218353 in 0.41s\n",
      " [-] epoch  169/250, train loss 0.210437 in 0.42s\n",
      " [-] epoch  170/250, train loss 0.198708 in 0.39s\n",
      " [-] epoch  171/250, train loss 0.209976 in 0.42s\n",
      " [-] epoch  172/250, train loss 0.230740 in 0.41s\n",
      " [-] epoch  173/250, train loss 0.227031 in 0.40s\n",
      " [-] epoch  174/250, train loss 0.238535 in 0.46s\n",
      " [-] epoch  175/250, train loss 0.221219 in 0.39s\n",
      " [-] epoch  176/250, train loss 0.235115 in 0.40s\n",
      " [-] epoch  177/250, train loss 0.214379 in 0.39s\n",
      " [-] epoch  178/250, train loss 0.206550 in 0.39s\n",
      " [-] epoch  179/250, train loss 0.203758 in 0.38s\n",
      " [-] epoch  180/250, train loss 0.199415 in 0.41s\n",
      " [-] epoch  181/250, train loss 0.218591 in 0.43s\n",
      " [-] epoch  182/250, train loss 0.223617 in 0.42s\n",
      " [-] epoch  183/250, train loss 0.202877 in 0.41s\n",
      " [-] epoch  184/250, train loss 0.215174 in 0.41s\n",
      " [-] epoch  185/250, train loss 0.209949 in 0.42s\n",
      " [-] epoch  186/250, train loss 0.196509 in 0.42s\n",
      " [-] epoch  187/250, train loss 0.195344 in 0.43s\n",
      " [-] epoch  188/250, train loss 0.198139 in 0.44s\n",
      " [-] epoch  189/250, train loss 0.187034 in 0.41s\n",
      " [-] epoch  190/250, train loss 0.197320 in 0.41s\n",
      " [-] epoch  191/250, train loss 0.219425 in 0.41s\n",
      " [-] epoch  192/250, train loss 0.205281 in 0.41s\n",
      " [-] epoch  193/250, train loss 0.221936 in 0.45s\n",
      " [-] epoch  194/250, train loss 0.224614 in 0.42s\n",
      " [-] epoch  195/250, train loss 0.209628 in 0.45s\n",
      " [-] epoch  196/250, train loss 0.188304 in 0.40s\n",
      " [-] epoch  197/250, train loss 0.199804 in 0.42s\n",
      " [-] epoch  198/250, train loss 0.193926 in 0.44s\n",
      " [-] epoch  199/250, train loss 0.217488 in 0.43s\n",
      " [-] epoch  200/250, train loss 0.216247 in 0.40s\n",
      " [-] epoch  201/250, train loss 0.212756 in 0.40s\n",
      " [-] epoch  202/250, train loss 0.218707 in 0.41s\n",
      " [-] epoch  203/250, train loss 0.208566 in 0.41s\n",
      " [-] epoch  204/250, train loss 0.215403 in 0.43s\n",
      " [-] epoch  205/250, train loss 0.191845 in 0.40s\n",
      " [-] epoch  206/250, train loss 0.207050 in 0.43s\n",
      " [-] epoch  207/250, train loss 0.213002 in 0.39s\n",
      " [-] epoch  208/250, train loss 0.205443 in 0.39s\n",
      " [-] epoch  209/250, train loss 0.210103 in 0.41s\n",
      " [-] epoch  210/250, train loss 0.217665 in 0.40s\n",
      " [-] epoch  211/250, train loss 0.211511 in 0.41s\n",
      " [-] epoch  212/250, train loss 0.196887 in 0.41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  213/250, train loss 0.212187 in 0.41s\n",
      " [-] epoch  214/250, train loss 0.204288 in 0.40s\n",
      " [-] epoch  215/250, train loss 0.213572 in 0.42s\n",
      " [-] epoch  216/250, train loss 0.211378 in 0.42s\n",
      " [-] epoch  217/250, train loss 0.226516 in 0.43s\n",
      " [-] epoch  218/250, train loss 0.215574 in 0.44s\n",
      " [-] epoch  219/250, train loss 0.191364 in 0.44s\n",
      " [-] epoch  220/250, train loss 0.203596 in 0.44s\n",
      " [-] epoch  221/250, train loss 0.211390 in 0.44s\n",
      " [-] epoch  222/250, train loss 0.196838 in 0.46s\n",
      " [-] epoch  223/250, train loss 0.219917 in 0.46s\n",
      " [-] epoch  224/250, train loss 0.210412 in 0.44s\n",
      " [-] epoch  225/250, train loss 0.201715 in 0.41s\n",
      " [-] epoch  226/250, train loss 0.205574 in 0.42s\n",
      " [-] epoch  227/250, train loss 0.197051 in 0.40s\n",
      " [-] epoch  228/250, train loss 0.185865 in 0.40s\n",
      " [-] epoch  229/250, train loss 0.192022 in 0.40s\n",
      " [-] epoch  230/250, train loss 0.200653 in 0.40s\n",
      " [-] epoch  231/250, train loss 0.195299 in 0.41s\n",
      " [-] epoch  232/250, train loss 0.193022 in 0.38s\n",
      " [-] epoch  233/250, train loss 0.201918 in 0.39s\n",
      " [-] epoch  234/250, train loss 0.194494 in 0.44s\n",
      " [-] epoch  235/250, train loss 0.187748 in 0.41s\n",
      " [-] epoch  236/250, train loss 0.206329 in 0.42s\n",
      " [-] epoch  237/250, train loss 0.193385 in 0.41s\n",
      " [-] epoch  238/250, train loss 0.188831 in 0.41s\n",
      " [-] epoch  239/250, train loss 0.176375 in 0.43s\n",
      " [-] epoch  240/250, train loss 0.214821 in 0.40s\n",
      " [-] epoch  241/250, train loss 0.199464 in 0.41s\n",
      " [-] epoch  242/250, train loss 0.189469 in 0.41s\n",
      " [-] epoch  243/250, train loss 0.193666 in 0.38s\n",
      " [-] epoch  244/250, train loss 0.200123 in 0.43s\n",
      " [-] epoch  245/250, train loss 0.207584 in 0.40s\n",
      " [-] epoch  246/250, train loss 0.205754 in 0.41s\n",
      " [-] epoch  247/250, train loss 0.216201 in 0.42s\n",
      " [-] epoch  248/250, train loss 0.203218 in 0.40s\n",
      " [-] epoch  249/250, train loss 0.204278 in 0.43s\n",
      " [-] epoch  250/250, train loss 0.189934 in 0.40s\n",
      " [-] test acc. 86.388889%\n",
      "Je vais utiliser 7 layers\n",
      " [-] epoch    1/250, train loss 0.587645 in 0.49s\n",
      " [-] epoch    2/250, train loss 0.461194 in 0.53s\n",
      " [-] epoch    3/250, train loss 0.430543 in 0.51s\n",
      " [-] epoch    4/250, train loss 0.403192 in 0.52s\n",
      " [-] epoch    5/250, train loss 0.414634 in 0.53s\n",
      " [-] epoch    6/250, train loss 0.423402 in 0.52s\n",
      " [-] epoch    7/250, train loss 0.372853 in 0.51s\n",
      " [-] epoch    8/250, train loss 0.368671 in 0.50s\n",
      " [-] epoch    9/250, train loss 0.342562 in 0.49s\n",
      " [-] epoch   10/250, train loss 0.362472 in 0.53s\n",
      " [-] epoch   11/250, train loss 0.352696 in 0.51s\n",
      " [-] epoch   12/250, train loss 0.342873 in 0.52s\n",
      " [-] epoch   13/250, train loss 0.330630 in 0.51s\n",
      " [-] epoch   14/250, train loss 0.326124 in 0.51s\n",
      " [-] epoch   15/250, train loss 0.357289 in 0.50s\n",
      " [-] epoch   16/250, train loss 0.339049 in 0.49s\n",
      " [-] epoch   17/250, train loss 0.335864 in 0.53s\n",
      " [-] epoch   18/250, train loss 0.337437 in 0.51s\n",
      " [-] epoch   19/250, train loss 0.319025 in 0.54s\n",
      " [-] epoch   20/250, train loss 0.308922 in 0.53s\n",
      " [-] epoch   21/250, train loss 0.325837 in 0.51s\n",
      " [-] epoch   22/250, train loss 0.313274 in 0.54s\n",
      " [-] epoch   23/250, train loss 0.313741 in 0.52s\n",
      " [-] epoch   24/250, train loss 0.292924 in 0.49s\n",
      " [-] epoch   25/250, train loss 0.299672 in 0.51s\n",
      " [-] epoch   26/250, train loss 0.307070 in 0.53s\n",
      " [-] epoch   27/250, train loss 0.326251 in 0.49s\n",
      " [-] epoch   28/250, train loss 0.311271 in 0.53s\n",
      " [-] epoch   29/250, train loss 0.294924 in 0.50s\n",
      " [-] epoch   30/250, train loss 0.326753 in 0.50s\n",
      " [-] epoch   31/250, train loss 0.285838 in 0.50s\n",
      " [-] epoch   32/250, train loss 0.301007 in 0.50s\n",
      " [-] epoch   33/250, train loss 0.300374 in 0.50s\n",
      " [-] epoch   34/250, train loss 0.308816 in 0.50s\n",
      " [-] epoch   35/250, train loss 0.292470 in 0.51s\n",
      " [-] epoch   36/250, train loss 0.292803 in 0.50s\n",
      " [-] epoch   37/250, train loss 0.275015 in 0.51s\n",
      " [-] epoch   38/250, train loss 0.293651 in 0.52s\n",
      " [-] epoch   39/250, train loss 0.294378 in 0.53s\n",
      " [-] epoch   40/250, train loss 0.299108 in 0.50s\n",
      " [-] epoch   41/250, train loss 0.280666 in 0.49s\n",
      " [-] epoch   42/250, train loss 0.271602 in 0.52s\n",
      " [-] epoch   43/250, train loss 0.270539 in 0.56s\n",
      " [-] epoch   44/250, train loss 0.279214 in 0.53s\n",
      " [-] epoch   45/250, train loss 0.279464 in 0.51s\n",
      " [-] epoch   46/250, train loss 0.294019 in 0.54s\n",
      " [-] epoch   47/250, train loss 0.260806 in 0.53s\n",
      " [-] epoch   48/250, train loss 0.253718 in 0.51s\n",
      " [-] epoch   49/250, train loss 0.283387 in 0.52s\n",
      " [-] epoch   50/250, train loss 0.258397 in 0.51s\n",
      " [-] epoch   51/250, train loss 0.274569 in 0.53s\n",
      " [-] epoch   52/250, train loss 0.272095 in 0.54s\n",
      " [-] epoch   53/250, train loss 0.281017 in 0.50s\n",
      " [-] epoch   54/250, train loss 0.296522 in 0.51s\n",
      " [-] epoch   55/250, train loss 0.269292 in 0.50s\n",
      " [-] epoch   56/250, train loss 0.273207 in 0.51s\n",
      " [-] epoch   57/250, train loss 0.258539 in 0.50s\n",
      " [-] epoch   58/250, train loss 0.252344 in 0.49s\n",
      " [-] epoch   59/250, train loss 0.260720 in 0.50s\n",
      " [-] epoch   60/250, train loss 0.265321 in 0.51s\n",
      " [-] epoch   61/250, train loss 0.261762 in 0.50s\n",
      " [-] epoch   62/250, train loss 0.265075 in 0.53s\n",
      " [-] epoch   63/250, train loss 0.272290 in 0.54s\n",
      " [-] epoch   64/250, train loss 0.287119 in 0.52s\n",
      " [-] epoch   65/250, train loss 0.273009 in 0.54s\n",
      " [-] epoch   66/250, train loss 0.237332 in 0.51s\n",
      " [-] epoch   67/250, train loss 0.249586 in 0.52s\n",
      " [-] epoch   68/250, train loss 0.237665 in 0.52s\n",
      " [-] epoch   69/250, train loss 0.250560 in 0.53s\n",
      " [-] epoch   70/250, train loss 0.255749 in 0.53s\n",
      " [-] epoch   71/250, train loss 0.260553 in 0.54s\n",
      " [-] epoch   72/250, train loss 0.237883 in 0.54s\n",
      " [-] epoch   73/250, train loss 0.278970 in 0.53s\n",
      " [-] epoch   74/250, train loss 0.254524 in 0.50s\n",
      " [-] epoch   75/250, train loss 0.238063 in 0.56s\n",
      " [-] epoch   76/250, train loss 0.255090 in 0.54s\n",
      " [-] epoch   77/250, train loss 0.277110 in 0.54s\n",
      " [-] epoch   78/250, train loss 0.252835 in 0.50s\n",
      " [-] epoch   79/250, train loss 0.262483 in 0.50s\n",
      " [-] epoch   80/250, train loss 0.270991 in 0.49s\n",
      " [-] epoch   81/250, train loss 0.241406 in 0.50s\n",
      " [-] epoch   82/250, train loss 0.271608 in 0.52s\n",
      " [-] epoch   83/250, train loss 0.247473 in 0.54s\n",
      " [-] epoch   84/250, train loss 0.235260 in 0.57s\n",
      " [-] epoch   85/250, train loss 0.245353 in 0.52s\n",
      " [-] epoch   86/250, train loss 0.238026 in 0.51s\n",
      " [-] epoch   87/250, train loss 0.245369 in 0.54s\n",
      " [-] epoch   88/250, train loss 0.231421 in 0.53s\n",
      " [-] epoch   89/250, train loss 0.241066 in 0.52s\n",
      " [-] epoch   90/250, train loss 0.241555 in 0.54s\n",
      " [-] epoch   91/250, train loss 0.249060 in 0.52s\n",
      " [-] epoch   92/250, train loss 0.223546 in 0.54s\n",
      " [-] epoch   93/250, train loss 0.240780 in 0.54s\n",
      " [-] epoch   94/250, train loss 0.246314 in 0.53s\n",
      " [-] epoch   95/250, train loss 0.239614 in 0.53s\n",
      " [-] epoch   96/250, train loss 0.230435 in 0.52s\n",
      " [-] epoch   97/250, train loss 0.217102 in 0.52s\n",
      " [-] epoch   98/250, train loss 0.263409 in 0.51s\n",
      " [-] epoch   99/250, train loss 0.242779 in 0.52s\n",
      " [-] epoch  100/250, train loss 0.250173 in 0.51s\n",
      " [-] epoch  101/250, train loss 0.272584 in 0.52s\n",
      " [-] epoch  102/250, train loss 0.234222 in 0.53s\n",
      " [-] epoch  103/250, train loss 0.239298 in 0.51s\n",
      " [-] epoch  104/250, train loss 0.216228 in 0.50s\n",
      " [-] epoch  105/250, train loss 0.211497 in 0.51s\n",
      " [-] epoch  106/250, train loss 0.247798 in 0.50s\n",
      " [-] epoch  107/250, train loss 0.236034 in 0.50s\n",
      " [-] epoch  108/250, train loss 0.267466 in 0.48s\n",
      " [-] epoch  109/250, train loss 0.260702 in 0.51s\n",
      " [-] epoch  110/250, train loss 0.233049 in 0.52s\n",
      " [-] epoch  111/250, train loss 0.236752 in 0.54s\n",
      " [-] epoch  112/250, train loss 0.229541 in 0.50s\n",
      " [-] epoch  113/250, train loss 0.217794 in 0.54s\n",
      " [-] epoch  114/250, train loss 0.235258 in 0.52s\n",
      " [-] epoch  115/250, train loss 0.242746 in 0.50s\n",
      " [-] epoch  116/250, train loss 0.244671 in 0.51s\n",
      " [-] epoch  117/250, train loss 0.229497 in 0.50s\n",
      " [-] epoch  118/250, train loss 0.226235 in 0.52s\n",
      " [-] epoch  119/250, train loss 0.213616 in 0.50s\n",
      " [-] epoch  120/250, train loss 0.237198 in 0.50s\n",
      " [-] epoch  121/250, train loss 0.235735 in 0.51s\n",
      " [-] epoch  122/250, train loss 0.230442 in 0.49s\n",
      " [-] epoch  123/250, train loss 0.222566 in 0.51s\n",
      " [-] epoch  124/250, train loss 0.210830 in 0.53s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  125/250, train loss 0.214755 in 0.51s\n",
      " [-] epoch  126/250, train loss 0.209020 in 0.53s\n",
      " [-] epoch  127/250, train loss 0.223222 in 0.47s\n",
      " [-] epoch  128/250, train loss 0.224888 in 0.51s\n",
      " [-] epoch  129/250, train loss 0.228024 in 0.50s\n",
      " [-] epoch  130/250, train loss 0.224813 in 0.49s\n",
      " [-] epoch  131/250, train loss 0.211195 in 0.51s\n",
      " [-] epoch  132/250, train loss 0.234628 in 0.50s\n",
      " [-] epoch  133/250, train loss 0.214915 in 0.50s\n",
      " [-] epoch  134/250, train loss 0.222965 in 0.51s\n",
      " [-] epoch  135/250, train loss 0.216649 in 0.50s\n",
      " [-] epoch  136/250, train loss 0.217235 in 0.52s\n",
      " [-] epoch  137/250, train loss 0.246501 in 0.52s\n",
      " [-] epoch  138/250, train loss 0.252069 in 0.53s\n",
      " [-] epoch  139/250, train loss 0.220427 in 0.49s\n",
      " [-] epoch  140/250, train loss 0.221364 in 0.54s\n",
      " [-] epoch  141/250, train loss 0.235168 in 0.49s\n",
      " [-] epoch  142/250, train loss 0.245440 in 0.51s\n",
      " [-] epoch  143/250, train loss 0.221284 in 0.49s\n",
      " [-] epoch  144/250, train loss 0.226793 in 0.50s\n",
      " [-] epoch  145/250, train loss 0.236317 in 0.56s\n",
      " [-] epoch  146/250, train loss 0.236921 in 0.52s\n",
      " [-] epoch  147/250, train loss 0.212128 in 0.53s\n",
      " [-] epoch  148/250, train loss 0.245291 in 0.53s\n",
      " [-] epoch  149/250, train loss 0.217424 in 0.50s\n",
      " [-] epoch  150/250, train loss 0.216537 in 0.50s\n",
      " [-] epoch  151/250, train loss 0.242403 in 0.53s\n",
      " [-] epoch  152/250, train loss 0.206971 in 0.49s\n",
      " [-] epoch  153/250, train loss 0.210848 in 0.49s\n",
      " [-] epoch  154/250, train loss 0.209021 in 0.49s\n",
      " [-] epoch  155/250, train loss 0.232668 in 0.50s\n",
      " [-] epoch  156/250, train loss 0.231037 in 0.53s\n",
      " [-] epoch  157/250, train loss 0.224565 in 0.48s\n",
      " [-] epoch  158/250, train loss 0.212592 in 0.51s\n",
      " [-] epoch  159/250, train loss 0.212630 in 0.52s\n",
      " [-] epoch  160/250, train loss 0.228749 in 0.50s\n",
      " [-] epoch  161/250, train loss 0.231435 in 0.53s\n",
      " [-] epoch  162/250, train loss 0.203861 in 0.51s\n",
      " [-] epoch  163/250, train loss 0.206603 in 0.51s\n",
      " [-] epoch  164/250, train loss 0.224171 in 0.50s\n",
      " [-] epoch  165/250, train loss 0.222481 in 0.52s\n",
      " [-] epoch  166/250, train loss 0.210138 in 0.52s\n",
      " [-] epoch  167/250, train loss 0.207690 in 0.50s\n",
      " [-] epoch  168/250, train loss 0.225296 in 0.50s\n",
      " [-] epoch  169/250, train loss 0.215253 in 0.48s\n",
      " [-] epoch  170/250, train loss 0.226864 in 0.51s\n",
      " [-] epoch  171/250, train loss 0.215305 in 0.51s\n",
      " [-] epoch  172/250, train loss 0.204669 in 0.52s\n",
      " [-] epoch  173/250, train loss 0.209487 in 0.50s\n",
      " [-] epoch  174/250, train loss 0.212544 in 0.50s\n",
      " [-] epoch  175/250, train loss 0.209268 in 0.53s\n",
      " [-] epoch  176/250, train loss 0.233489 in 0.47s\n",
      " [-] epoch  177/250, train loss 0.214290 in 0.53s\n",
      " [-] epoch  178/250, train loss 0.202781 in 0.52s\n",
      " [-] epoch  179/250, train loss 0.216146 in 0.50s\n",
      " [-] epoch  180/250, train loss 0.196860 in 0.51s\n",
      " [-] epoch  181/250, train loss 0.215050 in 0.51s\n",
      " [-] epoch  182/250, train loss 0.232369 in 0.55s\n",
      " [-] epoch  183/250, train loss 0.216504 in 0.54s\n",
      " [-] epoch  184/250, train loss 0.213897 in 0.52s\n",
      " [-] epoch  185/250, train loss 0.214031 in 0.53s\n",
      " [-] epoch  186/250, train loss 0.208392 in 0.52s\n",
      " [-] epoch  187/250, train loss 0.220456 in 0.49s\n",
      " [-] epoch  188/250, train loss 0.207563 in 0.51s\n",
      " [-] epoch  189/250, train loss 0.186168 in 0.49s\n",
      " [-] epoch  190/250, train loss 0.214470 in 0.56s\n",
      " [-] epoch  191/250, train loss 0.219326 in 0.51s\n",
      " [-] epoch  192/250, train loss 0.204208 in 0.54s\n",
      " [-] epoch  193/250, train loss 0.204020 in 0.54s\n",
      " [-] epoch  194/250, train loss 0.212882 in 0.52s\n",
      " [-] epoch  195/250, train loss 0.222564 in 0.50s\n",
      " [-] epoch  196/250, train loss 0.219394 in 0.55s\n",
      " [-] epoch  197/250, train loss 0.209521 in 0.53s\n",
      " [-] epoch  198/250, train loss 0.198640 in 0.52s\n",
      " [-] epoch  199/250, train loss 0.207838 in 0.50s\n",
      " [-] epoch  200/250, train loss 0.225229 in 0.50s\n",
      " [-] epoch  201/250, train loss 0.231417 in 0.50s\n",
      " [-] epoch  202/250, train loss 0.216197 in 0.52s\n",
      " [-] epoch  203/250, train loss 0.231188 in 0.51s\n",
      " [-] epoch  204/250, train loss 0.209009 in 0.49s\n",
      " [-] epoch  205/250, train loss 0.194066 in 0.53s\n",
      " [-] epoch  206/250, train loss 0.200033 in 0.49s\n",
      " [-] epoch  207/250, train loss 0.203182 in 0.51s\n",
      " [-] epoch  208/250, train loss 0.199735 in 0.52s\n",
      " [-] epoch  209/250, train loss 0.200544 in 0.52s\n",
      " [-] epoch  210/250, train loss 0.197381 in 0.54s\n",
      " [-] epoch  211/250, train loss 0.221408 in 0.54s\n",
      " [-] epoch  212/250, train loss 0.229438 in 0.51s\n",
      " [-] epoch  213/250, train loss 0.218640 in 0.55s\n",
      " [-] epoch  214/250, train loss 0.204565 in 0.56s\n",
      " [-] epoch  215/250, train loss 0.206221 in 0.52s\n",
      " [-] epoch  216/250, train loss 0.205486 in 0.52s\n",
      " [-] epoch  217/250, train loss 0.208803 in 0.53s\n",
      " [-] epoch  218/250, train loss 0.195544 in 0.50s\n",
      " [-] epoch  219/250, train loss 0.202680 in 0.53s\n",
      " [-] epoch  220/250, train loss 0.201989 in 0.52s\n",
      " [-] epoch  221/250, train loss 0.203110 in 0.53s\n",
      " [-] epoch  222/250, train loss 0.214666 in 0.54s\n",
      " [-] epoch  223/250, train loss 0.216722 in 0.52s\n",
      " [-] epoch  224/250, train loss 0.236692 in 0.52s\n",
      " [-] epoch  225/250, train loss 0.206823 in 0.51s\n",
      " [-] epoch  226/250, train loss 0.200563 in 0.51s\n",
      " [-] epoch  227/250, train loss 0.193843 in 0.53s\n",
      " [-] epoch  228/250, train loss 0.194731 in 0.52s\n",
      " [-] epoch  229/250, train loss 0.203159 in 0.53s\n",
      " [-] epoch  230/250, train loss 0.202606 in 0.53s\n",
      " [-] epoch  231/250, train loss 0.199505 in 0.53s\n",
      " [-] epoch  232/250, train loss 0.199850 in 0.53s\n",
      " [-] epoch  233/250, train loss 0.256905 in 0.50s\n",
      " [-] epoch  234/250, train loss 0.232398 in 0.52s\n",
      " [-] epoch  235/250, train loss 0.233603 in 0.54s\n",
      " [-] epoch  236/250, train loss 0.205610 in 0.52s\n",
      " [-] epoch  237/250, train loss 0.203261 in 0.53s\n",
      " [-] epoch  238/250, train loss 0.195996 in 0.57s\n",
      " [-] epoch  239/250, train loss 0.208273 in 0.52s\n",
      " [-] epoch  240/250, train loss 0.223871 in 0.54s\n",
      " [-] epoch  241/250, train loss 0.216945 in 0.56s\n",
      " [-] epoch  242/250, train loss 0.221513 in 0.55s\n",
      " [-] epoch  243/250, train loss 0.207684 in 0.50s\n",
      " [-] epoch  244/250, train loss 0.205580 in 0.51s\n",
      " [-] epoch  245/250, train loss 0.204327 in 0.49s\n",
      " [-] epoch  246/250, train loss 0.205336 in 0.53s\n",
      " [-] epoch  247/250, train loss 0.207258 in 0.55s\n",
      " [-] epoch  248/250, train loss 0.196987 in 0.51s\n",
      " [-] epoch  249/250, train loss 0.203350 in 0.49s\n",
      " [-] epoch  250/250, train loss 0.192920 in 0.52s\n",
      " [-] test acc. 80.833333%\n",
      "Je vais utiliser 8 layers\n",
      " [-] epoch    1/250, train loss 0.564720 in 0.64s\n",
      " [-] epoch    2/250, train loss 0.453819 in 0.63s\n",
      " [-] epoch    3/250, train loss 0.417062 in 0.60s\n",
      " [-] epoch    4/250, train loss 0.420235 in 0.62s\n",
      " [-] epoch    5/250, train loss 0.392055 in 0.63s\n",
      " [-] epoch    6/250, train loss 0.388487 in 0.63s\n",
      " [-] epoch    7/250, train loss 0.360408 in 0.60s\n",
      " [-] epoch    8/250, train loss 0.351484 in 0.62s\n",
      " [-] epoch    9/250, train loss 0.374430 in 0.60s\n",
      " [-] epoch   10/250, train loss 0.354512 in 0.61s\n",
      " [-] epoch   11/250, train loss 0.365076 in 0.61s\n",
      " [-] epoch   12/250, train loss 0.397389 in 0.59s\n",
      " [-] epoch   13/250, train loss 0.359420 in 0.61s\n",
      " [-] epoch   14/250, train loss 0.326838 in 0.62s\n",
      " [-] epoch   15/250, train loss 0.342613 in 0.64s\n",
      " [-] epoch   16/250, train loss 0.333517 in 0.61s\n",
      " [-] epoch   17/250, train loss 0.338860 in 0.61s\n",
      " [-] epoch   18/250, train loss 0.329182 in 0.62s\n",
      " [-] epoch   19/250, train loss 0.324076 in 0.60s\n",
      " [-] epoch   20/250, train loss 0.329985 in 0.65s\n",
      " [-] epoch   21/250, train loss 0.319268 in 0.62s\n",
      " [-] epoch   22/250, train loss 0.320245 in 0.60s\n",
      " [-] epoch   23/250, train loss 0.343033 in 0.57s\n",
      " [-] epoch   24/250, train loss 0.333669 in 0.61s\n",
      " [-] epoch   25/250, train loss 0.296384 in 0.59s\n",
      " [-] epoch   26/250, train loss 0.301937 in 0.68s\n",
      " [-] epoch   27/250, train loss 0.308114 in 0.65s\n",
      " [-] epoch   28/250, train loss 0.298124 in 0.62s\n",
      " [-] epoch   29/250, train loss 0.310185 in 0.63s\n",
      " [-] epoch   30/250, train loss 0.293064 in 0.60s\n",
      " [-] epoch   31/250, train loss 0.295379 in 0.64s\n",
      " [-] epoch   32/250, train loss 0.300918 in 0.64s\n",
      " [-] epoch   33/250, train loss 0.324746 in 0.61s\n",
      " [-] epoch   34/250, train loss 0.290195 in 0.68s\n",
      " [-] epoch   35/250, train loss 0.298597 in 0.65s\n",
      " [-] epoch   36/250, train loss 0.291852 in 0.62s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   37/250, train loss 0.297765 in 0.63s\n",
      " [-] epoch   38/250, train loss 0.281749 in 0.62s\n",
      " [-] epoch   39/250, train loss 0.311174 in 0.60s\n",
      " [-] epoch   40/250, train loss 0.330169 in 0.60s\n",
      " [-] epoch   41/250, train loss 0.285001 in 0.60s\n",
      " [-] epoch   42/250, train loss 0.287069 in 0.58s\n",
      " [-] epoch   43/250, train loss 0.288924 in 0.61s\n",
      " [-] epoch   44/250, train loss 0.284677 in 0.62s\n",
      " [-] epoch   45/250, train loss 0.287342 in 0.60s\n",
      " [-] epoch   46/250, train loss 0.265765 in 0.62s\n",
      " [-] epoch   47/250, train loss 0.272805 in 0.60s\n",
      " [-] epoch   48/250, train loss 0.266803 in 0.62s\n",
      " [-] epoch   49/250, train loss 0.258879 in 0.59s\n",
      " [-] epoch   50/250, train loss 0.264823 in 0.61s\n",
      " [-] epoch   51/250, train loss 0.266151 in 0.60s\n",
      " [-] epoch   52/250, train loss 0.301971 in 0.63s\n",
      " [-] epoch   53/250, train loss 0.274453 in 0.61s\n",
      " [-] epoch   54/250, train loss 0.283118 in 0.62s\n",
      " [-] epoch   55/250, train loss 0.275868 in 0.62s\n",
      " [-] epoch   56/250, train loss 0.284721 in 0.65s\n",
      " [-] epoch   57/250, train loss 0.284578 in 0.59s\n",
      " [-] epoch   58/250, train loss 0.262823 in 0.59s\n",
      " [-] epoch   59/250, train loss 0.267371 in 0.62s\n",
      " [-] epoch   60/250, train loss 0.263546 in 0.63s\n",
      " [-] epoch   61/250, train loss 0.276443 in 0.60s\n",
      " [-] epoch   62/250, train loss 0.274481 in 0.60s\n",
      " [-] epoch   63/250, train loss 0.283313 in 0.59s\n",
      " [-] epoch   64/250, train loss 0.257727 in 0.62s\n",
      " [-] epoch   65/250, train loss 0.265265 in 0.65s\n",
      " [-] epoch   66/250, train loss 0.269462 in 0.62s\n",
      " [-] epoch   67/250, train loss 0.243006 in 0.60s\n",
      " [-] epoch   68/250, train loss 0.279582 in 0.62s\n",
      " [-] epoch   69/250, train loss 0.240721 in 0.60s\n",
      " [-] epoch   70/250, train loss 0.277019 in 0.60s\n",
      " [-] epoch   71/250, train loss 0.266339 in 0.59s\n",
      " [-] epoch   72/250, train loss 0.263018 in 0.61s\n",
      " [-] epoch   73/250, train loss 0.258375 in 0.61s\n",
      " [-] epoch   74/250, train loss 0.246666 in 0.60s\n",
      " [-] epoch   75/250, train loss 0.259814 in 0.61s\n",
      " [-] epoch   76/250, train loss 0.264764 in 0.65s\n",
      " [-] epoch   77/250, train loss 0.253602 in 0.59s\n",
      " [-] epoch   78/250, train loss 0.278246 in 0.57s\n",
      " [-] epoch   79/250, train loss 0.252541 in 0.59s\n",
      " [-] epoch   80/250, train loss 0.265046 in 0.62s\n",
      " [-] epoch   81/250, train loss 0.260435 in 0.61s\n",
      " [-] epoch   82/250, train loss 0.266321 in 0.58s\n",
      " [-] epoch   83/250, train loss 0.249177 in 0.56s\n",
      " [-] epoch   84/250, train loss 0.242862 in 0.59s\n",
      " [-] epoch   85/250, train loss 0.250183 in 0.55s\n",
      " [-] epoch   86/250, train loss 0.277104 in 0.58s\n",
      " [-] epoch   87/250, train loss 0.256148 in 0.58s\n",
      " [-] epoch   88/250, train loss 0.257471 in 0.58s\n",
      " [-] epoch   89/250, train loss 0.272689 in 0.60s\n",
      " [-] epoch   90/250, train loss 0.248414 in 0.58s\n",
      " [-] epoch   91/250, train loss 0.251407 in 0.59s\n",
      " [-] epoch   92/250, train loss 0.248288 in 0.62s\n",
      " [-] epoch   93/250, train loss 0.247105 in 0.59s\n",
      " [-] epoch   94/250, train loss 0.243577 in 0.59s\n",
      " [-] epoch   95/250, train loss 0.256307 in 0.59s\n",
      " [-] epoch   96/250, train loss 0.256743 in 0.58s\n",
      " [-] epoch   97/250, train loss 0.273903 in 0.58s\n",
      " [-] epoch   98/250, train loss 0.252399 in 0.59s\n",
      " [-] epoch   99/250, train loss 0.230184 in 0.55s\n",
      " [-] epoch  100/250, train loss 0.260357 in 0.56s\n",
      " [-] epoch  101/250, train loss 0.222581 in 0.55s\n",
      " [-] epoch  102/250, train loss 0.273306 in 0.56s\n",
      " [-] epoch  103/250, train loss 0.241477 in 0.59s\n",
      " [-] epoch  104/250, train loss 0.257790 in 0.60s\n",
      " [-] epoch  105/250, train loss 0.294001 in 0.58s\n",
      " [-] epoch  106/250, train loss 0.239718 in 0.58s\n",
      " [-] epoch  107/250, train loss 0.222672 in 0.60s\n",
      " [-] epoch  108/250, train loss 0.223142 in 0.58s\n",
      " [-] epoch  109/250, train loss 0.233957 in 0.58s\n",
      " [-] epoch  110/250, train loss 0.251932 in 0.55s\n",
      " [-] epoch  111/250, train loss 0.239887 in 0.57s\n",
      " [-] epoch  112/250, train loss 0.226430 in 0.56s\n",
      " [-] epoch  113/250, train loss 0.223594 in 0.57s\n",
      " [-] epoch  114/250, train loss 0.244947 in 0.56s\n",
      " [-] epoch  115/250, train loss 0.227554 in 0.60s\n",
      " [-] epoch  116/250, train loss 0.235995 in 0.53s\n",
      " [-] epoch  117/250, train loss 0.267829 in 0.59s\n",
      " [-] epoch  118/250, train loss 0.235041 in 0.59s\n",
      " [-] epoch  119/250, train loss 0.237296 in 0.59s\n",
      " [-] epoch  120/250, train loss 0.242666 in 0.59s\n",
      " [-] epoch  121/250, train loss 0.258164 in 0.56s\n",
      " [-] epoch  122/250, train loss 0.261808 in 0.57s\n",
      " [-] epoch  123/250, train loss 0.273178 in 0.55s\n",
      " [-] epoch  124/250, train loss 0.216978 in 0.56s\n",
      " [-] epoch  125/250, train loss 0.231417 in 0.56s\n",
      " [-] epoch  126/250, train loss 0.230180 in 0.56s\n",
      " [-] epoch  127/250, train loss 0.235526 in 0.59s\n",
      " [-] epoch  128/250, train loss 0.225470 in 0.55s\n",
      " [-] epoch  129/250, train loss 0.242953 in 0.56s\n",
      " [-] epoch  130/250, train loss 0.225479 in 0.59s\n",
      " [-] epoch  131/250, train loss 0.218845 in 0.54s\n",
      " [-] epoch  132/250, train loss 0.231830 in 0.59s\n",
      " [-] epoch  133/250, train loss 0.218456 in 0.55s\n",
      " [-] epoch  134/250, train loss 0.213899 in 0.56s\n",
      " [-] epoch  135/250, train loss 0.250080 in 0.59s\n",
      " [-] epoch  136/250, train loss 0.218547 in 0.57s\n",
      " [-] epoch  137/250, train loss 0.235415 in 0.57s\n",
      " [-] epoch  138/250, train loss 0.242864 in 0.56s\n",
      " [-] epoch  139/250, train loss 0.238615 in 0.57s\n",
      " [-] epoch  140/250, train loss 0.232981 in 0.58s\n",
      " [-] epoch  141/250, train loss 0.263905 in 0.55s\n",
      " [-] epoch  142/250, train loss 0.230302 in 0.55s\n",
      " [-] epoch  143/250, train loss 0.217840 in 0.55s\n",
      " [-] epoch  144/250, train loss 0.231147 in 0.56s\n",
      " [-] epoch  145/250, train loss 0.222845 in 0.56s\n",
      " [-] epoch  146/250, train loss 0.231974 in 0.55s\n",
      " [-] epoch  147/250, train loss 0.234688 in 0.55s\n",
      " [-] epoch  148/250, train loss 0.204208 in 0.65s\n",
      " [-] epoch  149/250, train loss 0.240153 in 0.70s\n",
      " [-] epoch  150/250, train loss 0.229251 in 0.72s\n",
      " [-] epoch  151/250, train loss 0.252702 in 0.70s\n",
      " [-] epoch  152/250, train loss 0.225729 in 0.73s\n",
      " [-] epoch  153/250, train loss 0.238628 in 0.71s\n",
      " [-] epoch  154/250, train loss 0.235201 in 0.70s\n",
      " [-] epoch  155/250, train loss 0.233446 in 0.68s\n",
      " [-] epoch  156/250, train loss 0.221340 in 0.69s\n",
      " [-] epoch  157/250, train loss 0.226821 in 0.69s\n",
      " [-] epoch  158/250, train loss 0.241579 in 0.67s\n",
      " [-] epoch  159/250, train loss 0.233230 in 0.69s\n",
      " [-] epoch  160/250, train loss 0.227518 in 0.66s\n",
      " [-] epoch  161/250, train loss 0.221712 in 0.63s\n",
      " [-] epoch  162/250, train loss 0.201403 in 0.67s\n",
      " [-] epoch  163/250, train loss 0.199991 in 0.67s\n",
      " [-] epoch  164/250, train loss 0.213473 in 0.64s\n",
      " [-] epoch  165/250, train loss 0.218586 in 0.65s\n",
      " [-] epoch  166/250, train loss 0.231750 in 0.63s\n",
      " [-] epoch  167/250, train loss 0.228386 in 0.64s\n",
      " [-] epoch  168/250, train loss 0.215804 in 0.63s\n",
      " [-] epoch  169/250, train loss 0.214196 in 0.63s\n",
      " [-] epoch  170/250, train loss 0.205555 in 0.60s\n",
      " [-] epoch  171/250, train loss 0.206255 in 0.64s\n",
      " [-] epoch  172/250, train loss 0.223754 in 0.64s\n",
      " [-] epoch  173/250, train loss 0.215773 in 0.66s\n",
      " [-] epoch  174/250, train loss 0.216582 in 0.62s\n",
      " [-] epoch  175/250, train loss 0.216179 in 0.63s\n",
      " [-] epoch  176/250, train loss 0.194530 in 0.63s\n",
      " [-] epoch  177/250, train loss 0.212852 in 0.61s\n",
      " [-] epoch  178/250, train loss 0.203312 in 0.60s\n",
      " [-] epoch  179/250, train loss 0.209675 in 0.62s\n",
      " [-] epoch  180/250, train loss 0.209877 in 0.60s\n",
      " [-] epoch  181/250, train loss 0.234316 in 0.60s\n",
      " [-] epoch  182/250, train loss 0.228032 in 0.62s\n",
      " [-] epoch  183/250, train loss 0.245436 in 0.59s\n",
      " [-] epoch  184/250, train loss 0.219705 in 0.59s\n",
      " [-] epoch  185/250, train loss 0.234845 in 0.60s\n",
      " [-] epoch  186/250, train loss 0.210444 in 0.57s\n",
      " [-] epoch  187/250, train loss 0.213614 in 0.58s\n",
      " [-] epoch  188/250, train loss 0.190536 in 0.59s\n",
      " [-] epoch  189/250, train loss 0.222874 in 0.58s\n",
      " [-] epoch  190/250, train loss 0.230261 in 0.59s\n",
      " [-] epoch  191/250, train loss 0.237706 in 0.59s\n",
      " [-] epoch  192/250, train loss 0.210849 in 0.59s\n",
      " [-] epoch  193/250, train loss 0.211565 in 0.63s\n",
      " [-] epoch  194/250, train loss 0.212679 in 0.59s\n",
      " [-] epoch  195/250, train loss 0.218479 in 0.60s\n",
      " [-] epoch  196/250, train loss 0.217003 in 0.58s\n",
      " [-] epoch  197/250, train loss 0.213081 in 0.59s\n",
      " [-] epoch  198/250, train loss 0.203035 in 0.57s\n",
      " [-] epoch  199/250, train loss 0.221476 in 0.55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  200/250, train loss 0.199431 in 0.57s\n",
      " [-] epoch  201/250, train loss 0.207515 in 0.58s\n",
      " [-] epoch  202/250, train loss 0.210722 in 0.55s\n",
      " [-] epoch  203/250, train loss 0.188055 in 0.55s\n",
      " [-] epoch  204/250, train loss 0.218553 in 0.59s\n",
      " [-] epoch  205/250, train loss 0.218676 in 0.54s\n",
      " [-] epoch  206/250, train loss 0.219890 in 0.55s\n",
      " [-] epoch  207/250, train loss 0.210153 in 0.56s\n",
      " [-] epoch  208/250, train loss 0.206388 in 0.56s\n",
      " [-] epoch  209/250, train loss 0.203166 in 0.57s\n",
      " [-] epoch  210/250, train loss 0.221590 in 0.58s\n",
      " [-] epoch  211/250, train loss 0.210335 in 0.58s\n",
      " [-] epoch  212/250, train loss 0.180481 in 0.56s\n",
      " [-] epoch  213/250, train loss 0.193248 in 0.59s\n",
      " [-] epoch  214/250, train loss 0.216077 in 0.57s\n",
      " [-] epoch  215/250, train loss 0.225552 in 0.57s\n",
      " [-] epoch  216/250, train loss 0.212106 in 0.58s\n",
      " [-] epoch  217/250, train loss 0.209134 in 0.53s\n",
      " [-] epoch  218/250, train loss 0.227465 in 0.54s\n",
      " [-] epoch  219/250, train loss 0.207920 in 0.55s\n",
      " [-] epoch  220/250, train loss 0.202221 in 0.55s\n",
      " [-] epoch  221/250, train loss 0.200923 in 0.56s\n",
      " [-] epoch  222/250, train loss 0.217608 in 0.56s\n",
      " [-] epoch  223/250, train loss 0.219551 in 0.57s\n",
      " [-] epoch  224/250, train loss 0.218796 in 0.55s\n",
      " [-] epoch  225/250, train loss 0.219715 in 0.55s\n",
      " [-] epoch  226/250, train loss 0.225451 in 0.57s\n",
      " [-] epoch  227/250, train loss 0.202877 in 0.58s\n",
      " [-] epoch  228/250, train loss 0.215624 in 0.55s\n",
      " [-] epoch  229/250, train loss 0.204220 in 0.55s\n",
      " [-] epoch  230/250, train loss 0.209940 in 0.56s\n",
      " [-] epoch  231/250, train loss 0.217050 in 0.57s\n",
      " [-] epoch  232/250, train loss 0.199439 in 0.57s\n",
      " [-] epoch  233/250, train loss 0.230043 in 0.55s\n",
      " [-] epoch  234/250, train loss 0.206000 in 0.56s\n",
      " [-] epoch  235/250, train loss 0.200116 in 0.57s\n",
      " [-] epoch  236/250, train loss 0.213250 in 0.57s\n",
      " [-] epoch  237/250, train loss 0.212177 in 0.56s\n",
      " [-] epoch  238/250, train loss 0.219470 in 0.56s\n",
      " [-] epoch  239/250, train loss 0.196523 in 0.55s\n",
      " [-] epoch  240/250, train loss 0.224707 in 0.57s\n",
      " [-] epoch  241/250, train loss 0.202434 in 0.57s\n",
      " [-] epoch  242/250, train loss 0.194863 in 0.58s\n",
      " [-] epoch  243/250, train loss 0.203686 in 0.55s\n",
      " [-] epoch  244/250, train loss 0.196800 in 0.57s\n",
      " [-] epoch  245/250, train loss 0.212616 in 0.59s\n",
      " [-] epoch  246/250, train loss 0.187875 in 0.56s\n",
      " [-] epoch  247/250, train loss 0.219241 in 0.57s\n",
      " [-] epoch  248/250, train loss 0.202344 in 0.55s\n",
      " [-] epoch  249/250, train loss 0.187835 in 0.60s\n",
      " [-] epoch  250/250, train loss 0.174939 in 0.57s\n",
      " [-] test acc. 83.611111%\n",
      "Je vais utiliser 9 layers\n",
      " [-] epoch    1/250, train loss 0.555856 in 0.63s\n",
      " [-] epoch    2/250, train loss 0.437428 in 0.64s\n",
      " [-] epoch    3/250, train loss 0.445288 in 0.66s\n",
      " [-] epoch    4/250, train loss 0.435531 in 0.65s\n",
      " [-] epoch    5/250, train loss 0.372753 in 0.63s\n",
      " [-] epoch    6/250, train loss 0.375250 in 0.61s\n",
      " [-] epoch    7/250, train loss 0.370759 in 0.64s\n",
      " [-] epoch    8/250, train loss 0.391773 in 0.63s\n",
      " [-] epoch    9/250, train loss 0.363912 in 0.67s\n",
      " [-] epoch   10/250, train loss 0.339759 in 0.63s\n",
      " [-] epoch   11/250, train loss 0.333902 in 0.64s\n",
      " [-] epoch   12/250, train loss 0.319343 in 0.62s\n",
      " [-] epoch   13/250, train loss 0.301697 in 0.61s\n",
      " [-] epoch   14/250, train loss 0.360802 in 0.63s\n",
      " [-] epoch   15/250, train loss 0.326256 in 0.64s\n",
      " [-] epoch   16/250, train loss 0.302673 in 0.66s\n",
      " [-] epoch   17/250, train loss 0.300359 in 0.67s\n",
      " [-] epoch   18/250, train loss 0.301007 in 0.63s\n",
      " [-] epoch   19/250, train loss 0.347803 in 0.63s\n",
      " [-] epoch   20/250, train loss 0.323980 in 0.63s\n",
      " [-] epoch   21/250, train loss 0.321208 in 0.63s\n",
      " [-] epoch   22/250, train loss 0.323538 in 0.65s\n",
      " [-] epoch   23/250, train loss 0.290301 in 0.63s\n",
      " [-] epoch   24/250, train loss 0.298392 in 0.64s\n",
      " [-] epoch   25/250, train loss 0.314207 in 0.65s\n",
      " [-] epoch   26/250, train loss 0.294838 in 0.61s\n",
      " [-] epoch   27/250, train loss 0.307247 in 0.62s\n",
      " [-] epoch   28/250, train loss 0.281770 in 0.61s\n",
      " [-] epoch   29/250, train loss 0.289816 in 0.62s\n",
      " [-] epoch   30/250, train loss 0.298822 in 0.61s\n",
      " [-] epoch   31/250, train loss 0.265845 in 0.64s\n",
      " [-] epoch   32/250, train loss 0.298534 in 0.65s\n",
      " [-] epoch   33/250, train loss 0.300876 in 0.61s\n",
      " [-] epoch   34/250, train loss 0.286878 in 0.61s\n",
      " [-] epoch   35/250, train loss 0.308743 in 0.63s\n",
      " [-] epoch   36/250, train loss 0.282310 in 0.63s\n",
      " [-] epoch   37/250, train loss 0.273390 in 0.66s\n",
      " [-] epoch   38/250, train loss 0.320489 in 0.64s\n",
      " [-] epoch   39/250, train loss 0.288622 in 0.64s\n",
      " [-] epoch   40/250, train loss 0.293877 in 0.63s\n",
      " [-] epoch   41/250, train loss 0.288547 in 0.63s\n",
      " [-] epoch   42/250, train loss 0.281348 in 0.63s\n",
      " [-] epoch   43/250, train loss 0.288188 in 0.63s\n",
      " [-] epoch   44/250, train loss 0.271966 in 0.67s\n",
      " [-] epoch   45/250, train loss 0.264787 in 0.64s\n",
      " [-] epoch   46/250, train loss 0.284321 in 0.63s\n",
      " [-] epoch   47/250, train loss 0.270274 in 0.63s\n",
      " [-] epoch   48/250, train loss 0.252721 in 0.65s\n",
      " [-] epoch   49/250, train loss 0.271558 in 0.64s\n",
      " [-] epoch   50/250, train loss 0.301310 in 0.64s\n",
      " [-] epoch   51/250, train loss 0.260859 in 0.63s\n",
      " [-] epoch   52/250, train loss 0.282178 in 0.65s\n",
      " [-] epoch   53/250, train loss 0.262266 in 0.62s\n",
      " [-] epoch   54/250, train loss 0.241681 in 0.61s\n",
      " [-] epoch   55/250, train loss 0.266476 in 0.62s\n",
      " [-] epoch   56/250, train loss 0.283720 in 0.63s\n",
      " [-] epoch   57/250, train loss 0.275788 in 0.65s\n",
      " [-] epoch   58/250, train loss 0.263079 in 0.59s\n",
      " [-] epoch   59/250, train loss 0.262309 in 0.63s\n",
      " [-] epoch   60/250, train loss 0.261629 in 0.61s\n",
      " [-] epoch   61/250, train loss 0.264027 in 0.62s\n",
      " [-] epoch   62/250, train loss 0.249790 in 0.62s\n",
      " [-] epoch   63/250, train loss 0.243825 in 0.66s\n",
      " [-] epoch   64/250, train loss 0.255910 in 0.62s\n",
      " [-] epoch   65/250, train loss 0.271419 in 0.62s\n",
      " [-] epoch   66/250, train loss 0.259871 in 0.62s\n",
      " [-] epoch   67/250, train loss 0.261965 in 0.63s\n",
      " [-] epoch   68/250, train loss 0.256608 in 0.62s\n",
      " [-] epoch   69/250, train loss 0.256085 in 0.64s\n",
      " [-] epoch   70/250, train loss 0.264754 in 0.63s\n",
      " [-] epoch   71/250, train loss 0.245963 in 0.61s\n",
      " [-] epoch   72/250, train loss 0.257348 in 0.65s\n",
      " [-] epoch   73/250, train loss 0.251899 in 0.62s\n",
      " [-] epoch   74/250, train loss 0.263048 in 0.64s\n",
      " [-] epoch   75/250, train loss 0.264703 in 0.62s\n",
      " [-] epoch   76/250, train loss 0.274588 in 0.63s\n",
      " [-] epoch   77/250, train loss 0.224381 in 0.66s\n",
      " [-] epoch   78/250, train loss 0.241188 in 0.64s\n",
      " [-] epoch   79/250, train loss 0.261885 in 0.67s\n",
      " [-] epoch   80/250, train loss 0.241438 in 0.67s\n",
      " [-] epoch   81/250, train loss 0.232151 in 0.64s\n",
      " [-] epoch   82/250, train loss 0.228734 in 0.64s\n",
      " [-] epoch   83/250, train loss 0.251434 in 0.62s\n",
      " [-] epoch   84/250, train loss 0.233457 in 0.63s\n",
      " [-] epoch   85/250, train loss 0.239385 in 0.62s\n",
      " [-] epoch   86/250, train loss 0.239556 in 0.64s\n",
      " [-] epoch   87/250, train loss 0.249041 in 0.63s\n",
      " [-] epoch   88/250, train loss 0.245192 in 0.62s\n",
      " [-] epoch   89/250, train loss 0.257073 in 0.61s\n",
      " [-] epoch   90/250, train loss 0.252517 in 0.64s\n",
      " [-] epoch   91/250, train loss 0.250688 in 0.63s\n",
      " [-] epoch   92/250, train loss 0.252122 in 0.63s\n",
      " [-] epoch   93/250, train loss 0.235344 in 0.64s\n",
      " [-] epoch   94/250, train loss 0.249943 in 0.64s\n",
      " [-] epoch   95/250, train loss 0.232685 in 0.62s\n",
      " [-] epoch   96/250, train loss 0.233507 in 0.60s\n",
      " [-] epoch   97/250, train loss 0.252516 in 0.65s\n",
      " [-] epoch   98/250, train loss 0.246582 in 0.64s\n",
      " [-] epoch   99/250, train loss 0.242760 in 0.63s\n",
      " [-] epoch  100/250, train loss 0.240232 in 0.62s\n",
      " [-] epoch  101/250, train loss 0.247852 in 0.65s\n",
      " [-] epoch  102/250, train loss 0.229159 in 0.60s\n",
      " [-] epoch  103/250, train loss 0.238326 in 0.63s\n",
      " [-] epoch  104/250, train loss 0.212630 in 0.62s\n",
      " [-] epoch  105/250, train loss 0.236736 in 0.63s\n",
      " [-] epoch  106/250, train loss 0.230514 in 0.64s\n",
      " [-] epoch  107/250, train loss 0.240215 in 0.63s\n",
      " [-] epoch  108/250, train loss 0.237308 in 0.64s\n",
      " [-] epoch  109/250, train loss 0.258834 in 0.66s\n",
      " [-] epoch  110/250, train loss 0.243295 in 0.64s\n",
      " [-] epoch  111/250, train loss 0.232525 in 0.65s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  112/250, train loss 0.237016 in 0.61s\n",
      " [-] epoch  113/250, train loss 0.227787 in 0.63s\n",
      " [-] epoch  114/250, train loss 0.242931 in 0.60s\n",
      " [-] epoch  115/250, train loss 0.256620 in 0.59s\n",
      " [-] epoch  116/250, train loss 0.232460 in 0.63s\n",
      " [-] epoch  117/250, train loss 0.242589 in 0.65s\n",
      " [-] epoch  118/250, train loss 0.239124 in 0.63s\n",
      " [-] epoch  119/250, train loss 0.224285 in 0.65s\n",
      " [-] epoch  120/250, train loss 0.217321 in 0.61s\n",
      " [-] epoch  121/250, train loss 0.227856 in 0.67s\n",
      " [-] epoch  122/250, train loss 0.219744 in 0.61s\n",
      " [-] epoch  123/250, train loss 0.225974 in 0.63s\n",
      " [-] epoch  124/250, train loss 0.230650 in 0.66s\n",
      " [-] epoch  125/250, train loss 0.225265 in 0.63s\n",
      " [-] epoch  126/250, train loss 0.220828 in 0.64s\n",
      " [-] epoch  127/250, train loss 0.206120 in 0.64s\n",
      " [-] epoch  128/250, train loss 0.222184 in 0.67s\n",
      " [-] epoch  129/250, train loss 0.216482 in 0.66s\n",
      " [-] epoch  130/250, train loss 0.209406 in 0.65s\n",
      " [-] epoch  131/250, train loss 0.226157 in 0.63s\n",
      " [-] epoch  132/250, train loss 0.237690 in 0.64s\n",
      " [-] epoch  133/250, train loss 0.228403 in 0.63s\n",
      " [-] epoch  134/250, train loss 0.219911 in 0.65s\n",
      " [-] epoch  135/250, train loss 0.241300 in 0.67s\n",
      " [-] epoch  136/250, train loss 0.207775 in 0.65s\n",
      " [-] epoch  137/250, train loss 0.214781 in 0.65s\n",
      " [-] epoch  138/250, train loss 0.225661 in 0.63s\n",
      " [-] epoch  139/250, train loss 0.216591 in 0.63s\n",
      " [-] epoch  140/250, train loss 0.231063 in 0.64s\n",
      " [-] epoch  141/250, train loss 0.219461 in 0.63s\n",
      " [-] epoch  142/250, train loss 0.219703 in 0.67s\n",
      " [-] epoch  143/250, train loss 0.231852 in 0.65s\n",
      " [-] epoch  144/250, train loss 0.229747 in 0.65s\n",
      " [-] epoch  145/250, train loss 0.218732 in 0.63s\n",
      " [-] epoch  146/250, train loss 0.216023 in 0.67s\n",
      " [-] epoch  147/250, train loss 0.214987 in 0.64s\n",
      " [-] epoch  148/250, train loss 0.223994 in 0.66s\n",
      " [-] epoch  149/250, train loss 0.208959 in 0.65s\n",
      " [-] epoch  150/250, train loss 0.220187 in 0.60s\n",
      " [-] epoch  151/250, train loss 0.220504 in 0.62s\n",
      " [-] epoch  152/250, train loss 0.210616 in 0.63s\n",
      " [-] epoch  153/250, train loss 0.202072 in 0.64s\n",
      " [-] epoch  154/250, train loss 0.221947 in 0.64s\n",
      " [-] epoch  155/250, train loss 0.218114 in 0.62s\n",
      " [-] epoch  156/250, train loss 0.215024 in 0.64s\n",
      " [-] epoch  157/250, train loss 0.210513 in 0.64s\n",
      " [-] epoch  158/250, train loss 0.223207 in 0.63s\n",
      " [-] epoch  159/250, train loss 0.230639 in 0.63s\n",
      " [-] epoch  160/250, train loss 0.221835 in 0.59s\n",
      " [-] epoch  161/250, train loss 0.221657 in 0.63s\n",
      " [-] epoch  162/250, train loss 0.205470 in 0.61s\n",
      " [-] epoch  163/250, train loss 0.220458 in 0.68s\n",
      " [-] epoch  164/250, train loss 0.219782 in 0.62s\n",
      " [-] epoch  165/250, train loss 0.218169 in 0.62s\n",
      " [-] epoch  166/250, train loss 0.227380 in 0.63s\n",
      " [-] epoch  167/250, train loss 0.194849 in 0.62s\n",
      " [-] epoch  168/250, train loss 0.212698 in 0.65s\n",
      " [-] epoch  169/250, train loss 0.211690 in 0.63s\n",
      " [-] epoch  170/250, train loss 0.213615 in 0.65s\n",
      " [-] epoch  171/250, train loss 0.214702 in 0.64s\n",
      " [-] epoch  172/250, train loss 0.231538 in 0.63s\n",
      " [-] epoch  173/250, train loss 0.209406 in 0.63s\n",
      " [-] epoch  174/250, train loss 0.217424 in 0.62s\n",
      " [-] epoch  175/250, train loss 0.206685 in 0.63s\n",
      " [-] epoch  176/250, train loss 0.209062 in 0.66s\n",
      " [-] epoch  177/250, train loss 0.203984 in 0.63s\n",
      " [-] epoch  178/250, train loss 0.218711 in 0.64s\n",
      " [-] epoch  179/250, train loss 0.222230 in 0.65s\n",
      " [-] epoch  180/250, train loss 0.218636 in 0.66s\n",
      " [-] epoch  181/250, train loss 0.206088 in 0.63s\n",
      " [-] epoch  182/250, train loss 0.218446 in 0.66s\n",
      " [-] epoch  183/250, train loss 0.198575 in 0.63s\n",
      " [-] epoch  184/250, train loss 0.217656 in 0.63s\n",
      " [-] epoch  185/250, train loss 0.212584 in 0.62s\n",
      " [-] epoch  186/250, train loss 0.191100 in 0.64s\n",
      " [-] epoch  187/250, train loss 0.218371 in 0.63s\n",
      " [-] epoch  188/250, train loss 0.211106 in 0.63s\n",
      " [-] epoch  189/250, train loss 0.236690 in 0.62s\n",
      " [-] epoch  190/250, train loss 0.219099 in 0.62s\n",
      " [-] epoch  191/250, train loss 0.208249 in 0.62s\n",
      " [-] epoch  192/250, train loss 0.197844 in 0.64s\n",
      " [-] epoch  193/250, train loss 0.207398 in 0.62s\n",
      " [-] epoch  194/250, train loss 0.216929 in 0.61s\n",
      " [-] epoch  195/250, train loss 0.213116 in 0.67s\n",
      " [-] epoch  196/250, train loss 0.196275 in 0.63s\n",
      " [-] epoch  197/250, train loss 0.203022 in 0.62s\n",
      " [-] epoch  198/250, train loss 0.187056 in 0.62s\n",
      " [-] epoch  199/250, train loss 0.205824 in 0.61s\n",
      " [-] epoch  200/250, train loss 0.201287 in 0.59s\n",
      " [-] epoch  201/250, train loss 0.190601 in 0.63s\n",
      " [-] epoch  202/250, train loss 0.204762 in 0.62s\n",
      " [-] epoch  203/250, train loss 0.193441 in 0.62s\n",
      " [-] epoch  204/250, train loss 0.198072 in 0.61s\n",
      " [-] epoch  205/250, train loss 0.191373 in 0.62s\n",
      " [-] epoch  206/250, train loss 0.208790 in 0.60s\n",
      " [-] epoch  207/250, train loss 0.204755 in 0.63s\n",
      " [-] epoch  208/250, train loss 0.202737 in 0.61s\n",
      " [-] epoch  209/250, train loss 0.218701 in 0.64s\n",
      " [-] epoch  210/250, train loss 0.216379 in 0.65s\n",
      " [-] epoch  211/250, train loss 0.212287 in 0.60s\n",
      " [-] epoch  212/250, train loss 0.202873 in 0.67s\n",
      " [-] epoch  213/250, train loss 0.205728 in 0.62s\n",
      " [-] epoch  214/250, train loss 0.203427 in 0.61s\n",
      " [-] epoch  215/250, train loss 0.185532 in 0.66s\n",
      " [-] epoch  216/250, train loss 0.198558 in 0.63s\n",
      " [-] epoch  217/250, train loss 0.203053 in 0.62s\n",
      " [-] epoch  218/250, train loss 0.206809 in 0.64s\n",
      " [-] epoch  219/250, train loss 0.203097 in 0.62s\n",
      " [-] epoch  220/250, train loss 0.208729 in 0.64s\n",
      " [-] epoch  221/250, train loss 0.188697 in 0.62s\n",
      " [-] epoch  222/250, train loss 0.190713 in 0.63s\n",
      " [-] epoch  223/250, train loss 0.201514 in 0.63s\n",
      " [-] epoch  224/250, train loss 0.192330 in 0.63s\n",
      " [-] epoch  225/250, train loss 0.199671 in 0.63s\n",
      " [-] epoch  226/250, train loss 0.211944 in 0.64s\n",
      " [-] epoch  227/250, train loss 0.204330 in 0.65s\n",
      " [-] epoch  228/250, train loss 0.203306 in 0.66s\n",
      " [-] epoch  229/250, train loss 0.194718 in 0.62s\n",
      " [-] epoch  230/250, train loss 0.219658 in 0.64s\n",
      " [-] epoch  231/250, train loss 0.243003 in 0.61s\n",
      " [-] epoch  232/250, train loss 0.220200 in 0.64s\n",
      " [-] epoch  233/250, train loss 0.195232 in 0.62s\n",
      " [-] epoch  234/250, train loss 0.188103 in 0.66s\n",
      " [-] epoch  235/250, train loss 0.197001 in 0.60s\n",
      " [-] epoch  236/250, train loss 0.197974 in 0.63s\n",
      " [-] epoch  237/250, train loss 0.206793 in 0.66s\n",
      " [-] epoch  238/250, train loss 0.202492 in 0.64s\n",
      " [-] epoch  239/250, train loss 0.198325 in 0.64s\n",
      " [-] epoch  240/250, train loss 0.206541 in 0.62s\n",
      " [-] epoch  241/250, train loss 0.213819 in 0.63s\n",
      " [-] epoch  242/250, train loss 0.208473 in 0.63s\n",
      " [-] epoch  243/250, train loss 0.191141 in 0.61s\n",
      " [-] epoch  244/250, train loss 0.209973 in 0.62s\n",
      " [-] epoch  245/250, train loss 0.205718 in 0.67s\n",
      " [-] epoch  246/250, train loss 0.205326 in 0.63s\n",
      " [-] epoch  247/250, train loss 0.205514 in 0.63s\n",
      " [-] epoch  248/250, train loss 0.190824 in 0.62s\n",
      " [-] epoch  249/250, train loss 0.191169 in 0.65s\n",
      " [-] epoch  250/250, train loss 0.196251 in 0.63s\n",
      " [-] test acc. 82.777778%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reducedData = numpy.load('labeled_reduit_100dim.npy')\n",
    "numberOfData = labeledData.shape[0] # 1441\n",
    "dimensions = labeledData.shape[1] # 1183\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train_reduced = minmax_scale(reducedData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test_reduced = minmax_scale(reducedData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données\n",
    "\n",
    "compute_results(X_train_reduced, X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on all dimensions: 0.8361111111111111\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM performance with all dimensions\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train[:, : -1], X_train[:, -1])\n",
    "score = clf.score(X_test[:, : -1], X_test[:, -1])\n",
    "print(\"Score on all dimensions:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenir compte du fait que l'on n'a pas beaucoup de données considérant le nombre de dimensions\n",
    "\n",
    "2 et 6 layers ressortent souvent (impression?) possibilité d'avoir une genre d'interférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning pour Classification\n",
    "\n",
    "Ce notebook regroupe quelques configurations d'algorithmes de Deep Learning pour la classification de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Variable                        Possibilités             Nombre de colonne (+1)\n",
      "Spécialité de médecins\t        33 codes de spécialité\t        1-33\n",
      "Sexe de médecin \t            2\t                            34-35\n",
      "Langue de correspondance \t    2\t                            36-37\n",
      "Université de graduation \t    14 universités\t                38-51\n",
      "Plage horaire de facturation\t3 (AM, PM, toute la journée)\t52-54\n",
      "Agence de représentation\t    845 codes d’agences             55-899\n",
      "Établissements                  241 établissements\t            900-1140\n",
      "ActeMedical                     2 (Oui et Non)\t                1141-1142\n",
      "Activités médico-admin          2 (Oui et Non)\t                1143-1144\n",
      "Activités d’enseignement\t    2 (Oui et Non)\t                1145-1146\n",
      "Entente LE\t                    2 (Oui et Non)\t                1147-1148\n",
      "Activité en santé communautaire\t2 (Oui et Non)\t                1149-1150\n",
      "Activité en santé comm. CSST\t2 (Oui et Non)\t                1151-1152\n",
      "Activité en santé comm. INSP\t2 (Oui et Non)\t                1153-1154\n",
      "Année de graduation\t                                            1155\n",
      "semaine de l’année associée à la facturation\t                1156\n",
      "Nombre de jours dès la dernière facture avant le Perdiem\t    1157\n",
      "Année de naissance\t                                            1158\n",
      "Année de début de pratique\t                                    1159\n",
      "Expérience\t                                                    1160\n",
      "Salaire régulier\t                                            1161\n",
      "Salaire payé\t                                                1162\n",
      "Nombre de Perdiem pendant l’année\t                            1163\n",
      "Jour de la semaine associé à la facturation\t                    1164\n",
      "Nombre de Perdiem\t                                            1165\n",
      "Nombre d’heures facturées\t                                    1166\n",
      "Nombre de services avec bénéficiaire\t                        1167\n",
      "Nombre de services sans bénéficiaire\t                        1168\n",
      "Nombre de patients\t                                            1169\n",
      "Montant réclamé avec bénéficiaire\t                            1170\n",
      "Montant réclamé sans bénéficiaire\t                            1171\n",
      "Nombre de service total\t                                        1172\n",
      "Nombre de Perdiem facturé le jour avant\t                        1173\n",
      "Nombre d’heures facturées le jour avant\t                        1174\n",
      "Nombre de services avec bénéficiaire le jour avant\t            1175\n",
      "Nombre de services sans bénéficiaire le jour avant\t            1176\n",
      "Nombre de patients le jour avant\t                            1177\n",
      "Montant réclamé avec bénéficiaire le jour avant\t                1178\n",
      "Montant réclamé sans bénéficiaire le jour avant\t                1179\n",
      "Nombre de services total le jour avant\t                        1180\n",
      "âge de médecin le jour avant \t                                1181\n",
      "Nombre de Perdiem facturés la semaine d’avant\t                1182\n",
      "\n",
      "Target:\n",
      "Conformité\n",
      "(-1 : non contrôlé, 1 : cas conforme, 0 : cas non-conforme)\t    1183\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enregistrement du jeu de données comportant les données avec étiquettes\n",
    "Ici, le jeu de données comportant uniquement les données avec des étiquettes de classe est téléchargé. Il est ensuite séparé en jeux d'entraînement et de test en plus d'être normalisé dans toutes les dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData = numpy.load('labeled.npy')\n",
    "numberOfData = labeledData.shape[0] # 1441\n",
    "dimensions = labeledData.shape[1] # 1183\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train = minmax_scale(labeledData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test = minmax_scale(labeledData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Déclaration de fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_sampler(dataset):\n",
    "    def make_weights_for_balanced_classes(images, n_classes):                        \n",
    "        count = [0] * n_classes                                                      \n",
    "        for item in images:                                                         \n",
    "            count[int(item[1])] += 1                                                     \n",
    "        weight_per_class = [0.] * n_classes                                      \n",
    "        N = float(sum(count))                                                   \n",
    "        for i in range(n_classes):                                                   \n",
    "            weight_per_class[i] = N/float(count[i])                                 \n",
    "        weight = [0] * len(images)                                              \n",
    "        for idx, val in enumerate(images):                                          \n",
    "            weight[idx] = weight_per_class[int(val[1])]                                  \n",
    "        return weight\n",
    "\n",
    "    n_classes = numpy.unique(dataset.targets)\n",
    "    weights = make_weights_for_balanced_classes(dataset.data, len(n_classes))                                                         \n",
    "    weights = torch.DoubleTensor(weights)                 \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    return sampler\n",
    "\n",
    "def compute_accuracy(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition du Dataset sans transformation de la forme des données\n",
    "Définition de la classe RAMQDataset, une classe qui hérite de la classe abstraite torch.utils.data.Dataset. Comme mentionné dans la documentation, les méthodes __getitem__ et __len__ sont surchargées afin d'avoir un jeu de données utilisable par PyTorch. Le data accepté en paramètres est un array numpy dont la dernière dimension est la valeur de l'étiquette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Cette classe sert à définir le dataset RAMQ pour PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for elem in data:\n",
    "            if isinstance(elem, numpy.ndarray):\n",
    "                elem = elem.tolist()\n",
    "                elem_data = torch.Tensor(elem[:-1])\n",
    "                elem_target = torch.FloatTensor([elem[-1]])\n",
    "            # garde les paramètres en mémoire\n",
    "            self.data += [(elem_data, elem_target[0])]\n",
    "            \n",
    "        self.targets = numpy.array(data[:, -1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Déclaration d'un réseau de neurones de base: 1 couche - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.F1 = nn.Linear(1182, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.F1(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Déclaration d'un réseau de neurones de type linéraire: multicouches\n",
    "Ce réseau est personnalisable à souhait: il est possible d'y indiquer, en paramètres d'entrée, le nombre de dimensions en input, le nombre de couches linéaires à avoir dans le réseau en question et le type de la fonction d'activation qui y sera relié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_layers, activation = nn.ELU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        reduction_ratio = (1/n_inputs)**(1/n_layers)\n",
    "        \n",
    "        layers = []\n",
    "        layer_lenght = n_inputs\n",
    "        \n",
    "        # Calculate nb of layers and nb of neurons\n",
    "        # Builds a list (ex:[1000, 500, 250, ... until 1])\n",
    "        while layer_lenght > 1:\n",
    "            layers.append(layer_lenght)\n",
    "            layer_lenght = int(layer_lenght*reduction_ratio)\n",
    "        layers.append(1)\n",
    "        \n",
    "        # Build network layers\n",
    "        network_layers = []\n",
    "        for i, n_neurons in enumerate(layers[:-1]):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            network_layers.append(layer)\n",
    "            network_layers.append(activation)\n",
    "            network_layers.append(nn.BatchNorm1d(layers[i+1]))\n",
    "        network_layers[-1] = nn.Sigmoid()\n",
    "            \n",
    "        # Build pytorch sequential network\n",
    "        self.network = nn.Sequential(\n",
    "            *network_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Exécute l'inférence du réseau. L'ordre \n",
    "        # d'exécution ici est important.\n",
    "        x = self.network(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Déclaration de la fonction permettant l'affichage du pourcentage d'efficacité en classement selon 1 à 9 couches d'un réseau de neurones \"x\"\n",
    "Cette méthode n'a besoin, en entrées, que d'un tableau des pourcentages d'efficacité pour 0 à 9 couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(test_accu, start, end):\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(test_accu, label='Test accuracy')\n",
    "    ax.set_xlim(start, end)\n",
    "    pyplot.grid()\n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Déclaration de la fonction permettant la classification par réseau de neurones profond de type multicouches linéaire\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_results(X_train, X_test):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cpu\" \n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 250\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Définit le nombre de dimensions des données avec lesquelles on travaille (la dernière dimension étant l'étiquette)\n",
    "    dimensions = X_train.shape[1] - 1  \n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "\n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "\n",
    "    # Instancier un réseau RAMQNetLinear\n",
    "    # dans une variable nommée \"model\"\n",
    "    for i in range(1,10):\n",
    "        print(\"Je vais utiliser \" + str(i) + \" layers\")\n",
    "        model = RAMQNetLinear(dimensions, i)\n",
    "\n",
    "        # Tranfert le réseau au bon endroit\n",
    "        model.to(device)\n",
    "\n",
    "        # Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "        # et la mettre dans une variable nommée criterion\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        # Ne pas oublier de lui donner les hyperparamètres\n",
    "        # d'entraînement : learning rate et momentum!\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer\n",
    "                # une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu, 1, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calcul du taux de réussite en classement d'un SVM linéaire de base sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on all dimensions: 0.8361111111111111\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM performance with all dimensions\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train[:, : -1], X_train[:, -1])\n",
    "score = clf.score(X_test[:, : -1], X_test[:, -1])\n",
    "print(\"Score on all dimensions:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on tente de classifier les 1441 données à 1182 dimensions avec un réseau de neurones à 1 couche linéaire, on obtient un pourcentage de classement de l'ordre d'environ 82%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calcul du taux de réussite en classement d'un réseau linéaire multicouches sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.686894 in 0.03s\n",
      " [-] epoch    2/250, train loss 0.658042 in 0.03s\n",
      " [-] epoch    3/250, train loss 0.630507 in 0.02s\n",
      " [-] epoch    4/250, train loss 0.617247 in 0.03s\n",
      " [-] epoch    5/250, train loss 0.598471 in 0.03s\n",
      " [-] epoch    6/250, train loss 0.585760 in 0.03s\n",
      " [-] epoch    7/250, train loss 0.585472 in 0.03s\n",
      " [-] epoch    8/250, train loss 0.578508 in 0.03s\n",
      " [-] epoch    9/250, train loss 0.559436 in 0.03s\n",
      " [-] epoch   10/250, train loss 0.566538 in 0.03s\n",
      " [-] epoch   11/250, train loss 0.546757 in 0.03s\n",
      " [-] epoch   12/250, train loss 0.543458 in 0.03s\n",
      " [-] epoch   13/250, train loss 0.539289 in 0.03s\n",
      " [-] epoch   14/250, train loss 0.532973 in 0.02s\n",
      " [-] epoch   15/250, train loss 0.528345 in 0.03s\n",
      " [-] epoch   16/250, train loss 0.514023 in 0.03s\n",
      " [-] epoch   17/250, train loss 0.529386 in 0.03s\n",
      " [-] epoch   18/250, train loss 0.517567 in 0.04s\n",
      " [-] epoch   19/250, train loss 0.519313 in 0.03s\n",
      " [-] epoch   20/250, train loss 0.512015 in 0.03s\n",
      " [-] epoch   21/250, train loss 0.506013 in 0.03s\n",
      " [-] epoch   22/250, train loss 0.528303 in 0.03s\n",
      " [-] epoch   23/250, train loss 0.513400 in 0.03s\n",
      " [-] epoch   24/250, train loss 0.506760 in 0.02s\n",
      " [-] epoch   25/250, train loss 0.525287 in 0.04s\n",
      " [-] epoch   26/250, train loss 0.493433 in 0.03s\n",
      " [-] epoch   27/250, train loss 0.483047 in 0.02s\n",
      " [-] epoch   28/250, train loss 0.514353 in 0.04s\n",
      " [-] epoch   29/250, train loss 0.486162 in 0.03s\n",
      " [-] epoch   30/250, train loss 0.486986 in 0.02s\n",
      " [-] epoch   31/250, train loss 0.481819 in 0.04s\n",
      " [-] epoch   32/250, train loss 0.474423 in 0.03s\n",
      " [-] epoch   33/250, train loss 0.496064 in 0.03s\n",
      " [-] epoch   34/250, train loss 0.486675 in 0.03s\n",
      " [-] epoch   35/250, train loss 0.481116 in 0.03s\n",
      " [-] epoch   36/250, train loss 0.475196 in 0.03s\n",
      " [-] epoch   37/250, train loss 0.470870 in 0.03s\n",
      " [-] epoch   38/250, train loss 0.487678 in 0.03s\n",
      " [-] epoch   39/250, train loss 0.465195 in 0.03s\n",
      " [-] epoch   40/250, train loss 0.481104 in 0.04s\n",
      " [-] epoch   41/250, train loss 0.477669 in 0.03s\n",
      " [-] epoch   42/250, train loss 0.468704 in 0.03s\n",
      " [-] epoch   43/250, train loss 0.469356 in 0.03s\n",
      " [-] epoch   44/250, train loss 0.468588 in 0.03s\n",
      " [-] epoch   45/250, train loss 0.469191 in 0.03s\n",
      " [-] epoch   46/250, train loss 0.486303 in 0.03s\n",
      " [-] epoch   47/250, train loss 0.471700 in 0.03s\n",
      " [-] epoch   48/250, train loss 0.477710 in 0.03s\n",
      " [-] epoch   49/250, train loss 0.458941 in 0.03s\n",
      " [-] epoch   50/250, train loss 0.462526 in 0.03s\n",
      " [-] epoch   51/250, train loss 0.464078 in 0.03s\n",
      " [-] epoch   52/250, train loss 0.475905 in 0.03s\n",
      " [-] epoch   53/250, train loss 0.470088 in 0.03s\n",
      " [-] epoch   54/250, train loss 0.464953 in 0.03s\n",
      " [-] epoch   55/250, train loss 0.454509 in 0.03s\n",
      " [-] epoch   56/250, train loss 0.462534 in 0.03s\n",
      " [-] epoch   57/250, train loss 0.440198 in 0.03s\n",
      " [-] epoch   58/250, train loss 0.453093 in 0.03s\n",
      " [-] epoch   59/250, train loss 0.434883 in 0.03s\n",
      " [-] epoch   60/250, train loss 0.469784 in 0.03s\n",
      " [-] epoch   61/250, train loss 0.462267 in 0.03s\n",
      " [-] epoch   62/250, train loss 0.445654 in 0.03s\n",
      " [-] epoch   63/250, train loss 0.439448 in 0.03s\n",
      " [-] epoch   64/250, train loss 0.463466 in 0.03s\n",
      " [-] epoch   65/250, train loss 0.443017 in 0.03s\n",
      " [-] epoch   66/250, train loss 0.427959 in 0.03s\n",
      " [-] epoch   67/250, train loss 0.431752 in 0.03s\n",
      " [-] epoch   68/250, train loss 0.460021 in 0.03s\n",
      " [-] epoch   69/250, train loss 0.447327 in 0.03s\n",
      " [-] epoch   70/250, train loss 0.441008 in 0.03s\n",
      " [-] epoch   71/250, train loss 0.444952 in 0.03s\n",
      " [-] epoch   72/250, train loss 0.449204 in 0.03s\n",
      " [-] epoch   73/250, train loss 0.443091 in 0.03s\n",
      " [-] epoch   74/250, train loss 0.443718 in 0.03s\n",
      " [-] epoch   75/250, train loss 0.443723 in 0.03s\n",
      " [-] epoch   76/250, train loss 0.442998 in 0.03s\n",
      " [-] epoch   77/250, train loss 0.438818 in 0.02s\n",
      " [-] epoch   78/250, train loss 0.425812 in 0.03s\n",
      " [-] epoch   79/250, train loss 0.455550 in 0.03s\n",
      " [-] epoch   80/250, train loss 0.461926 in 0.03s\n",
      " [-] epoch   81/250, train loss 0.437535 in 0.03s\n",
      " [-] epoch   82/250, train loss 0.435652 in 0.02s\n",
      " [-] epoch   83/250, train loss 0.446051 in 0.03s\n",
      " [-] epoch   84/250, train loss 0.454365 in 0.03s\n",
      " [-] epoch   85/250, train loss 0.426782 in 0.03s\n",
      " [-] epoch   86/250, train loss 0.449189 in 0.03s\n",
      " [-] epoch   87/250, train loss 0.439182 in 0.03s\n",
      " [-] epoch   88/250, train loss 0.439271 in 0.03s\n",
      " [-] epoch   89/250, train loss 0.446835 in 0.03s\n",
      " [-] epoch   90/250, train loss 0.432996 in 0.03s\n",
      " [-] epoch   91/250, train loss 0.423893 in 0.04s\n",
      " [-] epoch   92/250, train loss 0.450408 in 0.03s\n",
      " [-] epoch   93/250, train loss 0.426811 in 0.03s\n",
      " [-] epoch   94/250, train loss 0.420783 in 0.03s\n",
      " [-] epoch   95/250, train loss 0.423785 in 0.03s\n",
      " [-] epoch   96/250, train loss 0.426003 in 0.03s\n",
      " [-] epoch   97/250, train loss 0.456690 in 0.03s\n",
      " [-] epoch   98/250, train loss 0.420610 in 0.03s\n",
      " [-] epoch   99/250, train loss 0.426513 in 0.03s\n",
      " [-] epoch  100/250, train loss 0.461158 in 0.03s\n",
      " [-] epoch  101/250, train loss 0.415989 in 0.03s\n",
      " [-] epoch  102/250, train loss 0.426366 in 0.03s\n",
      " [-] epoch  103/250, train loss 0.448316 in 0.03s\n",
      " [-] epoch  104/250, train loss 0.439311 in 0.03s\n",
      " [-] epoch  105/250, train loss 0.445333 in 0.03s\n",
      " [-] epoch  106/250, train loss 0.423601 in 0.03s\n",
      " [-] epoch  107/250, train loss 0.439187 in 0.03s\n",
      " [-] epoch  108/250, train loss 0.433433 in 0.03s\n",
      " [-] epoch  109/250, train loss 0.441506 in 0.03s\n",
      " [-] epoch  110/250, train loss 0.419462 in 0.03s\n",
      " [-] epoch  111/250, train loss 0.432694 in 0.03s\n",
      " [-] epoch  112/250, train loss 0.415232 in 0.03s\n",
      " [-] epoch  113/250, train loss 0.432503 in 0.03s\n",
      " [-] epoch  114/250, train loss 0.418831 in 0.03s\n",
      " [-] epoch  115/250, train loss 0.399350 in 0.03s\n",
      " [-] epoch  116/250, train loss 0.431033 in 0.03s\n",
      " [-] epoch  117/250, train loss 0.415211 in 0.03s\n",
      " [-] epoch  118/250, train loss 0.422882 in 0.03s\n",
      " [-] epoch  119/250, train loss 0.434474 in 0.03s\n",
      " [-] epoch  120/250, train loss 0.415012 in 0.03s\n",
      " [-] epoch  121/250, train loss 0.424558 in 0.03s\n",
      " [-] epoch  122/250, train loss 0.421011 in 0.03s\n",
      " [-] epoch  123/250, train loss 0.417624 in 0.02s\n",
      " [-] epoch  124/250, train loss 0.420680 in 0.03s\n",
      " [-] epoch  125/250, train loss 0.415637 in 0.03s\n",
      " [-] epoch  126/250, train loss 0.412918 in 0.03s\n",
      " [-] epoch  127/250, train loss 0.414603 in 0.03s\n",
      " [-] epoch  128/250, train loss 0.416057 in 0.03s\n",
      " [-] epoch  129/250, train loss 0.412618 in 0.03s\n",
      " [-] epoch  130/250, train loss 0.404668 in 0.03s\n",
      " [-] epoch  131/250, train loss 0.422584 in 0.03s\n",
      " [-] epoch  132/250, train loss 0.426590 in 0.03s\n",
      " [-] epoch  133/250, train loss 0.407028 in 0.03s\n",
      " [-] epoch  134/250, train loss 0.407906 in 0.03s\n",
      " [-] epoch  135/250, train loss 0.409173 in 0.03s\n",
      " [-] epoch  136/250, train loss 0.413573 in 0.03s\n",
      " [-] epoch  137/250, train loss 0.419781 in 0.03s\n",
      " [-] epoch  138/250, train loss 0.418465 in 0.03s\n",
      " [-] epoch  139/250, train loss 0.438541 in 0.03s\n",
      " [-] epoch  140/250, train loss 0.401110 in 0.03s\n",
      " [-] epoch  141/250, train loss 0.435644 in 0.02s\n",
      " [-] epoch  142/250, train loss 0.406052 in 0.03s\n",
      " [-] epoch  143/250, train loss 0.425998 in 0.03s\n",
      " [-] epoch  144/250, train loss 0.390823 in 0.03s\n",
      " [-] epoch  145/250, train loss 0.437552 in 0.03s\n",
      " [-] epoch  146/250, train loss 0.448306 in 0.03s\n",
      " [-] epoch  147/250, train loss 0.410591 in 0.03s\n",
      " [-] epoch  148/250, train loss 0.428829 in 0.03s\n",
      " [-] epoch  149/250, train loss 0.424597 in 0.03s\n",
      " [-] epoch  150/250, train loss 0.414360 in 0.03s\n",
      " [-] epoch  151/250, train loss 0.411201 in 0.03s\n",
      " [-] epoch  152/250, train loss 0.416670 in 0.03s\n",
      " [-] epoch  153/250, train loss 0.407897 in 0.03s\n",
      " [-] epoch  154/250, train loss 0.421654 in 0.03s\n",
      " [-] epoch  155/250, train loss 0.410982 in 0.03s\n",
      " [-] epoch  156/250, train loss 0.409894 in 0.03s\n",
      " [-] epoch  157/250, train loss 0.405318 in 0.03s\n",
      " [-] epoch  158/250, train loss 0.410543 in 0.03s\n",
      " [-] epoch  159/250, train loss 0.418293 in 0.03s\n",
      " [-] epoch  160/250, train loss 0.416968 in 0.03s\n",
      " [-] epoch  161/250, train loss 0.424800 in 0.03s\n",
      " [-] epoch  162/250, train loss 0.418563 in 0.03s\n",
      " [-] epoch  163/250, train loss 0.433209 in 0.03s\n",
      " [-] epoch  164/250, train loss 0.408729 in 0.03s\n",
      " [-] epoch  165/250, train loss 0.422436 in 0.03s\n",
      " [-] epoch  166/250, train loss 0.426027 in 0.03s\n",
      " [-] epoch  167/250, train loss 0.409895 in 0.03s\n",
      " [-] epoch  168/250, train loss 0.409231 in 0.03s\n",
      " [-] epoch  169/250, train loss 0.395823 in 0.03s\n",
      " [-] epoch  170/250, train loss 0.414587 in 0.03s\n",
      " [-] epoch  171/250, train loss 0.411114 in 0.03s\n",
      " [-] epoch  172/250, train loss 0.399019 in 0.03s\n",
      " [-] epoch  173/250, train loss 0.394968 in 0.03s\n",
      " [-] epoch  174/250, train loss 0.406293 in 0.03s\n",
      " [-] epoch  175/250, train loss 0.410021 in 0.03s\n",
      " [-] epoch  176/250, train loss 0.411000 in 0.03s\n",
      " [-] epoch  177/250, train loss 0.426811 in 0.03s\n",
      " [-] epoch  178/250, train loss 0.423709 in 0.03s\n",
      " [-] epoch  179/250, train loss 0.400071 in 0.03s\n",
      " [-] epoch  180/250, train loss 0.390296 in 0.03s\n",
      " [-] epoch  181/250, train loss 0.416739 in 0.03s\n",
      " [-] epoch  182/250, train loss 0.410546 in 0.03s\n",
      " [-] epoch  183/250, train loss 0.399930 in 0.03s\n",
      " [-] epoch  184/250, train loss 0.405117 in 0.03s\n",
      " [-] epoch  185/250, train loss 0.414516 in 0.04s\n",
      " [-] epoch  186/250, train loss 0.417465 in 0.03s\n",
      " [-] epoch  187/250, train loss 0.411473 in 0.03s\n",
      " [-] epoch  188/250, train loss 0.401347 in 0.03s\n",
      " [-] epoch  189/250, train loss 0.407957 in 0.03s\n",
      " [-] epoch  190/250, train loss 0.390860 in 0.03s\n",
      " [-] epoch  191/250, train loss 0.409210 in 0.03s\n",
      " [-] epoch  192/250, train loss 0.389607 in 0.03s\n",
      " [-] epoch  193/250, train loss 0.395724 in 0.03s\n",
      " [-] epoch  194/250, train loss 0.439176 in 0.03s\n",
      " [-] epoch  195/250, train loss 0.408678 in 0.02s\n",
      " [-] epoch  196/250, train loss 0.409444 in 0.03s\n",
      " [-] epoch  197/250, train loss 0.400692 in 0.03s\n",
      " [-] epoch  198/250, train loss 0.386844 in 0.03s\n",
      " [-] epoch  199/250, train loss 0.409035 in 0.03s\n",
      " [-] epoch  200/250, train loss 0.399440 in 0.03s\n",
      " [-] epoch  201/250, train loss 0.400205 in 0.02s\n",
      " [-] epoch  202/250, train loss 0.408856 in 0.03s\n",
      " [-] epoch  203/250, train loss 0.404271 in 0.03s\n",
      " [-] epoch  204/250, train loss 0.402758 in 0.03s\n",
      " [-] epoch  205/250, train loss 0.428574 in 0.03s\n",
      " [-] epoch  206/250, train loss 0.428718 in 0.03s\n",
      " [-] epoch  207/250, train loss 0.388471 in 0.03s\n",
      " [-] epoch  208/250, train loss 0.400042 in 0.03s\n",
      " [-] epoch  209/250, train loss 0.391695 in 0.03s\n",
      " [-] epoch  210/250, train loss 0.427544 in 0.03s\n",
      " [-] epoch  211/250, train loss 0.400224 in 0.03s\n",
      " [-] epoch  212/250, train loss 0.395737 in 0.03s\n",
      " [-] epoch  213/250, train loss 0.407511 in 0.03s\n",
      " [-] epoch  214/250, train loss 0.407626 in 0.03s\n",
      " [-] epoch  215/250, train loss 0.377390 in 0.03s\n",
      " [-] epoch  216/250, train loss 0.412704 in 0.03s\n",
      " [-] epoch  217/250, train loss 0.395338 in 0.03s\n",
      " [-] epoch  218/250, train loss 0.406954 in 0.03s\n",
      " [-] epoch  219/250, train loss 0.393453 in 0.03s\n",
      " [-] epoch  220/250, train loss 0.393603 in 0.03s\n",
      " [-] epoch  221/250, train loss 0.415229 in 0.03s\n",
      " [-] epoch  222/250, train loss 0.399749 in 0.03s\n",
      " [-] epoch  223/250, train loss 0.388951 in 0.02s\n",
      " [-] epoch  224/250, train loss 0.386051 in 0.02s\n",
      " [-] epoch  225/250, train loss 0.414744 in 0.03s\n",
      " [-] epoch  226/250, train loss 0.390653 in 0.03s\n",
      " [-] epoch  227/250, train loss 0.404790 in 0.03s\n",
      " [-] epoch  228/250, train loss 0.377873 in 0.03s\n",
      " [-] epoch  229/250, train loss 0.387934 in 0.03s\n",
      " [-] epoch  230/250, train loss 0.396379 in 0.03s\n",
      " [-] epoch  231/250, train loss 0.424489 in 0.03s\n",
      " [-] epoch  232/250, train loss 0.380163 in 0.03s\n",
      " [-] epoch  233/250, train loss 0.395174 in 0.03s\n",
      " [-] epoch  234/250, train loss 0.400887 in 0.03s\n",
      " [-] epoch  235/250, train loss 0.401479 in 0.03s\n",
      " [-] epoch  236/250, train loss 0.373238 in 0.03s\n",
      " [-] epoch  237/250, train loss 0.378884 in 0.03s\n",
      " [-] epoch  238/250, train loss 0.394642 in 0.02s\n",
      " [-] epoch  239/250, train loss 0.409679 in 0.03s\n",
      " [-] epoch  240/250, train loss 0.375524 in 0.03s\n",
      " [-] epoch  241/250, train loss 0.400764 in 0.03s\n",
      " [-] epoch  242/250, train loss 0.388357 in 0.03s\n",
      " [-] epoch  243/250, train loss 0.394262 in 0.03s\n",
      " [-] epoch  244/250, train loss 0.380884 in 0.03s\n",
      " [-] epoch  245/250, train loss 0.395338 in 0.03s\n",
      " [-] epoch  246/250, train loss 0.413390 in 0.03s\n",
      " [-] epoch  247/250, train loss 0.419087 in 0.02s\n",
      " [-] epoch  248/250, train loss 0.408124 in 0.03s\n",
      " [-] epoch  249/250, train loss 0.370555 in 0.03s\n",
      " [-] epoch  250/250, train loss 0.395318 in 0.03s\n",
      " [-] test acc. 83.055556%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.589388 in 0.06s\n",
      " [-] epoch    2/250, train loss 0.459193 in 0.08s\n",
      " [-] epoch    3/250, train loss 0.472389 in 0.07s\n",
      " [-] epoch    4/250, train loss 0.453155 in 0.06s\n",
      " [-] epoch    5/250, train loss 0.421463 in 0.08s\n",
      " [-] epoch    6/250, train loss 0.393128 in 0.07s\n",
      " [-] epoch    7/250, train loss 0.428014 in 0.07s\n",
      " [-] epoch    8/250, train loss 0.410945 in 0.07s\n",
      " [-] epoch    9/250, train loss 0.375544 in 0.07s\n",
      " [-] epoch   10/250, train loss 0.402918 in 0.07s\n",
      " [-] epoch   11/250, train loss 0.408163 in 0.07s\n",
      " [-] epoch   12/250, train loss 0.363794 in 0.06s\n",
      " [-] epoch   13/250, train loss 0.403182 in 0.07s\n",
      " [-] epoch   14/250, train loss 0.376326 in 0.06s\n",
      " [-] epoch   15/250, train loss 0.365432 in 0.08s\n",
      " [-] epoch   16/250, train loss 0.371595 in 0.07s\n",
      " [-] epoch   17/250, train loss 0.391871 in 0.07s\n",
      " [-] epoch   18/250, train loss 0.355608 in 0.06s\n",
      " [-] epoch   19/250, train loss 0.370593 in 0.07s\n",
      " [-] epoch   20/250, train loss 0.394204 in 0.06s\n",
      " [-] epoch   21/250, train loss 0.362525 in 0.08s\n",
      " [-] epoch   22/250, train loss 0.362596 in 0.06s\n",
      " [-] epoch   23/250, train loss 0.346626 in 0.07s\n",
      " [-] epoch   24/250, train loss 0.376250 in 0.06s\n",
      " [-] epoch   25/250, train loss 0.363399 in 0.08s\n",
      " [-] epoch   26/250, train loss 0.372603 in 0.07s\n",
      " [-] epoch   27/250, train loss 0.371687 in 0.08s\n",
      " [-] epoch   28/250, train loss 0.342641 in 0.06s\n",
      " [-] epoch   29/250, train loss 0.354405 in 0.08s\n",
      " [-] epoch   30/250, train loss 0.357196 in 0.07s\n",
      " [-] epoch   31/250, train loss 0.368604 in 0.08s\n",
      " [-] epoch   32/250, train loss 0.339927 in 0.07s\n",
      " [-] epoch   33/250, train loss 0.365285 in 0.06s\n",
      " [-] epoch   34/250, train loss 0.347617 in 0.08s\n",
      " [-] epoch   35/250, train loss 0.345027 in 0.07s\n",
      " [-] epoch   36/250, train loss 0.369485 in 0.08s\n",
      " [-] epoch   37/250, train loss 0.369957 in 0.07s\n",
      " [-] epoch   38/250, train loss 0.377549 in 0.08s\n",
      " [-] epoch   39/250, train loss 0.350288 in 0.08s\n",
      " [-] epoch   40/250, train loss 0.354258 in 0.08s\n",
      " [-] epoch   41/250, train loss 0.374277 in 0.08s\n",
      " [-] epoch   42/250, train loss 0.360221 in 0.08s\n",
      " [-] epoch   43/250, train loss 0.298430 in 0.07s\n",
      " [-] epoch   44/250, train loss 0.360025 in 0.07s\n",
      " [-] epoch   45/250, train loss 0.306287 in 0.07s\n",
      " [-] epoch   46/250, train loss 0.318142 in 0.07s\n",
      " [-] epoch   47/250, train loss 0.336756 in 0.06s\n",
      " [-] epoch   48/250, train loss 0.322262 in 0.07s\n",
      " [-] epoch   49/250, train loss 0.353159 in 0.07s\n",
      " [-] epoch   50/250, train loss 0.342092 in 0.07s\n",
      " [-] epoch   51/250, train loss 0.346992 in 0.06s\n",
      " [-] epoch   52/250, train loss 0.359186 in 0.07s\n",
      " [-] epoch   53/250, train loss 0.337083 in 0.06s\n",
      " [-] epoch   54/250, train loss 0.336179 in 0.07s\n",
      " [-] epoch   55/250, train loss 0.354946 in 0.08s\n",
      " [-] epoch   56/250, train loss 0.340803 in 0.07s\n",
      " [-] epoch   57/250, train loss 0.339750 in 0.06s\n",
      " [-] epoch   58/250, train loss 0.331449 in 0.08s\n",
      " [-] epoch   59/250, train loss 0.334794 in 0.07s\n",
      " [-] epoch   60/250, train loss 0.372095 in 0.06s\n",
      " [-] epoch   61/250, train loss 0.354032 in 0.07s\n",
      " [-] epoch   62/250, train loss 0.339908 in 0.07s\n",
      " [-] epoch   63/250, train loss 0.329310 in 0.07s\n",
      " [-] epoch   64/250, train loss 0.345802 in 0.07s\n",
      " [-] epoch   65/250, train loss 0.321532 in 0.06s\n",
      " [-] epoch   66/250, train loss 0.335564 in 0.07s\n",
      " [-] epoch   67/250, train loss 0.328030 in 0.06s\n",
      " [-] epoch   68/250, train loss 0.329418 in 0.08s\n",
      " [-] epoch   69/250, train loss 0.344038 in 0.07s\n",
      " [-] epoch   70/250, train loss 0.340202 in 0.07s\n",
      " [-] epoch   71/250, train loss 0.331445 in 0.07s\n",
      " [-] epoch   72/250, train loss 0.366268 in 0.07s\n",
      " [-] epoch   73/250, train loss 0.332901 in 0.07s\n",
      " [-] epoch   74/250, train loss 0.371943 in 0.07s\n",
      " [-] epoch   75/250, train loss 0.326040 in 0.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.333345 in 0.08s\n",
      " [-] epoch   77/250, train loss 0.311390 in 0.07s\n",
      " [-] epoch   78/250, train loss 0.329150 in 0.07s\n",
      " [-] epoch   79/250, train loss 0.329106 in 0.07s\n",
      " [-] epoch   80/250, train loss 0.360876 in 0.07s\n",
      " [-] epoch   81/250, train loss 0.333102 in 0.06s\n",
      " [-] epoch   82/250, train loss 0.335034 in 0.08s\n",
      " [-] epoch   83/250, train loss 0.334150 in 0.08s\n",
      " [-] epoch   84/250, train loss 0.321516 in 0.07s\n",
      " [-] epoch   85/250, train loss 0.308439 in 0.07s\n",
      " [-] epoch   86/250, train loss 0.342272 in 0.08s\n",
      " [-] epoch   87/250, train loss 0.315602 in 0.07s\n",
      " [-] epoch   88/250, train loss 0.310725 in 0.07s\n",
      " [-] epoch   89/250, train loss 0.285735 in 0.07s\n",
      " [-] epoch   90/250, train loss 0.315211 in 0.07s\n",
      " [-] epoch   91/250, train loss 0.354974 in 0.07s\n",
      " [-] epoch   92/250, train loss 0.318265 in 0.07s\n",
      " [-] epoch   93/250, train loss 0.321508 in 0.07s\n",
      " [-] epoch   94/250, train loss 0.317926 in 0.07s\n",
      " [-] epoch   95/250, train loss 0.306919 in 0.08s\n",
      " [-] epoch   96/250, train loss 0.324599 in 0.07s\n",
      " [-] epoch   97/250, train loss 0.337846 in 0.06s\n",
      " [-] epoch   98/250, train loss 0.319472 in 0.07s\n",
      " [-] epoch   99/250, train loss 0.318448 in 0.07s\n",
      " [-] epoch  100/250, train loss 0.300456 in 0.07s\n",
      " [-] epoch  101/250, train loss 0.306435 in 0.07s\n",
      " [-] epoch  102/250, train loss 0.329245 in 0.07s\n",
      " [-] epoch  103/250, train loss 0.314801 in 0.07s\n",
      " [-] epoch  104/250, train loss 0.327013 in 0.07s\n",
      " [-] epoch  105/250, train loss 0.311309 in 0.07s\n",
      " [-] epoch  106/250, train loss 0.316106 in 0.07s\n",
      " [-] epoch  107/250, train loss 0.305279 in 0.06s\n",
      " [-] epoch  108/250, train loss 0.310491 in 0.07s\n",
      " [-] epoch  109/250, train loss 0.327569 in 0.08s\n",
      " [-] epoch  110/250, train loss 0.332822 in 0.07s\n",
      " [-] epoch  111/250, train loss 0.307179 in 0.08s\n",
      " [-] epoch  112/250, train loss 0.294819 in 0.08s\n",
      " [-] epoch  113/250, train loss 0.309085 in 0.07s\n",
      " [-] epoch  114/250, train loss 0.300326 in 0.07s\n",
      " [-] epoch  115/250, train loss 0.302809 in 0.07s\n",
      " [-] epoch  116/250, train loss 0.310339 in 0.07s\n",
      " [-] epoch  117/250, train loss 0.314488 in 0.07s\n",
      " [-] epoch  118/250, train loss 0.306241 in 0.07s\n",
      " [-] epoch  119/250, train loss 0.314247 in 0.07s\n",
      " [-] epoch  120/250, train loss 0.351496 in 0.07s\n",
      " [-] epoch  121/250, train loss 0.320036 in 0.07s\n",
      " [-] epoch  122/250, train loss 0.317873 in 0.06s\n",
      " [-] epoch  123/250, train loss 0.296083 in 0.07s\n",
      " [-] epoch  124/250, train loss 0.321607 in 0.06s\n",
      " [-] epoch  125/250, train loss 0.277357 in 0.08s\n",
      " [-] epoch  126/250, train loss 0.308154 in 0.07s\n",
      " [-] epoch  127/250, train loss 0.312035 in 0.06s\n",
      " [-] epoch  128/250, train loss 0.293939 in 0.08s\n",
      " [-] epoch  129/250, train loss 0.290185 in 0.07s\n",
      " [-] epoch  130/250, train loss 0.321380 in 0.08s\n",
      " [-] epoch  131/250, train loss 0.315790 in 0.07s\n",
      " [-] epoch  132/250, train loss 0.310334 in 0.07s\n",
      " [-] epoch  133/250, train loss 0.295770 in 0.07s\n",
      " [-] epoch  134/250, train loss 0.307225 in 0.07s\n",
      " [-] epoch  135/250, train loss 0.269859 in 0.08s\n",
      " [-] epoch  136/250, train loss 0.300428 in 0.07s\n",
      " [-] epoch  137/250, train loss 0.323631 in 0.07s\n",
      " [-] epoch  138/250, train loss 0.293592 in 0.07s\n",
      " [-] epoch  139/250, train loss 0.289810 in 0.07s\n",
      " [-] epoch  140/250, train loss 0.306743 in 0.07s\n",
      " [-] epoch  141/250, train loss 0.301681 in 0.06s\n",
      " [-] epoch  142/250, train loss 0.297407 in 0.08s\n",
      " [-] epoch  143/250, train loss 0.304401 in 0.07s\n",
      " [-] epoch  144/250, train loss 0.283707 in 0.08s\n",
      " [-] epoch  145/250, train loss 0.269838 in 0.07s\n",
      " [-] epoch  146/250, train loss 0.299356 in 0.08s\n",
      " [-] epoch  147/250, train loss 0.296061 in 0.07s\n",
      " [-] epoch  148/250, train loss 0.307497 in 0.07s\n",
      " [-] epoch  149/250, train loss 0.301179 in 0.08s\n",
      " [-] epoch  150/250, train loss 0.283223 in 0.07s\n",
      " [-] epoch  151/250, train loss 0.267941 in 0.07s\n",
      " [-] epoch  152/250, train loss 0.287276 in 0.07s\n",
      " [-] epoch  153/250, train loss 0.291224 in 0.07s\n",
      " [-] epoch  154/250, train loss 0.298835 in 0.08s\n",
      " [-] epoch  155/250, train loss 0.298897 in 0.08s\n",
      " [-] epoch  156/250, train loss 0.304863 in 0.07s\n",
      " [-] epoch  157/250, train loss 0.282436 in 0.07s\n",
      " [-] epoch  158/250, train loss 0.281890 in 0.07s\n",
      " [-] epoch  159/250, train loss 0.318245 in 0.07s\n",
      " [-] epoch  160/250, train loss 0.318915 in 0.07s\n",
      " [-] epoch  161/250, train loss 0.287761 in 0.07s\n",
      " [-] epoch  162/250, train loss 0.302645 in 0.07s\n",
      " [-] epoch  163/250, train loss 0.279151 in 0.08s\n",
      " [-] epoch  164/250, train loss 0.284099 in 0.07s\n",
      " [-] epoch  165/250, train loss 0.289034 in 0.07s\n",
      " [-] epoch  166/250, train loss 0.297657 in 0.07s\n",
      " [-] epoch  167/250, train loss 0.299889 in 0.08s\n",
      " [-] epoch  168/250, train loss 0.289059 in 0.08s\n",
      " [-] epoch  169/250, train loss 0.301907 in 0.07s\n",
      " [-] epoch  170/250, train loss 0.305862 in 0.07s\n",
      " [-] epoch  171/250, train loss 0.302918 in 0.07s\n",
      " [-] epoch  172/250, train loss 0.292983 in 0.07s\n",
      " [-] epoch  173/250, train loss 0.291203 in 0.08s\n",
      " [-] epoch  174/250, train loss 0.292020 in 0.07s\n",
      " [-] epoch  175/250, train loss 0.287331 in 0.07s\n",
      " [-] epoch  176/250, train loss 0.297907 in 0.07s\n",
      " [-] epoch  177/250, train loss 0.296119 in 0.07s\n",
      " [-] epoch  178/250, train loss 0.296267 in 0.07s\n",
      " [-] epoch  179/250, train loss 0.309634 in 0.08s\n",
      " [-] epoch  180/250, train loss 0.296431 in 0.07s\n",
      " [-] epoch  181/250, train loss 0.287648 in 0.07s\n",
      " [-] epoch  182/250, train loss 0.293996 in 0.08s\n",
      " [-] epoch  183/250, train loss 0.268670 in 0.08s\n",
      " [-] epoch  184/250, train loss 0.324164 in 0.08s\n",
      " [-] epoch  185/250, train loss 0.268052 in 0.07s\n",
      " [-] epoch  186/250, train loss 0.281531 in 0.07s\n",
      " [-] epoch  187/250, train loss 0.288383 in 0.07s\n",
      " [-] epoch  188/250, train loss 0.308977 in 0.07s\n",
      " [-] epoch  189/250, train loss 0.289011 in 0.08s\n",
      " [-] epoch  190/250, train loss 0.275585 in 0.08s\n",
      " [-] epoch  191/250, train loss 0.300489 in 0.07s\n",
      " [-] epoch  192/250, train loss 0.300325 in 0.07s\n",
      " [-] epoch  193/250, train loss 0.279927 in 0.07s\n",
      " [-] epoch  194/250, train loss 0.294871 in 0.07s\n",
      " [-] epoch  195/250, train loss 0.289069 in 0.08s\n",
      " [-] epoch  196/250, train loss 0.293895 in 0.08s\n",
      " [-] epoch  197/250, train loss 0.273155 in 0.07s\n",
      " [-] epoch  198/250, train loss 0.299189 in 0.07s\n",
      " [-] epoch  199/250, train loss 0.279941 in 0.07s\n",
      " [-] epoch  200/250, train loss 0.300155 in 0.07s\n",
      " [-] epoch  201/250, train loss 0.289988 in 0.07s\n",
      " [-] epoch  202/250, train loss 0.284975 in 0.07s\n",
      " [-] epoch  203/250, train loss 0.291274 in 0.07s\n",
      " [-] epoch  204/250, train loss 0.285387 in 0.07s\n",
      " [-] epoch  205/250, train loss 0.284815 in 0.06s\n",
      " [-] epoch  206/250, train loss 0.300427 in 0.07s\n",
      " [-] epoch  207/250, train loss 0.299631 in 0.07s\n",
      " [-] epoch  208/250, train loss 0.270530 in 0.07s\n",
      " [-] epoch  209/250, train loss 0.280674 in 0.07s\n",
      " [-] epoch  210/250, train loss 0.285312 in 0.06s\n",
      " [-] epoch  211/250, train loss 0.279595 in 0.09s\n",
      " [-] epoch  212/250, train loss 0.273938 in 0.08s\n",
      " [-] epoch  213/250, train loss 0.296658 in 0.08s\n",
      " [-] epoch  214/250, train loss 0.270728 in 0.07s\n",
      " [-] epoch  215/250, train loss 0.282286 in 0.07s\n",
      " [-] epoch  216/250, train loss 0.280587 in 0.08s\n",
      " [-] epoch  217/250, train loss 0.282202 in 0.07s\n",
      " [-] epoch  218/250, train loss 0.279520 in 0.07s\n",
      " [-] epoch  219/250, train loss 0.274696 in 0.07s\n",
      " [-] epoch  220/250, train loss 0.271196 in 0.08s\n",
      " [-] epoch  221/250, train loss 0.297711 in 0.07s\n",
      " [-] epoch  222/250, train loss 0.299969 in 0.07s\n",
      " [-] epoch  223/250, train loss 0.293038 in 0.07s\n",
      " [-] epoch  224/250, train loss 0.279633 in 0.07s\n",
      " [-] epoch  225/250, train loss 0.278264 in 0.07s\n",
      " [-] epoch  226/250, train loss 0.264108 in 0.07s\n",
      " [-] epoch  227/250, train loss 0.265084 in 0.07s\n",
      " [-] epoch  228/250, train loss 0.300871 in 0.07s\n",
      " [-] epoch  229/250, train loss 0.261103 in 0.07s\n",
      " [-] epoch  230/250, train loss 0.269833 in 0.07s\n",
      " [-] epoch  231/250, train loss 0.272480 in 0.06s\n",
      " [-] epoch  232/250, train loss 0.291648 in 0.07s\n",
      " [-] epoch  233/250, train loss 0.301868 in 0.07s\n",
      " [-] epoch  234/250, train loss 0.264062 in 0.07s\n",
      " [-] epoch  235/250, train loss 0.308281 in 0.07s\n",
      " [-] epoch  236/250, train loss 0.281242 in 0.07s\n",
      " [-] epoch  237/250, train loss 0.277029 in 0.07s\n",
      " [-] epoch  238/250, train loss 0.264663 in 0.08s\n",
      " [-] epoch  239/250, train loss 0.264546 in 0.07s\n",
      " [-] epoch  240/250, train loss 0.270482 in 0.07s\n",
      " [-] epoch  241/250, train loss 0.262180 in 0.06s\n",
      " [-] epoch  242/250, train loss 0.284999 in 0.07s\n",
      " [-] epoch  243/250, train loss 0.266501 in 0.07s\n",
      " [-] epoch  244/250, train loss 0.288543 in 0.08s\n",
      " [-] epoch  245/250, train loss 0.281126 in 0.06s\n",
      " [-] epoch  246/250, train loss 0.272339 in 0.08s\n",
      " [-] epoch  247/250, train loss 0.311172 in 0.07s\n",
      " [-] epoch  248/250, train loss 0.270497 in 0.07s\n",
      " [-] epoch  249/250, train loss 0.278764 in 0.07s\n",
      " [-] epoch  250/250, train loss 0.278198 in 0.06s\n",
      " [-] test acc. 81.666667%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.548998 in 0.13s\n",
      " [-] epoch    2/250, train loss 0.481776 in 0.12s\n",
      " [-] epoch    3/250, train loss 0.429799 in 0.13s\n",
      " [-] epoch    4/250, train loss 0.413195 in 0.10s\n",
      " [-] epoch    5/250, train loss 0.406080 in 0.13s\n",
      " [-] epoch    6/250, train loss 0.389216 in 0.13s\n",
      " [-] epoch    7/250, train loss 0.394210 in 0.14s\n",
      " [-] epoch    8/250, train loss 0.353340 in 0.13s\n",
      " [-] epoch    9/250, train loss 0.377068 in 0.13s\n",
      " [-] epoch   10/250, train loss 0.359617 in 0.13s\n",
      " [-] epoch   11/250, train loss 0.357816 in 0.12s\n",
      " [-] epoch   12/250, train loss 0.348653 in 0.12s\n",
      " [-] epoch   13/250, train loss 0.351575 in 0.13s\n",
      " [-] epoch   14/250, train loss 0.366971 in 0.13s\n",
      " [-] epoch   15/250, train loss 0.351504 in 0.11s\n",
      " [-] epoch   16/250, train loss 0.323480 in 0.14s\n",
      " [-] epoch   17/250, train loss 0.319987 in 0.12s\n",
      " [-] epoch   18/250, train loss 0.340573 in 0.13s\n",
      " [-] epoch   19/250, train loss 0.326697 in 0.12s\n",
      " [-] epoch   20/250, train loss 0.309886 in 0.13s\n",
      " [-] epoch   21/250, train loss 0.330867 in 0.13s\n",
      " [-] epoch   22/250, train loss 0.305773 in 0.12s\n",
      " [-] epoch   23/250, train loss 0.324815 in 0.13s\n",
      " [-] epoch   24/250, train loss 0.298604 in 0.13s\n",
      " [-] epoch   25/250, train loss 0.307371 in 0.13s\n",
      " [-] epoch   26/250, train loss 0.306367 in 0.12s\n",
      " [-] epoch   27/250, train loss 0.299351 in 0.13s\n",
      " [-] epoch   28/250, train loss 0.309949 in 0.13s\n",
      " [-] epoch   29/250, train loss 0.327910 in 0.12s\n",
      " [-] epoch   30/250, train loss 0.314412 in 0.12s\n",
      " [-] epoch   31/250, train loss 0.334165 in 0.12s\n",
      " [-] epoch   32/250, train loss 0.294656 in 0.12s\n",
      " [-] epoch   33/250, train loss 0.316993 in 0.13s\n",
      " [-] epoch   34/250, train loss 0.311328 in 0.13s\n",
      " [-] epoch   35/250, train loss 0.299661 in 0.12s\n",
      " [-] epoch   36/250, train loss 0.303548 in 0.13s\n",
      " [-] epoch   37/250, train loss 0.303151 in 0.12s\n",
      " [-] epoch   38/250, train loss 0.271339 in 0.13s\n",
      " [-] epoch   39/250, train loss 0.294569 in 0.13s\n",
      " [-] epoch   40/250, train loss 0.274751 in 0.12s\n",
      " [-] epoch   41/250, train loss 0.282481 in 0.12s\n",
      " [-] epoch   42/250, train loss 0.281721 in 0.13s\n",
      " [-] epoch   43/250, train loss 0.280698 in 0.12s\n",
      " [-] epoch   44/250, train loss 0.280372 in 0.13s\n",
      " [-] epoch   45/250, train loss 0.269192 in 0.12s\n",
      " [-] epoch   46/250, train loss 0.263785 in 0.13s\n",
      " [-] epoch   47/250, train loss 0.256362 in 0.14s\n",
      " [-] epoch   48/250, train loss 0.298738 in 0.14s\n",
      " [-] epoch   49/250, train loss 0.293382 in 0.12s\n",
      " [-] epoch   50/250, train loss 0.281513 in 0.12s\n",
      " [-] epoch   51/250, train loss 0.264538 in 0.13s\n",
      " [-] epoch   52/250, train loss 0.260880 in 0.13s\n",
      " [-] epoch   53/250, train loss 0.287145 in 0.11s\n",
      " [-] epoch   54/250, train loss 0.275537 in 0.13s\n",
      " [-] epoch   55/250, train loss 0.274719 in 0.12s\n",
      " [-] epoch   56/250, train loss 0.281327 in 0.12s\n",
      " [-] epoch   57/250, train loss 0.271480 in 0.13s\n",
      " [-] epoch   58/250, train loss 0.266263 in 0.13s\n",
      " [-] epoch   59/250, train loss 0.298652 in 0.14s\n",
      " [-] epoch   60/250, train loss 0.286609 in 0.11s\n",
      " [-] epoch   61/250, train loss 0.293065 in 0.12s\n",
      " [-] epoch   62/250, train loss 0.282532 in 0.15s\n",
      " [-] epoch   63/250, train loss 0.268423 in 0.11s\n",
      " [-] epoch   64/250, train loss 0.284838 in 0.14s\n",
      " [-] epoch   65/250, train loss 0.275953 in 0.13s\n",
      " [-] epoch   66/250, train loss 0.274088 in 0.13s\n",
      " [-] epoch   67/250, train loss 0.261729 in 0.12s\n",
      " [-] epoch   68/250, train loss 0.268225 in 0.14s\n",
      " [-] epoch   69/250, train loss 0.267037 in 0.13s\n",
      " [-] epoch   70/250, train loss 0.273024 in 0.13s\n",
      " [-] epoch   71/250, train loss 0.258149 in 0.13s\n",
      " [-] epoch   72/250, train loss 0.259101 in 0.12s\n",
      " [-] epoch   73/250, train loss 0.277705 in 0.13s\n",
      " [-] epoch   74/250, train loss 0.264052 in 0.13s\n",
      " [-] epoch   75/250, train loss 0.273115 in 0.14s\n",
      " [-] epoch   76/250, train loss 0.268949 in 0.11s\n",
      " [-] epoch   77/250, train loss 0.254993 in 0.13s\n",
      " [-] epoch   78/250, train loss 0.265457 in 0.13s\n",
      " [-] epoch   79/250, train loss 0.272922 in 0.12s\n",
      " [-] epoch   80/250, train loss 0.253913 in 0.12s\n",
      " [-] epoch   81/250, train loss 0.270146 in 0.13s\n",
      " [-] epoch   82/250, train loss 0.249337 in 0.13s\n",
      " [-] epoch   83/250, train loss 0.262249 in 0.12s\n",
      " [-] epoch   84/250, train loss 0.244123 in 0.13s\n",
      " [-] epoch   85/250, train loss 0.279192 in 0.12s\n",
      " [-] epoch   86/250, train loss 0.258330 in 0.13s\n",
      " [-] epoch   87/250, train loss 0.261496 in 0.12s\n",
      " [-] epoch   88/250, train loss 0.248889 in 0.13s\n",
      " [-] epoch   89/250, train loss 0.255721 in 0.12s\n",
      " [-] epoch   90/250, train loss 0.273524 in 0.12s\n",
      " [-] epoch   91/250, train loss 0.253215 in 0.13s\n",
      " [-] epoch   92/250, train loss 0.262645 in 0.13s\n",
      " [-] epoch   93/250, train loss 0.270474 in 0.14s\n",
      " [-] epoch   94/250, train loss 0.241591 in 0.12s\n",
      " [-] epoch   95/250, train loss 0.245767 in 0.13s\n",
      " [-] epoch   96/250, train loss 0.266584 in 0.14s\n",
      " [-] epoch   97/250, train loss 0.258896 in 0.13s\n",
      " [-] epoch   98/250, train loss 0.236993 in 0.13s\n",
      " [-] epoch   99/250, train loss 0.244091 in 0.12s\n",
      " [-] epoch  100/250, train loss 0.244552 in 0.13s\n",
      " [-] epoch  101/250, train loss 0.252870 in 0.12s\n",
      " [-] epoch  102/250, train loss 0.253335 in 0.12s\n",
      " [-] epoch  103/250, train loss 0.247400 in 0.14s\n",
      " [-] epoch  104/250, train loss 0.258899 in 0.13s\n",
      " [-] epoch  105/250, train loss 0.244171 in 0.11s\n",
      " [-] epoch  106/250, train loss 0.236502 in 0.13s\n",
      " [-] epoch  107/250, train loss 0.252546 in 0.13s\n",
      " [-] epoch  108/250, train loss 0.263293 in 0.12s\n",
      " [-] epoch  109/250, train loss 0.247907 in 0.14s\n",
      " [-] epoch  110/250, train loss 0.263875 in 0.13s\n",
      " [-] epoch  111/250, train loss 0.256266 in 0.13s\n",
      " [-] epoch  112/250, train loss 0.251174 in 0.15s\n",
      " [-] epoch  113/250, train loss 0.233793 in 0.13s\n",
      " [-] epoch  114/250, train loss 0.246245 in 0.13s\n",
      " [-] epoch  115/250, train loss 0.254213 in 0.13s\n",
      " [-] epoch  116/250, train loss 0.248738 in 0.12s\n",
      " [-] epoch  117/250, train loss 0.246713 in 0.13s\n",
      " [-] epoch  118/250, train loss 0.268509 in 0.12s\n",
      " [-] epoch  119/250, train loss 0.246201 in 0.13s\n",
      " [-] epoch  120/250, train loss 0.234906 in 0.14s\n",
      " [-] epoch  121/250, train loss 0.237309 in 0.13s\n",
      " [-] epoch  122/250, train loss 0.244022 in 0.12s\n",
      " [-] epoch  123/250, train loss 0.271000 in 0.15s\n",
      " [-] epoch  124/250, train loss 0.248293 in 0.11s\n",
      " [-] epoch  125/250, train loss 0.272604 in 0.15s\n",
      " [-] epoch  126/250, train loss 0.236063 in 0.13s\n",
      " [-] epoch  127/250, train loss 0.249643 in 0.12s\n",
      " [-] epoch  128/250, train loss 0.225916 in 0.11s\n",
      " [-] epoch  129/250, train loss 0.271095 in 0.12s\n",
      " [-] epoch  130/250, train loss 0.243716 in 0.12s\n",
      " [-] epoch  131/250, train loss 0.243114 in 0.13s\n",
      " [-] epoch  132/250, train loss 0.244895 in 0.11s\n",
      " [-] epoch  133/250, train loss 0.254783 in 0.12s\n",
      " [-] epoch  134/250, train loss 0.254038 in 0.14s\n",
      " [-] epoch  135/250, train loss 0.224123 in 0.13s\n",
      " [-] epoch  136/250, train loss 0.249336 in 0.14s\n",
      " [-] epoch  137/250, train loss 0.242031 in 0.13s\n",
      " [-] epoch  138/250, train loss 0.228579 in 0.15s\n",
      " [-] epoch  139/250, train loss 0.260474 in 0.13s\n",
      " [-] epoch  140/250, train loss 0.256818 in 0.13s\n",
      " [-] epoch  141/250, train loss 0.231893 in 0.13s\n",
      " [-] epoch  142/250, train loss 0.245649 in 0.12s\n",
      " [-] epoch  143/250, train loss 0.234837 in 0.13s\n",
      " [-] epoch  144/250, train loss 0.256730 in 0.13s\n",
      " [-] epoch  145/250, train loss 0.225744 in 0.13s\n",
      " [-] epoch  146/250, train loss 0.242575 in 0.12s\n",
      " [-] epoch  147/250, train loss 0.255873 in 0.13s\n",
      " [-] epoch  148/250, train loss 0.242225 in 0.14s\n",
      " [-] epoch  149/250, train loss 0.235139 in 0.11s\n",
      " [-] epoch  150/250, train loss 0.225856 in 0.12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.234843 in 0.13s\n",
      " [-] epoch  152/250, train loss 0.266849 in 0.14s\n",
      " [-] epoch  153/250, train loss 0.239830 in 0.13s\n",
      " [-] epoch  154/250, train loss 0.241307 in 0.13s\n",
      " [-] epoch  155/250, train loss 0.242555 in 0.13s\n",
      " [-] epoch  156/250, train loss 0.246355 in 0.14s\n",
      " [-] epoch  157/250, train loss 0.228121 in 0.13s\n",
      " [-] epoch  158/250, train loss 0.232341 in 0.11s\n",
      " [-] epoch  159/250, train loss 0.233410 in 0.13s\n",
      " [-] epoch  160/250, train loss 0.235731 in 0.13s\n",
      " [-] epoch  161/250, train loss 0.254208 in 0.14s\n",
      " [-] epoch  162/250, train loss 0.249817 in 0.12s\n",
      " [-] epoch  163/250, train loss 0.239944 in 0.12s\n",
      " [-] epoch  164/250, train loss 0.234931 in 0.13s\n",
      " [-] epoch  165/250, train loss 0.231438 in 0.11s\n",
      " [-] epoch  166/250, train loss 0.222969 in 0.13s\n",
      " [-] epoch  167/250, train loss 0.222429 in 0.13s\n",
      " [-] epoch  168/250, train loss 0.206306 in 0.14s\n",
      " [-] epoch  169/250, train loss 0.231362 in 0.11s\n",
      " [-] epoch  170/250, train loss 0.235616 in 0.13s\n",
      " [-] epoch  171/250, train loss 0.246580 in 0.13s\n",
      " [-] epoch  172/250, train loss 0.212873 in 0.14s\n",
      " [-] epoch  173/250, train loss 0.202897 in 0.11s\n",
      " [-] epoch  174/250, train loss 0.238768 in 0.12s\n",
      " [-] epoch  175/250, train loss 0.250133 in 0.13s\n",
      " [-] epoch  176/250, train loss 0.241574 in 0.13s\n",
      " [-] epoch  177/250, train loss 0.227147 in 0.13s\n",
      " [-] epoch  178/250, train loss 0.223253 in 0.12s\n",
      " [-] epoch  179/250, train loss 0.223683 in 0.12s\n",
      " [-] epoch  180/250, train loss 0.254161 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.250416 in 0.13s\n",
      " [-] epoch  182/250, train loss 0.228742 in 0.13s\n",
      " [-] epoch  183/250, train loss 0.225841 in 0.13s\n",
      " [-] epoch  184/250, train loss 0.234085 in 0.13s\n",
      " [-] epoch  185/250, train loss 0.243198 in 0.12s\n",
      " [-] epoch  186/250, train loss 0.235568 in 0.13s\n",
      " [-] epoch  187/250, train loss 0.222218 in 0.13s\n",
      " [-] epoch  188/250, train loss 0.222604 in 0.14s\n",
      " [-] epoch  189/250, train loss 0.231385 in 0.12s\n",
      " [-] epoch  190/250, train loss 0.231110 in 0.13s\n",
      " [-] epoch  191/250, train loss 0.240156 in 0.12s\n",
      " [-] epoch  192/250, train loss 0.233919 in 0.13s\n",
      " [-] epoch  193/250, train loss 0.238423 in 0.13s\n",
      " [-] epoch  194/250, train loss 0.229160 in 0.12s\n",
      " [-] epoch  195/250, train loss 0.230515 in 0.13s\n",
      " [-] epoch  196/250, train loss 0.235833 in 0.13s\n",
      " [-] epoch  197/250, train loss 0.241754 in 0.12s\n",
      " [-] epoch  198/250, train loss 0.213245 in 0.13s\n",
      " [-] epoch  199/250, train loss 0.220915 in 0.14s\n",
      " [-] epoch  200/250, train loss 0.232894 in 0.13s\n",
      " [-] epoch  201/250, train loss 0.224880 in 0.14s\n",
      " [-] epoch  202/250, train loss 0.222788 in 0.13s\n",
      " [-] epoch  203/250, train loss 0.233985 in 0.12s\n",
      " [-] epoch  204/250, train loss 0.235830 in 0.13s\n",
      " [-] epoch  205/250, train loss 0.216534 in 0.14s\n",
      " [-] epoch  206/250, train loss 0.219374 in 0.11s\n",
      " [-] epoch  207/250, train loss 0.220148 in 0.12s\n",
      " [-] epoch  208/250, train loss 0.217893 in 0.13s\n",
      " [-] epoch  209/250, train loss 0.244591 in 0.13s\n",
      " [-] epoch  210/250, train loss 0.249782 in 0.12s\n",
      " [-] epoch  211/250, train loss 0.226870 in 0.13s\n",
      " [-] epoch  212/250, train loss 0.233950 in 0.13s\n",
      " [-] epoch  213/250, train loss 0.222219 in 0.13s\n",
      " [-] epoch  214/250, train loss 0.218724 in 0.12s\n",
      " [-] epoch  215/250, train loss 0.242135 in 0.15s\n",
      " [-] epoch  216/250, train loss 0.227638 in 0.12s\n",
      " [-] epoch  217/250, train loss 0.241059 in 0.12s\n",
      " [-] epoch  218/250, train loss 0.228109 in 0.13s\n",
      " [-] epoch  219/250, train loss 0.228997 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.209795 in 0.12s\n",
      " [-] epoch  221/250, train loss 0.237620 in 0.12s\n",
      " [-] epoch  222/250, train loss 0.217040 in 0.13s\n",
      " [-] epoch  223/250, train loss 0.239071 in 0.15s\n",
      " [-] epoch  224/250, train loss 0.214784 in 0.13s\n",
      " [-] epoch  225/250, train loss 0.226682 in 0.12s\n",
      " [-] epoch  226/250, train loss 0.213082 in 0.12s\n",
      " [-] epoch  227/250, train loss 0.270233 in 0.12s\n",
      " [-] epoch  228/250, train loss 0.216631 in 0.15s\n",
      " [-] epoch  229/250, train loss 0.217919 in 0.12s\n",
      " [-] epoch  230/250, train loss 0.229882 in 0.12s\n",
      " [-] epoch  231/250, train loss 0.227598 in 0.15s\n",
      " [-] epoch  232/250, train loss 0.230850 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.223608 in 0.13s\n",
      " [-] epoch  234/250, train loss 0.259461 in 0.13s\n",
      " [-] epoch  235/250, train loss 0.214967 in 0.12s\n",
      " [-] epoch  236/250, train loss 0.220748 in 0.13s\n",
      " [-] epoch  237/250, train loss 0.216516 in 0.13s\n",
      " [-] epoch  238/250, train loss 0.215944 in 0.14s\n",
      " [-] epoch  239/250, train loss 0.237326 in 0.12s\n",
      " [-] epoch  240/250, train loss 0.222661 in 0.14s\n",
      " [-] epoch  241/250, train loss 0.228059 in 0.13s\n",
      " [-] epoch  242/250, train loss 0.232370 in 0.13s\n",
      " [-] epoch  243/250, train loss 0.214242 in 0.13s\n",
      " [-] epoch  244/250, train loss 0.262731 in 0.14s\n",
      " [-] epoch  245/250, train loss 0.223283 in 0.13s\n",
      " [-] epoch  246/250, train loss 0.231175 in 0.13s\n",
      " [-] epoch  247/250, train loss 0.213466 in 0.14s\n",
      " [-] epoch  248/250, train loss 0.224650 in 0.11s\n",
      " [-] epoch  249/250, train loss 0.230365 in 0.14s\n",
      " [-] epoch  250/250, train loss 0.212486 in 0.14s\n",
      " [-] test acc. 75.833333%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.572536 in 0.20s\n",
      " [-] epoch    2/250, train loss 0.460830 in 0.21s\n",
      " [-] epoch    3/250, train loss 0.459472 in 0.20s\n",
      " [-] epoch    4/250, train loss 0.412910 in 0.21s\n",
      " [-] epoch    5/250, train loss 0.396901 in 0.21s\n",
      " [-] epoch    6/250, train loss 0.406658 in 0.19s\n",
      " [-] epoch    7/250, train loss 0.405068 in 0.19s\n",
      " [-] epoch    8/250, train loss 0.373955 in 0.20s\n",
      " [-] epoch    9/250, train loss 0.370191 in 0.21s\n",
      " [-] epoch   10/250, train loss 0.368443 in 0.20s\n",
      " [-] epoch   11/250, train loss 0.319609 in 0.21s\n",
      " [-] epoch   12/250, train loss 0.388943 in 0.21s\n",
      " [-] epoch   13/250, train loss 0.348861 in 0.19s\n",
      " [-] epoch   14/250, train loss 0.336172 in 0.22s\n",
      " [-] epoch   15/250, train loss 0.317303 in 0.20s\n",
      " [-] epoch   16/250, train loss 0.320564 in 0.21s\n",
      " [-] epoch   17/250, train loss 0.307452 in 0.20s\n",
      " [-] epoch   18/250, train loss 0.330408 in 0.21s\n",
      " [-] epoch   19/250, train loss 0.318575 in 0.18s\n",
      " [-] epoch   20/250, train loss 0.319807 in 0.21s\n",
      " [-] epoch   21/250, train loss 0.331253 in 0.20s\n",
      " [-] epoch   22/250, train loss 0.300432 in 0.22s\n",
      " [-] epoch   23/250, train loss 0.331274 in 0.22s\n",
      " [-] epoch   24/250, train loss 0.328785 in 0.20s\n",
      " [-] epoch   25/250, train loss 0.314998 in 0.22s\n",
      " [-] epoch   26/250, train loss 0.305745 in 0.21s\n",
      " [-] epoch   27/250, train loss 0.305001 in 0.18s\n",
      " [-] epoch   28/250, train loss 0.290372 in 0.20s\n",
      " [-] epoch   29/250, train loss 0.295855 in 0.23s\n",
      " [-] epoch   30/250, train loss 0.309256 in 0.19s\n",
      " [-] epoch   31/250, train loss 0.320750 in 0.19s\n",
      " [-] epoch   32/250, train loss 0.275873 in 0.21s\n",
      " [-] epoch   33/250, train loss 0.293191 in 0.21s\n",
      " [-] epoch   34/250, train loss 0.287080 in 0.20s\n",
      " [-] epoch   35/250, train loss 0.282889 in 0.20s\n",
      " [-] epoch   36/250, train loss 0.247381 in 0.21s\n",
      " [-] epoch   37/250, train loss 0.299133 in 0.20s\n",
      " [-] epoch   38/250, train loss 0.262154 in 0.21s\n",
      " [-] epoch   39/250, train loss 0.285471 in 0.19s\n",
      " [-] epoch   40/250, train loss 0.271809 in 0.23s\n",
      " [-] epoch   41/250, train loss 0.272481 in 0.19s\n",
      " [-] epoch   42/250, train loss 0.276730 in 0.21s\n",
      " [-] epoch   43/250, train loss 0.277207 in 0.20s\n",
      " [-] epoch   44/250, train loss 0.294034 in 0.22s\n",
      " [-] epoch   45/250, train loss 0.295843 in 0.21s\n",
      " [-] epoch   46/250, train loss 0.271388 in 0.21s\n",
      " [-] epoch   47/250, train loss 0.275502 in 0.22s\n",
      " [-] epoch   48/250, train loss 0.276516 in 0.22s\n",
      " [-] epoch   49/250, train loss 0.266488 in 0.21s\n",
      " [-] epoch   50/250, train loss 0.263982 in 0.21s\n",
      " [-] epoch   51/250, train loss 0.262155 in 0.22s\n",
      " [-] epoch   52/250, train loss 0.266975 in 0.21s\n",
      " [-] epoch   53/250, train loss 0.254053 in 0.22s\n",
      " [-] epoch   54/250, train loss 0.262670 in 0.22s\n",
      " [-] epoch   55/250, train loss 0.263575 in 0.20s\n",
      " [-] epoch   56/250, train loss 0.261415 in 0.21s\n",
      " [-] epoch   57/250, train loss 0.250957 in 0.21s\n",
      " [-] epoch   58/250, train loss 0.260746 in 0.21s\n",
      " [-] epoch   59/250, train loss 0.255542 in 0.21s\n",
      " [-] epoch   60/250, train loss 0.264430 in 0.22s\n",
      " [-] epoch   61/250, train loss 0.261399 in 0.19s\n",
      " [-] epoch   62/250, train loss 0.272335 in 0.22s\n",
      " [-] epoch   63/250, train loss 0.260636 in 0.22s\n",
      " [-] epoch   64/250, train loss 0.261913 in 0.22s\n",
      " [-] epoch   65/250, train loss 0.267876 in 0.22s\n",
      " [-] epoch   66/250, train loss 0.274463 in 0.21s\n",
      " [-] epoch   67/250, train loss 0.257273 in 0.21s\n",
      " [-] epoch   68/250, train loss 0.277112 in 0.21s\n",
      " [-] epoch   69/250, train loss 0.241130 in 0.20s\n",
      " [-] epoch   70/250, train loss 0.262461 in 0.22s\n",
      " [-] epoch   71/250, train loss 0.254816 in 0.21s\n",
      " [-] epoch   72/250, train loss 0.249694 in 0.20s\n",
      " [-] epoch   73/250, train loss 0.265656 in 0.23s\n",
      " [-] epoch   74/250, train loss 0.257928 in 0.21s\n",
      " [-] epoch   75/250, train loss 0.258374 in 0.22s\n",
      " [-] epoch   76/250, train loss 0.250761 in 0.21s\n",
      " [-] epoch   77/250, train loss 0.258341 in 0.21s\n",
      " [-] epoch   78/250, train loss 0.253590 in 0.19s\n",
      " [-] epoch   79/250, train loss 0.243209 in 0.22s\n",
      " [-] epoch   80/250, train loss 0.220189 in 0.18s\n",
      " [-] epoch   81/250, train loss 0.276070 in 0.20s\n",
      " [-] epoch   82/250, train loss 0.264100 in 0.21s\n",
      " [-] epoch   83/250, train loss 0.248488 in 0.19s\n",
      " [-] epoch   84/250, train loss 0.256401 in 0.22s\n",
      " [-] epoch   85/250, train loss 0.234013 in 0.23s\n",
      " [-] epoch   86/250, train loss 0.254598 in 0.21s\n",
      " [-] epoch   87/250, train loss 0.256024 in 0.22s\n",
      " [-] epoch   88/250, train loss 0.240368 in 0.21s\n",
      " [-] epoch   89/250, train loss 0.251275 in 0.21s\n",
      " [-] epoch   90/250, train loss 0.263941 in 0.21s\n",
      " [-] epoch   91/250, train loss 0.239445 in 0.19s\n",
      " [-] epoch   92/250, train loss 0.251200 in 0.21s\n",
      " [-] epoch   93/250, train loss 0.222514 in 0.19s\n",
      " [-] epoch   94/250, train loss 0.260541 in 0.22s\n",
      " [-] epoch   95/250, train loss 0.236762 in 0.21s\n",
      " [-] epoch   96/250, train loss 0.261780 in 0.22s\n",
      " [-] epoch   97/250, train loss 0.252940 in 0.22s\n",
      " [-] epoch   98/250, train loss 0.220798 in 0.20s\n",
      " [-] epoch   99/250, train loss 0.236238 in 0.21s\n",
      " [-] epoch  100/250, train loss 0.234967 in 0.19s\n",
      " [-] epoch  101/250, train loss 0.228432 in 0.21s\n",
      " [-] epoch  102/250, train loss 0.238935 in 0.22s\n",
      " [-] epoch  103/250, train loss 0.245825 in 0.20s\n",
      " [-] epoch  104/250, train loss 0.248076 in 0.19s\n",
      " [-] epoch  105/250, train loss 0.235966 in 0.21s\n",
      " [-] epoch  106/250, train loss 0.252643 in 0.20s\n",
      " [-] epoch  107/250, train loss 0.232637 in 0.20s\n",
      " [-] epoch  108/250, train loss 0.227560 in 0.22s\n",
      " [-] epoch  109/250, train loss 0.240151 in 0.22s\n",
      " [-] epoch  110/250, train loss 0.233474 in 0.18s\n",
      " [-] epoch  111/250, train loss 0.249697 in 0.21s\n",
      " [-] epoch  112/250, train loss 0.225642 in 0.21s\n",
      " [-] epoch  113/250, train loss 0.229563 in 0.20s\n",
      " [-] epoch  114/250, train loss 0.242120 in 0.21s\n",
      " [-] epoch  115/250, train loss 0.231313 in 0.20s\n",
      " [-] epoch  116/250, train loss 0.247088 in 0.22s\n",
      " [-] epoch  117/250, train loss 0.230189 in 0.19s\n",
      " [-] epoch  118/250, train loss 0.236221 in 0.23s\n",
      " [-] epoch  119/250, train loss 0.226462 in 0.21s\n",
      " [-] epoch  120/250, train loss 0.230407 in 0.21s\n",
      " [-] epoch  121/250, train loss 0.245042 in 0.20s\n",
      " [-] epoch  122/250, train loss 0.219223 in 0.20s\n",
      " [-] epoch  123/250, train loss 0.214356 in 0.18s\n",
      " [-] epoch  124/250, train loss 0.235815 in 0.23s\n",
      " [-] epoch  125/250, train loss 0.228870 in 0.21s\n",
      " [-] epoch  126/250, train loss 0.232052 in 0.21s\n",
      " [-] epoch  127/250, train loss 0.235717 in 0.20s\n",
      " [-] epoch  128/250, train loss 0.225924 in 0.20s\n",
      " [-] epoch  129/250, train loss 0.225592 in 0.20s\n",
      " [-] epoch  130/250, train loss 0.248042 in 0.22s\n",
      " [-] epoch  131/250, train loss 0.253737 in 0.21s\n",
      " [-] epoch  132/250, train loss 0.232231 in 0.20s\n",
      " [-] epoch  133/250, train loss 0.243212 in 0.20s\n",
      " [-] epoch  134/250, train loss 0.228813 in 0.22s\n",
      " [-] epoch  135/250, train loss 0.236813 in 0.21s\n",
      " [-] epoch  136/250, train loss 0.219754 in 0.20s\n",
      " [-] epoch  137/250, train loss 0.251903 in 0.23s\n",
      " [-] epoch  138/250, train loss 0.231044 in 0.20s\n",
      " [-] epoch  139/250, train loss 0.230129 in 0.22s\n",
      " [-] epoch  140/250, train loss 0.210270 in 0.20s\n",
      " [-] epoch  141/250, train loss 0.240337 in 0.21s\n",
      " [-] epoch  142/250, train loss 0.223637 in 0.21s\n",
      " [-] epoch  143/250, train loss 0.230004 in 0.21s\n",
      " [-] epoch  144/250, train loss 0.213134 in 0.19s\n",
      " [-] epoch  145/250, train loss 0.223877 in 0.21s\n",
      " [-] epoch  146/250, train loss 0.231445 in 0.22s\n",
      " [-] epoch  147/250, train loss 0.211316 in 0.19s\n",
      " [-] epoch  148/250, train loss 0.215750 in 0.23s\n",
      " [-] epoch  149/250, train loss 0.228533 in 0.23s\n",
      " [-] epoch  150/250, train loss 0.242415 in 0.21s\n",
      " [-] epoch  151/250, train loss 0.222777 in 0.21s\n",
      " [-] epoch  152/250, train loss 0.224288 in 0.20s\n",
      " [-] epoch  153/250, train loss 0.232864 in 0.19s\n",
      " [-] epoch  154/250, train loss 0.228096 in 0.22s\n",
      " [-] epoch  155/250, train loss 0.214649 in 0.19s\n",
      " [-] epoch  156/250, train loss 0.242022 in 0.21s\n",
      " [-] epoch  157/250, train loss 0.220512 in 0.21s\n",
      " [-] epoch  158/250, train loss 0.211764 in 0.20s\n",
      " [-] epoch  159/250, train loss 0.244099 in 0.22s\n",
      " [-] epoch  160/250, train loss 0.217886 in 0.21s\n",
      " [-] epoch  161/250, train loss 0.216576 in 0.20s\n",
      " [-] epoch  162/250, train loss 0.226786 in 0.21s\n",
      " [-] epoch  163/250, train loss 0.214976 in 0.19s\n",
      " [-] epoch  164/250, train loss 0.229954 in 0.21s\n",
      " [-] epoch  165/250, train loss 0.206799 in 0.21s\n",
      " [-] epoch  166/250, train loss 0.204038 in 0.20s\n",
      " [-] epoch  167/250, train loss 0.203469 in 0.21s\n",
      " [-] epoch  168/250, train loss 0.215607 in 0.19s\n",
      " [-] epoch  169/250, train loss 0.208664 in 0.20s\n",
      " [-] epoch  170/250, train loss 0.220816 in 0.23s\n",
      " [-] epoch  171/250, train loss 0.239777 in 0.21s\n",
      " [-] epoch  172/250, train loss 0.235499 in 0.19s\n",
      " [-] epoch  173/250, train loss 0.226642 in 0.22s\n",
      " [-] epoch  174/250, train loss 0.221699 in 0.21s\n",
      " [-] epoch  175/250, train loss 0.217432 in 0.21s\n",
      " [-] epoch  176/250, train loss 0.223745 in 0.20s\n",
      " [-] epoch  177/250, train loss 0.218495 in 0.20s\n",
      " [-] epoch  178/250, train loss 0.206457 in 0.20s\n",
      " [-] epoch  179/250, train loss 0.212051 in 0.22s\n",
      " [-] epoch  180/250, train loss 0.216446 in 0.21s\n",
      " [-] epoch  181/250, train loss 0.227065 in 0.22s\n",
      " [-] epoch  182/250, train loss 0.234663 in 0.20s\n",
      " [-] epoch  183/250, train loss 0.223362 in 0.20s\n",
      " [-] epoch  184/250, train loss 0.221404 in 0.21s\n",
      " [-] epoch  185/250, train loss 0.232794 in 0.21s\n",
      " [-] epoch  186/250, train loss 0.228450 in 0.21s\n",
      " [-] epoch  187/250, train loss 0.232802 in 0.21s\n",
      " [-] epoch  188/250, train loss 0.218412 in 0.20s\n",
      " [-] epoch  189/250, train loss 0.220684 in 0.22s\n",
      " [-] epoch  190/250, train loss 0.228358 in 0.21s\n",
      " [-] epoch  191/250, train loss 0.227049 in 0.19s\n",
      " [-] epoch  192/250, train loss 0.211411 in 0.21s\n",
      " [-] epoch  193/250, train loss 0.204909 in 0.21s\n",
      " [-] epoch  194/250, train loss 0.211154 in 0.22s\n",
      " [-] epoch  195/250, train loss 0.211739 in 0.21s\n",
      " [-] epoch  196/250, train loss 0.210431 in 0.18s\n",
      " [-] epoch  197/250, train loss 0.206050 in 0.21s\n",
      " [-] epoch  198/250, train loss 0.217859 in 0.23s\n",
      " [-] epoch  199/250, train loss 0.222315 in 0.19s\n",
      " [-] epoch  200/250, train loss 0.217859 in 0.20s\n",
      " [-] epoch  201/250, train loss 0.214647 in 0.24s\n",
      " [-] epoch  202/250, train loss 0.197206 in 0.21s\n",
      " [-] epoch  203/250, train loss 0.215422 in 0.20s\n",
      " [-] epoch  204/250, train loss 0.186006 in 0.21s\n",
      " [-] epoch  205/250, train loss 0.229395 in 0.19s\n",
      " [-] epoch  206/250, train loss 0.217400 in 0.19s\n",
      " [-] epoch  207/250, train loss 0.232518 in 0.22s\n",
      " [-] epoch  208/250, train loss 0.222540 in 0.19s\n",
      " [-] epoch  209/250, train loss 0.215649 in 0.22s\n",
      " [-] epoch  210/250, train loss 0.208649 in 0.20s\n",
      " [-] epoch  211/250, train loss 0.215550 in 0.20s\n",
      " [-] epoch  212/250, train loss 0.205078 in 0.20s\n",
      " [-] epoch  213/250, train loss 0.221486 in 0.22s\n",
      " [-] epoch  214/250, train loss 0.207135 in 0.18s\n",
      " [-] epoch  215/250, train loss 0.228184 in 0.20s\n",
      " [-] epoch  216/250, train loss 0.212064 in 0.20s\n",
      " [-] epoch  217/250, train loss 0.216566 in 0.21s\n",
      " [-] epoch  218/250, train loss 0.214187 in 0.21s\n",
      " [-] epoch  219/250, train loss 0.220381 in 0.20s\n",
      " [-] epoch  220/250, train loss 0.226419 in 0.22s\n",
      " [-] epoch  221/250, train loss 0.210906 in 0.19s\n",
      " [-] epoch  222/250, train loss 0.209546 in 0.21s\n",
      " [-] epoch  223/250, train loss 0.199316 in 0.20s\n",
      " [-] epoch  224/250, train loss 0.198530 in 0.22s\n",
      " [-] epoch  225/250, train loss 0.222716 in 0.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.202808 in 0.19s\n",
      " [-] epoch  227/250, train loss 0.209161 in 0.22s\n",
      " [-] epoch  228/250, train loss 0.212249 in 0.22s\n",
      " [-] epoch  229/250, train loss 0.214586 in 0.21s\n",
      " [-] epoch  230/250, train loss 0.209930 in 0.21s\n",
      " [-] epoch  231/250, train loss 0.212893 in 0.20s\n",
      " [-] epoch  232/250, train loss 0.198827 in 0.19s\n",
      " [-] epoch  233/250, train loss 0.208853 in 0.22s\n",
      " [-] epoch  234/250, train loss 0.210772 in 0.20s\n",
      " [-] epoch  235/250, train loss 0.217120 in 0.20s\n",
      " [-] epoch  236/250, train loss 0.217848 in 0.22s\n",
      " [-] epoch  237/250, train loss 0.208374 in 0.21s\n",
      " [-] epoch  238/250, train loss 0.213925 in 0.21s\n",
      " [-] epoch  239/250, train loss 0.214252 in 0.20s\n",
      " [-] epoch  240/250, train loss 0.208597 in 0.22s\n",
      " [-] epoch  241/250, train loss 0.200781 in 0.19s\n",
      " [-] epoch  242/250, train loss 0.202604 in 0.21s\n",
      " [-] epoch  243/250, train loss 0.219412 in 0.20s\n",
      " [-] epoch  244/250, train loss 0.201550 in 0.22s\n",
      " [-] epoch  245/250, train loss 0.243404 in 0.20s\n",
      " [-] epoch  246/250, train loss 0.202552 in 0.21s\n",
      " [-] epoch  247/250, train loss 0.225751 in 0.21s\n",
      " [-] epoch  248/250, train loss 0.209562 in 0.20s\n",
      " [-] epoch  249/250, train loss 0.198260 in 0.21s\n",
      " [-] epoch  250/250, train loss 0.216227 in 0.21s\n",
      " [-] test acc. 81.111111%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.559365 in 0.30s\n",
      " [-] epoch    2/250, train loss 0.474303 in 0.30s\n",
      " [-] epoch    3/250, train loss 0.449787 in 0.30s\n",
      " [-] epoch    4/250, train loss 0.406066 in 0.29s\n",
      " [-] epoch    5/250, train loss 0.397710 in 0.28s\n",
      " [-] epoch    6/250, train loss 0.366343 in 0.29s\n",
      " [-] epoch    7/250, train loss 0.360887 in 0.31s\n",
      " [-] epoch    8/250, train loss 0.353513 in 0.32s\n",
      " [-] epoch    9/250, train loss 0.368263 in 0.32s\n",
      " [-] epoch   10/250, train loss 0.322781 in 0.30s\n",
      " [-] epoch   11/250, train loss 0.331909 in 0.30s\n",
      " [-] epoch   12/250, train loss 0.345786 in 0.32s\n",
      " [-] epoch   13/250, train loss 0.336821 in 0.29s\n",
      " [-] epoch   14/250, train loss 0.320412 in 0.34s\n",
      " [-] epoch   15/250, train loss 0.342665 in 0.29s\n",
      " [-] epoch   16/250, train loss 0.355893 in 0.30s\n",
      " [-] epoch   17/250, train loss 0.310402 in 0.31s\n",
      " [-] epoch   18/250, train loss 0.338297 in 0.32s\n",
      " [-] epoch   19/250, train loss 0.323058 in 0.32s\n",
      " [-] epoch   20/250, train loss 0.306038 in 0.29s\n",
      " [-] epoch   21/250, train loss 0.300765 in 0.28s\n",
      " [-] epoch   22/250, train loss 0.298256 in 0.32s\n",
      " [-] epoch   23/250, train loss 0.294804 in 0.31s\n",
      " [-] epoch   24/250, train loss 0.268979 in 0.29s\n",
      " [-] epoch   25/250, train loss 0.291884 in 0.32s\n",
      " [-] epoch   26/250, train loss 0.322528 in 0.31s\n",
      " [-] epoch   27/250, train loss 0.275973 in 0.29s\n",
      " [-] epoch   28/250, train loss 0.313792 in 0.30s\n",
      " [-] epoch   29/250, train loss 0.293669 in 0.31s\n",
      " [-] epoch   30/250, train loss 0.290170 in 0.30s\n",
      " [-] epoch   31/250, train loss 0.320042 in 0.30s\n",
      " [-] epoch   32/250, train loss 0.302409 in 0.29s\n",
      " [-] epoch   33/250, train loss 0.310814 in 0.30s\n",
      " [-] epoch   34/250, train loss 0.271252 in 0.30s\n",
      " [-] epoch   35/250, train loss 0.291185 in 0.32s\n",
      " [-] epoch   36/250, train loss 0.281220 in 0.31s\n",
      " [-] epoch   37/250, train loss 0.291094 in 0.29s\n",
      " [-] epoch   38/250, train loss 0.266366 in 0.35s\n",
      " [-] epoch   39/250, train loss 0.261049 in 0.31s\n",
      " [-] epoch   40/250, train loss 0.279213 in 0.31s\n",
      " [-] epoch   41/250, train loss 0.277932 in 0.30s\n",
      " [-] epoch   42/250, train loss 0.303160 in 0.31s\n",
      " [-] epoch   43/250, train loss 0.259338 in 0.30s\n",
      " [-] epoch   44/250, train loss 0.263279 in 0.29s\n",
      " [-] epoch   45/250, train loss 0.263264 in 0.29s\n",
      " [-] epoch   46/250, train loss 0.288649 in 0.29s\n",
      " [-] epoch   47/250, train loss 0.292007 in 0.33s\n",
      " [-] epoch   48/250, train loss 0.273959 in 0.31s\n",
      " [-] epoch   49/250, train loss 0.283108 in 0.30s\n",
      " [-] epoch   50/250, train loss 0.262154 in 0.30s\n",
      " [-] epoch   51/250, train loss 0.241147 in 0.31s\n",
      " [-] epoch   52/250, train loss 0.259903 in 0.30s\n",
      " [-] epoch   53/250, train loss 0.269300 in 0.31s\n",
      " [-] epoch   54/250, train loss 0.261374 in 0.31s\n",
      " [-] epoch   55/250, train loss 0.260212 in 0.31s\n",
      " [-] epoch   56/250, train loss 0.264297 in 0.31s\n",
      " [-] epoch   57/250, train loss 0.264470 in 0.32s\n",
      " [-] epoch   58/250, train loss 0.261110 in 0.30s\n",
      " [-] epoch   59/250, train loss 0.276469 in 0.31s\n",
      " [-] epoch   60/250, train loss 0.251565 in 0.29s\n",
      " [-] epoch   61/250, train loss 0.250381 in 0.27s\n",
      " [-] epoch   62/250, train loss 0.261358 in 0.31s\n",
      " [-] epoch   63/250, train loss 0.262186 in 0.31s\n",
      " [-] epoch   64/250, train loss 0.270290 in 0.33s\n",
      " [-] epoch   65/250, train loss 0.234046 in 0.31s\n",
      " [-] epoch   66/250, train loss 0.251998 in 0.33s\n",
      " [-] epoch   67/250, train loss 0.245488 in 0.32s\n",
      " [-] epoch   68/250, train loss 0.249951 in 0.32s\n",
      " [-] epoch   69/250, train loss 0.239619 in 0.31s\n",
      " [-] epoch   70/250, train loss 0.258894 in 0.31s\n",
      " [-] epoch   71/250, train loss 0.226679 in 0.31s\n",
      " [-] epoch   72/250, train loss 0.273140 in 0.30s\n",
      " [-] epoch   73/250, train loss 0.267879 in 0.30s\n",
      " [-] epoch   74/250, train loss 0.240409 in 0.31s\n",
      " [-] epoch   75/250, train loss 0.240212 in 0.29s\n",
      " [-] epoch   76/250, train loss 0.240801 in 0.33s\n",
      " [-] epoch   77/250, train loss 0.242408 in 0.32s\n",
      " [-] epoch   78/250, train loss 0.252674 in 0.30s\n",
      " [-] epoch   79/250, train loss 0.234116 in 0.30s\n",
      " [-] epoch   80/250, train loss 0.226366 in 0.31s\n",
      " [-] epoch   81/250, train loss 0.254069 in 0.31s\n",
      " [-] epoch   82/250, train loss 0.253929 in 0.29s\n",
      " [-] epoch   83/250, train loss 0.241328 in 0.32s\n",
      " [-] epoch   84/250, train loss 0.235021 in 0.31s\n",
      " [-] epoch   85/250, train loss 0.246328 in 0.30s\n",
      " [-] epoch   86/250, train loss 0.261377 in 0.32s\n",
      " [-] epoch   87/250, train loss 0.211541 in 0.31s\n",
      " [-] epoch   88/250, train loss 0.236196 in 0.32s\n",
      " [-] epoch   89/250, train loss 0.238637 in 0.29s\n",
      " [-] epoch   90/250, train loss 0.227984 in 0.30s\n",
      " [-] epoch   91/250, train loss 0.250745 in 0.28s\n",
      " [-] epoch   92/250, train loss 0.261745 in 0.34s\n",
      " [-] epoch   93/250, train loss 0.243565 in 0.29s\n",
      " [-] epoch   94/250, train loss 0.235681 in 0.31s\n",
      " [-] epoch   95/250, train loss 0.247769 in 0.31s\n",
      " [-] epoch   96/250, train loss 0.257544 in 0.28s\n",
      " [-] epoch   97/250, train loss 0.245783 in 0.29s\n",
      " [-] epoch   98/250, train loss 0.230524 in 0.34s\n",
      " [-] epoch   99/250, train loss 0.249954 in 0.30s\n",
      " [-] epoch  100/250, train loss 0.224078 in 0.32s\n",
      " [-] epoch  101/250, train loss 0.230256 in 0.32s\n",
      " [-] epoch  102/250, train loss 0.217713 in 0.30s\n",
      " [-] epoch  103/250, train loss 0.234390 in 0.31s\n",
      " [-] epoch  104/250, train loss 0.223650 in 0.31s\n",
      " [-] epoch  105/250, train loss 0.253263 in 0.32s\n",
      " [-] epoch  106/250, train loss 0.239015 in 0.31s\n",
      " [-] epoch  107/250, train loss 0.237491 in 0.32s\n",
      " [-] epoch  108/250, train loss 0.235398 in 0.32s\n",
      " [-] epoch  109/250, train loss 0.241581 in 0.32s\n",
      " [-] epoch  110/250, train loss 0.236286 in 0.32s\n",
      " [-] epoch  111/250, train loss 0.226700 in 0.31s\n",
      " [-] epoch  112/250, train loss 0.230768 in 0.28s\n",
      " [-] epoch  113/250, train loss 0.215925 in 0.31s\n",
      " [-] epoch  114/250, train loss 0.205119 in 0.31s\n",
      " [-] epoch  115/250, train loss 0.223898 in 0.33s\n",
      " [-] epoch  116/250, train loss 0.220197 in 0.31s\n",
      " [-] epoch  117/250, train loss 0.219428 in 0.31s\n",
      " [-] epoch  118/250, train loss 0.221842 in 0.30s\n",
      " [-] epoch  119/250, train loss 0.235064 in 0.31s\n",
      " [-] epoch  120/250, train loss 0.217315 in 0.31s\n",
      " [-] epoch  121/250, train loss 0.220712 in 0.31s\n",
      " [-] epoch  122/250, train loss 0.233920 in 0.31s\n",
      " [-] epoch  123/250, train loss 0.212119 in 0.33s\n",
      " [-] epoch  124/250, train loss 0.226997 in 0.31s\n",
      " [-] epoch  125/250, train loss 0.237278 in 0.33s\n",
      " [-] epoch  126/250, train loss 0.233133 in 0.30s\n",
      " [-] epoch  127/250, train loss 0.233827 in 0.31s\n",
      " [-] epoch  128/250, train loss 0.220809 in 0.30s\n",
      " [-] epoch  129/250, train loss 0.226778 in 0.30s\n",
      " [-] epoch  130/250, train loss 0.215021 in 0.29s\n",
      " [-] epoch  131/250, train loss 0.224337 in 0.32s\n",
      " [-] epoch  132/250, train loss 0.219975 in 0.29s\n",
      " [-] epoch  133/250, train loss 0.207293 in 0.32s\n",
      " [-] epoch  134/250, train loss 0.211402 in 0.30s\n",
      " [-] epoch  135/250, train loss 0.215959 in 0.31s\n",
      " [-] epoch  136/250, train loss 0.254175 in 0.31s\n",
      " [-] epoch  137/250, train loss 0.216193 in 0.32s\n",
      " [-] epoch  138/250, train loss 0.225178 in 0.30s\n",
      " [-] epoch  139/250, train loss 0.238280 in 0.30s\n",
      " [-] epoch  140/250, train loss 0.205123 in 0.31s\n",
      " [-] epoch  141/250, train loss 0.215887 in 0.30s\n",
      " [-] epoch  142/250, train loss 0.217657 in 0.30s\n",
      " [-] epoch  143/250, train loss 0.235512 in 0.32s\n",
      " [-] epoch  144/250, train loss 0.227026 in 0.28s\n",
      " [-] epoch  145/250, train loss 0.205447 in 0.28s\n",
      " [-] epoch  146/250, train loss 0.212684 in 0.32s\n",
      " [-] epoch  147/250, train loss 0.245781 in 0.31s\n",
      " [-] epoch  148/250, train loss 0.216611 in 0.32s\n",
      " [-] epoch  149/250, train loss 0.212425 in 0.31s\n",
      " [-] epoch  150/250, train loss 0.212128 in 0.30s\n",
      " [-] epoch  151/250, train loss 0.227811 in 0.30s\n",
      " [-] epoch  152/250, train loss 0.213126 in 0.31s\n",
      " [-] epoch  153/250, train loss 0.236706 in 0.33s\n",
      " [-] epoch  154/250, train loss 0.216155 in 0.30s\n",
      " [-] epoch  155/250, train loss 0.221536 in 0.31s\n",
      " [-] epoch  156/250, train loss 0.206334 in 0.28s\n",
      " [-] epoch  157/250, train loss 0.195101 in 0.30s\n",
      " [-] epoch  158/250, train loss 0.204277 in 0.32s\n",
      " [-] epoch  159/250, train loss 0.201154 in 0.30s\n",
      " [-] epoch  160/250, train loss 0.211628 in 0.30s\n",
      " [-] epoch  161/250, train loss 0.218148 in 0.31s\n",
      " [-] epoch  162/250, train loss 0.224066 in 0.29s\n",
      " [-] epoch  163/250, train loss 0.236891 in 0.31s\n",
      " [-] epoch  164/250, train loss 0.221148 in 0.29s\n",
      " [-] epoch  165/250, train loss 0.207004 in 0.31s\n",
      " [-] epoch  166/250, train loss 0.214705 in 0.29s\n",
      " [-] epoch  167/250, train loss 0.211414 in 0.33s\n",
      " [-] epoch  168/250, train loss 0.215535 in 0.31s\n",
      " [-] epoch  169/250, train loss 0.209496 in 0.33s\n",
      " [-] epoch  170/250, train loss 0.198050 in 0.30s\n",
      " [-] epoch  171/250, train loss 0.205008 in 0.29s\n",
      " [-] epoch  172/250, train loss 0.205621 in 0.31s\n",
      " [-] epoch  173/250, train loss 0.202651 in 0.33s\n",
      " [-] epoch  174/250, train loss 0.208220 in 0.32s\n",
      " [-] epoch  175/250, train loss 0.214223 in 0.30s\n",
      " [-] epoch  176/250, train loss 0.217584 in 0.30s\n",
      " [-] epoch  177/250, train loss 0.216680 in 0.29s\n",
      " [-] epoch  178/250, train loss 0.214155 in 0.31s\n",
      " [-] epoch  179/250, train loss 0.193137 in 0.30s\n",
      " [-] epoch  180/250, train loss 0.202276 in 0.32s\n",
      " [-] epoch  181/250, train loss 0.217257 in 0.30s\n",
      " [-] epoch  182/250, train loss 0.222422 in 0.31s\n",
      " [-] epoch  183/250, train loss 0.207103 in 0.31s\n",
      " [-] epoch  184/250, train loss 0.208362 in 0.30s\n",
      " [-] epoch  185/250, train loss 0.207237 in 0.31s\n",
      " [-] epoch  186/250, train loss 0.194299 in 0.30s\n",
      " [-] epoch  187/250, train loss 0.209790 in 0.29s\n",
      " [-] epoch  188/250, train loss 0.202818 in 0.31s\n",
      " [-] epoch  189/250, train loss 0.225664 in 0.30s\n",
      " [-] epoch  190/250, train loss 0.204240 in 0.31s\n",
      " [-] epoch  191/250, train loss 0.197380 in 0.32s\n",
      " [-] epoch  192/250, train loss 0.204932 in 0.33s\n",
      " [-] epoch  193/250, train loss 0.204571 in 0.30s\n",
      " [-] epoch  194/250, train loss 0.221280 in 0.32s\n",
      " [-] epoch  195/250, train loss 0.201124 in 0.28s\n",
      " [-] epoch  196/250, train loss 0.202120 in 0.30s\n",
      " [-] epoch  197/250, train loss 0.206508 in 0.30s\n",
      " [-] epoch  198/250, train loss 0.200598 in 0.31s\n",
      " [-] epoch  199/250, train loss 0.204162 in 0.31s\n",
      " [-] epoch  200/250, train loss 0.207757 in 0.31s\n",
      " [-] epoch  201/250, train loss 0.224401 in 0.30s\n",
      " [-] epoch  202/250, train loss 0.191727 in 0.32s\n",
      " [-] epoch  203/250, train loss 0.198030 in 0.32s\n",
      " [-] epoch  204/250, train loss 0.194447 in 0.30s\n",
      " [-] epoch  205/250, train loss 0.200706 in 0.27s\n",
      " [-] epoch  206/250, train loss 0.216124 in 0.30s\n",
      " [-] epoch  207/250, train loss 0.204666 in 0.31s\n",
      " [-] epoch  208/250, train loss 0.202701 in 0.31s\n",
      " [-] epoch  209/250, train loss 0.216062 in 0.31s\n",
      " [-] epoch  210/250, train loss 0.211360 in 0.32s\n",
      " [-] epoch  211/250, train loss 0.198946 in 0.31s\n",
      " [-] epoch  212/250, train loss 0.191063 in 0.31s\n",
      " [-] epoch  213/250, train loss 0.202620 in 0.32s\n",
      " [-] epoch  214/250, train loss 0.226943 in 0.31s\n",
      " [-] epoch  215/250, train loss 0.223208 in 0.31s\n",
      " [-] epoch  216/250, train loss 0.203237 in 0.32s\n",
      " [-] epoch  217/250, train loss 0.191074 in 0.32s\n",
      " [-] epoch  218/250, train loss 0.201627 in 0.31s\n",
      " [-] epoch  219/250, train loss 0.194927 in 0.28s\n",
      " [-] epoch  220/250, train loss 0.203719 in 0.31s\n",
      " [-] epoch  221/250, train loss 0.201420 in 0.31s\n",
      " [-] epoch  222/250, train loss 0.202563 in 0.30s\n",
      " [-] epoch  223/250, train loss 0.191297 in 0.33s\n",
      " [-] epoch  224/250, train loss 0.203249 in 0.28s\n",
      " [-] epoch  225/250, train loss 0.184479 in 0.32s\n",
      " [-] epoch  226/250, train loss 0.197731 in 0.30s\n",
      " [-] epoch  227/250, train loss 0.196310 in 0.30s\n",
      " [-] epoch  228/250, train loss 0.202073 in 0.30s\n",
      " [-] epoch  229/250, train loss 0.184929 in 0.30s\n",
      " [-] epoch  230/250, train loss 0.189830 in 0.29s\n",
      " [-] epoch  231/250, train loss 0.192879 in 0.31s\n",
      " [-] epoch  232/250, train loss 0.213231 in 0.31s\n",
      " [-] epoch  233/250, train loss 0.205662 in 0.30s\n",
      " [-] epoch  234/250, train loss 0.203057 in 0.30s\n",
      " [-] epoch  235/250, train loss 0.202783 in 0.31s\n",
      " [-] epoch  236/250, train loss 0.195748 in 0.31s\n",
      " [-] epoch  237/250, train loss 0.212649 in 0.33s\n",
      " [-] epoch  238/250, train loss 0.196296 in 0.30s\n",
      " [-] epoch  239/250, train loss 0.185566 in 0.30s\n",
      " [-] epoch  240/250, train loss 0.198370 in 0.32s\n",
      " [-] epoch  241/250, train loss 0.194606 in 0.31s\n",
      " [-] epoch  242/250, train loss 0.203354 in 0.29s\n",
      " [-] epoch  243/250, train loss 0.236682 in 0.30s\n",
      " [-] epoch  244/250, train loss 0.195741 in 0.30s\n",
      " [-] epoch  245/250, train loss 0.196701 in 0.31s\n",
      " [-] epoch  246/250, train loss 0.203554 in 0.31s\n",
      " [-] epoch  247/250, train loss 0.202776 in 0.31s\n",
      " [-] epoch  248/250, train loss 0.184484 in 0.31s\n",
      " [-] epoch  249/250, train loss 0.206273 in 0.31s\n",
      " [-] epoch  250/250, train loss 0.187154 in 0.34s\n",
      " [-] test acc. 80.833333%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.606700 in 0.40s\n",
      " [-] epoch    2/250, train loss 0.453514 in 0.41s\n",
      " [-] epoch    3/250, train loss 0.460443 in 0.42s\n",
      " [-] epoch    4/250, train loss 0.387659 in 0.42s\n",
      " [-] epoch    5/250, train loss 0.382538 in 0.41s\n",
      " [-] epoch    6/250, train loss 0.368306 in 0.42s\n",
      " [-] epoch    7/250, train loss 0.385961 in 0.41s\n",
      " [-] epoch    8/250, train loss 0.379175 in 0.40s\n",
      " [-] epoch    9/250, train loss 0.361280 in 0.40s\n",
      " [-] epoch   10/250, train loss 0.345835 in 0.42s\n",
      " [-] epoch   11/250, train loss 0.345948 in 0.39s\n",
      " [-] epoch   12/250, train loss 0.353230 in 0.37s\n",
      " [-] epoch   13/250, train loss 0.328064 in 0.40s\n",
      " [-] epoch   14/250, train loss 0.334069 in 0.38s\n",
      " [-] epoch   15/250, train loss 0.335272 in 0.40s\n",
      " [-] epoch   16/250, train loss 0.318465 in 0.41s\n",
      " [-] epoch   17/250, train loss 0.349641 in 0.42s\n",
      " [-] epoch   18/250, train loss 0.322776 in 0.41s\n",
      " [-] epoch   19/250, train loss 0.311687 in 0.41s\n",
      " [-] epoch   20/250, train loss 0.331588 in 0.37s\n",
      " [-] epoch   21/250, train loss 0.331169 in 0.40s\n",
      " [-] epoch   22/250, train loss 0.295651 in 0.38s\n",
      " [-] epoch   23/250, train loss 0.341105 in 0.41s\n",
      " [-] epoch   24/250, train loss 0.283923 in 0.41s\n",
      " [-] epoch   25/250, train loss 0.303558 in 0.38s\n",
      " [-] epoch   26/250, train loss 0.291925 in 0.40s\n",
      " [-] epoch   27/250, train loss 0.312684 in 0.40s\n",
      " [-] epoch   28/250, train loss 0.313453 in 0.39s\n",
      " [-] epoch   29/250, train loss 0.287105 in 0.41s\n",
      " [-] epoch   30/250, train loss 0.312255 in 0.40s\n",
      " [-] epoch   31/250, train loss 0.309914 in 0.37s\n",
      " [-] epoch   32/250, train loss 0.287069 in 0.43s\n",
      " [-] epoch   33/250, train loss 0.283529 in 0.41s\n",
      " [-] epoch   34/250, train loss 0.277173 in 0.39s\n",
      " [-] epoch   35/250, train loss 0.274375 in 0.39s\n",
      " [-] epoch   36/250, train loss 0.291746 in 0.38s\n",
      " [-] epoch   37/250, train loss 0.294398 in 0.38s\n",
      " [-] epoch   38/250, train loss 0.301046 in 0.41s\n",
      " [-] epoch   39/250, train loss 0.266659 in 0.39s\n",
      " [-] epoch   40/250, train loss 0.260513 in 0.39s\n",
      " [-] epoch   41/250, train loss 0.292940 in 0.39s\n",
      " [-] epoch   42/250, train loss 0.261621 in 0.40s\n",
      " [-] epoch   43/250, train loss 0.305818 in 0.41s\n",
      " [-] epoch   44/250, train loss 0.285822 in 0.39s\n",
      " [-] epoch   45/250, train loss 0.262707 in 0.39s\n",
      " [-] epoch   46/250, train loss 0.275028 in 0.37s\n",
      " [-] epoch   47/250, train loss 0.282441 in 0.40s\n",
      " [-] epoch   48/250, train loss 0.271743 in 0.40s\n",
      " [-] epoch   49/250, train loss 0.268962 in 0.40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.271757 in 0.41s\n",
      " [-] epoch   51/250, train loss 0.308569 in 0.43s\n",
      " [-] epoch   52/250, train loss 0.268918 in 0.43s\n",
      " [-] epoch   53/250, train loss 0.272214 in 0.40s\n",
      " [-] epoch   54/250, train loss 0.263229 in 0.41s\n",
      " [-] epoch   55/250, train loss 0.256736 in 0.41s\n",
      " [-] epoch   56/250, train loss 0.253083 in 0.38s\n",
      " [-] epoch   57/250, train loss 0.247069 in 0.42s\n",
      " [-] epoch   58/250, train loss 0.265275 in 0.44s\n",
      " [-] epoch   59/250, train loss 0.270713 in 0.38s\n",
      " [-] epoch   60/250, train loss 0.276519 in 0.40s\n",
      " [-] epoch   61/250, train loss 0.248532 in 0.41s\n",
      " [-] epoch   62/250, train loss 0.247417 in 0.42s\n",
      " [-] epoch   63/250, train loss 0.280878 in 0.40s\n",
      " [-] epoch   64/250, train loss 0.253168 in 0.44s\n",
      " [-] epoch   65/250, train loss 0.256112 in 0.42s\n",
      " [-] epoch   66/250, train loss 0.254210 in 0.40s\n",
      " [-] epoch   67/250, train loss 0.247717 in 0.41s\n",
      " [-] epoch   68/250, train loss 0.259737 in 0.38s\n",
      " [-] epoch   69/250, train loss 0.243274 in 0.40s\n",
      " [-] epoch   70/250, train loss 0.232747 in 0.44s\n",
      " [-] epoch   71/250, train loss 0.245174 in 0.39s\n",
      " [-] epoch   72/250, train loss 0.264671 in 0.40s\n",
      " [-] epoch   73/250, train loss 0.226685 in 0.41s\n",
      " [-] epoch   74/250, train loss 0.280451 in 0.40s\n",
      " [-] epoch   75/250, train loss 0.261491 in 0.39s\n",
      " [-] epoch   76/250, train loss 0.230690 in 0.40s\n",
      " [-] epoch   77/250, train loss 0.234880 in 0.41s\n",
      " [-] epoch   78/250, train loss 0.237076 in 0.39s\n",
      " [-] epoch   79/250, train loss 0.263785 in 0.38s\n",
      " [-] epoch   80/250, train loss 0.263984 in 0.41s\n",
      " [-] epoch   81/250, train loss 0.256483 in 0.42s\n",
      " [-] epoch   82/250, train loss 0.259302 in 0.41s\n",
      " [-] epoch   83/250, train loss 0.248889 in 0.40s\n",
      " [-] epoch   84/250, train loss 0.229319 in 0.42s\n",
      " [-] epoch   85/250, train loss 0.250740 in 0.42s\n",
      " [-] epoch   86/250, train loss 0.237367 in 0.42s\n",
      " [-] epoch   87/250, train loss 0.238393 in 0.40s\n",
      " [-] epoch   88/250, train loss 0.231575 in 0.42s\n",
      " [-] epoch   89/250, train loss 0.239834 in 0.39s\n",
      " [-] epoch   90/250, train loss 0.245359 in 0.38s\n",
      " [-] epoch   91/250, train loss 0.237799 in 0.40s\n",
      " [-] epoch   92/250, train loss 0.227273 in 0.39s\n",
      " [-] epoch   93/250, train loss 0.245116 in 0.40s\n",
      " [-] epoch   94/250, train loss 0.255412 in 0.35s\n",
      " [-] epoch   95/250, train loss 0.263461 in 0.38s\n",
      " [-] epoch   96/250, train loss 0.236792 in 0.39s\n",
      " [-] epoch   97/250, train loss 0.225396 in 0.37s\n",
      " [-] epoch   98/250, train loss 0.238571 in 0.41s\n",
      " [-] epoch   99/250, train loss 0.257403 in 0.40s\n",
      " [-] epoch  100/250, train loss 0.240021 in 0.40s\n",
      " [-] epoch  101/250, train loss 0.238627 in 0.39s\n",
      " [-] epoch  102/250, train loss 0.239762 in 0.40s\n",
      " [-] epoch  103/250, train loss 0.240352 in 0.41s\n",
      " [-] epoch  104/250, train loss 0.232972 in 0.39s\n",
      " [-] epoch  105/250, train loss 0.256917 in 0.40s\n",
      " [-] epoch  106/250, train loss 0.242607 in 0.40s\n",
      " [-] epoch  107/250, train loss 0.253125 in 0.38s\n",
      " [-] epoch  108/250, train loss 0.239590 in 0.40s\n",
      " [-] epoch  109/250, train loss 0.230715 in 0.40s\n",
      " [-] epoch  110/250, train loss 0.251490 in 0.40s\n",
      " [-] epoch  111/250, train loss 0.218639 in 0.40s\n",
      " [-] epoch  112/250, train loss 0.235512 in 0.38s\n",
      " [-] epoch  113/250, train loss 0.219310 in 0.38s\n",
      " [-] epoch  114/250, train loss 0.218812 in 0.39s\n",
      " [-] epoch  115/250, train loss 0.217758 in 0.40s\n",
      " [-] epoch  116/250, train loss 0.230783 in 0.39s\n",
      " [-] epoch  117/250, train loss 0.214615 in 0.40s\n",
      " [-] epoch  118/250, train loss 0.231715 in 0.38s\n",
      " [-] epoch  119/250, train loss 0.194774 in 0.38s\n",
      " [-] epoch  120/250, train loss 0.228888 in 0.41s\n",
      " [-] epoch  121/250, train loss 0.238188 in 0.40s\n",
      " [-] epoch  122/250, train loss 0.224556 in 0.40s\n",
      " [-] epoch  123/250, train loss 0.260977 in 0.41s\n",
      " [-] epoch  124/250, train loss 0.209321 in 0.41s\n",
      " [-] epoch  125/250, train loss 0.216028 in 0.40s\n",
      " [-] epoch  126/250, train loss 0.220493 in 0.42s\n",
      " [-] epoch  127/250, train loss 0.216323 in 0.44s\n",
      " [-] epoch  128/250, train loss 0.205393 in 0.42s\n",
      " [-] epoch  129/250, train loss 0.217060 in 0.44s\n",
      " [-] epoch  130/250, train loss 0.220411 in 0.38s\n",
      " [-] epoch  131/250, train loss 0.208253 in 0.39s\n",
      " [-] epoch  132/250, train loss 0.238756 in 0.39s\n",
      " [-] epoch  133/250, train loss 0.219243 in 0.40s\n",
      " [-] epoch  134/250, train loss 0.215019 in 0.39s\n",
      " [-] epoch  135/250, train loss 0.236422 in 0.37s\n",
      " [-] epoch  136/250, train loss 0.228433 in 0.41s\n",
      " [-] epoch  137/250, train loss 0.246448 in 0.43s\n",
      " [-] epoch  138/250, train loss 0.213095 in 0.42s\n",
      " [-] epoch  139/250, train loss 0.223189 in 0.41s\n",
      " [-] epoch  140/250, train loss 0.222444 in 0.41s\n",
      " [-] epoch  141/250, train loss 0.225862 in 0.39s\n",
      " [-] epoch  142/250, train loss 0.243187 in 0.41s\n",
      " [-] epoch  143/250, train loss 0.222916 in 0.39s\n",
      " [-] epoch  144/250, train loss 0.246214 in 0.40s\n",
      " [-] epoch  145/250, train loss 0.219005 in 0.39s\n",
      " [-] epoch  146/250, train loss 0.225158 in 0.43s\n",
      " [-] epoch  147/250, train loss 0.222127 in 0.41s\n",
      " [-] epoch  148/250, train loss 0.231210 in 0.41s\n",
      " [-] epoch  149/250, train loss 0.206749 in 0.39s\n",
      " [-] epoch  150/250, train loss 0.212790 in 0.40s\n",
      " [-] epoch  151/250, train loss 0.234194 in 0.42s\n",
      " [-] epoch  152/250, train loss 0.231821 in 0.41s\n",
      " [-] epoch  153/250, train loss 0.203613 in 0.42s\n",
      " [-] epoch  154/250, train loss 0.217449 in 0.40s\n",
      " [-] epoch  155/250, train loss 0.232436 in 0.40s\n",
      " [-] epoch  156/250, train loss 0.210756 in 0.42s\n",
      " [-] epoch  157/250, train loss 0.228298 in 0.42s\n",
      " [-] epoch  158/250, train loss 0.209545 in 0.41s\n",
      " [-] epoch  159/250, train loss 0.213645 in 0.42s\n",
      " [-] epoch  160/250, train loss 0.217901 in 0.42s\n",
      " [-] epoch  161/250, train loss 0.220076 in 0.40s\n",
      " [-] epoch  162/250, train loss 0.221281 in 0.41s\n",
      " [-] epoch  163/250, train loss 0.227406 in 0.41s\n",
      " [-] epoch  164/250, train loss 0.210814 in 0.38s\n",
      " [-] epoch  165/250, train loss 0.227018 in 0.37s\n",
      " [-] epoch  166/250, train loss 0.226211 in 0.41s\n",
      " [-] epoch  167/250, train loss 0.215977 in 0.38s\n",
      " [-] epoch  168/250, train loss 0.202118 in 0.39s\n",
      " [-] epoch  169/250, train loss 0.213054 in 0.43s\n",
      " [-] epoch  170/250, train loss 0.213283 in 0.40s\n",
      " [-] epoch  171/250, train loss 0.211885 in 0.42s\n",
      " [-] epoch  172/250, train loss 0.213308 in 0.41s\n",
      " [-] epoch  173/250, train loss 0.213610 in 0.41s\n",
      " [-] epoch  174/250, train loss 0.220448 in 0.39s\n",
      " [-] epoch  175/250, train loss 0.216171 in 0.41s\n",
      " [-] epoch  176/250, train loss 0.218126 in 0.40s\n",
      " [-] epoch  177/250, train loss 0.215108 in 0.40s\n",
      " [-] epoch  178/250, train loss 0.234245 in 0.39s\n",
      " [-] epoch  179/250, train loss 0.219616 in 0.42s\n",
      " [-] epoch  180/250, train loss 0.228013 in 0.42s\n",
      " [-] epoch  181/250, train loss 0.208991 in 0.40s\n",
      " [-] epoch  182/250, train loss 0.206174 in 0.39s\n",
      " [-] epoch  183/250, train loss 0.225221 in 0.38s\n",
      " [-] epoch  184/250, train loss 0.199344 in 0.43s\n",
      " [-] epoch  185/250, train loss 0.209099 in 0.37s\n",
      " [-] epoch  186/250, train loss 0.219488 in 0.39s\n",
      " [-] epoch  187/250, train loss 0.207243 in 0.39s\n",
      " [-] epoch  188/250, train loss 0.222200 in 0.39s\n",
      " [-] epoch  189/250, train loss 0.234067 in 0.40s\n",
      " [-] epoch  190/250, train loss 0.199049 in 0.39s\n",
      " [-] epoch  191/250, train loss 0.208947 in 0.39s\n",
      " [-] epoch  192/250, train loss 0.213520 in 0.41s\n",
      " [-] epoch  193/250, train loss 0.218820 in 0.39s\n",
      " [-] epoch  194/250, train loss 0.216508 in 0.40s\n",
      " [-] epoch  195/250, train loss 0.207567 in 0.41s\n",
      " [-] epoch  196/250, train loss 0.229386 in 0.43s\n",
      " [-] epoch  197/250, train loss 0.191775 in 0.43s\n",
      " [-] epoch  198/250, train loss 0.224843 in 0.40s\n",
      " [-] epoch  199/250, train loss 0.208162 in 0.41s\n",
      " [-] epoch  200/250, train loss 0.235003 in 0.40s\n",
      " [-] epoch  201/250, train loss 0.209682 in 0.39s\n",
      " [-] epoch  202/250, train loss 0.213581 in 0.40s\n",
      " [-] epoch  203/250, train loss 0.243684 in 0.41s\n",
      " [-] epoch  204/250, train loss 0.214649 in 0.41s\n",
      " [-] epoch  205/250, train loss 0.229872 in 0.40s\n",
      " [-] epoch  206/250, train loss 0.213881 in 0.40s\n",
      " [-] epoch  207/250, train loss 0.195453 in 0.40s\n",
      " [-] epoch  208/250, train loss 0.202909 in 0.40s\n",
      " [-] epoch  209/250, train loss 0.224735 in 0.40s\n",
      " [-] epoch  210/250, train loss 0.198440 in 0.40s\n",
      " [-] epoch  211/250, train loss 0.197920 in 0.40s\n",
      " [-] epoch  212/250, train loss 0.209454 in 0.40s\n",
      " [-] epoch  213/250, train loss 0.209138 in 0.40s\n",
      " [-] epoch  214/250, train loss 0.206808 in 0.40s\n",
      " [-] epoch  215/250, train loss 0.216036 in 0.39s\n",
      " [-] epoch  216/250, train loss 0.201850 in 0.39s\n",
      " [-] epoch  217/250, train loss 0.190416 in 0.42s\n",
      " [-] epoch  218/250, train loss 0.210053 in 0.40s\n",
      " [-] epoch  219/250, train loss 0.213334 in 0.40s\n",
      " [-] epoch  220/250, train loss 0.210501 in 0.41s\n",
      " [-] epoch  221/250, train loss 0.211895 in 0.41s\n",
      " [-] epoch  222/250, train loss 0.209103 in 0.41s\n",
      " [-] epoch  223/250, train loss 0.192471 in 0.40s\n",
      " [-] epoch  224/250, train loss 0.231747 in 0.41s\n",
      " [-] epoch  225/250, train loss 0.213436 in 0.42s\n",
      " [-] epoch  226/250, train loss 0.213688 in 0.42s\n",
      " [-] epoch  227/250, train loss 0.219267 in 0.42s\n",
      " [-] epoch  228/250, train loss 0.206710 in 0.40s\n",
      " [-] epoch  229/250, train loss 0.225297 in 0.39s\n",
      " [-] epoch  230/250, train loss 0.204479 in 0.43s\n",
      " [-] epoch  231/250, train loss 0.211790 in 0.41s\n",
      " [-] epoch  232/250, train loss 0.199844 in 0.40s\n",
      " [-] epoch  233/250, train loss 0.229870 in 0.42s\n",
      " [-] epoch  234/250, train loss 0.210197 in 0.41s\n",
      " [-] epoch  235/250, train loss 0.204607 in 0.42s\n",
      " [-] epoch  236/250, train loss 0.190714 in 0.42s\n",
      " [-] epoch  237/250, train loss 0.199299 in 0.40s\n",
      " [-] epoch  238/250, train loss 0.216164 in 0.41s\n",
      " [-] epoch  239/250, train loss 0.220309 in 0.39s\n",
      " [-] epoch  240/250, train loss 0.209018 in 0.40s\n",
      " [-] epoch  241/250, train loss 0.196271 in 0.40s\n",
      " [-] epoch  242/250, train loss 0.190388 in 0.40s\n",
      " [-] epoch  243/250, train loss 0.199562 in 0.40s\n",
      " [-] epoch  244/250, train loss 0.193718 in 0.40s\n",
      " [-] epoch  245/250, train loss 0.207596 in 0.41s\n",
      " [-] epoch  246/250, train loss 0.214986 in 0.41s\n",
      " [-] epoch  247/250, train loss 0.192085 in 0.39s\n",
      " [-] epoch  248/250, train loss 0.203006 in 0.40s\n",
      " [-] epoch  249/250, train loss 0.198135 in 0.39s\n",
      " [-] epoch  250/250, train loss 0.181156 in 0.39s\n",
      " [-] test acc. 84.444444%\n",
      "Je vais utiliser 7 layers\n",
      " [-] epoch    1/250, train loss 0.591937 in 0.48s\n",
      " [-] epoch    2/250, train loss 0.479185 in 0.48s\n",
      " [-] epoch    3/250, train loss 0.420637 in 0.50s\n",
      " [-] epoch    4/250, train loss 0.417657 in 0.50s\n",
      " [-] epoch    5/250, train loss 0.386232 in 0.50s\n",
      " [-] epoch    6/250, train loss 0.377837 in 0.49s\n",
      " [-] epoch    7/250, train loss 0.393516 in 0.49s\n",
      " [-] epoch    8/250, train loss 0.356557 in 0.49s\n",
      " [-] epoch    9/250, train loss 0.334191 in 0.49s\n",
      " [-] epoch   10/250, train loss 0.353813 in 0.51s\n",
      " [-] epoch   11/250, train loss 0.351998 in 0.46s\n",
      " [-] epoch   12/250, train loss 0.370767 in 0.47s\n",
      " [-] epoch   13/250, train loss 0.340188 in 0.49s\n",
      " [-] epoch   14/250, train loss 0.344200 in 0.50s\n",
      " [-] epoch   15/250, train loss 0.329723 in 0.49s\n",
      " [-] epoch   16/250, train loss 0.341649 in 0.47s\n",
      " [-] epoch   17/250, train loss 0.294993 in 0.50s\n",
      " [-] epoch   18/250, train loss 0.351132 in 0.49s\n",
      " [-] epoch   19/250, train loss 0.331827 in 0.52s\n",
      " [-] epoch   20/250, train loss 0.325866 in 0.49s\n",
      " [-] epoch   21/250, train loss 0.324567 in 0.48s\n",
      " [-] epoch   22/250, train loss 0.311482 in 0.49s\n",
      " [-] epoch   23/250, train loss 0.296660 in 0.49s\n",
      " [-] epoch   24/250, train loss 0.338996 in 0.52s\n",
      " [-] epoch   25/250, train loss 0.298048 in 0.50s\n",
      " [-] epoch   26/250, train loss 0.313347 in 0.47s\n",
      " [-] epoch   27/250, train loss 0.321584 in 0.50s\n",
      " [-] epoch   28/250, train loss 0.306441 in 0.51s\n",
      " [-] epoch   29/250, train loss 0.286516 in 0.48s\n",
      " [-] epoch   30/250, train loss 0.303972 in 0.48s\n",
      " [-] epoch   31/250, train loss 0.285196 in 0.52s\n",
      " [-] epoch   32/250, train loss 0.304406 in 0.50s\n",
      " [-] epoch   33/250, train loss 0.302084 in 0.49s\n",
      " [-] epoch   34/250, train loss 0.308401 in 0.48s\n",
      " [-] epoch   35/250, train loss 0.302933 in 0.51s\n",
      " [-] epoch   36/250, train loss 0.301210 in 0.50s\n",
      " [-] epoch   37/250, train loss 0.304352 in 0.50s\n",
      " [-] epoch   38/250, train loss 0.282934 in 0.48s\n",
      " [-] epoch   39/250, train loss 0.283927 in 0.51s\n",
      " [-] epoch   40/250, train loss 0.283809 in 0.49s\n",
      " [-] epoch   41/250, train loss 0.301537 in 0.49s\n",
      " [-] epoch   42/250, train loss 0.296064 in 0.50s\n",
      " [-] epoch   43/250, train loss 0.290242 in 0.49s\n",
      " [-] epoch   44/250, train loss 0.283893 in 0.49s\n",
      " [-] epoch   45/250, train loss 0.286689 in 0.51s\n",
      " [-] epoch   46/250, train loss 0.305901 in 0.49s\n",
      " [-] epoch   47/250, train loss 0.282119 in 0.51s\n",
      " [-] epoch   48/250, train loss 0.279309 in 0.50s\n",
      " [-] epoch   49/250, train loss 0.274727 in 0.50s\n",
      " [-] epoch   50/250, train loss 0.273823 in 0.50s\n",
      " [-] epoch   51/250, train loss 0.301153 in 0.52s\n",
      " [-] epoch   52/250, train loss 0.306574 in 0.51s\n",
      " [-] epoch   53/250, train loss 0.283610 in 0.50s\n",
      " [-] epoch   54/250, train loss 0.248850 in 0.50s\n",
      " [-] epoch   55/250, train loss 0.256658 in 0.49s\n",
      " [-] epoch   56/250, train loss 0.250672 in 0.51s\n",
      " [-] epoch   57/250, train loss 0.286213 in 0.52s\n",
      " [-] epoch   58/250, train loss 0.258981 in 0.47s\n",
      " [-] epoch   59/250, train loss 0.251278 in 0.48s\n",
      " [-] epoch   60/250, train loss 0.258270 in 0.50s\n",
      " [-] epoch   61/250, train loss 0.281365 in 0.47s\n",
      " [-] epoch   62/250, train loss 0.258540 in 0.50s\n",
      " [-] epoch   63/250, train loss 0.289785 in 0.50s\n",
      " [-] epoch   64/250, train loss 0.274304 in 0.50s\n",
      " [-] epoch   65/250, train loss 0.256370 in 0.51s\n",
      " [-] epoch   66/250, train loss 0.242608 in 0.49s\n",
      " [-] epoch   67/250, train loss 0.248337 in 0.47s\n",
      " [-] epoch   68/250, train loss 0.262131 in 0.52s\n",
      " [-] epoch   69/250, train loss 0.264429 in 0.48s\n",
      " [-] epoch   70/250, train loss 0.253881 in 0.50s\n",
      " [-] epoch   71/250, train loss 0.241341 in 0.51s\n",
      " [-] epoch   72/250, train loss 0.259194 in 0.51s\n",
      " [-] epoch   73/250, train loss 0.244279 in 0.49s\n",
      " [-] epoch   74/250, train loss 0.271213 in 0.46s\n",
      " [-] epoch   75/250, train loss 0.246655 in 0.45s\n",
      " [-] epoch   76/250, train loss 0.239493 in 0.46s\n",
      " [-] epoch   77/250, train loss 0.213061 in 0.51s\n",
      " [-] epoch   78/250, train loss 0.235117 in 0.49s\n",
      " [-] epoch   79/250, train loss 0.229936 in 0.47s\n",
      " [-] epoch   80/250, train loss 0.272065 in 0.48s\n",
      " [-] epoch   81/250, train loss 0.240168 in 0.48s\n",
      " [-] epoch   82/250, train loss 0.237052 in 0.50s\n",
      " [-] epoch   83/250, train loss 0.261930 in 0.50s\n",
      " [-] epoch   84/250, train loss 0.249265 in 0.49s\n",
      " [-] epoch   85/250, train loss 0.239507 in 0.48s\n",
      " [-] epoch   86/250, train loss 0.257148 in 0.49s\n",
      " [-] epoch   87/250, train loss 0.257462 in 0.48s\n",
      " [-] epoch   88/250, train loss 0.226485 in 0.51s\n",
      " [-] epoch   89/250, train loss 0.225596 in 0.49s\n",
      " [-] epoch   90/250, train loss 0.231730 in 0.46s\n",
      " [-] epoch   91/250, train loss 0.263489 in 0.50s\n",
      " [-] epoch   92/250, train loss 0.240061 in 0.48s\n",
      " [-] epoch   93/250, train loss 0.235708 in 0.46s\n",
      " [-] epoch   94/250, train loss 0.265646 in 0.49s\n",
      " [-] epoch   95/250, train loss 0.226092 in 0.48s\n",
      " [-] epoch   96/250, train loss 0.255318 in 0.50s\n",
      " [-] epoch   97/250, train loss 0.259138 in 0.49s\n",
      " [-] epoch   98/250, train loss 0.224987 in 0.49s\n",
      " [-] epoch   99/250, train loss 0.224230 in 0.53s\n",
      " [-] epoch  100/250, train loss 0.238999 in 0.49s\n",
      " [-] epoch  101/250, train loss 0.245427 in 0.47s\n",
      " [-] epoch  102/250, train loss 0.234186 in 0.50s\n",
      " [-] epoch  103/250, train loss 0.241411 in 0.48s\n",
      " [-] epoch  104/250, train loss 0.242729 in 0.46s\n",
      " [-] epoch  105/250, train loss 0.253305 in 0.47s\n",
      " [-] epoch  106/250, train loss 0.241339 in 0.48s\n",
      " [-] epoch  107/250, train loss 0.251111 in 0.47s\n",
      " [-] epoch  108/250, train loss 0.226365 in 0.50s\n",
      " [-] epoch  109/250, train loss 0.224902 in 0.48s\n",
      " [-] epoch  110/250, train loss 0.225639 in 0.51s\n",
      " [-] epoch  111/250, train loss 0.232258 in 0.49s\n",
      " [-] epoch  112/250, train loss 0.211369 in 0.51s\n",
      " [-] epoch  113/250, train loss 0.232703 in 0.50s\n",
      " [-] epoch  114/250, train loss 0.246934 in 0.50s\n",
      " [-] epoch  115/250, train loss 0.252784 in 0.49s\n",
      " [-] epoch  116/250, train loss 0.232642 in 0.51s\n",
      " [-] epoch  117/250, train loss 0.215730 in 0.48s\n",
      " [-] epoch  118/250, train loss 0.224764 in 0.49s\n",
      " [-] epoch  119/250, train loss 0.238410 in 0.49s\n",
      " [-] epoch  120/250, train loss 0.234945 in 0.48s\n",
      " [-] epoch  121/250, train loss 0.236579 in 0.48s\n",
      " [-] epoch  122/250, train loss 0.217316 in 0.45s\n",
      " [-] epoch  123/250, train loss 0.238877 in 0.50s\n",
      " [-] epoch  124/250, train loss 0.250522 in 0.52s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  125/250, train loss 0.239737 in 0.51s\n",
      " [-] epoch  126/250, train loss 0.222563 in 0.49s\n",
      " [-] epoch  127/250, train loss 0.212647 in 0.48s\n",
      " [-] epoch  128/250, train loss 0.228657 in 0.49s\n",
      " [-] epoch  129/250, train loss 0.220578 in 0.47s\n",
      " [-] epoch  130/250, train loss 0.223361 in 0.47s\n",
      " [-] epoch  131/250, train loss 0.209026 in 0.46s\n",
      " [-] epoch  132/250, train loss 0.206038 in 0.47s\n",
      " [-] epoch  133/250, train loss 0.210330 in 0.49s\n",
      " [-] epoch  134/250, train loss 0.207651 in 0.50s\n",
      " [-] epoch  135/250, train loss 0.206162 in 0.50s\n",
      " [-] epoch  136/250, train loss 0.204399 in 0.49s\n",
      " [-] epoch  137/250, train loss 0.207253 in 0.49s\n",
      " [-] epoch  138/250, train loss 0.195897 in 0.47s\n",
      " [-] epoch  139/250, train loss 0.251061 in 0.46s\n",
      " [-] epoch  140/250, train loss 0.219254 in 0.48s\n",
      " [-] epoch  141/250, train loss 0.212212 in 0.48s\n",
      " [-] epoch  142/250, train loss 0.206394 in 0.47s\n",
      " [-] epoch  143/250, train loss 0.214266 in 0.49s\n",
      " [-] epoch  144/250, train loss 0.216988 in 0.49s\n",
      " [-] epoch  145/250, train loss 0.202108 in 0.49s\n",
      " [-] epoch  146/250, train loss 0.225979 in 0.46s\n",
      " [-] epoch  147/250, train loss 0.216196 in 0.50s\n",
      " [-] epoch  148/250, train loss 0.226853 in 0.49s\n",
      " [-] epoch  149/250, train loss 0.218104 in 0.48s\n",
      " [-] epoch  150/250, train loss 0.224674 in 0.47s\n",
      " [-] epoch  151/250, train loss 0.228340 in 0.48s\n",
      " [-] epoch  152/250, train loss 0.207931 in 0.47s\n",
      " [-] epoch  153/250, train loss 0.201611 in 0.51s\n",
      " [-] epoch  154/250, train loss 0.220452 in 0.50s\n",
      " [-] epoch  155/250, train loss 0.236123 in 0.49s\n",
      " [-] epoch  156/250, train loss 0.209144 in 0.50s\n",
      " [-] epoch  157/250, train loss 0.208008 in 0.49s\n",
      " [-] epoch  158/250, train loss 0.232478 in 0.48s\n",
      " [-] epoch  159/250, train loss 0.210889 in 0.46s\n",
      " [-] epoch  160/250, train loss 0.208149 in 0.45s\n",
      " [-] epoch  161/250, train loss 0.218570 in 0.49s\n",
      " [-] epoch  162/250, train loss 0.203810 in 0.48s\n",
      " [-] epoch  163/250, train loss 0.201822 in 0.48s\n",
      " [-] epoch  164/250, train loss 0.215841 in 0.47s\n",
      " [-] epoch  165/250, train loss 0.199246 in 0.48s\n",
      " [-] epoch  166/250, train loss 0.201324 in 0.49s\n",
      " [-] epoch  167/250, train loss 0.204690 in 0.49s\n",
      " [-] epoch  168/250, train loss 0.212359 in 0.50s\n",
      " [-] epoch  169/250, train loss 0.213641 in 0.48s\n",
      " [-] epoch  170/250, train loss 0.207144 in 0.48s\n",
      " [-] epoch  171/250, train loss 0.194154 in 0.48s\n",
      " [-] epoch  172/250, train loss 0.213622 in 0.50s\n",
      " [-] epoch  173/250, train loss 0.206390 in 0.47s\n",
      " [-] epoch  174/250, train loss 0.224563 in 0.47s\n",
      " [-] epoch  175/250, train loss 0.232660 in 0.48s\n",
      " [-] epoch  176/250, train loss 0.200014 in 0.48s\n",
      " [-] epoch  177/250, train loss 0.199508 in 0.49s\n",
      " [-] epoch  178/250, train loss 0.217705 in 0.47s\n",
      " [-] epoch  179/250, train loss 0.219301 in 0.48s\n",
      " [-] epoch  180/250, train loss 0.214192 in 0.45s\n",
      " [-] epoch  181/250, train loss 0.192122 in 0.48s\n",
      " [-] epoch  182/250, train loss 0.207378 in 0.49s\n",
      " [-] epoch  183/250, train loss 0.199464 in 0.49s\n",
      " [-] epoch  184/250, train loss 0.194929 in 0.48s\n",
      " [-] epoch  185/250, train loss 0.200407 in 0.47s\n",
      " [-] epoch  186/250, train loss 0.204262 in 0.48s\n",
      " [-] epoch  187/250, train loss 0.203976 in 0.47s\n",
      " [-] epoch  188/250, train loss 0.200786 in 0.47s\n",
      " [-] epoch  189/250, train loss 0.185956 in 0.47s\n",
      " [-] epoch  190/250, train loss 0.188540 in 0.47s\n",
      " [-] epoch  191/250, train loss 0.198994 in 0.50s\n",
      " [-] epoch  192/250, train loss 0.209611 in 0.48s\n",
      " [-] epoch  193/250, train loss 0.202554 in 0.48s\n",
      " [-] epoch  194/250, train loss 0.218512 in 0.49s\n",
      " [-] epoch  195/250, train loss 0.216133 in 0.49s\n",
      " [-] epoch  196/250, train loss 0.208885 in 0.50s\n",
      " [-] epoch  197/250, train loss 0.216077 in 0.46s\n",
      " [-] epoch  198/250, train loss 0.209834 in 0.47s\n",
      " [-] epoch  199/250, train loss 0.204073 in 0.47s\n",
      " [-] epoch  200/250, train loss 0.190042 in 0.48s\n",
      " [-] epoch  201/250, train loss 0.215396 in 0.50s\n",
      " [-] epoch  202/250, train loss 0.216460 in 0.49s\n",
      " [-] epoch  203/250, train loss 0.206644 in 0.48s\n",
      " [-] epoch  204/250, train loss 0.212148 in 0.49s\n",
      " [-] epoch  205/250, train loss 0.199014 in 0.47s\n",
      " [-] epoch  206/250, train loss 0.200953 in 0.48s\n",
      " [-] epoch  207/250, train loss 0.204239 in 0.49s\n",
      " [-] epoch  208/250, train loss 0.193073 in 0.49s\n",
      " [-] epoch  209/250, train loss 0.205402 in 0.47s\n",
      " [-] epoch  210/250, train loss 0.191655 in 0.49s\n",
      " [-] epoch  211/250, train loss 0.187047 in 0.46s\n",
      " [-] epoch  212/250, train loss 0.199553 in 0.49s\n",
      " [-] epoch  213/250, train loss 0.193877 in 0.50s\n",
      " [-] epoch  214/250, train loss 0.189429 in 0.47s\n",
      " [-] epoch  215/250, train loss 0.196355 in 0.49s\n",
      " [-] epoch  216/250, train loss 0.202303 in 0.47s\n",
      " [-] epoch  217/250, train loss 0.194568 in 0.49s\n",
      " [-] epoch  218/250, train loss 0.197684 in 0.49s\n",
      " [-] epoch  219/250, train loss 0.195988 in 0.47s\n",
      " [-] epoch  220/250, train loss 0.182194 in 0.48s\n",
      " [-] epoch  221/250, train loss 0.204256 in 0.49s\n",
      " [-] epoch  222/250, train loss 0.198072 in 0.47s\n",
      " [-] epoch  223/250, train loss 0.211821 in 0.51s\n",
      " [-] epoch  224/250, train loss 0.179679 in 0.50s\n",
      " [-] epoch  225/250, train loss 0.179910 in 0.49s\n",
      " [-] epoch  226/250, train loss 0.188993 in 0.49s\n",
      " [-] epoch  227/250, train loss 0.201424 in 0.49s\n",
      " [-] epoch  228/250, train loss 0.217801 in 0.48s\n",
      " [-] epoch  229/250, train loss 0.201064 in 0.47s\n",
      " [-] epoch  230/250, train loss 0.201355 in 0.46s\n",
      " [-] epoch  231/250, train loss 0.191063 in 0.46s\n",
      " [-] epoch  232/250, train loss 0.191830 in 0.48s\n",
      " [-] epoch  233/250, train loss 0.181575 in 0.48s\n",
      " [-] epoch  234/250, train loss 0.190550 in 0.49s\n",
      " [-] epoch  235/250, train loss 0.188232 in 0.51s\n",
      " [-] epoch  236/250, train loss 0.218292 in 0.49s\n",
      " [-] epoch  237/250, train loss 0.214141 in 0.50s\n",
      " [-] epoch  238/250, train loss 0.200774 in 0.49s\n",
      " [-] epoch  239/250, train loss 0.195989 in 0.50s\n",
      " [-] epoch  240/250, train loss 0.201526 in 0.48s\n",
      " [-] epoch  241/250, train loss 0.177497 in 0.49s\n",
      " [-] epoch  242/250, train loss 0.191480 in 0.44s\n",
      " [-] epoch  243/250, train loss 0.200789 in 0.48s\n",
      " [-] epoch  244/250, train loss 0.203483 in 0.47s\n",
      " [-] epoch  245/250, train loss 0.208318 in 0.46s\n",
      " [-] epoch  246/250, train loss 0.202241 in 0.49s\n",
      " [-] epoch  247/250, train loss 0.199686 in 0.47s\n",
      " [-] epoch  248/250, train loss 0.179736 in 0.49s\n",
      " [-] epoch  249/250, train loss 0.201283 in 0.50s\n",
      " [-] epoch  250/250, train loss 0.199451 in 0.51s\n",
      " [-] test acc. 81.388889%\n",
      "Je vais utiliser 8 layers\n",
      " [-] epoch    1/250, train loss 0.616213 in 0.55s\n",
      " [-] epoch    2/250, train loss 0.483413 in 0.55s\n",
      " [-] epoch    3/250, train loss 0.408916 in 0.58s\n",
      " [-] epoch    4/250, train loss 0.407015 in 0.59s\n",
      " [-] epoch    5/250, train loss 0.396657 in 0.57s\n",
      " [-] epoch    6/250, train loss 0.366113 in 0.56s\n",
      " [-] epoch    7/250, train loss 0.383451 in 0.56s\n",
      " [-] epoch    8/250, train loss 0.376036 in 0.58s\n",
      " [-] epoch    9/250, train loss 0.340128 in 0.57s\n",
      " [-] epoch   10/250, train loss 0.335437 in 0.59s\n",
      " [-] epoch   11/250, train loss 0.354910 in 0.57s\n",
      " [-] epoch   12/250, train loss 0.315041 in 0.56s\n",
      " [-] epoch   13/250, train loss 0.364426 in 0.58s\n",
      " [-] epoch   14/250, train loss 0.344044 in 0.57s\n",
      " [-] epoch   15/250, train loss 0.341144 in 0.58s\n",
      " [-] epoch   16/250, train loss 0.338323 in 0.60s\n",
      " [-] epoch   17/250, train loss 0.292721 in 0.58s\n",
      " [-] epoch   18/250, train loss 0.328544 in 0.57s\n",
      " [-] epoch   19/250, train loss 0.342875 in 0.58s\n",
      " [-] epoch   20/250, train loss 0.300572 in 0.55s\n",
      " [-] epoch   21/250, train loss 0.301667 in 0.57s\n",
      " [-] epoch   22/250, train loss 0.320062 in 0.59s\n",
      " [-] epoch   23/250, train loss 0.337105 in 0.57s\n",
      " [-] epoch   24/250, train loss 0.310677 in 0.56s\n",
      " [-] epoch   25/250, train loss 0.295013 in 0.59s\n",
      " [-] epoch   26/250, train loss 0.295951 in 0.59s\n",
      " [-] epoch   27/250, train loss 0.312645 in 0.58s\n",
      " [-] epoch   28/250, train loss 0.323268 in 0.57s\n",
      " [-] epoch   29/250, train loss 0.314625 in 0.58s\n",
      " [-] epoch   30/250, train loss 0.280283 in 0.57s\n",
      " [-] epoch   31/250, train loss 0.333539 in 0.58s\n",
      " [-] epoch   32/250, train loss 0.289253 in 0.58s\n",
      " [-] epoch   33/250, train loss 0.290666 in 0.57s\n",
      " [-] epoch   34/250, train loss 0.299020 in 0.55s\n",
      " [-] epoch   35/250, train loss 0.300239 in 0.57s\n",
      " [-] epoch   36/250, train loss 0.284712 in 0.56s\n",
      " [-] epoch   37/250, train loss 0.299779 in 0.61s\n",
      " [-] epoch   38/250, train loss 0.291516 in 0.56s\n",
      " [-] epoch   39/250, train loss 0.269598 in 0.57s\n",
      " [-] epoch   40/250, train loss 0.314528 in 0.55s\n",
      " [-] epoch   41/250, train loss 0.321698 in 0.58s\n",
      " [-] epoch   42/250, train loss 0.282191 in 0.56s\n",
      " [-] epoch   43/250, train loss 0.273541 in 0.58s\n",
      " [-] epoch   44/250, train loss 0.266670 in 0.55s\n",
      " [-] epoch   45/250, train loss 0.286236 in 0.59s\n",
      " [-] epoch   46/250, train loss 0.299021 in 0.60s\n",
      " [-] epoch   47/250, train loss 0.259978 in 0.59s\n",
      " [-] epoch   48/250, train loss 0.316587 in 0.54s\n",
      " [-] epoch   49/250, train loss 0.266233 in 0.56s\n",
      " [-] epoch   50/250, train loss 0.290170 in 0.59s\n",
      " [-] epoch   51/250, train loss 0.277379 in 0.57s\n",
      " [-] epoch   52/250, train loss 0.270742 in 0.58s\n",
      " [-] epoch   53/250, train loss 0.263643 in 0.55s\n",
      " [-] epoch   54/250, train loss 0.272798 in 0.58s\n",
      " [-] epoch   55/250, train loss 0.276323 in 0.57s\n",
      " [-] epoch   56/250, train loss 0.266972 in 0.58s\n",
      " [-] epoch   57/250, train loss 0.252322 in 0.58s\n",
      " [-] epoch   58/250, train loss 0.294091 in 0.57s\n",
      " [-] epoch   59/250, train loss 0.279456 in 0.60s\n",
      " [-] epoch   60/250, train loss 0.254924 in 0.56s\n",
      " [-] epoch   61/250, train loss 0.262692 in 0.61s\n",
      " [-] epoch   62/250, train loss 0.270726 in 0.62s\n",
      " [-] epoch   63/250, train loss 0.245592 in 0.57s\n",
      " [-] epoch   64/250, train loss 0.269077 in 0.59s\n",
      " [-] epoch   65/250, train loss 0.250774 in 0.60s\n",
      " [-] epoch   66/250, train loss 0.255796 in 0.57s\n",
      " [-] epoch   67/250, train loss 0.278130 in 0.59s\n",
      " [-] epoch   68/250, train loss 0.274250 in 0.57s\n",
      " [-] epoch   69/250, train loss 0.255594 in 0.59s\n",
      " [-] epoch   70/250, train loss 0.248773 in 0.57s\n",
      " [-] epoch   71/250, train loss 0.247502 in 0.56s\n",
      " [-] epoch   72/250, train loss 0.250067 in 0.58s\n",
      " [-] epoch   73/250, train loss 0.239165 in 0.58s\n",
      " [-] epoch   74/250, train loss 0.279295 in 0.57s\n",
      " [-] epoch   75/250, train loss 0.236805 in 0.56s\n",
      " [-] epoch   76/250, train loss 0.272102 in 0.59s\n",
      " [-] epoch   77/250, train loss 0.263479 in 0.55s\n",
      " [-] epoch   78/250, train loss 0.260750 in 0.58s\n",
      " [-] epoch   79/250, train loss 0.271498 in 0.60s\n",
      " [-] epoch   80/250, train loss 0.272093 in 0.58s\n",
      " [-] epoch   81/250, train loss 0.230526 in 0.55s\n",
      " [-] epoch   82/250, train loss 0.249283 in 0.57s\n",
      " [-] epoch   83/250, train loss 0.268851 in 0.56s\n",
      " [-] epoch   84/250, train loss 0.267701 in 0.59s\n",
      " [-] epoch   85/250, train loss 0.242505 in 0.61s\n",
      " [-] epoch   86/250, train loss 0.238967 in 0.56s\n",
      " [-] epoch   87/250, train loss 0.251073 in 0.58s\n",
      " [-] epoch   88/250, train loss 0.270773 in 0.56s\n",
      " [-] epoch   89/250, train loss 0.254323 in 0.59s\n",
      " [-] epoch   90/250, train loss 0.241496 in 0.57s\n",
      " [-] epoch   91/250, train loss 0.232942 in 0.56s\n",
      " [-] epoch   92/250, train loss 0.271167 in 0.59s\n",
      " [-] epoch   93/250, train loss 0.220678 in 0.60s\n",
      " [-] epoch   94/250, train loss 0.230052 in 0.58s\n",
      " [-] epoch   95/250, train loss 0.230887 in 0.58s\n",
      " [-] epoch   96/250, train loss 0.267939 in 0.59s\n",
      " [-] epoch   97/250, train loss 0.248116 in 0.58s\n",
      " [-] epoch   98/250, train loss 0.259179 in 0.58s\n",
      " [-] epoch   99/250, train loss 0.249922 in 0.56s\n",
      " [-] epoch  100/250, train loss 0.235528 in 0.57s\n",
      " [-] epoch  101/250, train loss 0.227025 in 0.56s\n",
      " [-] epoch  102/250, train loss 0.234813 in 0.58s\n",
      " [-] epoch  103/250, train loss 0.254326 in 0.58s\n",
      " [-] epoch  104/250, train loss 0.244222 in 0.62s\n",
      " [-] epoch  105/250, train loss 0.234947 in 0.59s\n",
      " [-] epoch  106/250, train loss 0.216742 in 0.59s\n",
      " [-] epoch  107/250, train loss 0.226034 in 0.58s\n",
      " [-] epoch  108/250, train loss 0.251198 in 0.61s\n",
      " [-] epoch  109/250, train loss 0.261218 in 0.60s\n",
      " [-] epoch  110/250, train loss 0.250654 in 0.60s\n",
      " [-] epoch  111/250, train loss 0.252785 in 0.59s\n",
      " [-] epoch  112/250, train loss 0.245189 in 0.58s\n",
      " [-] epoch  113/250, train loss 0.223775 in 0.57s\n",
      " [-] epoch  114/250, train loss 0.224828 in 0.57s\n",
      " [-] epoch  115/250, train loss 0.277755 in 0.58s\n",
      " [-] epoch  116/250, train loss 0.240072 in 0.60s\n",
      " [-] epoch  117/250, train loss 0.240581 in 0.58s\n",
      " [-] epoch  118/250, train loss 0.224391 in 0.58s\n",
      " [-] epoch  119/250, train loss 0.214005 in 0.56s\n",
      " [-] epoch  120/250, train loss 0.225255 in 0.55s\n",
      " [-] epoch  121/250, train loss 0.245933 in 0.57s\n",
      " [-] epoch  122/250, train loss 0.254401 in 0.58s\n",
      " [-] epoch  123/250, train loss 0.236581 in 0.56s\n",
      " [-] epoch  124/250, train loss 0.220929 in 0.59s\n",
      " [-] epoch  125/250, train loss 0.257823 in 0.58s\n",
      " [-] epoch  126/250, train loss 0.242898 in 0.61s\n",
      " [-] epoch  127/250, train loss 0.242448 in 0.56s\n",
      " [-] epoch  128/250, train loss 0.230927 in 0.58s\n",
      " [-] epoch  129/250, train loss 0.219465 in 0.56s\n",
      " [-] epoch  130/250, train loss 0.246433 in 0.57s\n",
      " [-] epoch  131/250, train loss 0.228713 in 0.58s\n",
      " [-] epoch  132/250, train loss 0.226956 in 0.59s\n",
      " [-] epoch  133/250, train loss 0.226488 in 0.59s\n",
      " [-] epoch  134/250, train loss 0.226709 in 0.57s\n",
      " [-] epoch  135/250, train loss 0.250415 in 0.59s\n",
      " [-] epoch  136/250, train loss 0.229784 in 0.56s\n",
      " [-] epoch  137/250, train loss 0.223749 in 0.58s\n",
      " [-] epoch  138/250, train loss 0.215539 in 0.58s\n",
      " [-] epoch  139/250, train loss 0.258058 in 0.59s\n",
      " [-] epoch  140/250, train loss 0.219164 in 0.56s\n",
      " [-] epoch  141/250, train loss 0.220361 in 0.55s\n",
      " [-] epoch  142/250, train loss 0.207597 in 0.56s\n",
      " [-] epoch  143/250, train loss 0.214276 in 0.60s\n",
      " [-] epoch  144/250, train loss 0.218826 in 0.58s\n",
      " [-] epoch  145/250, train loss 0.224957 in 0.60s\n",
      " [-] epoch  146/250, train loss 0.198736 in 0.61s\n",
      " [-] epoch  147/250, train loss 0.208145 in 0.59s\n",
      " [-] epoch  148/250, train loss 0.227875 in 1.40s\n",
      " [-] epoch  149/250, train loss 0.224663 in 0.57s\n",
      " [-] epoch  150/250, train loss 0.223615 in 0.64s\n",
      " [-] epoch  151/250, train loss 0.231538 in 0.61s\n",
      " [-] epoch  152/250, train loss 0.224113 in 0.64s\n",
      " [-] epoch  153/250, train loss 0.238493 in 0.63s\n",
      " [-] epoch  154/250, train loss 0.229699 in 0.60s\n",
      " [-] epoch  155/250, train loss 0.217996 in 0.58s\n",
      " [-] epoch  156/250, train loss 0.224237 in 0.59s\n",
      " [-] epoch  157/250, train loss 0.216829 in 0.61s\n",
      " [-] epoch  158/250, train loss 0.208842 in 0.61s\n",
      " [-] epoch  159/250, train loss 0.217542 in 0.62s\n",
      " [-] epoch  160/250, train loss 0.224661 in 0.60s\n",
      " [-] epoch  161/250, train loss 0.209661 in 0.63s\n",
      " [-] epoch  162/250, train loss 0.215434 in 0.60s\n",
      " [-] epoch  163/250, train loss 0.226331 in 0.62s\n",
      " [-] epoch  164/250, train loss 0.227830 in 0.63s\n",
      " [-] epoch  165/250, train loss 0.222442 in 0.60s\n",
      " [-] epoch  166/250, train loss 0.215356 in 0.60s\n",
      " [-] epoch  167/250, train loss 0.249449 in 0.58s\n",
      " [-] epoch  168/250, train loss 0.219558 in 0.59s\n",
      " [-] epoch  169/250, train loss 0.210925 in 0.62s\n",
      " [-] epoch  170/250, train loss 0.223666 in 0.58s\n",
      " [-] epoch  171/250, train loss 0.217438 in 0.57s\n",
      " [-] epoch  172/250, train loss 0.221340 in 0.63s\n",
      " [-] epoch  173/250, train loss 0.218945 in 0.62s\n",
      " [-] epoch  174/250, train loss 0.211500 in 0.59s\n",
      " [-] epoch  175/250, train loss 0.209791 in 0.57s\n",
      " [-] epoch  176/250, train loss 0.237698 in 0.56s\n",
      " [-] epoch  177/250, train loss 0.214638 in 0.57s\n",
      " [-] epoch  178/250, train loss 0.212401 in 0.57s\n",
      " [-] epoch  179/250, train loss 0.210982 in 0.57s\n",
      " [-] epoch  180/250, train loss 0.231963 in 0.60s\n",
      " [-] epoch  181/250, train loss 0.222586 in 0.59s\n",
      " [-] epoch  182/250, train loss 0.207480 in 0.58s\n",
      " [-] epoch  183/250, train loss 0.208328 in 0.57s\n",
      " [-] epoch  184/250, train loss 0.228910 in 0.57s\n",
      " [-] epoch  185/250, train loss 0.218615 in 0.55s\n",
      " [-] epoch  186/250, train loss 0.202587 in 0.55s\n",
      " [-] epoch  187/250, train loss 0.235842 in 0.58s\n",
      " [-] epoch  188/250, train loss 0.203604 in 0.57s\n",
      " [-] epoch  189/250, train loss 0.216416 in 0.55s\n",
      " [-] epoch  190/250, train loss 0.207159 in 0.56s\n",
      " [-] epoch  191/250, train loss 0.213399 in 0.57s\n",
      " [-] epoch  192/250, train loss 0.198932 in 0.57s\n",
      " [-] epoch  193/250, train loss 0.207996 in 0.57s\n",
      " [-] epoch  194/250, train loss 0.221641 in 0.58s\n",
      " [-] epoch  195/250, train loss 0.217739 in 0.55s\n",
      " [-] epoch  196/250, train loss 0.215844 in 0.58s\n",
      " [-] epoch  197/250, train loss 0.226283 in 0.60s\n",
      " [-] epoch  198/250, train loss 0.214398 in 0.56s\n",
      " [-] epoch  199/250, train loss 0.198907 in 0.57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  200/250, train loss 0.214673 in 0.58s\n",
      " [-] epoch  201/250, train loss 0.214800 in 0.58s\n",
      " [-] epoch  202/250, train loss 0.208075 in 0.59s\n",
      " [-] epoch  203/250, train loss 0.226570 in 0.58s\n",
      " [-] epoch  204/250, train loss 0.221069 in 0.61s\n",
      " [-] epoch  205/250, train loss 0.213942 in 0.59s\n",
      " [-] epoch  206/250, train loss 0.215976 in 0.60s\n",
      " [-] epoch  207/250, train loss 0.208525 in 0.61s\n",
      " [-] epoch  208/250, train loss 0.237495 in 0.59s\n",
      " [-] epoch  209/250, train loss 0.211913 in 0.57s\n",
      " [-] epoch  210/250, train loss 0.239292 in 0.59s\n",
      " [-] epoch  211/250, train loss 0.200678 in 0.59s\n",
      " [-] epoch  212/250, train loss 0.224072 in 0.59s\n",
      " [-] epoch  213/250, train loss 0.227767 in 0.60s\n",
      " [-] epoch  214/250, train loss 0.202910 in 0.56s\n",
      " [-] epoch  215/250, train loss 0.209129 in 0.61s\n",
      " [-] epoch  216/250, train loss 0.206841 in 0.58s\n",
      " [-] epoch  217/250, train loss 0.210480 in 0.59s\n",
      " [-] epoch  218/250, train loss 0.203098 in 0.57s\n",
      " [-] epoch  219/250, train loss 0.192173 in 0.59s\n",
      " [-] epoch  220/250, train loss 0.217930 in 0.59s\n",
      " [-] epoch  221/250, train loss 0.229103 in 0.57s\n",
      " [-] epoch  222/250, train loss 0.215128 in 0.58s\n",
      " [-] epoch  223/250, train loss 0.196934 in 0.58s\n",
      " [-] epoch  224/250, train loss 0.197197 in 0.54s\n",
      " [-] epoch  225/250, train loss 0.203478 in 0.58s\n",
      " [-] epoch  226/250, train loss 0.194193 in 0.61s\n",
      " [-] epoch  227/250, train loss 0.214508 in 0.57s\n",
      " [-] epoch  228/250, train loss 0.198409 in 0.58s\n",
      " [-] epoch  229/250, train loss 0.202089 in 0.57s\n",
      " [-] epoch  230/250, train loss 0.228223 in 0.59s\n",
      " [-] epoch  231/250, train loss 0.203344 in 0.60s\n",
      " [-] epoch  232/250, train loss 0.208016 in 0.57s\n",
      " [-] epoch  233/250, train loss 0.205473 in 0.57s\n",
      " [-] epoch  234/250, train loss 0.217558 in 0.58s\n",
      " [-] epoch  235/250, train loss 0.228977 in 0.58s\n",
      " [-] epoch  236/250, train loss 0.186410 in 0.61s\n",
      " [-] epoch  237/250, train loss 0.186610 in 0.61s\n",
      " [-] epoch  238/250, train loss 0.215958 in 0.57s\n",
      " [-] epoch  239/250, train loss 0.203208 in 0.58s\n",
      " [-] epoch  240/250, train loss 0.198448 in 0.57s\n",
      " [-] epoch  241/250, train loss 0.215309 in 0.59s\n",
      " [-] epoch  242/250, train loss 0.197670 in 0.59s\n",
      " [-] epoch  243/250, train loss 0.189917 in 0.57s\n",
      " [-] epoch  244/250, train loss 0.211341 in 0.61s\n",
      " [-] epoch  245/250, train loss 0.204141 in 0.60s\n",
      " [-] epoch  246/250, train loss 0.211105 in 0.58s\n",
      " [-] epoch  247/250, train loss 0.219167 in 0.55s\n",
      " [-] epoch  248/250, train loss 0.214459 in 0.57s\n",
      " [-] epoch  249/250, train loss 0.213074 in 0.61s\n",
      " [-] epoch  250/250, train loss 0.227424 in 0.61s\n",
      " [-] test acc. 82.777778%\n",
      "Je vais utiliser 9 layers\n",
      " [-] epoch    1/250, train loss 0.546251 in 0.64s\n",
      " [-] epoch    2/250, train loss 0.459414 in 0.66s\n",
      " [-] epoch    3/250, train loss 0.424939 in 0.65s\n",
      " [-] epoch    4/250, train loss 0.417108 in 0.63s\n",
      " [-] epoch    5/250, train loss 0.414335 in 0.67s\n",
      " [-] epoch    6/250, train loss 0.397302 in 0.66s\n",
      " [-] epoch    7/250, train loss 0.348281 in 0.65s\n",
      " [-] epoch    8/250, train loss 0.379900 in 0.65s\n",
      " [-] epoch    9/250, train loss 0.372328 in 0.65s\n",
      " [-] epoch   10/250, train loss 0.374787 in 0.66s\n",
      " [-] epoch   11/250, train loss 0.374740 in 0.66s\n",
      " [-] epoch   12/250, train loss 0.342352 in 0.65s\n",
      " [-] epoch   13/250, train loss 0.328286 in 0.64s\n",
      " [-] epoch   14/250, train loss 0.328217 in 0.67s\n",
      " [-] epoch   15/250, train loss 0.311887 in 0.66s\n",
      " [-] epoch   16/250, train loss 0.342111 in 0.63s\n",
      " [-] epoch   17/250, train loss 0.338922 in 0.67s\n",
      " [-] epoch   18/250, train loss 0.349616 in 0.64s\n",
      " [-] epoch   19/250, train loss 0.319776 in 0.64s\n",
      " [-] epoch   20/250, train loss 0.325586 in 0.64s\n",
      " [-] epoch   21/250, train loss 0.310156 in 0.65s\n",
      " [-] epoch   22/250, train loss 0.333342 in 0.65s\n",
      " [-] epoch   23/250, train loss 0.330995 in 0.65s\n",
      " [-] epoch   24/250, train loss 0.307687 in 0.65s\n",
      " [-] epoch   25/250, train loss 0.298159 in 0.65s\n",
      " [-] epoch   26/250, train loss 0.299943 in 0.65s\n",
      " [-] epoch   27/250, train loss 0.310173 in 0.65s\n",
      " [-] epoch   28/250, train loss 0.305567 in 0.65s\n",
      " [-] epoch   29/250, train loss 0.311666 in 0.63s\n",
      " [-] epoch   30/250, train loss 0.305794 in 0.66s\n",
      " [-] epoch   31/250, train loss 0.301834 in 0.65s\n",
      " [-] epoch   32/250, train loss 0.305430 in 0.64s\n",
      " [-] epoch   33/250, train loss 0.294749 in 0.63s\n",
      " [-] epoch   34/250, train loss 0.313157 in 0.64s\n",
      " [-] epoch   35/250, train loss 0.297973 in 0.64s\n",
      " [-] epoch   36/250, train loss 0.271468 in 0.65s\n",
      " [-] epoch   37/250, train loss 0.265160 in 0.66s\n",
      " [-] epoch   38/250, train loss 0.264115 in 0.64s\n",
      " [-] epoch   39/250, train loss 0.267407 in 0.63s\n",
      " [-] epoch   40/250, train loss 0.294956 in 0.64s\n",
      " [-] epoch   41/250, train loss 0.293474 in 0.63s\n",
      " [-] epoch   42/250, train loss 0.285384 in 0.64s\n",
      " [-] epoch   43/250, train loss 0.273016 in 0.62s\n",
      " [-] epoch   44/250, train loss 0.282820 in 0.65s\n",
      " [-] epoch   45/250, train loss 0.286726 in 0.65s\n",
      " [-] epoch   46/250, train loss 0.284823 in 0.65s\n",
      " [-] epoch   47/250, train loss 0.255793 in 0.62s\n",
      " [-] epoch   48/250, train loss 0.284713 in 0.65s\n",
      " [-] epoch   49/250, train loss 0.268988 in 0.66s\n",
      " [-] epoch   50/250, train loss 0.258425 in 0.63s\n",
      " [-] epoch   51/250, train loss 0.283257 in 0.65s\n",
      " [-] epoch   52/250, train loss 0.271468 in 0.64s\n",
      " [-] epoch   53/250, train loss 0.259428 in 0.67s\n",
      " [-] epoch   54/250, train loss 0.279385 in 0.65s\n",
      " [-] epoch   55/250, train loss 0.272660 in 0.66s\n",
      " [-] epoch   56/250, train loss 0.261790 in 0.66s\n",
      " [-] epoch   57/250, train loss 0.291125 in 0.69s\n",
      " [-] epoch   58/250, train loss 0.280818 in 0.68s\n",
      " [-] epoch   59/250, train loss 0.263964 in 0.66s\n",
      " [-] epoch   60/250, train loss 0.275688 in 0.67s\n",
      " [-] epoch   61/250, train loss 0.255807 in 0.67s\n",
      " [-] epoch   62/250, train loss 0.258189 in 0.63s\n",
      " [-] epoch   63/250, train loss 0.263859 in 0.65s\n",
      " [-] epoch   64/250, train loss 0.295290 in 0.68s\n",
      " [-] epoch   65/250, train loss 0.273281 in 0.66s\n",
      " [-] epoch   66/250, train loss 0.249287 in 0.66s\n",
      " [-] epoch   67/250, train loss 0.248453 in 0.66s\n",
      " [-] epoch   68/250, train loss 0.267408 in 0.66s\n",
      " [-] epoch   69/250, train loss 0.260972 in 0.64s\n",
      " [-] epoch   70/250, train loss 0.272227 in 0.69s\n",
      " [-] epoch   71/250, train loss 0.273932 in 0.67s\n",
      " [-] epoch   72/250, train loss 0.280199 in 0.66s\n",
      " [-] epoch   73/250, train loss 0.251397 in 0.64s\n",
      " [-] epoch   74/250, train loss 0.254109 in 0.66s\n",
      " [-] epoch   75/250, train loss 0.263793 in 0.67s\n",
      " [-] epoch   76/250, train loss 0.243663 in 0.63s\n",
      " [-] epoch   77/250, train loss 0.277482 in 0.67s\n",
      " [-] epoch   78/250, train loss 0.254269 in 0.66s\n",
      " [-] epoch   79/250, train loss 0.229119 in 0.67s\n",
      " [-] epoch   80/250, train loss 0.241030 in 0.62s\n",
      " [-] epoch   81/250, train loss 0.232231 in 0.64s\n",
      " [-] epoch   82/250, train loss 0.249952 in 0.68s\n",
      " [-] epoch   83/250, train loss 0.255646 in 0.68s\n",
      " [-] epoch   84/250, train loss 0.266183 in 0.65s\n",
      " [-] epoch   85/250, train loss 0.254743 in 0.65s\n",
      " [-] epoch   86/250, train loss 0.261809 in 0.64s\n",
      " [-] epoch   87/250, train loss 0.233894 in 0.63s\n",
      " [-] epoch   88/250, train loss 0.234176 in 0.66s\n",
      " [-] epoch   89/250, train loss 0.234443 in 0.66s\n",
      " [-] epoch   90/250, train loss 0.249661 in 0.65s\n",
      " [-] epoch   91/250, train loss 0.248524 in 0.65s\n",
      " [-] epoch   92/250, train loss 0.214799 in 0.65s\n",
      " [-] epoch   93/250, train loss 0.246609 in 0.68s\n",
      " [-] epoch   94/250, train loss 0.235516 in 0.68s\n",
      " [-] epoch   95/250, train loss 0.245852 in 0.66s\n",
      " [-] epoch   96/250, train loss 0.239470 in 0.64s\n",
      " [-] epoch   97/250, train loss 0.242883 in 0.68s\n",
      " [-] epoch   98/250, train loss 0.248428 in 0.68s\n",
      " [-] epoch   99/250, train loss 0.239207 in 0.65s\n",
      " [-] epoch  100/250, train loss 0.236659 in 0.64s\n",
      " [-] epoch  101/250, train loss 0.235656 in 0.66s\n",
      " [-] epoch  102/250, train loss 0.234036 in 0.65s\n",
      " [-] epoch  103/250, train loss 0.225030 in 0.65s\n",
      " [-] epoch  104/250, train loss 0.222554 in 0.63s\n",
      " [-] epoch  105/250, train loss 0.238939 in 0.62s\n",
      " [-] epoch  106/250, train loss 0.270166 in 0.67s\n",
      " [-] epoch  107/250, train loss 0.250537 in 0.66s\n",
      " [-] epoch  108/250, train loss 0.247351 in 0.68s\n",
      " [-] epoch  109/250, train loss 0.226334 in 0.68s\n",
      " [-] epoch  110/250, train loss 0.225956 in 0.67s\n",
      " [-] epoch  111/250, train loss 0.208791 in 0.65s\n",
      " [-] epoch  112/250, train loss 0.219947 in 0.67s\n",
      " [-] epoch  113/250, train loss 0.222286 in 0.63s\n",
      " [-] epoch  114/250, train loss 0.217177 in 0.64s\n",
      " [-] epoch  115/250, train loss 0.250263 in 0.66s\n",
      " [-] epoch  116/250, train loss 0.261881 in 0.65s\n",
      " [-] epoch  117/250, train loss 0.244741 in 0.65s\n",
      " [-] epoch  118/250, train loss 0.227767 in 0.64s\n",
      " [-] epoch  119/250, train loss 0.229019 in 0.65s\n",
      " [-] epoch  120/250, train loss 0.229182 in 0.66s\n",
      " [-] epoch  121/250, train loss 0.221093 in 0.64s\n",
      " [-] epoch  122/250, train loss 0.212851 in 0.68s\n",
      " [-] epoch  123/250, train loss 0.224749 in 0.66s\n",
      " [-] epoch  124/250, train loss 0.218725 in 0.66s\n",
      " [-] epoch  125/250, train loss 0.219165 in 0.66s\n",
      " [-] epoch  126/250, train loss 0.222087 in 0.63s\n",
      " [-] epoch  127/250, train loss 0.242980 in 0.66s\n",
      " [-] epoch  128/250, train loss 0.236165 in 0.65s\n",
      " [-] epoch  129/250, train loss 0.254408 in 0.67s\n",
      " [-] epoch  130/250, train loss 0.221239 in 0.65s\n",
      " [-] epoch  131/250, train loss 0.244963 in 0.66s\n",
      " [-] epoch  132/250, train loss 0.225788 in 0.66s\n",
      " [-] epoch  133/250, train loss 0.254189 in 0.67s\n",
      " [-] epoch  134/250, train loss 0.221333 in 0.65s\n",
      " [-] epoch  135/250, train loss 0.223956 in 0.67s\n",
      " [-] epoch  136/250, train loss 0.214772 in 0.67s\n",
      " [-] epoch  137/250, train loss 0.218113 in 0.65s\n",
      " [-] epoch  138/250, train loss 0.212121 in 0.67s\n",
      " [-] epoch  139/250, train loss 0.218068 in 0.65s\n",
      " [-] epoch  140/250, train loss 0.229265 in 0.65s\n",
      " [-] epoch  141/250, train loss 0.227914 in 0.61s\n",
      " [-] epoch  142/250, train loss 0.214215 in 0.67s\n",
      " [-] epoch  143/250, train loss 0.217437 in 0.67s\n",
      " [-] epoch  144/250, train loss 0.205202 in 0.65s\n",
      " [-] epoch  145/250, train loss 0.216867 in 0.63s\n",
      " [-] epoch  146/250, train loss 0.207274 in 0.64s\n",
      " [-] epoch  147/250, train loss 0.218029 in 0.66s\n",
      " [-] epoch  148/250, train loss 0.234817 in 0.64s\n",
      " [-] epoch  149/250, train loss 0.228869 in 0.67s\n",
      " [-] epoch  150/250, train loss 0.217003 in 0.65s\n",
      " [-] epoch  151/250, train loss 0.225089 in 0.63s\n",
      " [-] epoch  152/250, train loss 0.192140 in 0.64s\n",
      " [-] epoch  153/250, train loss 0.242755 in 0.66s\n",
      " [-] epoch  154/250, train loss 0.230665 in 0.68s\n",
      " [-] epoch  155/250, train loss 0.209902 in 0.64s\n",
      " [-] epoch  156/250, train loss 0.223983 in 0.66s\n",
      " [-] epoch  157/250, train loss 0.221927 in 0.66s\n",
      " [-] epoch  158/250, train loss 0.211966 in 0.66s\n",
      " [-] epoch  159/250, train loss 0.229507 in 0.66s\n",
      " [-] epoch  160/250, train loss 0.230010 in 0.66s\n",
      " [-] epoch  161/250, train loss 0.221764 in 0.67s\n",
      " [-] epoch  162/250, train loss 0.238751 in 0.66s\n",
      " [-] epoch  163/250, train loss 0.214824 in 0.67s\n",
      " [-] epoch  164/250, train loss 0.224800 in 0.68s\n",
      " [-] epoch  165/250, train loss 0.210375 in 0.67s\n",
      " [-] epoch  166/250, train loss 0.212587 in 0.66s\n",
      " [-] epoch  167/250, train loss 0.221106 in 0.65s\n",
      " [-] epoch  168/250, train loss 0.199001 in 0.66s\n",
      " [-] epoch  169/250, train loss 0.206457 in 0.69s\n",
      " [-] epoch  170/250, train loss 0.192192 in 0.65s\n",
      " [-] epoch  171/250, train loss 0.212904 in 0.65s\n",
      " [-] epoch  172/250, train loss 0.204371 in 0.66s\n",
      " [-] epoch  173/250, train loss 0.222085 in 0.64s\n",
      " [-] epoch  174/250, train loss 0.216415 in 0.66s\n",
      " [-] epoch  175/250, train loss 0.206479 in 0.65s\n",
      " [-] epoch  176/250, train loss 0.208424 in 0.65s\n",
      " [-] epoch  177/250, train loss 0.202903 in 0.66s\n",
      " [-] epoch  178/250, train loss 0.216646 in 0.66s\n",
      " [-] epoch  179/250, train loss 0.205825 in 0.65s\n",
      " [-] epoch  180/250, train loss 0.229041 in 0.67s\n",
      " [-] epoch  181/250, train loss 0.193595 in 0.63s\n",
      " [-] epoch  182/250, train loss 0.211190 in 0.61s\n",
      " [-] epoch  183/250, train loss 0.234175 in 0.65s\n",
      " [-] epoch  184/250, train loss 0.203041 in 0.64s\n",
      " [-] epoch  185/250, train loss 0.207518 in 0.63s\n",
      " [-] epoch  186/250, train loss 0.205973 in 0.65s\n",
      " [-] epoch  187/250, train loss 0.210523 in 0.64s\n",
      " [-] epoch  188/250, train loss 0.191772 in 0.63s\n",
      " [-] epoch  189/250, train loss 0.202828 in 0.65s\n",
      " [-] epoch  190/250, train loss 0.188556 in 0.66s\n",
      " [-] epoch  191/250, train loss 0.204347 in 0.64s\n",
      " [-] epoch  192/250, train loss 0.203378 in 0.66s\n",
      " [-] epoch  193/250, train loss 0.198037 in 0.67s\n",
      " [-] epoch  194/250, train loss 0.212428 in 0.70s\n",
      " [-] epoch  195/250, train loss 0.214295 in 0.71s\n",
      " [-] epoch  196/250, train loss 0.208756 in 0.67s\n",
      " [-] epoch  197/250, train loss 0.200137 in 0.67s\n",
      " [-] epoch  198/250, train loss 0.229618 in 0.65s\n",
      " [-] epoch  199/250, train loss 0.207485 in 0.67s\n",
      " [-] epoch  200/250, train loss 0.229762 in 0.66s\n",
      " [-] epoch  201/250, train loss 0.199144 in 0.68s\n",
      " [-] epoch  202/250, train loss 0.221109 in 0.65s\n",
      " [-] epoch  203/250, train loss 0.224342 in 0.63s\n",
      " [-] epoch  204/250, train loss 0.198324 in 0.64s\n",
      " [-] epoch  205/250, train loss 0.202090 in 0.63s\n",
      " [-] epoch  206/250, train loss 0.209954 in 0.61s\n",
      " [-] epoch  207/250, train loss 0.219596 in 0.66s\n",
      " [-] epoch  208/250, train loss 0.207544 in 0.69s\n",
      " [-] epoch  209/250, train loss 0.211461 in 0.68s\n",
      " [-] epoch  210/250, train loss 0.204275 in 0.65s\n",
      " [-] epoch  211/250, train loss 0.203228 in 0.62s\n",
      " [-] epoch  212/250, train loss 0.203832 in 0.66s\n",
      " [-] epoch  213/250, train loss 0.191327 in 0.65s\n",
      " [-] epoch  214/250, train loss 0.194773 in 0.64s\n",
      " [-] epoch  215/250, train loss 0.185193 in 0.63s\n",
      " [-] epoch  216/250, train loss 0.204839 in 0.63s\n",
      " [-] epoch  217/250, train loss 0.216171 in 0.63s\n",
      " [-] epoch  218/250, train loss 0.223021 in 0.62s\n",
      " [-] epoch  219/250, train loss 0.207417 in 0.66s\n",
      " [-] epoch  220/250, train loss 0.196738 in 0.65s\n",
      " [-] epoch  221/250, train loss 0.201172 in 0.65s\n",
      " [-] epoch  222/250, train loss 0.210403 in 0.65s\n",
      " [-] epoch  223/250, train loss 0.201821 in 0.64s\n",
      " [-] epoch  224/250, train loss 0.200721 in 0.65s\n",
      " [-] epoch  225/250, train loss 0.212590 in 0.63s\n",
      " [-] epoch  226/250, train loss 0.215002 in 0.67s\n",
      " [-] epoch  227/250, train loss 0.210116 in 0.63s\n",
      " [-] epoch  228/250, train loss 0.215797 in 0.64s\n",
      " [-] epoch  229/250, train loss 0.211706 in 0.66s\n",
      " [-] epoch  230/250, train loss 0.205333 in 0.65s\n",
      " [-] epoch  231/250, train loss 0.229323 in 0.65s\n",
      " [-] epoch  232/250, train loss 0.191890 in 0.64s\n",
      " [-] epoch  233/250, train loss 0.204198 in 0.64s\n",
      " [-] epoch  234/250, train loss 0.193622 in 0.65s\n",
      " [-] epoch  235/250, train loss 0.202900 in 0.64s\n",
      " [-] epoch  236/250, train loss 0.203300 in 0.64s\n",
      " [-] epoch  237/250, train loss 0.226142 in 0.64s\n",
      " [-] epoch  238/250, train loss 0.211688 in 0.66s\n",
      " [-] epoch  239/250, train loss 0.202726 in 0.66s\n",
      " [-] epoch  240/250, train loss 0.199189 in 0.67s\n",
      " [-] epoch  241/250, train loss 0.193989 in 0.66s\n",
      " [-] epoch  242/250, train loss 0.208169 in 0.64s\n",
      " [-] epoch  243/250, train loss 0.208205 in 0.67s\n",
      " [-] epoch  244/250, train loss 0.197655 in 0.67s\n",
      " [-] epoch  245/250, train loss 0.215348 in 0.67s\n",
      " [-] epoch  246/250, train loss 0.215167 in 0.66s\n",
      " [-] epoch  247/250, train loss 0.214619 in 0.65s\n",
      " [-] epoch  248/250, train loss 0.192809 in 0.65s\n",
      " [-] epoch  249/250, train loss 0.195971 in 0.65s\n",
      " [-] epoch  250/250, train loss 0.191923 in 0.67s\n",
      " [-] test acc. 84.166667%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_linear_results(X_train, X_test) # all dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calcul du taux de réussite en classement d'un réseau linéaire multicouches sur les données réduites\n",
    "Cette deuxième étape consiste à avoir les résultats sur les dimensions réduites engendrées par la réduction de dimensionnalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reducedData = numpy.load('labeled_reduit_100dim.npy')\n",
    "numberOfData = reducedData.shape[0]\n",
    "dimensions = reducedData.shape[1]\n",
    "\n",
    "#print(numberOfData, dimensions)\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train_reduced = minmax_scale(reducedData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test_reduced = minmax_scale(reducedData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_linear_results(X_train_reduced, X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification avec fonction d'activation elu à 9 couches: +- 70% de taux de réussite\n",
    "\n",
    "Classification avec fonction d'activation relu à 9 couches: +- 50% de taux de réussite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenir compte du fait que l'on n'a pas beaucoup de données considérant le nombre de dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Expérimentations - Déclaration d'un réseau de neurones de type convolution\n",
    "Cette partie agit à titre d'expérimentation.\n",
    "\n",
    "Le réseau de neurones à convolution est facile à paramétrer: il est possible d'y indiquer, en paramètres d'entrée, le nombre de dimensions en input, le nombre de convolutions à avoir dans le réseau en question et le type de la fonction d'activation qui y sera relié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau par convolution permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numberOfFilters):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.numberOfFilters = numberOfFilters\n",
    "        \n",
    "        self.C1 = nn.Conv1d(1, 32, kernel_size=251, stride=2, padding=125, bias=False)\n",
    "        self.B1 = nn.BatchNorm1d(32)\n",
    "        self.C2 = nn.Conv1d(32, 64, kernel_size=175, stride=2, padding=87, bias=False)\n",
    "        \n",
    "        self.B = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.C = nn.Conv1d(64, 64, kernel_size=175, stride=2, padding=87, bias=False)\n",
    "        \n",
    "        self.activation = nn.ELU()\n",
    "        self.F1 = nn.Linear(64, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Sélectionne la taille batch à l'entrée\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.activation(self.B1(self.C1(x)))\n",
    "        x = self.activation(self.B(self.C2(x)))\n",
    "        \n",
    "        for i in range(self.numberOfFilters):\n",
    "            x = self.activation(self.B(self.C(x)))\n",
    "        \n",
    "        # Fait un average pooling sur les caractéristiques\n",
    "        # de chaque filtre\n",
    "        x = x.view(batch_size, 64, -1).mean(dim=2)\n",
    "        \n",
    "        # Couche lineaire et sigmoide\n",
    "        x = self.F1(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Déclaration de la fonction permettant la classification par réseau de neurones profond de type convolution\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_for_convolution(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        values, targets = batch\n",
    "        values = values.unsqueeze(1)\n",
    "        values = values.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(values)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()\n",
    "\n",
    "def compute_convolution_results(X_train, X_test):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 10\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "    \n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    test_accu.append(0)\n",
    "    \n",
    "    for i in range(1, 10):\n",
    "        print(\"Je vais ajouter \" + str(i) + \" filters\")\n",
    "        \n",
    "        # Instancier un réseau RAMQNetConvolution\n",
    "        # dans une variable nommée \"model\"\n",
    "        model = RAMQNetConvolution(i)\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "\n",
    "                values = values.unsqueeze(1)\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy_for_convolution(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calcul du taux de réussite en classement d'un réseau de convolution sur les données non réduites - référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais ajouter 6 filters\n",
      " [-] epoch    1/10, train loss 0.484058 in 4.02s\n",
      " [-] epoch    2/10, train loss 0.381153 in 3.96s\n",
      " [-] epoch    3/10, train loss 0.364269 in 4.00s\n",
      " [-] epoch    4/10, train loss 0.306918 in 4.01s\n",
      " [-] epoch    5/10, train loss 0.278638 in 4.04s\n",
      " [-] epoch    6/10, train loss 0.272154 in 4.03s\n",
      " [-] epoch    7/10, train loss 0.286477 in 4.03s\n",
      " [-] epoch    8/10, train loss 0.253794 in 4.01s\n",
      " [-] epoch    9/10, train loss 0.275955 in 4.03s\n",
      " [-] epoch   10/10, train loss 0.255494 in 4.04s\n",
      " [-] test acc. 82.777778%\n",
      "Je vais ajouter 7 filters\n",
      " [-] epoch    1/10, train loss 0.524414 in 4.28s\n",
      " [-] epoch    2/10, train loss 0.406604 in 4.29s\n",
      " [-] epoch    3/10, train loss 0.385529 in 4.30s\n",
      " [-] epoch    4/10, train loss 0.335149 in 4.31s\n",
      " [-] epoch    5/10, train loss 0.304158 in 4.33s\n",
      " [-] epoch    6/10, train loss 0.285415 in 4.32s\n",
      " [-] epoch    7/10, train loss 0.303692 in 4.32s\n",
      " [-] epoch    8/10, train loss 0.293430 in 4.35s\n",
      " [-] epoch    9/10, train loss 0.240349 in 4.32s\n",
      " [-] epoch   10/10, train loss 0.239113 in 4.29s\n",
      " [-] test acc. 85.000000%\n",
      "Je vais ajouter 8 filters\n",
      " [-] epoch    1/10, train loss 0.540620 in 4.50s\n",
      " [-] epoch    2/10, train loss 0.439046 in 4.51s\n",
      " [-] epoch    3/10, train loss 0.355697 in 4.55s\n",
      " [-] epoch    4/10, train loss 0.323742 in 4.61s\n",
      " [-] epoch    5/10, train loss 0.269040 in 4.57s\n",
      " [-] epoch    6/10, train loss 0.294889 in 4.57s\n",
      " [-] epoch    7/10, train loss 0.290408 in 4.62s\n",
      " [-] epoch    8/10, train loss 0.242971 in 4.59s\n",
      " [-] epoch    9/10, train loss 0.240234 in 4.59s\n",
      " [-] epoch   10/10, train loss 0.258803 in 4.63s\n",
      " [-] test acc. 83.888889%\n",
      "Je vais ajouter 9 filters\n",
      " [-] epoch    1/10, train loss 0.538614 in 4.86s\n",
      " [-] epoch    2/10, train loss 0.390530 in 4.88s\n",
      " [-] epoch    3/10, train loss 0.341876 in 4.83s\n",
      " [-] epoch    4/10, train loss 0.312966 in 4.88s\n",
      " [-] epoch    5/10, train loss 0.325115 in 4.92s\n",
      " [-] epoch    6/10, train loss 0.305183 in 4.82s\n",
      " [-] epoch    7/10, train loss 0.266637 in 4.81s\n",
      " [-] epoch    8/10, train loss 0.259080 in 4.83s\n",
      " [-] epoch    9/10, train loss 0.261704 in 4.83s\n",
      " [-] epoch   10/10, train loss 0.206695 in 4.88s\n",
      " [-] test acc. 81.944444%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_convolution_results(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais ajouter 1 filters\n",
      " [-] epoch    1/50, train loss 0.644824 in 3.62s\n",
      " [-] epoch    2/50, train loss 0.623846 in 3.55s\n",
      " [-] epoch    3/50, train loss 0.609674 in 3.64s\n",
      " [-] epoch    4/50, train loss 0.591891 in 4.23s\n",
      " [-] epoch    5/50, train loss 0.569576 in 4.30s\n",
      " [-] epoch    6/50, train loss 0.568190 in 3.67s\n",
      " [-] epoch    7/50, train loss 0.534449 in 4.10s\n",
      " [-] epoch    8/50, train loss 0.550036 in 3.78s\n",
      " [-] epoch    9/50, train loss 0.563778 in 3.72s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1d07223a5811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_convolution_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_reduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-ce8f6392cd94>\u001b[0m in \u001b[0;36mcompute_convolution_results\u001b[0;34m(X_train, X_test)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;31m# 1. l'inférence dans une variable \"predictions\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;31m# 2. l'erreur dans une variable \"loss\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/machine-learning/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-60515366930a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumberOfFilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/machine-learning/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/machine-learning/venv/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compute_convolution_results(X_train_reduced, X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning pour Classification\n",
    "\n",
    "Ce notebook regroupe quelques configurations d'algorithmes de Deep Learning pour la classification de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Variable                        Possibilités             Nombre de colonne (+1)\n",
      "Spécialité de médecins\t        33 codes de spécialité\t        1-33\n",
      "Sexe de médecin \t            2\t                            34-35\n",
      "Langue de correspondance \t    2\t                            36-37\n",
      "Université de graduation \t    14 universités\t                38-51\n",
      "Plage horaire de facturation\t3 (AM, PM, toute la journée)\t52-54\n",
      "Agence de représentation\t    845 codes d’agences             55-899\n",
      "Établissements                  241 établissements\t            900-1140\n",
      "ActeMedical                     2 (Oui et Non)\t                1141-1142\n",
      "Activités médico-admin          2 (Oui et Non)\t                1143-1144\n",
      "Activités d’enseignement\t    2 (Oui et Non)\t                1145-1146\n",
      "Entente LE\t                    2 (Oui et Non)\t                1147-1148\n",
      "Activité en santé communautaire\t2 (Oui et Non)\t                1149-1150\n",
      "Activité en santé comm. CSST\t2 (Oui et Non)\t                1151-1152\n",
      "Activité en santé comm. INSP\t2 (Oui et Non)\t                1153-1154\n",
      "Année de graduation\t                                            1155\n",
      "semaine de l’année associée à la facturation\t                1156\n",
      "Nombre de jours dès la dernière facture avant le Perdiem\t    1157\n",
      "Année de naissance\t                                            1158\n",
      "Année de début de pratique\t                                    1159\n",
      "Expérience\t                                                    1160\n",
      "Salaire régulier\t                                            1161\n",
      "Salaire payé\t                                                1162\n",
      "Nombre de Perdiem pendant l’année\t                            1163\n",
      "Jour de la semaine associé à la facturation\t                    1164\n",
      "Nombre de Perdiem\t                                            1165\n",
      "Nombre d’heures facturées\t                                    1166\n",
      "Nombre de services avec bénéficiaire\t                        1167\n",
      "Nombre de services sans bénéficiaire\t                        1168\n",
      "Nombre de patients\t                                            1169\n",
      "Montant réclamé avec bénéficiaire\t                            1170\n",
      "Montant réclamé sans bénéficiaire\t                            1171\n",
      "Nombre de service total\t                                        1172\n",
      "Nombre de Perdiem facturé le jour avant\t                        1173\n",
      "Nombre d’heures facturées le jour avant\t                        1174\n",
      "Nombre de services avec bénéficiaire le jour avant\t            1175\n",
      "Nombre de services sans bénéficiaire le jour avant\t            1176\n",
      "Nombre de patients le jour avant\t                            1177\n",
      "Montant réclamé avec bénéficiaire le jour avant\t                1178\n",
      "Montant réclamé sans bénéficiaire le jour avant\t                1179\n",
      "Nombre de services total le jour avant\t                        1180\n",
      "âge de médecin le jour avant \t                                1181\n",
      "Nombre de Perdiem facturés la semaine d’avant\t                1182\n",
      "\n",
      "Target:\n",
      "Conformité\n",
      "(-1 : non contrôlé, 1 : cas conforme, 0 : cas non-conforme)\t    1183\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enregistrement du jeu de données comportant les données avec étiquettes\n",
    "Ici, le jeu de données comportant uniquement les données avec des étiquettes de classe est téléchargé. Il est ensuite séparé en jeux d'entraînement et de test en plus d'être normalisé dans toutes les dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData = numpy.load('labeled.npy')\n",
    "numberOfData = labeledData.shape[0] # 1441\n",
    "dimensions = labeledData.shape[1] # 1183\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train = minmax_scale(labeledData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test = minmax_scale(labeledData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Déclaration de fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_sampler(dataset):\n",
    "    def make_weights_for_balanced_classes(images, n_classes):                        \n",
    "        count = [0] * n_classes                                                      \n",
    "        for item in images:                                                         \n",
    "            count[int(item[1])] += 1                                                     \n",
    "        weight_per_class = [0.] * n_classes                                      \n",
    "        N = float(sum(count))                                                   \n",
    "        for i in range(n_classes):                                                   \n",
    "            weight_per_class[i] = N/float(count[i])                                 \n",
    "        weight = [0] * len(images)                                              \n",
    "        for idx, val in enumerate(images):                                          \n",
    "            weight[idx] = weight_per_class[int(val[1])]                                  \n",
    "        return weight\n",
    "\n",
    "    n_classes = numpy.unique(dataset.targets)\n",
    "    weights = make_weights_for_balanced_classes(dataset.data, len(n_classes))                                                         \n",
    "    weights = torch.DoubleTensor(weights)                 \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "    return sampler\n",
    "\n",
    "def compute_accuracy(model, dataloader, device='cpu'):\n",
    "    training_before = model.training\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if all_predictions[0].shape[-1] > 1:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions, axis=0)\n",
    "        predictions_numpy = predictions_numpy.argmax(axis=1)\n",
    "        targets_numpy = numpy.concatenate(all_targets, axis=0)\n",
    "    else:\n",
    "        predictions_numpy = numpy.concatenate(all_predictions).squeeze(-1)\n",
    "        targets_numpy = numpy.concatenate(all_targets)\n",
    "        predictions_numpy[predictions_numpy >= 0.5] = 1.0\n",
    "        predictions_numpy[predictions_numpy < 0.5] = 0.0\n",
    "\n",
    "    if training_before:\n",
    "        model.train()\n",
    "\n",
    "    return (predictions_numpy == targets_numpy).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définition du Dataset\n",
    "Définition de la classe RAMQDataset, une classe qui hérite de la classe abstraite torch.utils.data.Dataset. Comme mentionné dans la documentation, les méthodes __getitem__ et __len__ sont surchargées afin d'avoir un jeu de données utilisable par PyTorch. Le data accepté en paramètres est un array numpy dont la dernière dimension est la valeur de l'étiquette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Cette classe sert à définir le dataset RAMQ pour PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for elem in data:\n",
    "            if isinstance(elem, numpy.ndarray):\n",
    "                elem = elem.tolist()\n",
    "                elem_data = torch.Tensor(elem[:-1])\n",
    "                elem_target = torch.FloatTensor([elem[-1]])\n",
    "            # garde les paramètres en mémoire\n",
    "            self.data += [(elem_data, elem_target[0])]\n",
    "            \n",
    "        self.targets = numpy.array(data[:, -1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Déclaration d'un réseau de neurones de base: 1 couche - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNetBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.F1 = nn.Linear(1182, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.F1(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Déclaration d'un réseau de neurones de type linéraire: multicouches\n",
    "Ce réseau est personnalisable à souhait: il est possible d'y indiquer, en paramètres d'entrée, le nombre de dimensions en input, le nombre de couches linéaires à avoir dans le réseau en question et le type de la fonction d'activation qui y sera relié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAMQNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe définit un réseau permettant de classifier des données de la RAMQ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_layers, activation = nn.ELU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        reduction_ratio = (1/n_inputs)**(1/n_layers)\n",
    "        \n",
    "        layers = []\n",
    "        layer_lenght = n_inputs\n",
    "        \n",
    "        # Calculate nb of layers and nb of neurons\n",
    "        # Builds a list (ex:[1000, 500, 250, ... until 1])\n",
    "        while layer_lenght > 1:\n",
    "            layers.append(layer_lenght)\n",
    "            layer_lenght = int(layer_lenght*reduction_ratio)\n",
    "        layers.append(1)\n",
    "        \n",
    "        # Build network layers\n",
    "        network_layers = []\n",
    "        for i, n_neurons in enumerate(layers[:-1]):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            network_layers.append(layer)\n",
    "            network_layers.append(activation)\n",
    "            network_layers.append(nn.BatchNorm1d(layers[i+1]))\n",
    "        network_layers[-1] = nn.Sigmoid()\n",
    "            \n",
    "        # Build pytorch sequential network\n",
    "        self.network = nn.Sequential(\n",
    "            *network_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Exécute l'inférence du réseau. L'ordre \n",
    "        # d'exécution ici est important.\n",
    "        x = self.network(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Déclaration de la fonction permettant l'affichage du pourcentage d'efficacité en classement selon 1 à 9 couches d'un réseau de neurones \"x\"\n",
    "Cette méthode n'a besoin, en entrées, que d'un tableau des pourcentages d'efficacité pour 0 à 9 couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(test_accu):\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(test_accu, label='Test accuracy')\n",
    "    ax.set_xlim(1, 9)\n",
    "    pyplot.grid()\n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Déclaration de la fonction permettant la classification par réseau de neurones profond\n",
    "Cette méthode n'a besoin, en entrées, que des jeux d'entraînement et de test normalisé, dont la dernière dimension de chaque donnée constitue la valeur de l'étiquette reliée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(X_train, X_test):\n",
    "    # Définit si cuda est utilisé ou non\n",
    "    # mettre cuda pour utiliser un GPU\n",
    "    device = \"cpu\" #if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Définit les paramètres d'entraînement \n",
    "    nb_epoch = 250\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "\n",
    "    # Charge les données d'entraînement et de test\n",
    "    train_set = RAMQDataset(X_train)\n",
    "    test_set = RAMQDataset(X_test)\n",
    "\n",
    "    # Crée le sampler avec les classes balancées\n",
    "    balanced_train_sampler = create_balanced_sampler(train_set)\n",
    "    balanced_test_sampler = create_balanced_sampler(test_set)\n",
    "\n",
    "    # Crée le dataloader d'entraînement\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=balanced_train_sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, sampler=balanced_test_sampler)\n",
    "\n",
    "    test_accu = []\n",
    "    test_accu.append(0)\n",
    "\n",
    "    # Instancier un réseau RAMQNet\n",
    "    # dans une variable nommée \"model\"\n",
    "    for i in range(1,7):\n",
    "        print(\"Je vais utiliser \" + str(i) + \" layers\")\n",
    "        model = RAMQNet(1182, i)\n",
    "\n",
    "        # Tranfert le réseau au bon endroit\n",
    "        model.to(device)\n",
    "\n",
    "        # Instancier une fonction d'erreur BinaryCrossEntropy\n",
    "        # et la mettre dans une variable nommée criterion\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # Instancier l'algorithme d'optimisation SGD\n",
    "        # Ne pas oublier de lui donner les hyperparamètres\n",
    "        # d'entraînement : learning rate et momentum!\n",
    "        optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        # Mettre le réseau en mode entraînement\n",
    "        model.train()\n",
    "\n",
    "        for i_epoch in range(nb_epoch):\n",
    "\n",
    "            start_time, train_losses = time.time(), []\n",
    "            for i_batch, batch in enumerate(train_loader):\n",
    "                values, targets = batch\n",
    "                targets = targets.type(torch.FloatTensor).unsqueeze(-1)\n",
    "\n",
    "                values = values.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Mettre les gradients à zéro\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Calculer:\n",
    "                # 1. l'inférence dans une variable \"predictions\"\n",
    "                # 2. l'erreur dans une variable \"loss\"\n",
    "                predictions = model(values)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                # Rétropropager l'erreur et effectuer\n",
    "                # une étape d'optimisation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Ajoute le loss de la batch\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            print(' [-] epoch {:4}/{:}, train loss {:.6f} in {:.2f}s'.format(\n",
    "            i_epoch+1, nb_epoch, numpy.mean(train_losses), time.time()-start_time))\n",
    "\n",
    "        # affiche le score à l'écran\n",
    "        test_acc = compute_accuracy(model, test_loader, device)\n",
    "        print(' [-] test acc. {:.6f}%'.format(test_acc * 100))\n",
    "        test_accu.append(test_acc)\n",
    "    plot_results(test_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calcul du taux de réussite en classement d'un SVM linéaire de base sur les données non réduites - pour référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on all dimensions: 0.8361111111111111\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM performance with all dimensions\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train[:, : -1], X_train[:, -1])\n",
    "score = clf.score(X_test[:, : -1], X_test[:, -1])\n",
    "print(\"Score on all dimensions:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on tente de classifier les 1441 données à 1182 dimensions avec un réseau de neurones à 1 couche linéaire, on obtient un pourcentage de classement de l'ordre d'environ 82%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(X_train, X_test) # all dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification avec fonction d'activation elu à 9 couches: +- 82% de taux de réussite\n",
    "\n",
    "Classification avec fonction d'activation relu à 9 couches: +- 50% de taux de réussite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette deuxième étape consiste à avoir les résultats sur les dimensions réduites engendrées par la réduction de dimensionnalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais utiliser 1 layers\n",
      " [-] epoch    1/250, train loss 0.681174 in 0.03s\n",
      " [-] epoch    2/250, train loss 0.649854 in 0.03s\n",
      " [-] epoch    3/250, train loss 0.636645 in 0.03s\n",
      " [-] epoch    4/250, train loss 0.625682 in 0.03s\n",
      " [-] epoch    5/250, train loss 0.607701 in 0.03s\n",
      " [-] epoch    6/250, train loss 0.589374 in 0.03s\n",
      " [-] epoch    7/250, train loss 0.594402 in 0.03s\n",
      " [-] epoch    8/250, train loss 0.573142 in 0.03s\n",
      " [-] epoch    9/250, train loss 0.566357 in 0.03s\n",
      " [-] epoch   10/250, train loss 0.565336 in 0.03s\n",
      " [-] epoch   11/250, train loss 0.557530 in 0.03s\n",
      " [-] epoch   12/250, train loss 0.559920 in 0.03s\n",
      " [-] epoch   13/250, train loss 0.527116 in 0.03s\n",
      " [-] epoch   14/250, train loss 0.532925 in 0.03s\n",
      " [-] epoch   15/250, train loss 0.548208 in 0.03s\n",
      " [-] epoch   16/250, train loss 0.534821 in 0.03s\n",
      " [-] epoch   17/250, train loss 0.529055 in 0.03s\n",
      " [-] epoch   18/250, train loss 0.516233 in 0.03s\n",
      " [-] epoch   19/250, train loss 0.534101 in 0.03s\n",
      " [-] epoch   20/250, train loss 0.499485 in 0.03s\n",
      " [-] epoch   21/250, train loss 0.515026 in 0.03s\n",
      " [-] epoch   22/250, train loss 0.512213 in 0.03s\n",
      " [-] epoch   23/250, train loss 0.509743 in 0.03s\n",
      " [-] epoch   24/250, train loss 0.501074 in 0.03s\n",
      " [-] epoch   25/250, train loss 0.493313 in 0.03s\n",
      " [-] epoch   26/250, train loss 0.497127 in 0.03s\n",
      " [-] epoch   27/250, train loss 0.498737 in 0.03s\n",
      " [-] epoch   28/250, train loss 0.497857 in 0.03s\n",
      " [-] epoch   29/250, train loss 0.492574 in 0.03s\n",
      " [-] epoch   30/250, train loss 0.474175 in 0.03s\n",
      " [-] epoch   31/250, train loss 0.479928 in 0.03s\n",
      " [-] epoch   32/250, train loss 0.484841 in 0.03s\n",
      " [-] epoch   33/250, train loss 0.485532 in 0.03s\n",
      " [-] epoch   34/250, train loss 0.493290 in 0.03s\n",
      " [-] epoch   35/250, train loss 0.487887 in 0.03s\n",
      " [-] epoch   36/250, train loss 0.503168 in 0.03s\n",
      " [-] epoch   37/250, train loss 0.483468 in 0.03s\n",
      " [-] epoch   38/250, train loss 0.476239 in 0.03s\n",
      " [-] epoch   39/250, train loss 0.456520 in 0.03s\n",
      " [-] epoch   40/250, train loss 0.466116 in 0.03s\n",
      " [-] epoch   41/250, train loss 0.489179 in 0.03s\n",
      " [-] epoch   42/250, train loss 0.474235 in 0.03s\n",
      " [-] epoch   43/250, train loss 0.451401 in 0.03s\n",
      " [-] epoch   44/250, train loss 0.466057 in 0.03s\n",
      " [-] epoch   45/250, train loss 0.453138 in 0.03s\n",
      " [-] epoch   46/250, train loss 0.477874 in 0.03s\n",
      " [-] epoch   47/250, train loss 0.447499 in 0.03s\n",
      " [-] epoch   48/250, train loss 0.461487 in 0.03s\n",
      " [-] epoch   49/250, train loss 0.474610 in 0.03s\n",
      " [-] epoch   50/250, train loss 0.463851 in 0.03s\n",
      " [-] epoch   51/250, train loss 0.457609 in 0.03s\n",
      " [-] epoch   52/250, train loss 0.470263 in 0.03s\n",
      " [-] epoch   53/250, train loss 0.465791 in 0.03s\n",
      " [-] epoch   54/250, train loss 0.462199 in 0.03s\n",
      " [-] epoch   55/250, train loss 0.469873 in 0.03s\n",
      " [-] epoch   56/250, train loss 0.458897 in 0.03s\n",
      " [-] epoch   57/250, train loss 0.455655 in 0.03s\n",
      " [-] epoch   58/250, train loss 0.464239 in 0.03s\n",
      " [-] epoch   59/250, train loss 0.454462 in 0.03s\n",
      " [-] epoch   60/250, train loss 0.461991 in 0.03s\n",
      " [-] epoch   61/250, train loss 0.466826 in 0.03s\n",
      " [-] epoch   62/250, train loss 0.458982 in 0.03s\n",
      " [-] epoch   63/250, train loss 0.466480 in 0.03s\n",
      " [-] epoch   64/250, train loss 0.445152 in 0.03s\n",
      " [-] epoch   65/250, train loss 0.457465 in 0.03s\n",
      " [-] epoch   66/250, train loss 0.451175 in 0.03s\n",
      " [-] epoch   67/250, train loss 0.451069 in 0.03s\n",
      " [-] epoch   68/250, train loss 0.438747 in 0.03s\n",
      " [-] epoch   69/250, train loss 0.446929 in 0.03s\n",
      " [-] epoch   70/250, train loss 0.464148 in 0.03s\n",
      " [-] epoch   71/250, train loss 0.442623 in 0.03s\n",
      " [-] epoch   72/250, train loss 0.443824 in 0.03s\n",
      " [-] epoch   73/250, train loss 0.446317 in 0.03s\n",
      " [-] epoch   74/250, train loss 0.439302 in 0.03s\n",
      " [-] epoch   75/250, train loss 0.436553 in 0.03s\n",
      " [-] epoch   76/250, train loss 0.449213 in 0.03s\n",
      " [-] epoch   77/250, train loss 0.436147 in 0.03s\n",
      " [-] epoch   78/250, train loss 0.449429 in 0.03s\n",
      " [-] epoch   79/250, train loss 0.466170 in 0.03s\n",
      " [-] epoch   80/250, train loss 0.437322 in 0.03s\n",
      " [-] epoch   81/250, train loss 0.452213 in 0.03s\n",
      " [-] epoch   82/250, train loss 0.463673 in 0.03s\n",
      " [-] epoch   83/250, train loss 0.432638 in 0.03s\n",
      " [-] epoch   84/250, train loss 0.455159 in 0.03s\n",
      " [-] epoch   85/250, train loss 0.441162 in 0.03s\n",
      " [-] epoch   86/250, train loss 0.451898 in 0.03s\n",
      " [-] epoch   87/250, train loss 0.424886 in 0.03s\n",
      " [-] epoch   88/250, train loss 0.431143 in 0.03s\n",
      " [-] epoch   89/250, train loss 0.422968 in 0.03s\n",
      " [-] epoch   90/250, train loss 0.440993 in 0.03s\n",
      " [-] epoch   91/250, train loss 0.452096 in 0.03s\n",
      " [-] epoch   92/250, train loss 0.428855 in 0.03s\n",
      " [-] epoch   93/250, train loss 0.448802 in 0.03s\n",
      " [-] epoch   94/250, train loss 0.415278 in 0.03s\n",
      " [-] epoch   95/250, train loss 0.430989 in 0.03s\n",
      " [-] epoch   96/250, train loss 0.430482 in 0.03s\n",
      " [-] epoch   97/250, train loss 0.428853 in 0.03s\n",
      " [-] epoch   98/250, train loss 0.438829 in 0.03s\n",
      " [-] epoch   99/250, train loss 0.425096 in 0.03s\n",
      " [-] epoch  100/250, train loss 0.436102 in 0.03s\n",
      " [-] epoch  101/250, train loss 0.416045 in 0.03s\n",
      " [-] epoch  102/250, train loss 0.440936 in 0.03s\n",
      " [-] epoch  103/250, train loss 0.460784 in 0.03s\n",
      " [-] epoch  104/250, train loss 0.414678 in 0.03s\n",
      " [-] epoch  105/250, train loss 0.423688 in 0.03s\n",
      " [-] epoch  106/250, train loss 0.436527 in 0.03s\n",
      " [-] epoch  107/250, train loss 0.413542 in 0.03s\n",
      " [-] epoch  108/250, train loss 0.434767 in 0.03s\n",
      " [-] epoch  109/250, train loss 0.447720 in 0.03s\n",
      " [-] epoch  110/250, train loss 0.419087 in 0.03s\n",
      " [-] epoch  111/250, train loss 0.432645 in 0.03s\n",
      " [-] epoch  112/250, train loss 0.425859 in 0.03s\n",
      " [-] epoch  113/250, train loss 0.423392 in 0.03s\n",
      " [-] epoch  114/250, train loss 0.423961 in 0.03s\n",
      " [-] epoch  115/250, train loss 0.418834 in 0.03s\n",
      " [-] epoch  116/250, train loss 0.431661 in 0.03s\n",
      " [-] epoch  117/250, train loss 0.412657 in 0.03s\n",
      " [-] epoch  118/250, train loss 0.428680 in 0.03s\n",
      " [-] epoch  119/250, train loss 0.440904 in 0.03s\n",
      " [-] epoch  120/250, train loss 0.421565 in 0.03s\n",
      " [-] epoch  121/250, train loss 0.435561 in 0.03s\n",
      " [-] epoch  122/250, train loss 0.414001 in 0.03s\n",
      " [-] epoch  123/250, train loss 0.429340 in 0.03s\n",
      " [-] epoch  124/250, train loss 0.411458 in 0.03s\n",
      " [-] epoch  125/250, train loss 0.434852 in 0.03s\n",
      " [-] epoch  126/250, train loss 0.424964 in 0.03s\n",
      " [-] epoch  127/250, train loss 0.437491 in 0.03s\n",
      " [-] epoch  128/250, train loss 0.412634 in 0.03s\n",
      " [-] epoch  129/250, train loss 0.426478 in 0.03s\n",
      " [-] epoch  130/250, train loss 0.407399 in 0.03s\n",
      " [-] epoch  131/250, train loss 0.426653 in 0.03s\n",
      " [-] epoch  132/250, train loss 0.405534 in 0.03s\n",
      " [-] epoch  133/250, train loss 0.422469 in 0.03s\n",
      " [-] epoch  134/250, train loss 0.409937 in 0.03s\n",
      " [-] epoch  135/250, train loss 0.403668 in 0.03s\n",
      " [-] epoch  136/250, train loss 0.434003 in 0.03s\n",
      " [-] epoch  137/250, train loss 0.408044 in 0.03s\n",
      " [-] epoch  138/250, train loss 0.404672 in 0.03s\n",
      " [-] epoch  139/250, train loss 0.433846 in 0.03s\n",
      " [-] epoch  140/250, train loss 0.405357 in 0.03s\n",
      " [-] epoch  141/250, train loss 0.420390 in 0.03s\n",
      " [-] epoch  142/250, train loss 0.434731 in 0.03s\n",
      " [-] epoch  143/250, train loss 0.418860 in 0.03s\n",
      " [-] epoch  144/250, train loss 0.405235 in 0.03s\n",
      " [-] epoch  145/250, train loss 0.402472 in 0.03s\n",
      " [-] epoch  146/250, train loss 0.436672 in 0.03s\n",
      " [-] epoch  147/250, train loss 0.424930 in 0.03s\n",
      " [-] epoch  148/250, train loss 0.428451 in 0.03s\n",
      " [-] epoch  149/250, train loss 0.391597 in 0.03s\n",
      " [-] epoch  150/250, train loss 0.390341 in 0.03s\n",
      " [-] epoch  151/250, train loss 0.409136 in 0.03s\n",
      " [-] epoch  152/250, train loss 0.429640 in 0.03s\n",
      " [-] epoch  153/250, train loss 0.428478 in 0.03s\n",
      " [-] epoch  154/250, train loss 0.422928 in 0.03s\n",
      " [-] epoch  155/250, train loss 0.411863 in 0.03s\n",
      " [-] epoch  156/250, train loss 0.425258 in 0.03s\n",
      " [-] epoch  157/250, train loss 0.411430 in 0.03s\n",
      " [-] epoch  158/250, train loss 0.411147 in 0.03s\n",
      " [-] epoch  159/250, train loss 0.414859 in 0.03s\n",
      " [-] epoch  160/250, train loss 0.418335 in 0.03s\n",
      " [-] epoch  161/250, train loss 0.423571 in 0.03s\n",
      " [-] epoch  162/250, train loss 0.413770 in 0.03s\n",
      " [-] epoch  163/250, train loss 0.404341 in 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  164/250, train loss 0.413451 in 0.03s\n",
      " [-] epoch  165/250, train loss 0.409731 in 0.03s\n",
      " [-] epoch  166/250, train loss 0.398691 in 0.03s\n",
      " [-] epoch  167/250, train loss 0.415245 in 0.03s\n",
      " [-] epoch  168/250, train loss 0.404444 in 0.03s\n",
      " [-] epoch  169/250, train loss 0.405321 in 0.03s\n",
      " [-] epoch  170/250, train loss 0.418321 in 0.03s\n",
      " [-] epoch  171/250, train loss 0.396513 in 0.03s\n",
      " [-] epoch  172/250, train loss 0.397534 in 0.03s\n",
      " [-] epoch  173/250, train loss 0.419444 in 0.03s\n",
      " [-] epoch  174/250, train loss 0.413334 in 0.03s\n",
      " [-] epoch  175/250, train loss 0.396426 in 0.03s\n",
      " [-] epoch  176/250, train loss 0.397165 in 0.03s\n",
      " [-] epoch  177/250, train loss 0.423761 in 0.03s\n",
      " [-] epoch  178/250, train loss 0.429319 in 0.03s\n",
      " [-] epoch  179/250, train loss 0.405979 in 0.03s\n",
      " [-] epoch  180/250, train loss 0.395398 in 0.03s\n",
      " [-] epoch  181/250, train loss 0.409948 in 0.03s\n",
      " [-] epoch  182/250, train loss 0.407170 in 0.03s\n",
      " [-] epoch  183/250, train loss 0.392045 in 0.03s\n",
      " [-] epoch  184/250, train loss 0.403519 in 0.03s\n",
      " [-] epoch  185/250, train loss 0.409776 in 0.03s\n",
      " [-] epoch  186/250, train loss 0.404990 in 0.03s\n",
      " [-] epoch  187/250, train loss 0.410550 in 0.03s\n",
      " [-] epoch  188/250, train loss 0.424962 in 0.03s\n",
      " [-] epoch  189/250, train loss 0.395187 in 0.03s\n",
      " [-] epoch  190/250, train loss 0.392453 in 0.03s\n",
      " [-] epoch  191/250, train loss 0.411565 in 0.03s\n",
      " [-] epoch  192/250, train loss 0.405803 in 0.03s\n",
      " [-] epoch  193/250, train loss 0.405794 in 0.03s\n",
      " [-] epoch  194/250, train loss 0.382835 in 0.03s\n",
      " [-] epoch  195/250, train loss 0.402708 in 0.03s\n",
      " [-] epoch  196/250, train loss 0.409484 in 0.03s\n",
      " [-] epoch  197/250, train loss 0.393694 in 0.03s\n",
      " [-] epoch  198/250, train loss 0.391909 in 0.03s\n",
      " [-] epoch  199/250, train loss 0.391674 in 0.03s\n",
      " [-] epoch  200/250, train loss 0.408289 in 0.03s\n",
      " [-] epoch  201/250, train loss 0.398310 in 0.03s\n",
      " [-] epoch  202/250, train loss 0.385352 in 0.03s\n",
      " [-] epoch  203/250, train loss 0.393146 in 0.03s\n",
      " [-] epoch  204/250, train loss 0.416032 in 0.03s\n",
      " [-] epoch  205/250, train loss 0.395481 in 0.03s\n",
      " [-] epoch  206/250, train loss 0.398430 in 0.03s\n",
      " [-] epoch  207/250, train loss 0.408290 in 0.03s\n",
      " [-] epoch  208/250, train loss 0.400835 in 0.03s\n",
      " [-] epoch  209/250, train loss 0.400142 in 0.03s\n",
      " [-] epoch  210/250, train loss 0.408214 in 0.03s\n",
      " [-] epoch  211/250, train loss 0.406128 in 0.02s\n",
      " [-] epoch  212/250, train loss 0.403692 in 0.03s\n",
      " [-] epoch  213/250, train loss 0.409333 in 0.03s\n",
      " [-] epoch  214/250, train loss 0.395949 in 0.03s\n",
      " [-] epoch  215/250, train loss 0.386040 in 0.03s\n",
      " [-] epoch  216/250, train loss 0.373271 in 0.03s\n",
      " [-] epoch  217/250, train loss 0.410913 in 0.03s\n",
      " [-] epoch  218/250, train loss 0.390037 in 0.03s\n",
      " [-] epoch  219/250, train loss 0.388511 in 0.03s\n",
      " [-] epoch  220/250, train loss 0.406870 in 0.03s\n",
      " [-] epoch  221/250, train loss 0.425003 in 0.03s\n",
      " [-] epoch  222/250, train loss 0.412422 in 0.03s\n",
      " [-] epoch  223/250, train loss 0.401754 in 0.03s\n",
      " [-] epoch  224/250, train loss 0.396161 in 0.03s\n",
      " [-] epoch  225/250, train loss 0.406004 in 0.03s\n",
      " [-] epoch  226/250, train loss 0.396976 in 0.03s\n",
      " [-] epoch  227/250, train loss 0.376947 in 0.03s\n",
      " [-] epoch  228/250, train loss 0.388864 in 0.03s\n",
      " [-] epoch  229/250, train loss 0.391992 in 0.03s\n",
      " [-] epoch  230/250, train loss 0.384232 in 0.03s\n",
      " [-] epoch  231/250, train loss 0.406996 in 0.03s\n",
      " [-] epoch  232/250, train loss 0.389963 in 0.03s\n",
      " [-] epoch  233/250, train loss 0.408970 in 0.03s\n",
      " [-] epoch  234/250, train loss 0.404527 in 0.03s\n",
      " [-] epoch  235/250, train loss 0.378223 in 0.03s\n",
      " [-] epoch  236/250, train loss 0.426960 in 0.03s\n",
      " [-] epoch  237/250, train loss 0.400394 in 0.03s\n",
      " [-] epoch  238/250, train loss 0.412722 in 0.03s\n",
      " [-] epoch  239/250, train loss 0.383099 in 0.03s\n",
      " [-] epoch  240/250, train loss 0.393671 in 0.03s\n",
      " [-] epoch  241/250, train loss 0.371934 in 0.03s\n",
      " [-] epoch  242/250, train loss 0.399252 in 0.03s\n",
      " [-] epoch  243/250, train loss 0.415087 in 0.03s\n",
      " [-] epoch  244/250, train loss 0.391200 in 0.03s\n",
      " [-] epoch  245/250, train loss 0.398924 in 0.03s\n",
      " [-] epoch  246/250, train loss 0.390147 in 0.03s\n",
      " [-] epoch  247/250, train loss 0.414034 in 0.03s\n",
      " [-] epoch  248/250, train loss 0.390874 in 0.03s\n",
      " [-] epoch  249/250, train loss 0.398504 in 0.03s\n",
      " [-] epoch  250/250, train loss 0.370277 in 0.03s\n",
      " [-] test acc. 79.722222%\n",
      "Je vais utiliser 2 layers\n",
      " [-] epoch    1/250, train loss 0.562596 in 0.08s\n",
      " [-] epoch    2/250, train loss 0.496429 in 0.08s\n",
      " [-] epoch    3/250, train loss 0.432346 in 0.07s\n",
      " [-] epoch    4/250, train loss 0.406772 in 0.08s\n",
      " [-] epoch    5/250, train loss 0.412825 in 0.07s\n",
      " [-] epoch    6/250, train loss 0.433424 in 0.08s\n",
      " [-] epoch    7/250, train loss 0.431469 in 0.07s\n",
      " [-] epoch    8/250, train loss 0.391425 in 0.08s\n",
      " [-] epoch    9/250, train loss 0.393916 in 0.08s\n",
      " [-] epoch   10/250, train loss 0.392822 in 0.07s\n",
      " [-] epoch   11/250, train loss 0.402804 in 0.06s\n",
      " [-] epoch   12/250, train loss 0.364458 in 0.07s\n",
      " [-] epoch   13/250, train loss 0.363819 in 0.07s\n",
      " [-] epoch   14/250, train loss 0.406710 in 0.07s\n",
      " [-] epoch   15/250, train loss 0.333207 in 0.07s\n",
      " [-] epoch   16/250, train loss 0.369067 in 0.07s\n",
      " [-] epoch   17/250, train loss 0.390157 in 0.07s\n",
      " [-] epoch   18/250, train loss 0.348584 in 0.07s\n",
      " [-] epoch   19/250, train loss 0.385565 in 0.07s\n",
      " [-] epoch   20/250, train loss 0.373761 in 0.07s\n",
      " [-] epoch   21/250, train loss 0.363643 in 0.08s\n",
      " [-] epoch   22/250, train loss 0.377068 in 0.08s\n",
      " [-] epoch   23/250, train loss 0.371496 in 0.08s\n",
      " [-] epoch   24/250, train loss 0.356405 in 0.08s\n",
      " [-] epoch   25/250, train loss 0.370884 in 0.08s\n",
      " [-] epoch   26/250, train loss 0.357751 in 0.08s\n",
      " [-] epoch   27/250, train loss 0.372666 in 0.08s\n",
      " [-] epoch   28/250, train loss 0.369675 in 0.07s\n",
      " [-] epoch   29/250, train loss 0.370091 in 0.08s\n",
      " [-] epoch   30/250, train loss 0.379059 in 0.08s\n",
      " [-] epoch   31/250, train loss 0.334211 in 0.08s\n",
      " [-] epoch   32/250, train loss 0.364838 in 0.08s\n",
      " [-] epoch   33/250, train loss 0.362734 in 0.08s\n",
      " [-] epoch   34/250, train loss 0.336094 in 0.07s\n",
      " [-] epoch   35/250, train loss 0.335320 in 0.08s\n",
      " [-] epoch   36/250, train loss 0.344922 in 0.07s\n",
      " [-] epoch   37/250, train loss 0.376409 in 0.08s\n",
      " [-] epoch   38/250, train loss 0.368276 in 0.08s\n",
      " [-] epoch   39/250, train loss 0.364070 in 0.08s\n",
      " [-] epoch   40/250, train loss 0.350962 in 0.07s\n",
      " [-] epoch   41/250, train loss 0.343544 in 0.07s\n",
      " [-] epoch   42/250, train loss 0.332653 in 0.07s\n",
      " [-] epoch   43/250, train loss 0.352152 in 0.07s\n",
      " [-] epoch   44/250, train loss 0.346262 in 0.07s\n",
      " [-] epoch   45/250, train loss 0.334147 in 0.08s\n",
      " [-] epoch   46/250, train loss 0.360150 in 0.08s\n",
      " [-] epoch   47/250, train loss 0.335238 in 0.08s\n",
      " [-] epoch   48/250, train loss 0.345407 in 0.08s\n",
      " [-] epoch   49/250, train loss 0.337034 in 0.08s\n",
      " [-] epoch   50/250, train loss 0.338828 in 0.08s\n",
      " [-] epoch   51/250, train loss 0.314785 in 0.07s\n",
      " [-] epoch   52/250, train loss 0.346518 in 0.07s\n",
      " [-] epoch   53/250, train loss 0.333969 in 0.07s\n",
      " [-] epoch   54/250, train loss 0.362654 in 0.07s\n",
      " [-] epoch   55/250, train loss 0.345313 in 0.07s\n",
      " [-] epoch   56/250, train loss 0.315298 in 0.08s\n",
      " [-] epoch   57/250, train loss 0.324153 in 0.08s\n",
      " [-] epoch   58/250, train loss 0.343245 in 0.08s\n",
      " [-] epoch   59/250, train loss 0.337291 in 0.08s\n",
      " [-] epoch   60/250, train loss 0.333030 in 0.08s\n",
      " [-] epoch   61/250, train loss 0.314217 in 0.08s\n",
      " [-] epoch   62/250, train loss 0.333223 in 0.08s\n",
      " [-] epoch   63/250, train loss 0.360721 in 0.08s\n",
      " [-] epoch   64/250, train loss 0.329224 in 0.07s\n",
      " [-] epoch   65/250, train loss 0.337539 in 0.07s\n",
      " [-] epoch   66/250, train loss 0.329698 in 0.07s\n",
      " [-] epoch   67/250, train loss 0.313982 in 0.08s\n",
      " [-] epoch   68/250, train loss 0.333022 in 0.08s\n",
      " [-] epoch   69/250, train loss 0.326017 in 0.08s\n",
      " [-] epoch   70/250, train loss 0.304949 in 0.09s\n",
      " [-] epoch   71/250, train loss 0.323970 in 0.07s\n",
      " [-] epoch   72/250, train loss 0.309922 in 0.07s\n",
      " [-] epoch   73/250, train loss 0.349658 in 0.07s\n",
      " [-] epoch   74/250, train loss 0.320972 in 0.07s\n",
      " [-] epoch   75/250, train loss 0.313760 in 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   76/250, train loss 0.367971 in 0.08s\n",
      " [-] epoch   77/250, train loss 0.323948 in 0.08s\n",
      " [-] epoch   78/250, train loss 0.288364 in 0.07s\n",
      " [-] epoch   79/250, train loss 0.349138 in 0.06s\n",
      " [-] epoch   80/250, train loss 0.324184 in 0.07s\n",
      " [-] epoch   81/250, train loss 0.313166 in 0.07s\n",
      " [-] epoch   82/250, train loss 0.316516 in 0.07s\n",
      " [-] epoch   83/250, train loss 0.332324 in 0.08s\n",
      " [-] epoch   84/250, train loss 0.342463 in 0.08s\n",
      " [-] epoch   85/250, train loss 0.313659 in 0.08s\n",
      " [-] epoch   86/250, train loss 0.327713 in 0.08s\n",
      " [-] epoch   87/250, train loss 0.298647 in 0.07s\n",
      " [-] epoch   88/250, train loss 0.344977 in 0.07s\n",
      " [-] epoch   89/250, train loss 0.312317 in 0.07s\n",
      " [-] epoch   90/250, train loss 0.338302 in 0.07s\n",
      " [-] epoch   91/250, train loss 0.320832 in 0.07s\n",
      " [-] epoch   92/250, train loss 0.306548 in 0.07s\n",
      " [-] epoch   93/250, train loss 0.353966 in 0.06s\n",
      " [-] epoch   94/250, train loss 0.305966 in 0.07s\n",
      " [-] epoch   95/250, train loss 0.308261 in 0.07s\n",
      " [-] epoch   96/250, train loss 0.323814 in 0.07s\n",
      " [-] epoch   97/250, train loss 0.323683 in 0.07s\n",
      " [-] epoch   98/250, train loss 0.331585 in 0.07s\n",
      " [-] epoch   99/250, train loss 0.316025 in 0.07s\n",
      " [-] epoch  100/250, train loss 0.288640 in 0.06s\n",
      " [-] epoch  101/250, train loss 0.338642 in 0.07s\n",
      " [-] epoch  102/250, train loss 0.296622 in 0.07s\n",
      " [-] epoch  103/250, train loss 0.329248 in 0.07s\n",
      " [-] epoch  104/250, train loss 0.315082 in 0.08s\n",
      " [-] epoch  105/250, train loss 0.289249 in 0.08s\n",
      " [-] epoch  106/250, train loss 0.320921 in 0.07s\n",
      " [-] epoch  107/250, train loss 0.321585 in 0.07s\n",
      " [-] epoch  108/250, train loss 0.327770 in 0.06s\n",
      " [-] epoch  109/250, train loss 0.314448 in 0.07s\n",
      " [-] epoch  110/250, train loss 0.297287 in 0.06s\n",
      " [-] epoch  111/250, train loss 0.308163 in 0.06s\n",
      " [-] epoch  112/250, train loss 0.296793 in 0.07s\n",
      " [-] epoch  113/250, train loss 0.314084 in 0.06s\n",
      " [-] epoch  114/250, train loss 0.314959 in 0.08s\n",
      " [-] epoch  115/250, train loss 0.305938 in 0.08s\n",
      " [-] epoch  116/250, train loss 0.322439 in 0.08s\n",
      " [-] epoch  117/250, train loss 0.270494 in 0.07s\n",
      " [-] epoch  118/250, train loss 0.304935 in 0.06s\n",
      " [-] epoch  119/250, train loss 0.297958 in 0.07s\n",
      " [-] epoch  120/250, train loss 0.308082 in 0.07s\n",
      " [-] epoch  121/250, train loss 0.312182 in 0.08s\n",
      " [-] epoch  122/250, train loss 0.280558 in 0.08s\n",
      " [-] epoch  123/250, train loss 0.302600 in 0.08s\n",
      " [-] epoch  124/250, train loss 0.309206 in 0.08s\n",
      " [-] epoch  125/250, train loss 0.313291 in 0.08s\n",
      " [-] epoch  126/250, train loss 0.333833 in 0.08s\n",
      " [-] epoch  127/250, train loss 0.326741 in 0.08s\n",
      " [-] epoch  128/250, train loss 0.297380 in 0.07s\n",
      " [-] epoch  129/250, train loss 0.286735 in 0.07s\n",
      " [-] epoch  130/250, train loss 0.291388 in 0.07s\n",
      " [-] epoch  131/250, train loss 0.334666 in 0.07s\n",
      " [-] epoch  132/250, train loss 0.314077 in 0.07s\n",
      " [-] epoch  133/250, train loss 0.321717 in 0.07s\n",
      " [-] epoch  134/250, train loss 0.293486 in 0.07s\n",
      " [-] epoch  135/250, train loss 0.300394 in 0.07s\n",
      " [-] epoch  136/250, train loss 0.303459 in 0.08s\n",
      " [-] epoch  137/250, train loss 0.289412 in 0.08s\n",
      " [-] epoch  138/250, train loss 0.312709 in 0.08s\n",
      " [-] epoch  139/250, train loss 0.286626 in 0.08s\n",
      " [-] epoch  140/250, train loss 0.305467 in 0.08s\n",
      " [-] epoch  141/250, train loss 0.293095 in 0.07s\n",
      " [-] epoch  142/250, train loss 0.296947 in 0.07s\n",
      " [-] epoch  143/250, train loss 0.295571 in 0.07s\n",
      " [-] epoch  144/250, train loss 0.289929 in 0.07s\n",
      " [-] epoch  145/250, train loss 0.294977 in 0.07s\n",
      " [-] epoch  146/250, train loss 0.296667 in 0.07s\n",
      " [-] epoch  147/250, train loss 0.288404 in 0.06s\n",
      " [-] epoch  148/250, train loss 0.277631 in 0.07s\n",
      " [-] epoch  149/250, train loss 0.321877 in 0.07s\n",
      " [-] epoch  150/250, train loss 0.295223 in 0.07s\n",
      " [-] epoch  151/250, train loss 0.291807 in 0.07s\n",
      " [-] epoch  152/250, train loss 0.280145 in 0.07s\n",
      " [-] epoch  153/250, train loss 0.283725 in 0.06s\n",
      " [-] epoch  154/250, train loss 0.290650 in 0.07s\n",
      " [-] epoch  155/250, train loss 0.287621 in 0.07s\n",
      " [-] epoch  156/250, train loss 0.298989 in 0.07s\n",
      " [-] epoch  157/250, train loss 0.290957 in 0.07s\n",
      " [-] epoch  158/250, train loss 0.287720 in 0.06s\n",
      " [-] epoch  159/250, train loss 0.306435 in 0.07s\n",
      " [-] epoch  160/250, train loss 0.286301 in 0.07s\n",
      " [-] epoch  161/250, train loss 0.293515 in 0.07s\n",
      " [-] epoch  162/250, train loss 0.298911 in 0.07s\n",
      " [-] epoch  163/250, train loss 0.307144 in 0.07s\n",
      " [-] epoch  164/250, train loss 0.288897 in 0.07s\n",
      " [-] epoch  165/250, train loss 0.272522 in 0.07s\n",
      " [-] epoch  166/250, train loss 0.318302 in 0.07s\n",
      " [-] epoch  167/250, train loss 0.303460 in 0.07s\n",
      " [-] epoch  168/250, train loss 0.280112 in 0.08s\n",
      " [-] epoch  169/250, train loss 0.296971 in 0.08s\n",
      " [-] epoch  170/250, train loss 0.283141 in 0.07s\n",
      " [-] epoch  171/250, train loss 0.271426 in 0.07s\n",
      " [-] epoch  172/250, train loss 0.293636 in 0.07s\n",
      " [-] epoch  173/250, train loss 0.284789 in 0.07s\n",
      " [-] epoch  174/250, train loss 0.284636 in 0.07s\n",
      " [-] epoch  175/250, train loss 0.321148 in 0.07s\n",
      " [-] epoch  176/250, train loss 0.272554 in 0.07s\n",
      " [-] epoch  177/250, train loss 0.296858 in 0.07s\n",
      " [-] epoch  178/250, train loss 0.284856 in 0.07s\n",
      " [-] epoch  179/250, train loss 0.266739 in 0.08s\n",
      " [-] epoch  180/250, train loss 0.289717 in 0.07s\n",
      " [-] epoch  181/250, train loss 0.298277 in 0.08s\n",
      " [-] epoch  182/250, train loss 0.286877 in 0.08s\n",
      " [-] epoch  183/250, train loss 0.273950 in 0.07s\n",
      " [-] epoch  184/250, train loss 0.297913 in 0.07s\n",
      " [-] epoch  185/250, train loss 0.285197 in 0.07s\n",
      " [-] epoch  186/250, train loss 0.281437 in 0.07s\n",
      " [-] epoch  187/250, train loss 0.269917 in 0.08s\n",
      " [-] epoch  188/250, train loss 0.277115 in 0.08s\n",
      " [-] epoch  189/250, train loss 0.271948 in 0.06s\n",
      " [-] epoch  190/250, train loss 0.272768 in 0.07s\n",
      " [-] epoch  191/250, train loss 0.302877 in 0.07s\n",
      " [-] epoch  192/250, train loss 0.283047 in 0.07s\n",
      " [-] epoch  193/250, train loss 0.277362 in 0.07s\n",
      " [-] epoch  194/250, train loss 0.271837 in 0.07s\n",
      " [-] epoch  195/250, train loss 0.279911 in 0.07s\n",
      " [-] epoch  196/250, train loss 0.272570 in 0.07s\n",
      " [-] epoch  197/250, train loss 0.284500 in 0.07s\n",
      " [-] epoch  198/250, train loss 0.294859 in 0.08s\n",
      " [-] epoch  199/250, train loss 0.274278 in 0.08s\n",
      " [-] epoch  200/250, train loss 0.298572 in 0.07s\n",
      " [-] epoch  201/250, train loss 0.250819 in 0.07s\n",
      " [-] epoch  202/250, train loss 0.285636 in 0.07s\n",
      " [-] epoch  203/250, train loss 0.262100 in 0.07s\n",
      " [-] epoch  204/250, train loss 0.272383 in 0.07s\n",
      " [-] epoch  205/250, train loss 0.296003 in 0.07s\n",
      " [-] epoch  206/250, train loss 0.268017 in 0.07s\n",
      " [-] epoch  207/250, train loss 0.296911 in 0.06s\n",
      " [-] epoch  208/250, train loss 0.259894 in 0.07s\n",
      " [-] epoch  209/250, train loss 0.275208 in 0.08s\n",
      " [-] epoch  210/250, train loss 0.274772 in 0.06s\n",
      " [-] epoch  211/250, train loss 0.265379 in 0.07s\n",
      " [-] epoch  212/250, train loss 0.261374 in 0.06s\n",
      " [-] epoch  213/250, train loss 0.264502 in 0.07s\n",
      " [-] epoch  214/250, train loss 0.281140 in 0.07s\n",
      " [-] epoch  215/250, train loss 0.278084 in 0.07s\n",
      " [-] epoch  216/250, train loss 0.281807 in 0.07s\n",
      " [-] epoch  217/250, train loss 0.292001 in 0.07s\n",
      " [-] epoch  218/250, train loss 0.259505 in 0.06s\n",
      " [-] epoch  219/250, train loss 0.277980 in 0.07s\n",
      " [-] epoch  220/250, train loss 0.298304 in 0.07s\n",
      " [-] epoch  221/250, train loss 0.305642 in 0.07s\n",
      " [-] epoch  222/250, train loss 0.256155 in 0.07s\n",
      " [-] epoch  223/250, train loss 0.289561 in 0.07s\n",
      " [-] epoch  224/250, train loss 0.279469 in 0.07s\n",
      " [-] epoch  225/250, train loss 0.293824 in 0.07s\n",
      " [-] epoch  226/250, train loss 0.283862 in 0.07s\n",
      " [-] epoch  227/250, train loss 0.279291 in 0.07s\n",
      " [-] epoch  228/250, train loss 0.283927 in 0.07s\n",
      " [-] epoch  229/250, train loss 0.281531 in 0.07s\n",
      " [-] epoch  230/250, train loss 0.254509 in 0.07s\n",
      " [-] epoch  231/250, train loss 0.295728 in 0.07s\n",
      " [-] epoch  232/250, train loss 0.266782 in 0.07s\n",
      " [-] epoch  233/250, train loss 0.260987 in 0.07s\n",
      " [-] epoch  234/250, train loss 0.285237 in 0.07s\n",
      " [-] epoch  235/250, train loss 0.268731 in 0.07s\n",
      " [-] epoch  236/250, train loss 0.292508 in 0.07s\n",
      " [-] epoch  237/250, train loss 0.286697 in 0.07s\n",
      " [-] epoch  238/250, train loss 0.272787 in 0.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  239/250, train loss 0.290206 in 0.07s\n",
      " [-] epoch  240/250, train loss 0.273417 in 0.07s\n",
      " [-] epoch  241/250, train loss 0.283512 in 0.06s\n",
      " [-] epoch  242/250, train loss 0.292146 in 0.07s\n",
      " [-] epoch  243/250, train loss 0.260995 in 0.07s\n",
      " [-] epoch  244/250, train loss 0.294238 in 0.07s\n",
      " [-] epoch  245/250, train loss 0.279016 in 0.07s\n",
      " [-] epoch  246/250, train loss 0.262468 in 0.07s\n",
      " [-] epoch  247/250, train loss 0.289475 in 0.06s\n",
      " [-] epoch  248/250, train loss 0.259517 in 0.07s\n",
      " [-] epoch  249/250, train loss 0.294304 in 0.07s\n",
      " [-] epoch  250/250, train loss 0.272489 in 0.07s\n",
      " [-] test acc. 75.555556%\n",
      "Je vais utiliser 3 layers\n",
      " [-] epoch    1/250, train loss 0.553874 in 0.12s\n",
      " [-] epoch    2/250, train loss 0.461123 in 0.13s\n",
      " [-] epoch    3/250, train loss 0.444183 in 0.13s\n",
      " [-] epoch    4/250, train loss 0.431757 in 0.12s\n",
      " [-] epoch    5/250, train loss 0.403494 in 0.13s\n",
      " [-] epoch    6/250, train loss 0.396650 in 0.13s\n",
      " [-] epoch    7/250, train loss 0.366216 in 0.12s\n",
      " [-] epoch    8/250, train loss 0.404686 in 0.11s\n",
      " [-] epoch    9/250, train loss 0.374766 in 0.13s\n",
      " [-] epoch   10/250, train loss 0.357123 in 0.13s\n",
      " [-] epoch   11/250, train loss 0.376898 in 0.12s\n",
      " [-] epoch   12/250, train loss 0.379691 in 0.13s\n",
      " [-] epoch   13/250, train loss 0.359567 in 0.13s\n",
      " [-] epoch   14/250, train loss 0.326237 in 0.13s\n",
      " [-] epoch   15/250, train loss 0.347902 in 0.13s\n",
      " [-] epoch   16/250, train loss 0.333879 in 0.15s\n",
      " [-] epoch   17/250, train loss 0.303815 in 0.15s\n",
      " [-] epoch   18/250, train loss 0.341815 in 0.13s\n",
      " [-] epoch   19/250, train loss 0.348873 in 0.12s\n",
      " [-] epoch   20/250, train loss 0.330510 in 0.12s\n",
      " [-] epoch   21/250, train loss 0.349249 in 0.13s\n",
      " [-] epoch   22/250, train loss 0.339846 in 0.12s\n",
      " [-] epoch   23/250, train loss 0.325971 in 0.12s\n",
      " [-] epoch   24/250, train loss 0.335346 in 0.14s\n",
      " [-] epoch   25/250, train loss 0.326591 in 0.14s\n",
      " [-] epoch   26/250, train loss 0.295789 in 0.13s\n",
      " [-] epoch   27/250, train loss 0.335642 in 0.12s\n",
      " [-] epoch   28/250, train loss 0.301927 in 0.13s\n",
      " [-] epoch   29/250, train loss 0.312992 in 0.12s\n",
      " [-] epoch   30/250, train loss 0.299038 in 0.12s\n",
      " [-] epoch   31/250, train loss 0.313025 in 0.13s\n",
      " [-] epoch   32/250, train loss 0.306124 in 0.13s\n",
      " [-] epoch   33/250, train loss 0.313374 in 0.12s\n",
      " [-] epoch   34/250, train loss 0.303656 in 0.12s\n",
      " [-] epoch   35/250, train loss 0.311864 in 0.13s\n",
      " [-] epoch   36/250, train loss 0.326830 in 0.13s\n",
      " [-] epoch   37/250, train loss 0.284511 in 0.12s\n",
      " [-] epoch   38/250, train loss 0.284672 in 0.13s\n",
      " [-] epoch   39/250, train loss 0.267576 in 0.12s\n",
      " [-] epoch   40/250, train loss 0.289458 in 0.13s\n",
      " [-] epoch   41/250, train loss 0.295105 in 0.12s\n",
      " [-] epoch   42/250, train loss 0.265768 in 0.15s\n",
      " [-] epoch   43/250, train loss 0.309100 in 0.14s\n",
      " [-] epoch   44/250, train loss 0.298673 in 0.12s\n",
      " [-] epoch   45/250, train loss 0.288975 in 0.12s\n",
      " [-] epoch   46/250, train loss 0.276585 in 0.13s\n",
      " [-] epoch   47/250, train loss 0.283880 in 0.11s\n",
      " [-] epoch   48/250, train loss 0.277130 in 0.12s\n",
      " [-] epoch   49/250, train loss 0.261753 in 0.13s\n",
      " [-] epoch   50/250, train loss 0.260528 in 0.13s\n",
      " [-] epoch   51/250, train loss 0.280199 in 0.13s\n",
      " [-] epoch   52/250, train loss 0.285962 in 0.13s\n",
      " [-] epoch   53/250, train loss 0.281244 in 0.12s\n",
      " [-] epoch   54/250, train loss 0.260066 in 0.12s\n",
      " [-] epoch   55/250, train loss 0.291307 in 0.12s\n",
      " [-] epoch   56/250, train loss 0.270311 in 0.13s\n",
      " [-] epoch   57/250, train loss 0.275889 in 0.12s\n",
      " [-] epoch   58/250, train loss 0.282538 in 0.13s\n",
      " [-] epoch   59/250, train loss 0.268779 in 0.13s\n",
      " [-] epoch   60/250, train loss 0.272222 in 0.14s\n",
      " [-] epoch   61/250, train loss 0.268833 in 0.14s\n",
      " [-] epoch   62/250, train loss 0.266184 in 0.13s\n",
      " [-] epoch   63/250, train loss 0.260655 in 0.14s\n",
      " [-] epoch   64/250, train loss 0.245007 in 0.12s\n",
      " [-] epoch   65/250, train loss 0.259838 in 0.12s\n",
      " [-] epoch   66/250, train loss 0.254326 in 0.12s\n",
      " [-] epoch   67/250, train loss 0.263682 in 0.13s\n",
      " [-] epoch   68/250, train loss 0.248820 in 0.13s\n",
      " [-] epoch   69/250, train loss 0.247414 in 0.13s\n",
      " [-] epoch   70/250, train loss 0.270510 in 0.14s\n",
      " [-] epoch   71/250, train loss 0.244121 in 0.12s\n",
      " [-] epoch   72/250, train loss 0.261214 in 0.12s\n",
      " [-] epoch   73/250, train loss 0.242743 in 0.13s\n",
      " [-] epoch   74/250, train loss 0.245900 in 0.13s\n",
      " [-] epoch   75/250, train loss 0.266747 in 0.12s\n",
      " [-] epoch   76/250, train loss 0.255612 in 0.13s\n",
      " [-] epoch   77/250, train loss 0.235079 in 0.13s\n",
      " [-] epoch   78/250, train loss 0.259172 in 0.12s\n",
      " [-] epoch   79/250, train loss 0.249023 in 0.13s\n",
      " [-] epoch   80/250, train loss 0.231008 in 0.13s\n",
      " [-] epoch   81/250, train loss 0.247302 in 0.13s\n",
      " [-] epoch   82/250, train loss 0.263376 in 0.13s\n",
      " [-] epoch   83/250, train loss 0.232752 in 0.15s\n",
      " [-] epoch   84/250, train loss 0.264596 in 0.14s\n",
      " [-] epoch   85/250, train loss 0.252247 in 0.13s\n",
      " [-] epoch   86/250, train loss 0.241527 in 0.13s\n",
      " [-] epoch   87/250, train loss 0.237593 in 0.13s\n",
      " [-] epoch   88/250, train loss 0.259289 in 0.13s\n",
      " [-] epoch   89/250, train loss 0.245908 in 0.12s\n",
      " [-] epoch   90/250, train loss 0.259538 in 0.13s\n",
      " [-] epoch   91/250, train loss 0.252811 in 0.13s\n",
      " [-] epoch   92/250, train loss 0.259049 in 0.13s\n",
      " [-] epoch   93/250, train loss 0.257204 in 0.14s\n",
      " [-] epoch   94/250, train loss 0.267276 in 0.13s\n",
      " [-] epoch   95/250, train loss 0.233129 in 0.13s\n",
      " [-] epoch   96/250, train loss 0.256264 in 0.13s\n",
      " [-] epoch   97/250, train loss 0.223770 in 0.13s\n",
      " [-] epoch   98/250, train loss 0.244900 in 0.13s\n",
      " [-] epoch   99/250, train loss 0.259807 in 0.13s\n",
      " [-] epoch  100/250, train loss 0.252558 in 0.14s\n",
      " [-] epoch  101/250, train loss 0.234875 in 0.15s\n",
      " [-] epoch  102/250, train loss 0.229583 in 0.13s\n",
      " [-] epoch  103/250, train loss 0.237800 in 0.13s\n",
      " [-] epoch  104/250, train loss 0.252771 in 0.13s\n",
      " [-] epoch  105/250, train loss 0.245369 in 0.13s\n",
      " [-] epoch  106/250, train loss 0.240592 in 0.13s\n",
      " [-] epoch  107/250, train loss 0.248382 in 0.14s\n",
      " [-] epoch  108/250, train loss 0.233704 in 0.12s\n",
      " [-] epoch  109/250, train loss 0.233605 in 0.13s\n",
      " [-] epoch  110/250, train loss 0.237877 in 0.13s\n",
      " [-] epoch  111/250, train loss 0.256980 in 0.13s\n",
      " [-] epoch  112/250, train loss 0.231538 in 0.12s\n",
      " [-] epoch  113/250, train loss 0.235609 in 0.12s\n",
      " [-] epoch  114/250, train loss 0.236464 in 0.13s\n",
      " [-] epoch  115/250, train loss 0.249258 in 0.13s\n",
      " [-] epoch  116/250, train loss 0.223258 in 0.14s\n",
      " [-] epoch  117/250, train loss 0.222163 in 0.12s\n",
      " [-] epoch  118/250, train loss 0.225279 in 0.13s\n",
      " [-] epoch  119/250, train loss 0.228245 in 0.14s\n",
      " [-] epoch  120/250, train loss 0.228829 in 0.13s\n",
      " [-] epoch  121/250, train loss 0.216695 in 0.12s\n",
      " [-] epoch  122/250, train loss 0.226405 in 0.13s\n",
      " [-] epoch  123/250, train loss 0.216705 in 0.13s\n",
      " [-] epoch  124/250, train loss 0.215579 in 0.12s\n",
      " [-] epoch  125/250, train loss 0.231336 in 0.13s\n",
      " [-] epoch  126/250, train loss 0.215930 in 0.14s\n",
      " [-] epoch  127/250, train loss 0.223975 in 0.14s\n",
      " [-] epoch  128/250, train loss 0.209337 in 0.13s\n",
      " [-] epoch  129/250, train loss 0.206325 in 0.14s\n",
      " [-] epoch  130/250, train loss 0.235117 in 0.12s\n",
      " [-] epoch  131/250, train loss 0.224584 in 0.13s\n",
      " [-] epoch  132/250, train loss 0.229052 in 0.14s\n",
      " [-] epoch  133/250, train loss 0.245760 in 0.13s\n",
      " [-] epoch  134/250, train loss 0.226071 in 0.13s\n",
      " [-] epoch  135/250, train loss 0.232519 in 0.12s\n",
      " [-] epoch  136/250, train loss 0.225556 in 0.13s\n",
      " [-] epoch  137/250, train loss 0.219282 in 0.14s\n",
      " [-] epoch  138/250, train loss 0.219881 in 0.14s\n",
      " [-] epoch  139/250, train loss 0.231322 in 0.15s\n",
      " [-] epoch  140/250, train loss 0.211043 in 0.15s\n",
      " [-] epoch  141/250, train loss 0.230685 in 0.15s\n",
      " [-] epoch  142/250, train loss 0.220795 in 0.15s\n",
      " [-] epoch  143/250, train loss 0.224646 in 0.13s\n",
      " [-] epoch  144/250, train loss 0.237823 in 0.13s\n",
      " [-] epoch  145/250, train loss 0.217755 in 0.13s\n",
      " [-] epoch  146/250, train loss 0.230704 in 0.12s\n",
      " [-] epoch  147/250, train loss 0.224600 in 0.13s\n",
      " [-] epoch  148/250, train loss 0.243933 in 0.15s\n",
      " [-] epoch  149/250, train loss 0.229369 in 0.14s\n",
      " [-] epoch  150/250, train loss 0.219303 in 0.14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  151/250, train loss 0.228568 in 0.14s\n",
      " [-] epoch  152/250, train loss 0.224999 in 0.13s\n",
      " [-] epoch  153/250, train loss 0.214095 in 0.12s\n",
      " [-] epoch  154/250, train loss 0.223079 in 0.13s\n",
      " [-] epoch  155/250, train loss 0.216447 in 0.13s\n",
      " [-] epoch  156/250, train loss 0.234672 in 0.15s\n",
      " [-] epoch  157/250, train loss 0.246944 in 0.14s\n",
      " [-] epoch  158/250, train loss 0.219849 in 0.12s\n",
      " [-] epoch  159/250, train loss 0.220514 in 0.12s\n",
      " [-] epoch  160/250, train loss 0.231953 in 0.13s\n",
      " [-] epoch  161/250, train loss 0.236833 in 0.13s\n",
      " [-] epoch  162/250, train loss 0.222173 in 0.13s\n",
      " [-] epoch  163/250, train loss 0.209209 in 0.12s\n",
      " [-] epoch  164/250, train loss 0.210276 in 0.12s\n",
      " [-] epoch  165/250, train loss 0.215301 in 0.13s\n",
      " [-] epoch  166/250, train loss 0.236436 in 0.13s\n",
      " [-] epoch  167/250, train loss 0.223329 in 0.12s\n",
      " [-] epoch  168/250, train loss 0.208833 in 0.14s\n",
      " [-] epoch  169/250, train loss 0.234037 in 0.14s\n",
      " [-] epoch  170/250, train loss 0.214889 in 0.14s\n",
      " [-] epoch  171/250, train loss 0.211094 in 0.15s\n",
      " [-] epoch  172/250, train loss 0.214388 in 0.15s\n",
      " [-] epoch  173/250, train loss 0.221393 in 0.15s\n",
      " [-] epoch  174/250, train loss 0.224832 in 0.14s\n",
      " [-] epoch  175/250, train loss 0.232744 in 0.15s\n",
      " [-] epoch  176/250, train loss 0.223880 in 0.14s\n",
      " [-] epoch  177/250, train loss 0.227776 in 0.14s\n",
      " [-] epoch  178/250, train loss 0.219606 in 0.13s\n",
      " [-] epoch  179/250, train loss 0.233883 in 0.12s\n",
      " [-] epoch  180/250, train loss 0.221315 in 0.13s\n",
      " [-] epoch  181/250, train loss 0.223863 in 0.12s\n",
      " [-] epoch  182/250, train loss 0.217020 in 0.13s\n",
      " [-] epoch  183/250, train loss 0.228035 in 0.15s\n",
      " [-] epoch  184/250, train loss 0.227429 in 0.13s\n",
      " [-] epoch  185/250, train loss 0.223339 in 0.12s\n",
      " [-] epoch  186/250, train loss 0.229917 in 0.12s\n",
      " [-] epoch  187/250, train loss 0.224968 in 0.13s\n",
      " [-] epoch  188/250, train loss 0.230242 in 0.12s\n",
      " [-] epoch  189/250, train loss 0.232270 in 0.13s\n",
      " [-] epoch  190/250, train loss 0.222863 in 0.13s\n",
      " [-] epoch  191/250, train loss 0.218734 in 0.13s\n",
      " [-] epoch  192/250, train loss 0.200178 in 0.15s\n",
      " [-] epoch  193/250, train loss 0.217591 in 0.13s\n",
      " [-] epoch  194/250, train loss 0.217132 in 0.15s\n",
      " [-] epoch  195/250, train loss 0.218817 in 0.15s\n",
      " [-] epoch  196/250, train loss 0.217888 in 0.15s\n",
      " [-] epoch  197/250, train loss 0.234224 in 0.14s\n",
      " [-] epoch  198/250, train loss 0.222337 in 0.13s\n",
      " [-] epoch  199/250, train loss 0.218569 in 0.13s\n",
      " [-] epoch  200/250, train loss 0.215384 in 0.15s\n",
      " [-] epoch  201/250, train loss 0.207218 in 0.15s\n",
      " [-] epoch  202/250, train loss 0.220117 in 0.14s\n",
      " [-] epoch  203/250, train loss 0.211009 in 0.14s\n",
      " [-] epoch  204/250, train loss 0.229090 in 0.14s\n",
      " [-] epoch  205/250, train loss 0.208301 in 0.14s\n",
      " [-] epoch  206/250, train loss 0.197489 in 0.14s\n",
      " [-] epoch  207/250, train loss 0.206149 in 0.14s\n",
      " [-] epoch  208/250, train loss 0.220862 in 0.13s\n",
      " [-] epoch  209/250, train loss 0.205133 in 0.12s\n",
      " [-] epoch  210/250, train loss 0.211771 in 0.12s\n",
      " [-] epoch  211/250, train loss 0.228067 in 0.12s\n",
      " [-] epoch  212/250, train loss 0.235804 in 0.13s\n",
      " [-] epoch  213/250, train loss 0.220559 in 0.13s\n",
      " [-] epoch  214/250, train loss 0.213276 in 0.13s\n",
      " [-] epoch  215/250, train loss 0.221973 in 0.13s\n",
      " [-] epoch  216/250, train loss 0.236488 in 0.15s\n",
      " [-] epoch  217/250, train loss 0.217764 in 0.13s\n",
      " [-] epoch  218/250, train loss 0.212073 in 0.13s\n",
      " [-] epoch  219/250, train loss 0.235471 in 0.12s\n",
      " [-] epoch  220/250, train loss 0.217472 in 0.12s\n",
      " [-] epoch  221/250, train loss 0.216953 in 0.14s\n",
      " [-] epoch  222/250, train loss 0.204185 in 0.14s\n",
      " [-] epoch  223/250, train loss 0.232379 in 0.14s\n",
      " [-] epoch  224/250, train loss 0.230384 in 0.14s\n",
      " [-] epoch  225/250, train loss 0.212993 in 0.14s\n",
      " [-] epoch  226/250, train loss 0.219131 in 0.14s\n",
      " [-] epoch  227/250, train loss 0.211124 in 0.14s\n",
      " [-] epoch  228/250, train loss 0.213230 in 0.14s\n",
      " [-] epoch  229/250, train loss 0.205737 in 0.13s\n",
      " [-] epoch  230/250, train loss 0.197476 in 0.14s\n",
      " [-] epoch  231/250, train loss 0.212161 in 0.14s\n",
      " [-] epoch  232/250, train loss 0.218352 in 0.14s\n",
      " [-] epoch  233/250, train loss 0.201486 in 0.14s\n",
      " [-] epoch  234/250, train loss 0.206641 in 0.14s\n",
      " [-] epoch  235/250, train loss 0.232319 in 0.14s\n",
      " [-] epoch  236/250, train loss 0.223363 in 0.13s\n",
      " [-] epoch  237/250, train loss 0.217353 in 0.12s\n",
      " [-] epoch  238/250, train loss 0.210782 in 0.12s\n",
      " [-] epoch  239/250, train loss 0.203827 in 0.12s\n",
      " [-] epoch  240/250, train loss 0.211611 in 0.12s\n",
      " [-] epoch  241/250, train loss 0.213721 in 0.12s\n",
      " [-] epoch  242/250, train loss 0.213979 in 0.13s\n",
      " [-] epoch  243/250, train loss 0.213009 in 0.13s\n",
      " [-] epoch  244/250, train loss 0.220392 in 0.13s\n",
      " [-] epoch  245/250, train loss 0.196367 in 0.13s\n",
      " [-] epoch  246/250, train loss 0.208005 in 0.13s\n",
      " [-] epoch  247/250, train loss 0.227998 in 0.14s\n",
      " [-] epoch  248/250, train loss 0.212829 in 0.13s\n",
      " [-] epoch  249/250, train loss 0.201459 in 0.13s\n",
      " [-] epoch  250/250, train loss 0.206956 in 0.14s\n",
      " [-] test acc. 84.444444%\n",
      "Je vais utiliser 4 layers\n",
      " [-] epoch    1/250, train loss 0.582442 in 0.21s\n",
      " [-] epoch    2/250, train loss 0.492798 in 0.20s\n",
      " [-] epoch    3/250, train loss 0.440057 in 0.21s\n",
      " [-] epoch    4/250, train loss 0.434914 in 0.20s\n",
      " [-] epoch    5/250, train loss 0.405150 in 0.22s\n",
      " [-] epoch    6/250, train loss 0.382625 in 0.21s\n",
      " [-] epoch    7/250, train loss 0.371506 in 0.23s\n",
      " [-] epoch    8/250, train loss 0.358458 in 0.21s\n",
      " [-] epoch    9/250, train loss 0.328048 in 0.21s\n",
      " [-] epoch   10/250, train loss 0.376221 in 0.20s\n",
      " [-] epoch   11/250, train loss 0.341510 in 0.21s\n",
      " [-] epoch   12/250, train loss 0.365039 in 0.21s\n",
      " [-] epoch   13/250, train loss 0.329606 in 0.20s\n",
      " [-] epoch   14/250, train loss 0.342487 in 0.22s\n",
      " [-] epoch   15/250, train loss 0.333079 in 0.20s\n",
      " [-] epoch   16/250, train loss 0.332196 in 0.22s\n",
      " [-] epoch   17/250, train loss 0.361577 in 0.20s\n",
      " [-] epoch   18/250, train loss 0.330057 in 0.20s\n",
      " [-] epoch   19/250, train loss 0.302604 in 0.19s\n",
      " [-] epoch   20/250, train loss 0.304956 in 0.19s\n",
      " [-] epoch   21/250, train loss 0.322944 in 0.19s\n",
      " [-] epoch   22/250, train loss 0.339513 in 0.20s\n",
      " [-] epoch   23/250, train loss 0.315499 in 0.20s\n",
      " [-] epoch   24/250, train loss 0.311207 in 0.20s\n",
      " [-] epoch   25/250, train loss 0.303566 in 0.21s\n",
      " [-] epoch   26/250, train loss 0.302461 in 0.20s\n",
      " [-] epoch   27/250, train loss 0.292256 in 0.21s\n",
      " [-] epoch   28/250, train loss 0.307554 in 0.22s\n",
      " [-] epoch   29/250, train loss 0.297270 in 0.20s\n",
      " [-] epoch   30/250, train loss 0.294579 in 0.22s\n",
      " [-] epoch   31/250, train loss 0.323270 in 0.21s\n",
      " [-] epoch   32/250, train loss 0.300666 in 0.21s\n",
      " [-] epoch   33/250, train loss 0.287256 in 0.20s\n",
      " [-] epoch   34/250, train loss 0.285730 in 0.22s\n",
      " [-] epoch   35/250, train loss 0.288509 in 0.21s\n",
      " [-] epoch   36/250, train loss 0.297837 in 0.22s\n",
      " [-] epoch   37/250, train loss 0.293473 in 0.21s\n",
      " [-] epoch   38/250, train loss 0.273124 in 0.21s\n",
      " [-] epoch   39/250, train loss 0.281745 in 0.21s\n",
      " [-] epoch   40/250, train loss 0.275014 in 0.20s\n",
      " [-] epoch   41/250, train loss 0.295137 in 0.22s\n",
      " [-] epoch   42/250, train loss 0.294879 in 0.21s\n",
      " [-] epoch   43/250, train loss 0.275490 in 0.22s\n",
      " [-] epoch   44/250, train loss 0.279744 in 0.21s\n",
      " [-] epoch   45/250, train loss 0.267886 in 0.20s\n",
      " [-] epoch   46/250, train loss 0.264038 in 0.22s\n",
      " [-] epoch   47/250, train loss 0.263714 in 0.21s\n",
      " [-] epoch   48/250, train loss 0.261248 in 0.21s\n",
      " [-] epoch   49/250, train loss 0.295872 in 0.21s\n",
      " [-] epoch   50/250, train loss 0.259497 in 0.21s\n",
      " [-] epoch   51/250, train loss 0.276549 in 0.22s\n",
      " [-] epoch   52/250, train loss 0.295456 in 0.19s\n",
      " [-] epoch   53/250, train loss 0.278627 in 0.20s\n",
      " [-] epoch   54/250, train loss 0.259125 in 0.22s\n",
      " [-] epoch   55/250, train loss 0.249929 in 0.21s\n",
      " [-] epoch   56/250, train loss 0.293366 in 0.21s\n",
      " [-] epoch   57/250, train loss 0.267785 in 0.21s\n",
      " [-] epoch   58/250, train loss 0.251312 in 0.20s\n",
      " [-] epoch   59/250, train loss 0.262594 in 0.21s\n",
      " [-] epoch   60/250, train loss 0.260827 in 0.20s\n",
      " [-] epoch   61/250, train loss 0.274811 in 0.20s\n",
      " [-] epoch   62/250, train loss 0.250839 in 0.20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   63/250, train loss 0.295649 in 0.20s\n",
      " [-] epoch   64/250, train loss 0.260071 in 0.20s\n",
      " [-] epoch   65/250, train loss 0.278125 in 0.20s\n",
      " [-] epoch   66/250, train loss 0.265549 in 0.20s\n",
      " [-] epoch   67/250, train loss 0.254802 in 0.20s\n",
      " [-] epoch   68/250, train loss 0.260268 in 0.22s\n",
      " [-] epoch   69/250, train loss 0.237790 in 0.20s\n",
      " [-] epoch   70/250, train loss 0.246237 in 0.22s\n",
      " [-] epoch   71/250, train loss 0.262067 in 0.21s\n",
      " [-] epoch   72/250, train loss 0.255403 in 0.21s\n",
      " [-] epoch   73/250, train loss 0.259956 in 0.20s\n",
      " [-] epoch   74/250, train loss 0.258645 in 0.20s\n",
      " [-] epoch   75/250, train loss 0.267035 in 0.21s\n",
      " [-] epoch   76/250, train loss 0.267758 in 0.20s\n",
      " [-] epoch   77/250, train loss 0.248896 in 0.20s\n",
      " [-] epoch   78/250, train loss 0.256746 in 0.19s\n",
      " [-] epoch   79/250, train loss 0.256755 in 0.19s\n",
      " [-] epoch   80/250, train loss 0.250041 in 0.20s\n",
      " [-] epoch   81/250, train loss 0.263991 in 0.20s\n",
      " [-] epoch   82/250, train loss 0.274533 in 0.20s\n",
      " [-] epoch   83/250, train loss 0.252670 in 0.20s\n",
      " [-] epoch   84/250, train loss 0.247271 in 0.20s\n",
      " [-] epoch   85/250, train loss 0.220345 in 0.20s\n",
      " [-] epoch   86/250, train loss 0.230476 in 0.19s\n",
      " [-] epoch   87/250, train loss 0.228000 in 0.20s\n",
      " [-] epoch   88/250, train loss 0.231200 in 0.20s\n",
      " [-] epoch   89/250, train loss 0.247398 in 0.20s\n",
      " [-] epoch   90/250, train loss 0.257975 in 0.20s\n",
      " [-] epoch   91/250, train loss 0.247273 in 0.20s\n",
      " [-] epoch   92/250, train loss 0.247032 in 0.20s\n",
      " [-] epoch   93/250, train loss 0.248399 in 0.20s\n",
      " [-] epoch   94/250, train loss 0.237607 in 0.19s\n",
      " [-] epoch   95/250, train loss 0.230315 in 0.20s\n",
      " [-] epoch   96/250, train loss 0.230516 in 0.21s\n",
      " [-] epoch   97/250, train loss 0.251100 in 0.22s\n",
      " [-] epoch   98/250, train loss 0.231119 in 0.22s\n",
      " [-] epoch   99/250, train loss 0.254014 in 0.22s\n",
      " [-] epoch  100/250, train loss 0.243843 in 0.22s\n",
      " [-] epoch  101/250, train loss 0.238166 in 0.21s\n",
      " [-] epoch  102/250, train loss 0.236621 in 0.20s\n",
      " [-] epoch  103/250, train loss 0.240154 in 0.21s\n",
      " [-] epoch  104/250, train loss 0.250470 in 0.20s\n",
      " [-] epoch  105/250, train loss 0.253479 in 0.21s\n",
      " [-] epoch  106/250, train loss 0.246432 in 0.20s\n",
      " [-] epoch  107/250, train loss 0.244756 in 0.20s\n",
      " [-] epoch  108/250, train loss 0.256452 in 0.19s\n",
      " [-] epoch  109/250, train loss 0.240277 in 0.20s\n",
      " [-] epoch  110/250, train loss 0.249660 in 0.20s\n",
      " [-] epoch  111/250, train loss 0.230852 in 0.19s\n",
      " [-] epoch  112/250, train loss 0.243557 in 0.20s\n",
      " [-] epoch  113/250, train loss 0.233603 in 0.20s\n",
      " [-] epoch  114/250, train loss 0.232164 in 0.20s\n",
      " [-] epoch  115/250, train loss 0.253267 in 0.20s\n",
      " [-] epoch  116/250, train loss 0.239086 in 0.20s\n",
      " [-] epoch  117/250, train loss 0.236233 in 0.19s\n",
      " [-] epoch  118/250, train loss 0.229717 in 0.20s\n",
      " [-] epoch  119/250, train loss 0.222285 in 0.20s\n",
      " [-] epoch  120/250, train loss 0.232720 in 0.19s\n",
      " [-] epoch  121/250, train loss 0.233126 in 0.20s\n",
      " [-] epoch  122/250, train loss 0.261883 in 0.20s\n",
      " [-] epoch  123/250, train loss 0.236243 in 0.20s\n",
      " [-] epoch  124/250, train loss 0.246713 in 0.20s\n",
      " [-] epoch  125/250, train loss 0.240595 in 0.20s\n",
      " [-] epoch  126/250, train loss 0.233213 in 0.19s\n",
      " [-] epoch  127/250, train loss 0.244565 in 0.20s\n",
      " [-] epoch  128/250, train loss 0.231957 in 0.20s\n",
      " [-] epoch  129/250, train loss 0.225396 in 0.20s\n",
      " [-] epoch  130/250, train loss 0.245722 in 0.20s\n",
      " [-] epoch  131/250, train loss 0.232296 in 0.19s\n",
      " [-] epoch  132/250, train loss 0.235938 in 0.20s\n",
      " [-] epoch  133/250, train loss 0.209778 in 0.22s\n",
      " [-] epoch  134/250, train loss 0.235445 in 0.20s\n",
      " [-] epoch  135/250, train loss 0.237187 in 0.20s\n",
      " [-] epoch  136/250, train loss 0.228245 in 0.20s\n",
      " [-] epoch  137/250, train loss 0.226172 in 0.19s\n",
      " [-] epoch  138/250, train loss 0.220181 in 0.19s\n",
      " [-] epoch  139/250, train loss 0.226919 in 0.20s\n",
      " [-] epoch  140/250, train loss 0.238606 in 0.20s\n",
      " [-] epoch  141/250, train loss 0.234443 in 0.21s\n",
      " [-] epoch  142/250, train loss 0.231063 in 0.20s\n",
      " [-] epoch  143/250, train loss 0.217543 in 0.20s\n",
      " [-] epoch  144/250, train loss 0.223371 in 0.21s\n",
      " [-] epoch  145/250, train loss 0.214683 in 0.21s\n",
      " [-] epoch  146/250, train loss 0.222917 in 0.21s\n",
      " [-] epoch  147/250, train loss 0.227465 in 0.20s\n",
      " [-] epoch  148/250, train loss 0.222181 in 0.22s\n",
      " [-] epoch  149/250, train loss 0.225701 in 0.20s\n",
      " [-] epoch  150/250, train loss 0.212582 in 0.21s\n",
      " [-] epoch  151/250, train loss 0.228925 in 0.21s\n",
      " [-] epoch  152/250, train loss 0.217411 in 0.21s\n",
      " [-] epoch  153/250, train loss 0.254231 in 0.20s\n",
      " [-] epoch  154/250, train loss 0.212018 in 0.21s\n",
      " [-] epoch  155/250, train loss 0.219250 in 0.20s\n",
      " [-] epoch  156/250, train loss 0.209623 in 0.20s\n",
      " [-] epoch  157/250, train loss 0.212908 in 0.21s\n",
      " [-] epoch  158/250, train loss 0.206046 in 0.20s\n",
      " [-] epoch  159/250, train loss 0.213687 in 0.20s\n",
      " [-] epoch  160/250, train loss 0.211612 in 0.21s\n",
      " [-] epoch  161/250, train loss 0.217202 in 0.22s\n",
      " [-] epoch  162/250, train loss 0.217861 in 0.22s\n",
      " [-] epoch  163/250, train loss 0.205871 in 0.21s\n",
      " [-] epoch  164/250, train loss 0.217359 in 0.22s\n",
      " [-] epoch  165/250, train loss 0.220603 in 0.21s\n",
      " [-] epoch  166/250, train loss 0.213332 in 0.22s\n",
      " [-] epoch  167/250, train loss 0.216450 in 0.21s\n",
      " [-] epoch  168/250, train loss 0.215084 in 0.20s\n",
      " [-] epoch  169/250, train loss 0.222504 in 0.21s\n",
      " [-] epoch  170/250, train loss 0.222174 in 0.20s\n",
      " [-] epoch  171/250, train loss 0.216042 in 0.21s\n",
      " [-] epoch  172/250, train loss 0.208676 in 0.22s\n",
      " [-] epoch  173/250, train loss 0.223447 in 0.20s\n",
      " [-] epoch  174/250, train loss 0.211759 in 0.21s\n",
      " [-] epoch  175/250, train loss 0.224807 in 0.20s\n",
      " [-] epoch  176/250, train loss 0.220020 in 0.20s\n",
      " [-] epoch  177/250, train loss 0.226070 in 0.21s\n",
      " [-] epoch  178/250, train loss 0.214212 in 0.21s\n",
      " [-] epoch  179/250, train loss 0.214282 in 0.19s\n",
      " [-] epoch  180/250, train loss 0.199058 in 0.20s\n",
      " [-] epoch  181/250, train loss 0.219414 in 0.20s\n",
      " [-] epoch  182/250, train loss 0.215314 in 0.20s\n",
      " [-] epoch  183/250, train loss 0.231287 in 0.19s\n",
      " [-] epoch  184/250, train loss 0.243073 in 0.20s\n",
      " [-] epoch  185/250, train loss 0.243645 in 0.20s\n",
      " [-] epoch  186/250, train loss 0.217050 in 0.20s\n",
      " [-] epoch  187/250, train loss 0.243461 in 0.20s\n",
      " [-] epoch  188/250, train loss 0.234978 in 0.19s\n",
      " [-] epoch  189/250, train loss 0.214232 in 0.20s\n",
      " [-] epoch  190/250, train loss 0.209865 in 0.20s\n",
      " [-] epoch  191/250, train loss 0.231713 in 0.19s\n",
      " [-] epoch  192/250, train loss 0.212483 in 0.20s\n",
      " [-] epoch  193/250, train loss 0.208792 in 0.20s\n",
      " [-] epoch  194/250, train loss 0.222536 in 0.20s\n",
      " [-] epoch  195/250, train loss 0.208212 in 0.19s\n",
      " [-] epoch  196/250, train loss 0.210452 in 0.20s\n",
      " [-] epoch  197/250, train loss 0.195858 in 0.21s\n",
      " [-] epoch  198/250, train loss 0.217182 in 0.20s\n",
      " [-] epoch  199/250, train loss 0.211941 in 0.20s\n",
      " [-] epoch  200/250, train loss 0.221631 in 0.19s\n",
      " [-] epoch  201/250, train loss 0.216382 in 0.20s\n",
      " [-] epoch  202/250, train loss 0.212286 in 0.20s\n",
      " [-] epoch  203/250, train loss 0.223608 in 0.20s\n",
      " [-] epoch  204/250, train loss 0.213618 in 0.20s\n",
      " [-] epoch  205/250, train loss 0.236924 in 0.19s\n",
      " [-] epoch  206/250, train loss 0.219366 in 0.20s\n",
      " [-] epoch  207/250, train loss 0.232140 in 0.19s\n",
      " [-] epoch  208/250, train loss 0.217055 in 0.20s\n",
      " [-] epoch  209/250, train loss 0.209259 in 0.20s\n",
      " [-] epoch  210/250, train loss 0.217232 in 0.20s\n",
      " [-] epoch  211/250, train loss 0.214112 in 0.20s\n",
      " [-] epoch  212/250, train loss 0.197044 in 0.20s\n",
      " [-] epoch  213/250, train loss 0.200569 in 0.21s\n",
      " [-] epoch  214/250, train loss 0.218432 in 0.20s\n",
      " [-] epoch  215/250, train loss 0.213733 in 0.20s\n",
      " [-] epoch  216/250, train loss 0.208059 in 0.19s\n",
      " [-] epoch  217/250, train loss 0.205814 in 0.20s\n",
      " [-] epoch  218/250, train loss 0.210946 in 0.20s\n",
      " [-] epoch  219/250, train loss 0.205631 in 0.19s\n",
      " [-] epoch  220/250, train loss 0.214394 in 0.20s\n",
      " [-] epoch  221/250, train loss 0.210085 in 0.19s\n",
      " [-] epoch  222/250, train loss 0.209602 in 0.20s\n",
      " [-] epoch  223/250, train loss 0.215884 in 0.20s\n",
      " [-] epoch  224/250, train loss 0.211967 in 0.20s\n",
      " [-] epoch  225/250, train loss 0.213257 in 0.21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  226/250, train loss 0.201855 in 0.20s\n",
      " [-] epoch  227/250, train loss 0.219707 in 0.21s\n",
      " [-] epoch  228/250, train loss 0.223354 in 0.20s\n",
      " [-] epoch  229/250, train loss 0.224684 in 0.19s\n",
      " [-] epoch  230/250, train loss 0.224969 in 0.19s\n",
      " [-] epoch  231/250, train loss 0.217994 in 0.20s\n",
      " [-] epoch  232/250, train loss 0.202532 in 0.20s\n",
      " [-] epoch  233/250, train loss 0.202972 in 0.19s\n",
      " [-] epoch  234/250, train loss 0.191314 in 0.20s\n",
      " [-] epoch  235/250, train loss 0.221948 in 0.19s\n",
      " [-] epoch  236/250, train loss 0.204767 in 0.19s\n",
      " [-] epoch  237/250, train loss 0.206682 in 0.19s\n",
      " [-] epoch  238/250, train loss 0.215774 in 0.19s\n",
      " [-] epoch  239/250, train loss 0.225817 in 0.19s\n",
      " [-] epoch  240/250, train loss 0.224281 in 0.20s\n",
      " [-] epoch  241/250, train loss 0.202285 in 0.19s\n",
      " [-] epoch  242/250, train loss 0.201137 in 0.20s\n",
      " [-] epoch  243/250, train loss 0.188453 in 0.20s\n",
      " [-] epoch  244/250, train loss 0.201246 in 0.20s\n",
      " [-] epoch  245/250, train loss 0.221330 in 0.18s\n",
      " [-] epoch  246/250, train loss 0.217766 in 0.20s\n",
      " [-] epoch  247/250, train loss 0.210317 in 0.19s\n",
      " [-] epoch  248/250, train loss 0.227149 in 0.20s\n",
      " [-] epoch  249/250, train loss 0.223800 in 0.20s\n",
      " [-] epoch  250/250, train loss 0.221255 in 0.20s\n",
      " [-] test acc. 78.055556%\n",
      "Je vais utiliser 5 layers\n",
      " [-] epoch    1/250, train loss 0.576923 in 0.28s\n",
      " [-] epoch    2/250, train loss 0.477285 in 0.29s\n",
      " [-] epoch    3/250, train loss 0.446783 in 0.32s\n",
      " [-] epoch    4/250, train loss 0.391338 in 0.32s\n",
      " [-] epoch    5/250, train loss 0.416023 in 0.31s\n",
      " [-] epoch    6/250, train loss 0.389512 in 0.29s\n",
      " [-] epoch    7/250, train loss 0.356967 in 0.30s\n",
      " [-] epoch    8/250, train loss 0.378313 in 0.30s\n",
      " [-] epoch    9/250, train loss 0.338553 in 0.30s\n",
      " [-] epoch   10/250, train loss 0.344337 in 0.29s\n",
      " [-] epoch   11/250, train loss 0.326398 in 0.31s\n",
      " [-] epoch   12/250, train loss 0.356850 in 0.31s\n",
      " [-] epoch   13/250, train loss 0.310558 in 0.30s\n",
      " [-] epoch   14/250, train loss 0.342473 in 0.30s\n",
      " [-] epoch   15/250, train loss 0.314630 in 0.29s\n",
      " [-] epoch   16/250, train loss 0.317284 in 0.29s\n",
      " [-] epoch   17/250, train loss 0.334289 in 0.31s\n",
      " [-] epoch   18/250, train loss 0.290513 in 0.31s\n",
      " [-] epoch   19/250, train loss 0.296798 in 0.30s\n",
      " [-] epoch   20/250, train loss 0.317323 in 0.32s\n",
      " [-] epoch   21/250, train loss 0.334036 in 0.31s\n",
      " [-] epoch   22/250, train loss 0.303911 in 0.30s\n",
      " [-] epoch   23/250, train loss 0.327257 in 0.30s\n",
      " [-] epoch   24/250, train loss 0.297339 in 0.30s\n",
      " [-] epoch   25/250, train loss 0.297123 in 0.33s\n",
      " [-] epoch   26/250, train loss 0.306638 in 0.33s\n",
      " [-] epoch   27/250, train loss 0.320856 in 0.32s\n",
      " [-] epoch   28/250, train loss 0.294511 in 0.32s\n",
      " [-] epoch   29/250, train loss 0.279749 in 0.30s\n",
      " [-] epoch   30/250, train loss 0.299905 in 0.30s\n",
      " [-] epoch   31/250, train loss 0.278461 in 0.30s\n",
      " [-] epoch   32/250, train loss 0.261255 in 0.32s\n",
      " [-] epoch   33/250, train loss 0.298826 in 0.30s\n",
      " [-] epoch   34/250, train loss 0.261763 in 0.29s\n",
      " [-] epoch   35/250, train loss 0.283665 in 0.31s\n",
      " [-] epoch   36/250, train loss 0.284277 in 0.31s\n",
      " [-] epoch   37/250, train loss 0.278750 in 0.30s\n",
      " [-] epoch   38/250, train loss 0.303633 in 0.29s\n",
      " [-] epoch   39/250, train loss 0.281026 in 0.28s\n",
      " [-] epoch   40/250, train loss 0.288015 in 0.30s\n",
      " [-] epoch   41/250, train loss 0.265350 in 0.29s\n",
      " [-] epoch   42/250, train loss 0.250302 in 0.28s\n",
      " [-] epoch   43/250, train loss 0.263767 in 0.29s\n",
      " [-] epoch   44/250, train loss 0.245295 in 0.29s\n",
      " [-] epoch   45/250, train loss 0.279671 in 0.29s\n",
      " [-] epoch   46/250, train loss 0.288464 in 0.30s\n",
      " [-] epoch   47/250, train loss 0.271755 in 0.29s\n",
      " [-] epoch   48/250, train loss 0.263192 in 0.33s\n",
      " [-] epoch   49/250, train loss 0.289877 in 0.30s\n",
      " [-] epoch   50/250, train loss 0.252572 in 0.29s\n",
      " [-] epoch   51/250, train loss 0.261420 in 0.31s\n",
      " [-] epoch   52/250, train loss 0.275162 in 0.31s\n",
      " [-] epoch   53/250, train loss 0.263293 in 0.30s\n",
      " [-] epoch   54/250, train loss 0.263327 in 0.30s\n",
      " [-] epoch   55/250, train loss 0.253769 in 0.29s\n",
      " [-] epoch   56/250, train loss 0.276341 in 0.28s\n",
      " [-] epoch   57/250, train loss 0.287257 in 0.30s\n",
      " [-] epoch   58/250, train loss 0.271884 in 0.31s\n",
      " [-] epoch   59/250, train loss 0.260661 in 0.32s\n",
      " [-] epoch   60/250, train loss 0.260126 in 0.30s\n",
      " [-] epoch   61/250, train loss 0.251851 in 0.29s\n",
      " [-] epoch   62/250, train loss 0.254609 in 0.30s\n",
      " [-] epoch   63/250, train loss 0.251522 in 0.30s\n",
      " [-] epoch   64/250, train loss 0.241349 in 0.30s\n",
      " [-] epoch   65/250, train loss 0.259349 in 0.29s\n",
      " [-] epoch   66/250, train loss 0.240386 in 0.30s\n",
      " [-] epoch   67/250, train loss 0.276302 in 0.31s\n",
      " [-] epoch   68/250, train loss 0.249775 in 0.32s\n",
      " [-] epoch   69/250, train loss 0.258138 in 0.31s\n",
      " [-] epoch   70/250, train loss 0.289162 in 0.30s\n",
      " [-] epoch   71/250, train loss 0.248294 in 0.30s\n",
      " [-] epoch   72/250, train loss 0.234778 in 0.30s\n",
      " [-] epoch   73/250, train loss 0.263152 in 0.30s\n",
      " [-] epoch   74/250, train loss 0.240804 in 0.32s\n",
      " [-] epoch   75/250, train loss 0.238308 in 0.30s\n",
      " [-] epoch   76/250, train loss 0.228325 in 0.29s\n",
      " [-] epoch   77/250, train loss 0.225378 in 0.30s\n",
      " [-] epoch   78/250, train loss 0.254834 in 0.30s\n",
      " [-] epoch   79/250, train loss 0.267870 in 0.30s\n",
      " [-] epoch   80/250, train loss 0.265708 in 0.32s\n",
      " [-] epoch   81/250, train loss 0.231995 in 0.30s\n",
      " [-] epoch   82/250, train loss 0.236730 in 0.30s\n",
      " [-] epoch   83/250, train loss 0.232760 in 0.31s\n",
      " [-] epoch   84/250, train loss 0.271163 in 0.30s\n",
      " [-] epoch   85/250, train loss 0.237793 in 0.29s\n",
      " [-] epoch   86/250, train loss 0.220838 in 0.29s\n",
      " [-] epoch   87/250, train loss 0.224560 in 0.31s\n",
      " [-] epoch   88/250, train loss 0.226301 in 0.30s\n",
      " [-] epoch   89/250, train loss 0.236146 in 0.31s\n",
      " [-] epoch   90/250, train loss 0.225431 in 0.30s\n",
      " [-] epoch   91/250, train loss 0.223960 in 0.30s\n",
      " [-] epoch   92/250, train loss 0.232043 in 0.33s\n",
      " [-] epoch   93/250, train loss 0.242643 in 0.29s\n",
      " [-] epoch   94/250, train loss 0.228602 in 0.30s\n",
      " [-] epoch   95/250, train loss 0.226771 in 0.32s\n",
      " [-] epoch   96/250, train loss 0.229108 in 0.30s\n",
      " [-] epoch   97/250, train loss 0.216815 in 0.32s\n",
      " [-] epoch   98/250, train loss 0.236821 in 0.29s\n",
      " [-] epoch   99/250, train loss 0.228596 in 0.30s\n",
      " [-] epoch  100/250, train loss 0.222553 in 0.31s\n",
      " [-] epoch  101/250, train loss 0.232140 in 0.30s\n",
      " [-] epoch  102/250, train loss 0.226021 in 0.32s\n",
      " [-] epoch  103/250, train loss 0.230927 in 0.31s\n",
      " [-] epoch  104/250, train loss 0.240017 in 0.28s\n",
      " [-] epoch  105/250, train loss 0.215869 in 0.29s\n",
      " [-] epoch  106/250, train loss 0.245497 in 0.30s\n",
      " [-] epoch  107/250, train loss 0.228947 in 0.29s\n",
      " [-] epoch  108/250, train loss 0.252545 in 0.31s\n",
      " [-] epoch  109/250, train loss 0.227795 in 0.30s\n",
      " [-] epoch  110/250, train loss 0.228509 in 0.31s\n",
      " [-] epoch  111/250, train loss 0.226678 in 0.28s\n",
      " [-] epoch  112/250, train loss 0.235739 in 0.29s\n",
      " [-] epoch  113/250, train loss 0.232881 in 0.31s\n",
      " [-] epoch  114/250, train loss 0.243603 in 0.29s\n",
      " [-] epoch  115/250, train loss 0.236304 in 0.29s\n",
      " [-] epoch  116/250, train loss 0.260786 in 0.29s\n",
      " [-] epoch  117/250, train loss 0.232751 in 0.29s\n",
      " [-] epoch  118/250, train loss 0.223739 in 0.30s\n",
      " [-] epoch  119/250, train loss 0.246682 in 0.30s\n",
      " [-] epoch  120/250, train loss 0.235063 in 0.28s\n",
      " [-] epoch  121/250, train loss 0.231659 in 0.30s\n",
      " [-] epoch  122/250, train loss 0.235551 in 0.31s\n",
      " [-] epoch  123/250, train loss 0.221119 in 0.31s\n",
      " [-] epoch  124/250, train loss 0.242109 in 0.32s\n",
      " [-] epoch  125/250, train loss 0.264917 in 0.30s\n",
      " [-] epoch  126/250, train loss 0.221164 in 0.30s\n",
      " [-] epoch  127/250, train loss 0.220791 in 0.29s\n",
      " [-] epoch  128/250, train loss 0.232626 in 0.30s\n",
      " [-] epoch  129/250, train loss 0.217856 in 0.29s\n",
      " [-] epoch  130/250, train loss 0.215731 in 0.31s\n",
      " [-] epoch  131/250, train loss 0.238584 in 0.30s\n",
      " [-] epoch  132/250, train loss 0.206286 in 0.31s\n",
      " [-] epoch  133/250, train loss 0.227743 in 0.30s\n",
      " [-] epoch  134/250, train loss 0.225866 in 0.28s\n",
      " [-] epoch  135/250, train loss 0.247735 in 0.28s\n",
      " [-] epoch  136/250, train loss 0.244153 in 0.29s\n",
      " [-] epoch  137/250, train loss 0.205369 in 0.30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  138/250, train loss 0.230038 in 0.29s\n",
      " [-] epoch  139/250, train loss 0.223506 in 0.32s\n",
      " [-] epoch  140/250, train loss 0.225600 in 0.32s\n",
      " [-] epoch  141/250, train loss 0.216313 in 0.29s\n",
      " [-] epoch  142/250, train loss 0.237266 in 0.30s\n",
      " [-] epoch  143/250, train loss 0.223278 in 0.28s\n",
      " [-] epoch  144/250, train loss 0.222468 in 0.29s\n",
      " [-] epoch  145/250, train loss 0.211625 in 0.29s\n",
      " [-] epoch  146/250, train loss 0.208012 in 0.28s\n",
      " [-] epoch  147/250, train loss 0.233515 in 0.30s\n",
      " [-] epoch  148/250, train loss 0.196764 in 0.30s\n",
      " [-] epoch  149/250, train loss 0.226686 in 0.28s\n",
      " [-] epoch  150/250, train loss 0.230235 in 0.30s\n",
      " [-] epoch  151/250, train loss 0.212335 in 0.30s\n",
      " [-] epoch  152/250, train loss 0.255121 in 0.34s\n",
      " [-] epoch  153/250, train loss 0.219124 in 0.30s\n",
      " [-] epoch  154/250, train loss 0.224470 in 0.31s\n",
      " [-] epoch  155/250, train loss 0.220358 in 0.30s\n",
      " [-] epoch  156/250, train loss 0.219031 in 0.31s\n",
      " [-] epoch  157/250, train loss 0.208823 in 0.29s\n",
      " [-] epoch  158/250, train loss 0.230712 in 0.32s\n",
      " [-] epoch  159/250, train loss 0.227100 in 0.32s\n",
      " [-] epoch  160/250, train loss 0.218437 in 0.31s\n",
      " [-] epoch  161/250, train loss 0.222746 in 0.31s\n",
      " [-] epoch  162/250, train loss 0.216358 in 0.32s\n",
      " [-] epoch  163/250, train loss 0.215060 in 0.33s\n",
      " [-] epoch  164/250, train loss 0.224478 in 0.32s\n",
      " [-] epoch  165/250, train loss 0.224712 in 0.33s\n",
      " [-] epoch  166/250, train loss 0.211358 in 0.32s\n",
      " [-] epoch  167/250, train loss 0.207937 in 0.31s\n",
      " [-] epoch  168/250, train loss 0.215284 in 0.30s\n",
      " [-] epoch  169/250, train loss 0.226609 in 0.32s\n",
      " [-] epoch  170/250, train loss 0.208203 in 0.29s\n",
      " [-] epoch  171/250, train loss 0.212491 in 0.30s\n",
      " [-] epoch  172/250, train loss 0.199759 in 0.30s\n",
      " [-] epoch  173/250, train loss 0.204315 in 0.30s\n",
      " [-] epoch  174/250, train loss 0.204890 in 0.30s\n",
      " [-] epoch  175/250, train loss 0.215614 in 0.32s\n",
      " [-] epoch  176/250, train loss 0.216856 in 0.31s\n",
      " [-] epoch  177/250, train loss 0.216483 in 0.30s\n",
      " [-] epoch  178/250, train loss 0.236138 in 0.31s\n",
      " [-] epoch  179/250, train loss 0.224018 in 0.30s\n",
      " [-] epoch  180/250, train loss 0.210085 in 0.30s\n",
      " [-] epoch  181/250, train loss 0.199035 in 0.32s\n",
      " [-] epoch  182/250, train loss 0.197489 in 0.30s\n",
      " [-] epoch  183/250, train loss 0.224796 in 0.30s\n",
      " [-] epoch  184/250, train loss 0.201076 in 0.29s\n",
      " [-] epoch  185/250, train loss 0.214351 in 0.29s\n",
      " [-] epoch  186/250, train loss 0.203122 in 0.32s\n",
      " [-] epoch  187/250, train loss 0.202985 in 0.31s\n",
      " [-] epoch  188/250, train loss 0.197197 in 0.30s\n",
      " [-] epoch  189/250, train loss 0.214022 in 0.31s\n",
      " [-] epoch  190/250, train loss 0.199054 in 0.31s\n",
      " [-] epoch  191/250, train loss 0.187618 in 0.29s\n",
      " [-] epoch  192/250, train loss 0.220156 in 0.31s\n",
      " [-] epoch  193/250, train loss 0.204802 in 0.30s\n",
      " [-] epoch  194/250, train loss 0.194156 in 0.30s\n",
      " [-] epoch  195/250, train loss 0.219818 in 0.30s\n",
      " [-] epoch  196/250, train loss 0.225685 in 0.29s\n",
      " [-] epoch  197/250, train loss 0.220459 in 0.30s\n",
      " [-] epoch  198/250, train loss 0.191466 in 0.30s\n",
      " [-] epoch  199/250, train loss 0.189527 in 0.31s\n",
      " [-] epoch  200/250, train loss 0.206332 in 0.29s\n",
      " [-] epoch  201/250, train loss 0.213506 in 0.30s\n",
      " [-] epoch  202/250, train loss 0.206572 in 0.30s\n",
      " [-] epoch  203/250, train loss 0.210869 in 0.29s\n",
      " [-] epoch  204/250, train loss 0.207432 in 0.28s\n",
      " [-] epoch  205/250, train loss 0.224985 in 0.30s\n",
      " [-] epoch  206/250, train loss 0.197708 in 0.29s\n",
      " [-] epoch  207/250, train loss 0.210670 in 0.29s\n",
      " [-] epoch  208/250, train loss 0.206158 in 0.30s\n",
      " [-] epoch  209/250, train loss 0.199156 in 0.29s\n",
      " [-] epoch  210/250, train loss 0.194049 in 0.29s\n",
      " [-] epoch  211/250, train loss 0.189125 in 0.30s\n",
      " [-] epoch  212/250, train loss 0.188015 in 0.28s\n",
      " [-] epoch  213/250, train loss 0.207286 in 0.29s\n",
      " [-] epoch  214/250, train loss 0.203917 in 0.30s\n",
      " [-] epoch  215/250, train loss 0.218665 in 0.31s\n",
      " [-] epoch  216/250, train loss 0.219243 in 0.30s\n",
      " [-] epoch  217/250, train loss 0.205753 in 0.30s\n",
      " [-] epoch  218/250, train loss 0.204818 in 0.28s\n",
      " [-] epoch  219/250, train loss 0.204448 in 0.30s\n",
      " [-] epoch  220/250, train loss 0.203790 in 0.30s\n",
      " [-] epoch  221/250, train loss 0.200628 in 0.32s\n",
      " [-] epoch  222/250, train loss 0.196559 in 0.30s\n",
      " [-] epoch  223/250, train loss 0.195692 in 0.30s\n",
      " [-] epoch  224/250, train loss 0.188924 in 0.30s\n",
      " [-] epoch  225/250, train loss 0.197734 in 0.30s\n",
      " [-] epoch  226/250, train loss 0.206318 in 0.29s\n",
      " [-] epoch  227/250, train loss 0.217533 in 0.32s\n",
      " [-] epoch  228/250, train loss 0.191452 in 0.32s\n",
      " [-] epoch  229/250, train loss 0.201046 in 0.31s\n",
      " [-] epoch  230/250, train loss 0.194426 in 0.30s\n",
      " [-] epoch  231/250, train loss 0.203574 in 0.30s\n",
      " [-] epoch  232/250, train loss 0.190406 in 0.29s\n",
      " [-] epoch  233/250, train loss 0.185577 in 0.31s\n",
      " [-] epoch  234/250, train loss 0.176339 in 1.34s\n",
      " [-] epoch  235/250, train loss 0.210952 in 0.30s\n",
      " [-] epoch  236/250, train loss 0.206017 in 0.30s\n",
      " [-] epoch  237/250, train loss 0.217858 in 0.31s\n",
      " [-] epoch  238/250, train loss 0.209722 in 0.32s\n",
      " [-] epoch  239/250, train loss 0.206672 in 0.30s\n",
      " [-] epoch  240/250, train loss 0.205333 in 0.30s\n",
      " [-] epoch  241/250, train loss 0.212512 in 0.31s\n",
      " [-] epoch  242/250, train loss 0.202297 in 0.29s\n",
      " [-] epoch  243/250, train loss 0.213542 in 0.31s\n",
      " [-] epoch  244/250, train loss 0.205904 in 0.32s\n",
      " [-] epoch  245/250, train loss 0.196493 in 0.32s\n",
      " [-] epoch  246/250, train loss 0.206786 in 0.31s\n",
      " [-] epoch  247/250, train loss 0.207879 in 0.31s\n",
      " [-] epoch  248/250, train loss 0.202737 in 0.31s\n",
      " [-] epoch  249/250, train loss 0.184465 in 0.29s\n",
      " [-] epoch  250/250, train loss 0.198648 in 0.30s\n",
      " [-] test acc. 81.944444%\n",
      "Je vais utiliser 6 layers\n",
      " [-] epoch    1/250, train loss 0.567216 in 0.39s\n",
      " [-] epoch    2/250, train loss 0.437143 in 0.39s\n",
      " [-] epoch    3/250, train loss 0.453975 in 0.37s\n",
      " [-] epoch    4/250, train loss 0.436186 in 0.38s\n",
      " [-] epoch    5/250, train loss 0.381885 in 0.39s\n",
      " [-] epoch    6/250, train loss 0.353925 in 0.38s\n",
      " [-] epoch    7/250, train loss 0.353615 in 0.39s\n",
      " [-] epoch    8/250, train loss 0.393462 in 0.39s\n",
      " [-] epoch    9/250, train loss 0.337636 in 0.38s\n",
      " [-] epoch   10/250, train loss 0.369052 in 0.38s\n",
      " [-] epoch   11/250, train loss 0.338754 in 0.38s\n",
      " [-] epoch   12/250, train loss 0.335405 in 0.37s\n",
      " [-] epoch   13/250, train loss 0.340610 in 0.39s\n",
      " [-] epoch   14/250, train loss 0.332581 in 0.42s\n",
      " [-] epoch   15/250, train loss 0.334284 in 0.38s\n",
      " [-] epoch   16/250, train loss 0.345345 in 0.38s\n",
      " [-] epoch   17/250, train loss 0.305260 in 0.38s\n",
      " [-] epoch   18/250, train loss 0.314571 in 0.38s\n",
      " [-] epoch   19/250, train loss 0.330294 in 0.41s\n",
      " [-] epoch   20/250, train loss 0.360807 in 0.44s\n",
      " [-] epoch   21/250, train loss 0.308578 in 0.40s\n",
      " [-] epoch   22/250, train loss 0.299876 in 0.39s\n",
      " [-] epoch   23/250, train loss 0.285718 in 0.41s\n",
      " [-] epoch   24/250, train loss 0.315232 in 0.39s\n",
      " [-] epoch   25/250, train loss 0.292723 in 0.38s\n",
      " [-] epoch   26/250, train loss 0.290340 in 0.41s\n",
      " [-] epoch   27/250, train loss 0.299005 in 0.41s\n",
      " [-] epoch   28/250, train loss 0.287932 in 0.38s\n",
      " [-] epoch   29/250, train loss 0.305845 in 0.38s\n",
      " [-] epoch   30/250, train loss 0.333067 in 0.38s\n",
      " [-] epoch   31/250, train loss 0.288320 in 0.40s\n",
      " [-] epoch   32/250, train loss 0.303773 in 0.41s\n",
      " [-] epoch   33/250, train loss 0.292561 in 0.41s\n",
      " [-] epoch   34/250, train loss 0.295749 in 0.38s\n",
      " [-] epoch   35/250, train loss 0.272114 in 0.39s\n",
      " [-] epoch   36/250, train loss 0.292176 in 0.36s\n",
      " [-] epoch   37/250, train loss 0.303432 in 0.38s\n",
      " [-] epoch   38/250, train loss 0.301008 in 0.39s\n",
      " [-] epoch   39/250, train loss 0.279484 in 0.37s\n",
      " [-] epoch   40/250, train loss 0.293639 in 0.38s\n",
      " [-] epoch   41/250, train loss 0.297994 in 0.40s\n",
      " [-] epoch   42/250, train loss 0.259676 in 0.38s\n",
      " [-] epoch   43/250, train loss 0.292236 in 0.38s\n",
      " [-] epoch   44/250, train loss 0.265057 in 0.39s\n",
      " [-] epoch   45/250, train loss 0.272032 in 0.38s\n",
      " [-] epoch   46/250, train loss 0.280058 in 0.41s\n",
      " [-] epoch   47/250, train loss 0.255995 in 0.40s\n",
      " [-] epoch   48/250, train loss 0.281874 in 0.39s\n",
      " [-] epoch   49/250, train loss 0.276173 in 0.40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch   50/250, train loss 0.258461 in 0.37s\n",
      " [-] epoch   51/250, train loss 0.282425 in 0.38s\n",
      " [-] epoch   52/250, train loss 0.269292 in 0.38s\n",
      " [-] epoch   53/250, train loss 0.271745 in 0.37s\n",
      " [-] epoch   54/250, train loss 0.283236 in 0.37s\n",
      " [-] epoch   55/250, train loss 0.279200 in 0.36s\n",
      " [-] epoch   56/250, train loss 0.268548 in 0.38s\n",
      " [-] epoch   57/250, train loss 0.254203 in 0.37s\n",
      " [-] epoch   58/250, train loss 0.266015 in 0.39s\n",
      " [-] epoch   59/250, train loss 0.281754 in 0.44s\n",
      " [-] epoch   60/250, train loss 0.251024 in 0.39s\n",
      " [-] epoch   61/250, train loss 0.264724 in 0.39s\n",
      " [-] epoch   62/250, train loss 0.232332 in 0.38s\n",
      " [-] epoch   63/250, train loss 0.253568 in 0.39s\n",
      " [-] epoch   64/250, train loss 0.246156 in 0.40s\n",
      " [-] epoch   65/250, train loss 0.248890 in 0.39s\n",
      " [-] epoch   66/250, train loss 0.257599 in 0.38s\n",
      " [-] epoch   67/250, train loss 0.271385 in 0.39s\n",
      " [-] epoch   68/250, train loss 0.270853 in 0.39s\n",
      " [-] epoch   69/250, train loss 0.230661 in 0.39s\n",
      " [-] epoch   70/250, train loss 0.277842 in 0.39s\n",
      " [-] epoch   71/250, train loss 0.244075 in 0.39s\n",
      " [-] epoch   72/250, train loss 0.265910 in 0.39s\n",
      " [-] epoch   73/250, train loss 0.287731 in 0.38s\n",
      " [-] epoch   74/250, train loss 0.246822 in 0.39s\n",
      " [-] epoch   75/250, train loss 0.243326 in 0.42s\n",
      " [-] epoch   76/250, train loss 0.244369 in 0.44s\n",
      " [-] epoch   77/250, train loss 0.256089 in 0.39s\n",
      " [-] epoch   78/250, train loss 0.245153 in 0.37s\n",
      " [-] epoch   79/250, train loss 0.238332 in 0.41s\n",
      " [-] epoch   80/250, train loss 0.251064 in 0.40s\n",
      " [-] epoch   81/250, train loss 0.255740 in 0.41s\n",
      " [-] epoch   82/250, train loss 0.253168 in 0.41s\n",
      " [-] epoch   83/250, train loss 0.247485 in 0.40s\n",
      " [-] epoch   84/250, train loss 0.247446 in 0.41s\n",
      " [-] epoch   85/250, train loss 0.251900 in 0.39s\n",
      " [-] epoch   86/250, train loss 0.273839 in 0.39s\n",
      " [-] epoch   87/250, train loss 0.247781 in 0.41s\n",
      " [-] epoch   88/250, train loss 0.250624 in 0.39s\n",
      " [-] epoch   89/250, train loss 0.249689 in 0.39s\n",
      " [-] epoch   90/250, train loss 0.221063 in 0.39s\n",
      " [-] epoch   91/250, train loss 0.236051 in 0.39s\n",
      " [-] epoch   92/250, train loss 0.233562 in 0.41s\n",
      " [-] epoch   93/250, train loss 0.243038 in 0.38s\n",
      " [-] epoch   94/250, train loss 0.243017 in 0.39s\n",
      " [-] epoch   95/250, train loss 0.241436 in 0.36s\n",
      " [-] epoch   96/250, train loss 0.242974 in 0.38s\n",
      " [-] epoch   97/250, train loss 0.234583 in 0.38s\n",
      " [-] epoch   98/250, train loss 0.256103 in 0.38s\n",
      " [-] epoch   99/250, train loss 0.248576 in 0.39s\n",
      " [-] epoch  100/250, train loss 0.225429 in 0.39s\n",
      " [-] epoch  101/250, train loss 0.222863 in 0.41s\n",
      " [-] epoch  102/250, train loss 0.228974 in 0.40s\n",
      " [-] epoch  103/250, train loss 0.241737 in 0.40s\n",
      " [-] epoch  104/250, train loss 0.251080 in 0.39s\n",
      " [-] epoch  105/250, train loss 0.229186 in 0.39s\n",
      " [-] epoch  106/250, train loss 0.235775 in 0.42s\n",
      " [-] epoch  107/250, train loss 0.267498 in 0.37s\n",
      " [-] epoch  108/250, train loss 0.265819 in 0.39s\n",
      " [-] epoch  109/250, train loss 0.263289 in 0.37s\n",
      " [-] epoch  110/250, train loss 0.239609 in 0.38s\n",
      " [-] epoch  111/250, train loss 0.228331 in 0.39s\n",
      " [-] epoch  112/250, train loss 0.219339 in 0.37s\n",
      " [-] epoch  113/250, train loss 0.237096 in 0.36s\n",
      " [-] epoch  114/250, train loss 0.228153 in 0.38s\n",
      " [-] epoch  115/250, train loss 0.209730 in 0.41s\n",
      " [-] epoch  116/250, train loss 0.225848 in 0.40s\n",
      " [-] epoch  117/250, train loss 0.236563 in 0.40s\n",
      " [-] epoch  118/250, train loss 0.216554 in 0.40s\n",
      " [-] epoch  119/250, train loss 0.221512 in 0.40s\n",
      " [-] epoch  120/250, train loss 0.240445 in 0.39s\n",
      " [-] epoch  121/250, train loss 0.232026 in 0.38s\n",
      " [-] epoch  122/250, train loss 0.227550 in 0.38s\n",
      " [-] epoch  123/250, train loss 0.245213 in 0.38s\n",
      " [-] epoch  124/250, train loss 0.243782 in 0.42s\n",
      " [-] epoch  125/250, train loss 0.211008 in 0.42s\n",
      " [-] epoch  126/250, train loss 0.227118 in 0.39s\n",
      " [-] epoch  127/250, train loss 0.219399 in 0.40s\n",
      " [-] epoch  128/250, train loss 0.266468 in 0.39s\n",
      " [-] epoch  129/250, train loss 0.247463 in 0.38s\n",
      " [-] epoch  130/250, train loss 0.240364 in 0.38s\n",
      " [-] epoch  131/250, train loss 0.227607 in 0.41s\n",
      " [-] epoch  132/250, train loss 0.226352 in 0.39s\n",
      " [-] epoch  133/250, train loss 0.201987 in 0.40s\n",
      " [-] epoch  134/250, train loss 0.239705 in 0.39s\n",
      " [-] epoch  135/250, train loss 0.204120 in 0.37s\n",
      " [-] epoch  136/250, train loss 0.209371 in 0.41s\n",
      " [-] epoch  137/250, train loss 0.218699 in 0.41s\n",
      " [-] epoch  138/250, train loss 0.200960 in 0.38s\n",
      " [-] epoch  139/250, train loss 0.217370 in 0.40s\n",
      " [-] epoch  140/250, train loss 0.221558 in 0.39s\n",
      " [-] epoch  141/250, train loss 0.223628 in 0.42s\n",
      " [-] epoch  142/250, train loss 0.229510 in 0.40s\n",
      " [-] epoch  143/250, train loss 0.218392 in 0.40s\n",
      " [-] epoch  144/250, train loss 0.214897 in 0.40s\n",
      " [-] epoch  145/250, train loss 0.212841 in 0.39s\n",
      " [-] epoch  146/250, train loss 0.213308 in 0.38s\n",
      " [-] epoch  147/250, train loss 0.216132 in 0.38s\n",
      " [-] epoch  148/250, train loss 0.210947 in 0.42s\n",
      " [-] epoch  149/250, train loss 0.218750 in 0.39s\n",
      " [-] epoch  150/250, train loss 0.219990 in 0.40s\n",
      " [-] epoch  151/250, train loss 0.208457 in 0.38s\n",
      " [-] epoch  152/250, train loss 0.200783 in 0.38s\n",
      " [-] epoch  153/250, train loss 0.214509 in 0.37s\n",
      " [-] epoch  154/250, train loss 0.203069 in 0.38s\n",
      " [-] epoch  155/250, train loss 0.230636 in 0.38s\n",
      " [-] epoch  156/250, train loss 0.212867 in 0.41s\n",
      " [-] epoch  157/250, train loss 0.212183 in 0.40s\n",
      " [-] epoch  158/250, train loss 0.219749 in 0.39s\n",
      " [-] epoch  159/250, train loss 0.212885 in 0.40s\n",
      " [-] epoch  160/250, train loss 0.240851 in 0.40s\n",
      " [-] epoch  161/250, train loss 0.210430 in 0.39s\n",
      " [-] epoch  162/250, train loss 0.223596 in 0.38s\n",
      " [-] epoch  163/250, train loss 0.204326 in 0.43s\n",
      " [-] epoch  164/250, train loss 0.229382 in 0.37s\n",
      " [-] epoch  165/250, train loss 0.220226 in 0.39s\n",
      " [-] epoch  166/250, train loss 0.216405 in 0.39s\n",
      " [-] epoch  167/250, train loss 0.213025 in 0.37s\n",
      " [-] epoch  168/250, train loss 0.197543 in 0.38s\n",
      " [-] epoch  169/250, train loss 0.216655 in 0.37s\n",
      " [-] epoch  170/250, train loss 0.219880 in 0.36s\n",
      " [-] epoch  171/250, train loss 0.240285 in 0.39s\n",
      " [-] epoch  172/250, train loss 0.221262 in 0.39s\n",
      " [-] epoch  173/250, train loss 0.210399 in 0.38s\n",
      " [-] epoch  174/250, train loss 0.213419 in 0.39s\n",
      " [-] epoch  175/250, train loss 0.222319 in 0.40s\n",
      " [-] epoch  176/250, train loss 0.206523 in 0.42s\n",
      " [-] epoch  177/250, train loss 0.204635 in 0.37s\n",
      " [-] epoch  178/250, train loss 0.204573 in 0.41s\n",
      " [-] epoch  179/250, train loss 0.224392 in 0.42s\n",
      " [-] epoch  180/250, train loss 0.212407 in 0.41s\n",
      " [-] epoch  181/250, train loss 0.199845 in 0.40s\n",
      " [-] epoch  182/250, train loss 0.217336 in 0.41s\n",
      " [-] epoch  183/250, train loss 0.226363 in 0.41s\n",
      " [-] epoch  184/250, train loss 0.218961 in 0.42s\n",
      " [-] epoch  185/250, train loss 0.212994 in 0.41s\n",
      " [-] epoch  186/250, train loss 0.208156 in 0.42s\n",
      " [-] epoch  187/250, train loss 0.217052 in 0.41s\n",
      " [-] epoch  188/250, train loss 0.216786 in 0.41s\n",
      " [-] epoch  189/250, train loss 0.215910 in 0.41s\n",
      " [-] epoch  190/250, train loss 0.225839 in 0.41s\n",
      " [-] epoch  191/250, train loss 0.208909 in 0.40s\n",
      " [-] epoch  192/250, train loss 0.219564 in 0.39s\n",
      " [-] epoch  193/250, train loss 0.218729 in 0.41s\n",
      " [-] epoch  194/250, train loss 0.202808 in 0.42s\n",
      " [-] epoch  195/250, train loss 0.208963 in 0.42s\n",
      " [-] epoch  196/250, train loss 0.202828 in 0.41s\n",
      " [-] epoch  197/250, train loss 0.209691 in 0.41s\n",
      " [-] epoch  198/250, train loss 0.196571 in 0.39s\n",
      " [-] epoch  199/250, train loss 0.213846 in 0.38s\n",
      " [-] epoch  200/250, train loss 0.203282 in 0.40s\n",
      " [-] epoch  201/250, train loss 0.210329 in 0.42s\n",
      " [-] epoch  202/250, train loss 0.204037 in 0.42s\n",
      " [-] epoch  203/250, train loss 0.213831 in 0.41s\n",
      " [-] epoch  204/250, train loss 0.199712 in 0.40s\n",
      " [-] epoch  205/250, train loss 0.213482 in 0.40s\n",
      " [-] epoch  206/250, train loss 0.201990 in 0.40s\n",
      " [-] epoch  207/250, train loss 0.194463 in 0.42s\n",
      " [-] epoch  208/250, train loss 0.194119 in 0.44s\n",
      " [-] epoch  209/250, train loss 0.210036 in 0.38s\n",
      " [-] epoch  210/250, train loss 0.214705 in 0.40s\n",
      " [-] epoch  211/250, train loss 0.200701 in 0.38s\n",
      " [-] epoch  212/250, train loss 0.205323 in 0.39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-] epoch  213/250, train loss 0.220140 in 0.38s\n",
      " [-] epoch  214/250, train loss 0.210910 in 0.38s\n",
      " [-] epoch  215/250, train loss 0.204035 in 0.39s\n",
      " [-] epoch  216/250, train loss 0.197512 in 0.39s\n",
      " [-] epoch  217/250, train loss 0.200409 in 0.40s\n",
      " [-] epoch  218/250, train loss 0.199239 in 0.39s\n",
      " [-] epoch  219/250, train loss 0.202037 in 0.39s\n",
      " [-] epoch  220/250, train loss 0.190934 in 0.39s\n",
      " [-] epoch  221/250, train loss 0.190667 in 0.39s\n",
      " [-] epoch  222/250, train loss 0.191863 in 0.40s\n",
      " [-] epoch  223/250, train loss 0.190365 in 0.41s\n",
      " [-] epoch  224/250, train loss 0.195627 in 0.42s\n",
      " [-] epoch  225/250, train loss 0.198428 in 0.39s\n",
      " [-] epoch  226/250, train loss 0.212874 in 0.41s\n",
      " [-] epoch  227/250, train loss 0.203619 in 0.41s\n",
      " [-] epoch  228/250, train loss 0.193401 in 0.40s\n",
      " [-] epoch  229/250, train loss 0.186667 in 0.39s\n",
      " [-] epoch  230/250, train loss 0.208650 in 0.41s\n",
      " [-] epoch  231/250, train loss 0.198395 in 0.41s\n",
      " [-] epoch  232/250, train loss 0.197775 in 0.38s\n",
      " [-] epoch  233/250, train loss 0.219115 in 0.40s\n",
      " [-] epoch  234/250, train loss 0.205838 in 0.40s\n",
      " [-] epoch  235/250, train loss 0.217657 in 0.39s\n",
      " [-] epoch  236/250, train loss 0.190664 in 0.38s\n",
      " [-] epoch  237/250, train loss 0.202585 in 0.37s\n",
      " [-] epoch  238/250, train loss 0.194256 in 0.40s\n",
      " [-] epoch  239/250, train loss 0.194015 in 0.41s\n",
      " [-] epoch  240/250, train loss 0.214439 in 0.47s\n",
      " [-] epoch  241/250, train loss 0.203053 in 0.39s\n",
      " [-] epoch  242/250, train loss 0.208571 in 0.39s\n",
      " [-] epoch  243/250, train loss 0.195855 in 0.38s\n",
      " [-] epoch  244/250, train loss 0.202479 in 0.38s\n",
      " [-] epoch  245/250, train loss 0.212785 in 0.39s\n",
      " [-] epoch  246/250, train loss 0.214173 in 0.38s\n",
      " [-] epoch  247/250, train loss 0.215532 in 0.38s\n",
      " [-] epoch  248/250, train loss 0.195378 in 0.38s\n",
      " [-] epoch  249/250, train loss 0.191099 in 0.39s\n",
      " [-] epoch  250/250, train loss 0.191421 in 0.40s\n",
      " [-] test acc. 83.611111%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reducedData = numpy.load('labeled_reduit_100dim.npy')\n",
    "numberOfData = reducedData.shape[0]\n",
    "dimensions = reducedData.shape[1]\n",
    "\n",
    "#print(numberOfData, dimensions)\n",
    "\n",
    "# normaliser les données d'entrée entre 0 et 1 pour toutes les dimensions.\n",
    "X_train_reduced = minmax_scale(reducedData[ : -((numberOfData//4)), :]) # 1081 premières données\n",
    "X_test_reduced = minmax_scale(reducedData[(numberOfData//4)*3 + 1 : ,  :]) # 360 dernières données\n",
    "\n",
    "#print(X_train_reduced.shape[1])\n",
    "\n",
    "compute_results(X_train_reduced, X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenir compte du fait que l'on n'a pas beaucoup de données considérant le nombre de dimensions\n",
    "\n",
    "2 et 6 layers ressortent souvent (impression?) possibilité d'avoir une genre d'interférence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
